{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv')\n",
    "test_data = pd.read_csv('./data/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0  1461          20       RH         80.0    11622   Pave   NaN      Reg   \n",
       "1  1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
       "2  1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
       "3  1464          60       RL         78.0     9978   Pave   NaN      IR1   \n",
       "4  1465         120       RL         43.0     5005   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature  \\\n",
       "0         Lvl    AllPub  ...         120        0    NaN  MnPrv         NaN   \n",
       "1         Lvl    AllPub  ...           0        0    NaN    NaN        Gar2   \n",
       "2         Lvl    AllPub  ...           0        0    NaN  MnPrv         NaN   \n",
       "3         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "4         HLS    AllPub  ...         144        0    NaN    NaN         NaN   \n",
       "\n",
       "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
       "0       0      6    2010        WD         Normal  \n",
       "1   12500      6    2010        WD         Normal  \n",
       "2       0      3    2010        WD         Normal  \n",
       "3       0      6    2010        WD         Normal  \n",
       "4       0      1    2010        WD         Normal  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning: 4, type: object\n",
      "LotFrontage: 227, type: float64\n",
      "Alley: 1352, type: object\n",
      "Utilities: 2, type: object\n",
      "Exterior1st: 1, type: object\n",
      "Exterior2nd: 1, type: object\n",
      "MasVnrType: 16, type: object\n",
      "MasVnrArea: 15, type: float64\n",
      "BsmtQual: 44, type: object\n",
      "BsmtCond: 45, type: object\n",
      "BsmtExposure: 44, type: object\n",
      "BsmtFinType1: 42, type: object\n",
      "BsmtFinSF1: 1, type: float64\n",
      "BsmtFinType2: 42, type: object\n",
      "BsmtFinSF2: 1, type: float64\n",
      "BsmtUnfSF: 1, type: float64\n",
      "TotalBsmtSF: 1, type: float64\n",
      "BsmtFullBath: 2, type: float64\n",
      "BsmtHalfBath: 2, type: float64\n",
      "KitchenQual: 1, type: object\n",
      "Functional: 2, type: object\n",
      "FireplaceQu: 730, type: object\n",
      "GarageType: 76, type: object\n",
      "GarageYrBlt: 78, type: float64\n",
      "GarageFinish: 78, type: object\n",
      "GarageCars: 1, type: float64\n",
      "GarageArea: 1, type: float64\n",
      "GarageQual: 78, type: object\n",
      "GarageCond: 78, type: object\n",
      "PoolQC: 1456, type: object\n",
      "Fence: 1169, type: object\n",
      "MiscFeature: 1408, type: object\n",
      "SaleType: 1, type: object\n",
      "['MSZoning', 'LotFrontage', 'Alley', 'Utilities', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType']\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "def missing_value_checker(data):\n",
    "    list = []\n",
    "    for feature, content in data.items():\n",
    "        if data[feature].isnull().values.any():\n",
    "            \n",
    "            sum = data[feature].isna().sum()\n",
    "\n",
    "            type = data[feature].dtype\n",
    "\n",
    "            print (f'{feature}: {sum}, type: {type}')\n",
    "            \n",
    "            list.append(feature)\n",
    "    print(list)\n",
    "\n",
    "    print(len(list))\n",
    "\n",
    "missing_value_checker(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edited = test_data.drop(['Alley','FireplaceQu','PoolQC', 'Fence', 'MiscFeature'], axis=1)\n",
    "train_edited = train_data.drop(['Alley','FireplaceQu','PoolQC', 'Fence', 'MiscFeature'], axis=1)\n",
    "\n",
    "def nan_filler(data):\n",
    "    for label, content in data.items():\n",
    "        if pd.api.types.is_numeric_dtype(content):\n",
    "            data[label] = content.fillna(content.median())\n",
    "        else:\n",
    "            data[label] = content.astype(\"category\").cat.as_ordered()\n",
    "            data[label] = pd.Categorical(content).codes+1\n",
    "\n",
    "nan_filler(test_edited)\n",
    "nan_filler(train_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "missing_value_checker(test_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "missing_value_checker(train_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 76), (1459, 75))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_edited.shape, test_edited.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1459 entries, 0 to 1458\n",
      "Data columns (total 75 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1459 non-null   int64  \n",
      " 1   MSSubClass     1459 non-null   int64  \n",
      " 2   MSZoning       1459 non-null   int8   \n",
      " 3   LotFrontage    1459 non-null   float64\n",
      " 4   LotArea        1459 non-null   int64  \n",
      " 5   Street         1459 non-null   int8   \n",
      " 6   LotShape       1459 non-null   int8   \n",
      " 7   LandContour    1459 non-null   int8   \n",
      " 8   Utilities      1459 non-null   int8   \n",
      " 9   LotConfig      1459 non-null   int8   \n",
      " 10  LandSlope      1459 non-null   int8   \n",
      " 11  Neighborhood   1459 non-null   int8   \n",
      " 12  Condition1     1459 non-null   int8   \n",
      " 13  Condition2     1459 non-null   int8   \n",
      " 14  BldgType       1459 non-null   int8   \n",
      " 15  HouseStyle     1459 non-null   int8   \n",
      " 16  OverallQual    1459 non-null   int64  \n",
      " 17  OverallCond    1459 non-null   int64  \n",
      " 18  YearBuilt      1459 non-null   int64  \n",
      " 19  YearRemodAdd   1459 non-null   int64  \n",
      " 20  RoofStyle      1459 non-null   int8   \n",
      " 21  RoofMatl       1459 non-null   int8   \n",
      " 22  Exterior1st    1459 non-null   int8   \n",
      " 23  Exterior2nd    1459 non-null   int8   \n",
      " 24  MasVnrType     1459 non-null   int8   \n",
      " 25  MasVnrArea     1459 non-null   float64\n",
      " 26  ExterQual      1459 non-null   int8   \n",
      " 27  ExterCond      1459 non-null   int8   \n",
      " 28  Foundation     1459 non-null   int8   \n",
      " 29  BsmtQual       1459 non-null   int8   \n",
      " 30  BsmtCond       1459 non-null   int8   \n",
      " 31  BsmtExposure   1459 non-null   int8   \n",
      " 32  BsmtFinType1   1459 non-null   int8   \n",
      " 33  BsmtFinSF1     1459 non-null   float64\n",
      " 34  BsmtFinType2   1459 non-null   int8   \n",
      " 35  BsmtFinSF2     1459 non-null   float64\n",
      " 36  BsmtUnfSF      1459 non-null   float64\n",
      " 37  TotalBsmtSF    1459 non-null   float64\n",
      " 38  Heating        1459 non-null   int8   \n",
      " 39  HeatingQC      1459 non-null   int8   \n",
      " 40  CentralAir     1459 non-null   int8   \n",
      " 41  Electrical     1459 non-null   int8   \n",
      " 42  1stFlrSF       1459 non-null   int64  \n",
      " 43  2ndFlrSF       1459 non-null   int64  \n",
      " 44  LowQualFinSF   1459 non-null   int64  \n",
      " 45  GrLivArea      1459 non-null   int64  \n",
      " 46  BsmtFullBath   1459 non-null   float64\n",
      " 47  BsmtHalfBath   1459 non-null   float64\n",
      " 48  FullBath       1459 non-null   int64  \n",
      " 49  HalfBath       1459 non-null   int64  \n",
      " 50  BedroomAbvGr   1459 non-null   int64  \n",
      " 51  KitchenAbvGr   1459 non-null   int64  \n",
      " 52  KitchenQual    1459 non-null   int8   \n",
      " 53  TotRmsAbvGrd   1459 non-null   int64  \n",
      " 54  Functional     1459 non-null   int8   \n",
      " 55  Fireplaces     1459 non-null   int64  \n",
      " 56  GarageType     1459 non-null   int8   \n",
      " 57  GarageYrBlt    1459 non-null   float64\n",
      " 58  GarageFinish   1459 non-null   int8   \n",
      " 59  GarageCars     1459 non-null   float64\n",
      " 60  GarageArea     1459 non-null   float64\n",
      " 61  GarageQual     1459 non-null   int8   \n",
      " 62  GarageCond     1459 non-null   int8   \n",
      " 63  PavedDrive     1459 non-null   int8   \n",
      " 64  WoodDeckSF     1459 non-null   int64  \n",
      " 65  OpenPorchSF    1459 non-null   int64  \n",
      " 66  EnclosedPorch  1459 non-null   int64  \n",
      " 67  3SsnPorch      1459 non-null   int64  \n",
      " 68  ScreenPorch    1459 non-null   int64  \n",
      " 69  PoolArea       1459 non-null   int64  \n",
      " 70  MiscVal        1459 non-null   int64  \n",
      " 71  MoSold         1459 non-null   int64  \n",
      " 72  YrSold         1459 non-null   int64  \n",
      " 73  SaleType       1459 non-null   int8   \n",
      " 74  SaleCondition  1459 non-null   int8   \n",
      "dtypes: float64(11), int64(26), int8(38)\n",
      "memory usage: 476.0 KB\n"
     ]
    }
   ],
   "source": [
    "test_edited.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 76 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   int8   \n",
      " 3   LotFrontage    1460 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   int8   \n",
      " 6   LotShape       1460 non-null   int8   \n",
      " 7   LandContour    1460 non-null   int8   \n",
      " 8   Utilities      1460 non-null   int8   \n",
      " 9   LotConfig      1460 non-null   int8   \n",
      " 10  LandSlope      1460 non-null   int8   \n",
      " 11  Neighborhood   1460 non-null   int8   \n",
      " 12  Condition1     1460 non-null   int8   \n",
      " 13  Condition2     1460 non-null   int8   \n",
      " 14  BldgType       1460 non-null   int8   \n",
      " 15  HouseStyle     1460 non-null   int8   \n",
      " 16  OverallQual    1460 non-null   int64  \n",
      " 17  OverallCond    1460 non-null   int64  \n",
      " 18  YearBuilt      1460 non-null   int64  \n",
      " 19  YearRemodAdd   1460 non-null   int64  \n",
      " 20  RoofStyle      1460 non-null   int8   \n",
      " 21  RoofMatl       1460 non-null   int8   \n",
      " 22  Exterior1st    1460 non-null   int8   \n",
      " 23  Exterior2nd    1460 non-null   int8   \n",
      " 24  MasVnrType     1460 non-null   int8   \n",
      " 25  MasVnrArea     1460 non-null   float64\n",
      " 26  ExterQual      1460 non-null   int8   \n",
      " 27  ExterCond      1460 non-null   int8   \n",
      " 28  Foundation     1460 non-null   int8   \n",
      " 29  BsmtQual       1460 non-null   int8   \n",
      " 30  BsmtCond       1460 non-null   int8   \n",
      " 31  BsmtExposure   1460 non-null   int8   \n",
      " 32  BsmtFinType1   1460 non-null   int8   \n",
      " 33  BsmtFinSF1     1460 non-null   int64  \n",
      " 34  BsmtFinType2   1460 non-null   int8   \n",
      " 35  BsmtFinSF2     1460 non-null   int64  \n",
      " 36  BsmtUnfSF      1460 non-null   int64  \n",
      " 37  TotalBsmtSF    1460 non-null   int64  \n",
      " 38  Heating        1460 non-null   int8   \n",
      " 39  HeatingQC      1460 non-null   int8   \n",
      " 40  CentralAir     1460 non-null   int8   \n",
      " 41  Electrical     1460 non-null   int8   \n",
      " 42  1stFlrSF       1460 non-null   int64  \n",
      " 43  2ndFlrSF       1460 non-null   int64  \n",
      " 44  LowQualFinSF   1460 non-null   int64  \n",
      " 45  GrLivArea      1460 non-null   int64  \n",
      " 46  BsmtFullBath   1460 non-null   int64  \n",
      " 47  BsmtHalfBath   1460 non-null   int64  \n",
      " 48  FullBath       1460 non-null   int64  \n",
      " 49  HalfBath       1460 non-null   int64  \n",
      " 50  BedroomAbvGr   1460 non-null   int64  \n",
      " 51  KitchenAbvGr   1460 non-null   int64  \n",
      " 52  KitchenQual    1460 non-null   int8   \n",
      " 53  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 54  Functional     1460 non-null   int8   \n",
      " 55  Fireplaces     1460 non-null   int64  \n",
      " 56  GarageType     1460 non-null   int8   \n",
      " 57  GarageYrBlt    1460 non-null   float64\n",
      " 58  GarageFinish   1460 non-null   int8   \n",
      " 59  GarageCars     1460 non-null   int64  \n",
      " 60  GarageArea     1460 non-null   int64  \n",
      " 61  GarageQual     1460 non-null   int8   \n",
      " 62  GarageCond     1460 non-null   int8   \n",
      " 63  PavedDrive     1460 non-null   int8   \n",
      " 64  WoodDeckSF     1460 non-null   int64  \n",
      " 65  OpenPorchSF    1460 non-null   int64  \n",
      " 66  EnclosedPorch  1460 non-null   int64  \n",
      " 67  3SsnPorch      1460 non-null   int64  \n",
      " 68  ScreenPorch    1460 non-null   int64  \n",
      " 69  PoolArea       1460 non-null   int64  \n",
      " 70  MiscVal        1460 non-null   int64  \n",
      " 71  MoSold         1460 non-null   int64  \n",
      " 72  YrSold         1460 non-null   int64  \n",
      " 73  SaleType       1460 non-null   int8   \n",
      " 74  SaleCondition  1460 non-null   int8   \n",
      " 75  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), int8(38)\n",
      "memory usage: 487.7 KB\n"
     ]
    }
   ],
   "source": [
    "train_edited.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_edited.drop('SalePrice', axis=1)\n",
    "y = train_edited['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>120</td>\n",
       "      <td>4</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id  MSSubClass  MSZoning  LotFrontage  LotArea  Street  LotShape  \\\n",
       "0  1461          20         3         80.0    11622       2         4   \n",
       "1  1462          20         4         81.0    14267       2         1   \n",
       "2  1463          60         4         74.0    13830       2         1   \n",
       "3  1464          60         4         78.0     9978       2         1   \n",
       "4  1465         120         4         43.0     5005       2         1   \n",
       "\n",
       "   LandContour  Utilities  LotConfig  ...  OpenPorchSF  EnclosedPorch  \\\n",
       "0            4          1          5  ...            0              0   \n",
       "1            4          1          1  ...           36              0   \n",
       "2            4          1          5  ...           34              0   \n",
       "3            4          1          5  ...           36              0   \n",
       "4            2          1          5  ...           82              0   \n",
       "\n",
       "   3SsnPorch  ScreenPorch  PoolArea  MiscVal  MoSold  YrSold  SaleType  \\\n",
       "0          0          120         0        0       6    2010         9   \n",
       "1          0            0         0    12500       6    2010         9   \n",
       "2          0            0         0        0       3    2010         9   \n",
       "3          0            0         0        0       6    2010         9   \n",
       "4          0          144         0        0       1    2010         9   \n",
       "\n",
       "   SaleCondition  \n",
       "0              5  \n",
       "1              5  \n",
       "2              5  \n",
       "3              5  \n",
       "4              5  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_edited.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data between 0 and 1\n",
    "for e in range(len(X.columns)): #iterate for each column\n",
    "    max1 = X.iloc[:, e].max()\n",
    "    max2 = test_edited.iloc[:, e].max()\n",
    "    num = 0\n",
    "    if max1 > max2:\n",
    "        num = max1\n",
    "    else:\n",
    "        num = max2 \n",
    "    \n",
    "    if num < 10:\n",
    "        X.iloc[:, e] /= 10\n",
    "        test_edited.iloc[:, e] /= 10\n",
    "    elif num < 100:\n",
    "        X.iloc[:, e] /= 100\n",
    "        test_edited.iloc[:, e] /= 100\n",
    "    elif num < 1000:\n",
    "        X.iloc[:, e] /= 1000\n",
    "        test_edited.iloc[:, e] /= 1000\n",
    "    elif num < 10000:\n",
    "        X.iloc[:, e] /= 10000\n",
    "        test_edited.iloc[:, e] /= 10000\n",
    "    elif num < 100000:\n",
    "        X.iloc[:, e] /= 100000\n",
    "        test_edited.iloc[:, e] /= 100000\n",
    "    elif num < 1000000:\n",
    "        X.iloc[:, e] /= 1000000\n",
    "        test_edited.iloc[:, e] /= 1000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Id  MSSubClass  MSZoning  LotFrontage   LotArea  Street  LotShape  \\\n",
      "0     0.0001        0.06       0.4        0.065  0.008450     0.2       0.4   \n",
      "1     0.0002        0.02       0.4        0.080  0.009600     0.2       0.4   \n",
      "2     0.0003        0.06       0.4        0.068  0.011250     0.2       0.1   \n",
      "3     0.0004        0.07       0.4        0.060  0.009550     0.2       0.1   \n",
      "4     0.0005        0.06       0.4        0.084  0.014260     0.2       0.1   \n",
      "...      ...         ...       ...          ...       ...     ...       ...   \n",
      "1455  0.1456        0.06       0.4        0.062  0.007917     0.2       0.4   \n",
      "1456  0.1457        0.02       0.4        0.085  0.013175     0.2       0.4   \n",
      "1457  0.1458        0.07       0.4        0.066  0.009042     0.2       0.4   \n",
      "1458  0.1459        0.02       0.4        0.068  0.009717     0.2       0.4   \n",
      "1459  0.1460        0.02       0.4        0.075  0.009937     0.2       0.4   \n",
      "\n",
      "      LandContour  Utilities  LotConfig  ...  OpenPorchSF  EnclosedPorch  \\\n",
      "0             0.4        0.1        0.5  ...        0.061         0.0000   \n",
      "1             0.4        0.1        0.3  ...        0.000         0.0000   \n",
      "2             0.4        0.1        0.5  ...        0.042         0.0000   \n",
      "3             0.4        0.1        0.1  ...        0.035         0.0272   \n",
      "4             0.4        0.1        0.3  ...        0.084         0.0000   \n",
      "...           ...        ...        ...  ...          ...            ...   \n",
      "1455          0.4        0.1        0.5  ...        0.040         0.0000   \n",
      "1456          0.4        0.1        0.5  ...        0.000         0.0000   \n",
      "1457          0.4        0.1        0.5  ...        0.060         0.0000   \n",
      "1458          0.4        0.1        0.5  ...        0.000         0.0112   \n",
      "1459          0.4        0.1        0.5  ...        0.068         0.0000   \n",
      "\n",
      "      3SsnPorch  ScreenPorch  PoolArea  MiscVal  MoSold  YrSold  SaleType  \\\n",
      "0           0.0          0.0       0.0    0.000    0.02  0.2008       0.9   \n",
      "1           0.0          0.0       0.0    0.000    0.05  0.2007       0.9   \n",
      "2           0.0          0.0       0.0    0.000    0.09  0.2008       0.9   \n",
      "3           0.0          0.0       0.0    0.000    0.02  0.2006       0.9   \n",
      "4           0.0          0.0       0.0    0.000    0.12  0.2008       0.9   \n",
      "...         ...          ...       ...      ...     ...     ...       ...   \n",
      "1455        0.0          0.0       0.0    0.000    0.08  0.2007       0.9   \n",
      "1456        0.0          0.0       0.0    0.000    0.02  0.2010       0.9   \n",
      "1457        0.0          0.0       0.0    0.025    0.05  0.2010       0.9   \n",
      "1458        0.0          0.0       0.0    0.000    0.04  0.2010       0.9   \n",
      "1459        0.0          0.0       0.0    0.000    0.06  0.2008       0.9   \n",
      "\n",
      "      SaleCondition  \n",
      "0               0.5  \n",
      "1               0.5  \n",
      "2               0.5  \n",
      "3               0.1  \n",
      "4               0.5  \n",
      "...             ...  \n",
      "1455            0.5  \n",
      "1456            0.5  \n",
      "1457            0.5  \n",
      "1458            0.5  \n",
      "1459            0.5  \n",
      "\n",
      "[1460 rows x 75 columns]\n",
      "          Id  MSSubClass  MSZoning  LotFrontage   LotArea  Street  LotShape  \\\n",
      "0     0.1461       0.020       0.3        0.080  0.011622     0.2       0.4   \n",
      "1     0.1462       0.020       0.4        0.081  0.014267     0.2       0.1   \n",
      "2     0.1463       0.060       0.4        0.074  0.013830     0.2       0.1   \n",
      "3     0.1464       0.060       0.4        0.078  0.009978     0.2       0.1   \n",
      "4     0.1465       0.120       0.4        0.043  0.005005     0.2       0.1   \n",
      "...      ...         ...       ...          ...       ...     ...       ...   \n",
      "1454  0.2915       0.160       0.5        0.021  0.001936     0.2       0.4   \n",
      "1455  0.2916       0.160       0.5        0.021  0.001894     0.2       0.4   \n",
      "1456  0.2917       0.020       0.4        0.160  0.020000     0.2       0.4   \n",
      "1457  0.2918       0.085       0.4        0.062  0.010441     0.2       0.4   \n",
      "1458  0.2919       0.060       0.4        0.074  0.009627     0.2       0.4   \n",
      "\n",
      "      LandContour  Utilities  LotConfig  ...  OpenPorchSF  EnclosedPorch  \\\n",
      "0             0.4        0.1        0.5  ...        0.000            0.0   \n",
      "1             0.4        0.1        0.1  ...        0.036            0.0   \n",
      "2             0.4        0.1        0.5  ...        0.034            0.0   \n",
      "3             0.4        0.1        0.5  ...        0.036            0.0   \n",
      "4             0.2        0.1        0.5  ...        0.082            0.0   \n",
      "...           ...        ...        ...  ...          ...            ...   \n",
      "1454          0.4        0.1        0.5  ...        0.000            0.0   \n",
      "1455          0.4        0.1        0.5  ...        0.024            0.0   \n",
      "1456          0.4        0.1        0.5  ...        0.000            0.0   \n",
      "1457          0.4        0.1        0.5  ...        0.032            0.0   \n",
      "1458          0.4        0.1        0.5  ...        0.048            0.0   \n",
      "\n",
      "      3SsnPorch  ScreenPorch  PoolArea  MiscVal  MoSold  YrSold  SaleType  \\\n",
      "0           0.0        0.120       0.0    0.000    0.06  0.2010       0.9   \n",
      "1           0.0        0.000       0.0    0.125    0.06  0.2010       0.9   \n",
      "2           0.0        0.000       0.0    0.000    0.03  0.2010       0.9   \n",
      "3           0.0        0.000       0.0    0.000    0.06  0.2010       0.9   \n",
      "4           0.0        0.144       0.0    0.000    0.01  0.2010       0.9   \n",
      "...         ...          ...       ...      ...     ...     ...       ...   \n",
      "1454        0.0        0.000       0.0    0.000    0.06  0.2006       0.9   \n",
      "1455        0.0        0.000       0.0    0.000    0.04  0.2006       0.9   \n",
      "1456        0.0        0.000       0.0    0.000    0.09  0.2006       0.9   \n",
      "1457        0.0        0.000       0.0    0.007    0.07  0.2006       0.9   \n",
      "1458        0.0        0.000       0.0    0.000    0.11  0.2006       0.9   \n",
      "\n",
      "      SaleCondition  \n",
      "0               0.5  \n",
      "1               0.5  \n",
      "2               0.5  \n",
      "3               0.5  \n",
      "4               0.5  \n",
      "...             ...  \n",
      "1454            0.5  \n",
      "1455            0.1  \n",
      "1456            0.1  \n",
      "1457            0.5  \n",
      "1458            0.5  \n",
      "\n",
      "[1459 rows x 75 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X) #print to check\n",
    "print(test_edited) #print to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.241078\n",
      "1       0.203583\n",
      "2       0.261908\n",
      "3       0.145952\n",
      "4       0.298709\n",
      "          ...   \n",
      "1455    0.194556\n",
      "1456    0.243161\n",
      "1457    0.321622\n",
      "1458    0.148903\n",
      "1459    0.156367\n",
      "Name: SalePrice, Length: 1460, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/homa/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "min1 = y.iloc[:].min()\n",
    "y.iloc[:] -= min1\n",
    "max1 = y.iloc[:].max()\n",
    "y.iloc[:] /= max1 \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1168, 75), (1459, 75))"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, test_edited.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()     \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(relu)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train.values)\n",
    "y_train = torch.FloatTensor(y_train.values)\n",
    "X_val = torch.FloatTensor(X_val.values)\n",
    "y_val = torch.FloatTensor(y_val.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.size()[1] # number of features selected\n",
    "hidden_size = 30 # number of nodes/neurons in the hidden layer\n",
    "model = Net(input_size, hidden_size) # create the model\n",
    "criterion = torch.nn.BCELoss() # works for binary classification# without momentum parameter\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1) #with momentum parameter\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training 0.7021608352661133\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(X_val)\n",
    "before_train = criterion(y_pred.squeeze(), y_val)\n",
    "print('Test loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.7015141844749451\n",
      "Epoch 1: train loss: 0.6822131276130676\n",
      "Epoch 2: train loss: 0.6616403460502625\n",
      "Epoch 3: train loss: 0.6428647041320801\n",
      "Epoch 4: train loss: 0.6257614493370056\n",
      "Epoch 5: train loss: 0.6099721193313599\n",
      "Epoch 6: train loss: 0.5954802632331848\n",
      "Epoch 7: train loss: 0.5823997259140015\n",
      "Epoch 8: train loss: 0.570745050907135\n",
      "Epoch 9: train loss: 0.5604522824287415\n",
      "Epoch 10: train loss: 0.5514528155326843\n",
      "Epoch 11: train loss: 0.543665885925293\n",
      "Epoch 12: train loss: 0.5370006561279297\n",
      "Epoch 13: train loss: 0.5313535928726196\n",
      "Epoch 14: train loss: 0.5266174077987671\n",
      "Epoch 15: train loss: 0.522682785987854\n",
      "Epoch 16: train loss: 0.5194462537765503\n",
      "Epoch 17: train loss: 0.5168078541755676\n",
      "Epoch 18: train loss: 0.5146760940551758\n",
      "Epoch 19: train loss: 0.5129651427268982\n",
      "Epoch 20: train loss: 0.5115999579429626\n",
      "Epoch 21: train loss: 0.5105170607566833\n",
      "Epoch 22: train loss: 0.5096621513366699\n",
      "Epoch 23: train loss: 0.5089884996414185\n",
      "Epoch 24: train loss: 0.5084589719772339\n",
      "Epoch 25: train loss: 0.5080431699752808\n",
      "Epoch 26: train loss: 0.5077164769172668\n",
      "Epoch 27: train loss: 0.5074589252471924\n",
      "Epoch 28: train loss: 0.5072553753852844\n",
      "Epoch 29: train loss: 0.5070935487747192\n",
      "Epoch 30: train loss: 0.5069637298583984\n",
      "Epoch 31: train loss: 0.5068586468696594\n",
      "Epoch 32: train loss: 0.5067725777626038\n",
      "Epoch 33: train loss: 0.5067009329795837\n",
      "Epoch 34: train loss: 0.506640613079071\n",
      "Epoch 35: train loss: 0.5065886378288269\n",
      "Epoch 36: train loss: 0.5065430998802185\n",
      "Epoch 37: train loss: 0.5065025687217712\n",
      "Epoch 38: train loss: 0.5064658522605896\n",
      "Epoch 39: train loss: 0.5064319968223572\n",
      "Epoch 40: train loss: 0.5064003467559814\n",
      "Epoch 41: train loss: 0.5063702464103699\n",
      "Epoch 42: train loss: 0.5063413977622986\n",
      "Epoch 43: train loss: 0.5063135623931885\n",
      "Epoch 44: train loss: 0.5062863826751709\n",
      "Epoch 45: train loss: 0.5062596797943115\n",
      "Epoch 46: train loss: 0.5062334537506104\n",
      "Epoch 47: train loss: 0.5062075257301331\n",
      "Epoch 48: train loss: 0.5061816573143005\n",
      "Epoch 49: train loss: 0.5061560869216919\n",
      "Epoch 50: train loss: 0.5061306357383728\n",
      "Epoch 51: train loss: 0.5061051845550537\n",
      "Epoch 52: train loss: 0.506079912185669\n",
      "Epoch 53: train loss: 0.5060545206069946\n",
      "Epoch 54: train loss: 0.5060291290283203\n",
      "Epoch 55: train loss: 0.5060039162635803\n",
      "Epoch 56: train loss: 0.5059787034988403\n",
      "Epoch 57: train loss: 0.5059534311294556\n",
      "Epoch 58: train loss: 0.5059281587600708\n",
      "Epoch 59: train loss: 0.5059029459953308\n",
      "Epoch 60: train loss: 0.5058777928352356\n",
      "Epoch 61: train loss: 0.505852460861206\n",
      "Epoch 62: train loss: 0.5058273077011108\n",
      "Epoch 63: train loss: 0.5058020949363708\n",
      "Epoch 64: train loss: 0.5057768225669861\n",
      "Epoch 65: train loss: 0.5057516098022461\n",
      "Epoch 66: train loss: 0.5057263374328613\n",
      "Epoch 67: train loss: 0.5057011246681213\n",
      "Epoch 68: train loss: 0.5056759119033813\n",
      "Epoch 69: train loss: 0.5056507587432861\n",
      "Epoch 70: train loss: 0.5056256651878357\n",
      "Epoch 71: train loss: 0.50560063123703\n",
      "Epoch 72: train loss: 0.5055757164955139\n",
      "Epoch 73: train loss: 0.5055508017539978\n",
      "Epoch 74: train loss: 0.5055258274078369\n",
      "Epoch 75: train loss: 0.505500853061676\n",
      "Epoch 76: train loss: 0.5054760575294495\n",
      "Epoch 77: train loss: 0.5054512023925781\n",
      "Epoch 78: train loss: 0.5054264068603516\n",
      "Epoch 79: train loss: 0.5054014921188354\n",
      "Epoch 80: train loss: 0.5053765773773193\n",
      "Epoch 81: train loss: 0.5053517818450928\n",
      "Epoch 82: train loss: 0.5053271055221558\n",
      "Epoch 83: train loss: 0.505302369594574\n",
      "Epoch 84: train loss: 0.5052776336669922\n",
      "Epoch 85: train loss: 0.5052530169487\n",
      "Epoch 86: train loss: 0.5052282810211182\n",
      "Epoch 87: train loss: 0.5052036643028259\n",
      "Epoch 88: train loss: 0.5051789879798889\n",
      "Epoch 89: train loss: 0.5051543116569519\n",
      "Epoch 90: train loss: 0.5051296949386597\n",
      "Epoch 91: train loss: 0.5051048994064331\n",
      "Epoch 92: train loss: 0.5050801038742065\n",
      "Epoch 93: train loss: 0.5050554871559143\n",
      "Epoch 94: train loss: 0.5050306916236877\n",
      "Epoch 95: train loss: 0.505005955696106\n",
      "Epoch 96: train loss: 0.5049812197685242\n",
      "Epoch 97: train loss: 0.5049564242362976\n",
      "Epoch 98: train loss: 0.5049317479133606\n",
      "Epoch 99: train loss: 0.5049068927764893\n",
      "Epoch 100: train loss: 0.5048820972442627\n",
      "Epoch 101: train loss: 0.5048573017120361\n",
      "Epoch 102: train loss: 0.5048325061798096\n",
      "Epoch 103: train loss: 0.5048076510429382\n",
      "Epoch 104: train loss: 0.5047828555107117\n",
      "Epoch 105: train loss: 0.5047580003738403\n",
      "Epoch 106: train loss: 0.5047330260276794\n",
      "Epoch 107: train loss: 0.5047082304954529\n",
      "Epoch 108: train loss: 0.5046833157539368\n",
      "Epoch 109: train loss: 0.5046583414077759\n",
      "Epoch 110: train loss: 0.5046333074569702\n",
      "Epoch 111: train loss: 0.5046082139015198\n",
      "Epoch 112: train loss: 0.5045832395553589\n",
      "Epoch 113: train loss: 0.5045581459999084\n",
      "Epoch 114: train loss: 0.504533052444458\n",
      "Epoch 115: train loss: 0.5045080780982971\n",
      "Epoch 116: train loss: 0.5044829845428467\n",
      "Epoch 117: train loss: 0.5044578909873962\n",
      "Epoch 118: train loss: 0.5044327974319458\n",
      "Epoch 119: train loss: 0.5044075846672058\n",
      "Epoch 120: train loss: 0.5043824315071106\n",
      "Epoch 121: train loss: 0.5043574571609497\n",
      "Epoch 122: train loss: 0.5043323636054993\n",
      "Epoch 123: train loss: 0.504307210445404\n",
      "Epoch 124: train loss: 0.5042821764945984\n",
      "Epoch 125: train loss: 0.5042569637298584\n",
      "Epoch 126: train loss: 0.5042316913604736\n",
      "Epoch 127: train loss: 0.5042065382003784\n",
      "Epoch 128: train loss: 0.5041812658309937\n",
      "Epoch 129: train loss: 0.5041559934616089\n",
      "Epoch 130: train loss: 0.5041308403015137\n",
      "Epoch 131: train loss: 0.5041056871414185\n",
      "Epoch 132: train loss: 0.5040804147720337\n",
      "Epoch 133: train loss: 0.5040552020072937\n",
      "Epoch 134: train loss: 0.5040299892425537\n",
      "Epoch 135: train loss: 0.5040047764778137\n",
      "Epoch 136: train loss: 0.5039796829223633\n",
      "Epoch 137: train loss: 0.5039544701576233\n",
      "Epoch 138: train loss: 0.5039293169975281\n",
      "Epoch 139: train loss: 0.5039041638374329\n",
      "Epoch 140: train loss: 0.5038790702819824\n",
      "Epoch 141: train loss: 0.503853976726532\n",
      "Epoch 142: train loss: 0.5038288831710815\n",
      "Epoch 143: train loss: 0.5038038492202759\n",
      "Epoch 144: train loss: 0.5037786364555359\n",
      "Epoch 145: train loss: 0.5037534832954407\n",
      "Epoch 146: train loss: 0.5037283897399902\n",
      "Epoch 147: train loss: 0.5037031769752502\n",
      "Epoch 148: train loss: 0.5036779046058655\n",
      "Epoch 149: train loss: 0.5036526918411255\n",
      "Epoch 150: train loss: 0.503627359867096\n",
      "Epoch 151: train loss: 0.5036020874977112\n",
      "Epoch 152: train loss: 0.5035766959190369\n",
      "Epoch 153: train loss: 0.5035514235496521\n",
      "Epoch 154: train loss: 0.5035260915756226\n",
      "Epoch 155: train loss: 0.503500759601593\n",
      "Epoch 156: train loss: 0.5034753680229187\n",
      "Epoch 157: train loss: 0.5034499168395996\n",
      "Epoch 158: train loss: 0.5034244656562805\n",
      "Epoch 159: train loss: 0.5033990144729614\n",
      "Epoch 160: train loss: 0.503373384475708\n",
      "Epoch 161: train loss: 0.5033478140830994\n",
      "Epoch 162: train loss: 0.5033223628997803\n",
      "Epoch 163: train loss: 0.5032968521118164\n",
      "Epoch 164: train loss: 0.503271222114563\n",
      "Epoch 165: train loss: 0.5032457709312439\n",
      "Epoch 166: train loss: 0.5032201409339905\n",
      "Epoch 167: train loss: 0.5031944513320923\n",
      "Epoch 168: train loss: 0.5031688809394836\n",
      "Epoch 169: train loss: 0.5031431913375854\n",
      "Epoch 170: train loss: 0.503117561340332\n",
      "Epoch 171: train loss: 0.5030917525291443\n",
      "Epoch 172: train loss: 0.5030660629272461\n",
      "Epoch 173: train loss: 0.5030402541160583\n",
      "Epoch 174: train loss: 0.5030145049095154\n",
      "Epoch 175: train loss: 0.5029886960983276\n",
      "Epoch 176: train loss: 0.5029628872871399\n",
      "Epoch 177: train loss: 0.5029370784759521\n",
      "Epoch 178: train loss: 0.5029112696647644\n",
      "Epoch 179: train loss: 0.5028854012489319\n",
      "Epoch 180: train loss: 0.5028595328330994\n",
      "Epoch 181: train loss: 0.5028335452079773\n",
      "Epoch 182: train loss: 0.5028075575828552\n",
      "Epoch 183: train loss: 0.5027816295623779\n",
      "Epoch 184: train loss: 0.5027556419372559\n",
      "Epoch 185: train loss: 0.5027297139167786\n",
      "Epoch 186: train loss: 0.5027036070823669\n",
      "Epoch 187: train loss: 0.5026775598526001\n",
      "Epoch 188: train loss: 0.5026514530181885\n",
      "Epoch 189: train loss: 0.5026252865791321\n",
      "Epoch 190: train loss: 0.5025992393493652\n",
      "Epoch 191: train loss: 0.5025730133056641\n",
      "Epoch 192: train loss: 0.5025467872619629\n",
      "Epoch 193: train loss: 0.5025206208229065\n",
      "Epoch 194: train loss: 0.5024943947792053\n",
      "Epoch 195: train loss: 0.5024680495262146\n",
      "Epoch 196: train loss: 0.5024417042732239\n",
      "Epoch 197: train loss: 0.5024153590202332\n",
      "Epoch 198: train loss: 0.5023890137672424\n",
      "Epoch 199: train loss: 0.5023626089096069\n",
      "Epoch 200: train loss: 0.5023360848426819\n",
      "Epoch 201: train loss: 0.5023096799850464\n",
      "Epoch 202: train loss: 0.5022830963134766\n",
      "Epoch 203: train loss: 0.5022565126419067\n",
      "Epoch 204: train loss: 0.5022299289703369\n",
      "Epoch 205: train loss: 0.5022032856941223\n",
      "Epoch 206: train loss: 0.5021766424179077\n",
      "Epoch 207: train loss: 0.5021499991416931\n",
      "Epoch 208: train loss: 0.502123236656189\n",
      "Epoch 209: train loss: 0.5020964741706848\n",
      "Epoch 210: train loss: 0.5020697116851807\n",
      "Epoch 211: train loss: 0.5020428895950317\n",
      "Epoch 212: train loss: 0.502016007900238\n",
      "Epoch 213: train loss: 0.5019890666007996\n",
      "Epoch 214: train loss: 0.5019622445106506\n",
      "Epoch 215: train loss: 0.5019351840019226\n",
      "Epoch 216: train loss: 0.5019081830978394\n",
      "Epoch 217: train loss: 0.5018810629844666\n",
      "Epoch 218: train loss: 0.5018540620803833\n",
      "Epoch 219: train loss: 0.5018268823623657\n",
      "Epoch 220: train loss: 0.5017997026443481\n",
      "Epoch 221: train loss: 0.5017725229263306\n",
      "Epoch 222: train loss: 0.5017452239990234\n",
      "Epoch 223: train loss: 0.5017180442810059\n",
      "Epoch 224: train loss: 0.501690685749054\n",
      "Epoch 225: train loss: 0.5016633868217468\n",
      "Epoch 226: train loss: 0.5016359686851501\n",
      "Epoch 227: train loss: 0.5016086101531982\n",
      "Epoch 228: train loss: 0.501581072807312\n",
      "Epoch 229: train loss: 0.5015536546707153\n",
      "Epoch 230: train loss: 0.5015261173248291\n",
      "Epoch 231: train loss: 0.5014985203742981\n",
      "Epoch 232: train loss: 0.5014709830284119\n",
      "Epoch 233: train loss: 0.5014433860778809\n",
      "Epoch 234: train loss: 0.5014157295227051\n",
      "Epoch 235: train loss: 0.5013879537582397\n",
      "Epoch 236: train loss: 0.501360297203064\n",
      "Epoch 237: train loss: 0.5013325214385986\n",
      "Epoch 238: train loss: 0.5013047456741333\n",
      "Epoch 239: train loss: 0.5012768507003784\n",
      "Epoch 240: train loss: 0.5012489557266235\n",
      "Epoch 241: train loss: 0.5012210607528687\n",
      "Epoch 242: train loss: 0.5011931657791138\n",
      "Epoch 243: train loss: 0.5011651515960693\n",
      "Epoch 244: train loss: 0.5011371374130249\n",
      "Epoch 245: train loss: 0.5011090040206909\n",
      "Epoch 246: train loss: 0.5010809302330017\n",
      "Epoch 247: train loss: 0.5010527968406677\n",
      "Epoch 248: train loss: 0.5010246634483337\n",
      "Epoch 249: train loss: 0.5009963512420654\n",
      "Epoch 250: train loss: 0.5009680986404419\n",
      "Epoch 251: train loss: 0.5009398460388184\n",
      "Epoch 252: train loss: 0.5009114146232605\n",
      "Epoch 253: train loss: 0.5008830428123474\n",
      "Epoch 254: train loss: 0.5008546113967896\n",
      "Epoch 255: train loss: 0.5008261799812317\n",
      "Epoch 256: train loss: 0.5007976293563843\n",
      "Epoch 257: train loss: 0.5007691383361816\n",
      "Epoch 258: train loss: 0.500740647315979\n",
      "Epoch 259: train loss: 0.5007119178771973\n",
      "Epoch 260: train loss: 0.5006833076477051\n",
      "Epoch 261: train loss: 0.5006545782089233\n",
      "Epoch 262: train loss: 0.5006258487701416\n",
      "Epoch 263: train loss: 0.5005970597267151\n",
      "Epoch 264: train loss: 0.5005682706832886\n",
      "Epoch 265: train loss: 0.5005393624305725\n",
      "Epoch 266: train loss: 0.5005105137825012\n",
      "Epoch 267: train loss: 0.5004816055297852\n",
      "Epoch 268: train loss: 0.5004525184631348\n",
      "Epoch 269: train loss: 0.5004235506057739\n",
      "Epoch 270: train loss: 0.5003944635391235\n",
      "Epoch 271: train loss: 0.5003653764724731\n",
      "Epoch 272: train loss: 0.5003361701965332\n",
      "Epoch 273: train loss: 0.5003069639205933\n",
      "Epoch 274: train loss: 0.5002777576446533\n",
      "Epoch 275: train loss: 0.5002484321594238\n",
      "Epoch 276: train loss: 0.5002191066741943\n",
      "Epoch 277: train loss: 0.5001896619796753\n",
      "Epoch 278: train loss: 0.5001603364944458\n",
      "Epoch 279: train loss: 0.500130832195282\n",
      "Epoch 280: train loss: 0.5001013875007629\n",
      "Epoch 281: train loss: 0.5000718235969543\n",
      "Epoch 282: train loss: 0.500042200088501\n",
      "Epoch 283: train loss: 0.5000126361846924\n",
      "Epoch 284: train loss: 0.49998295307159424\n",
      "Epoch 285: train loss: 0.4999532699584961\n",
      "Epoch 286: train loss: 0.4999234974384308\n",
      "Epoch 287: train loss: 0.4998936653137207\n",
      "Epoch 288: train loss: 0.4998638331890106\n",
      "Epoch 289: train loss: 0.49983394145965576\n",
      "Epoch 290: train loss: 0.4998040497303009\n",
      "Epoch 291: train loss: 0.4997740387916565\n",
      "Epoch 292: train loss: 0.4997440576553345\n",
      "Epoch 293: train loss: 0.4997140169143677\n",
      "Epoch 294: train loss: 0.4996839165687561\n",
      "Epoch 295: train loss: 0.49965381622314453\n",
      "Epoch 296: train loss: 0.4996236562728882\n",
      "Epoch 297: train loss: 0.49959343671798706\n",
      "Epoch 298: train loss: 0.4995631277561188\n",
      "Epoch 299: train loss: 0.4995328187942505\n",
      "Epoch 300: train loss: 0.4995024800300598\n",
      "Epoch 301: train loss: 0.49947211146354675\n",
      "Epoch 302: train loss: 0.4994416832923889\n",
      "Epoch 303: train loss: 0.4994111657142639\n",
      "Epoch 304: train loss: 0.4993806481361389\n",
      "Epoch 305: train loss: 0.4993501901626587\n",
      "Epoch 306: train loss: 0.49931952357292175\n",
      "Epoch 307: train loss: 0.4992888867855072\n",
      "Epoch 308: train loss: 0.49925827980041504\n",
      "Epoch 309: train loss: 0.4992275536060333\n",
      "Epoch 310: train loss: 0.4991968274116516\n",
      "Epoch 311: train loss: 0.49916598200798035\n",
      "Epoch 312: train loss: 0.49913522601127625\n",
      "Epoch 313: train loss: 0.4991043210029602\n",
      "Epoch 314: train loss: 0.4990733861923218\n",
      "Epoch 315: train loss: 0.49904245138168335\n",
      "Epoch 316: train loss: 0.49901142716407776\n",
      "Epoch 317: train loss: 0.4989803731441498\n",
      "Epoch 318: train loss: 0.4989493489265442\n",
      "Epoch 319: train loss: 0.49891823530197144\n",
      "Epoch 320: train loss: 0.4988870620727539\n",
      "Epoch 321: train loss: 0.49885591864585876\n",
      "Epoch 322: train loss: 0.4988246560096741\n",
      "Epoch 323: train loss: 0.498793363571167\n",
      "Epoch 324: train loss: 0.4987620413303375\n",
      "Epoch 325: train loss: 0.49873068928718567\n",
      "Epoch 326: train loss: 0.4986993074417114\n",
      "Epoch 327: train loss: 0.4986678957939148\n",
      "Epoch 328: train loss: 0.4986364245414734\n",
      "Epoch 329: train loss: 0.4986048638820648\n",
      "Epoch 330: train loss: 0.498573362827301\n",
      "Epoch 331: train loss: 0.49854180216789246\n",
      "Epoch 332: train loss: 0.49851006269454956\n",
      "Epoch 333: train loss: 0.49847841262817383\n",
      "Epoch 334: train loss: 0.4984467327594757\n",
      "Epoch 335: train loss: 0.4984149634838104\n",
      "Epoch 336: train loss: 0.49838319420814514\n",
      "Epoch 337: train loss: 0.4983513653278351\n",
      "Epoch 338: train loss: 0.49831950664520264\n",
      "Epoch 339: train loss: 0.4982876181602478\n",
      "Epoch 340: train loss: 0.4982556402683258\n",
      "Epoch 341: train loss: 0.4982237219810486\n",
      "Epoch 342: train loss: 0.4981916844844818\n",
      "Epoch 343: train loss: 0.49815964698791504\n",
      "Epoch 344: train loss: 0.49812760949134827\n",
      "Epoch 345: train loss: 0.4980955123901367\n",
      "Epoch 346: train loss: 0.4980632960796356\n",
      "Epoch 347: train loss: 0.4980311393737793\n",
      "Epoch 348: train loss: 0.49799880385398865\n",
      "Epoch 349: train loss: 0.49796655774116516\n",
      "Epoch 350: train loss: 0.4979343116283417\n",
      "Epoch 351: train loss: 0.49790191650390625\n",
      "Epoch 352: train loss: 0.49786946177482605\n",
      "Epoch 353: train loss: 0.4978370666503906\n",
      "Epoch 354: train loss: 0.4978046119213104\n",
      "Epoch 355: train loss: 0.49777209758758545\n",
      "Epoch 356: train loss: 0.49773961305618286\n",
      "Epoch 357: train loss: 0.4977070093154907\n",
      "Epoch 358: train loss: 0.4976744055747986\n",
      "Epoch 359: train loss: 0.49764174222946167\n",
      "Epoch 360: train loss: 0.49760907888412476\n",
      "Epoch 361: train loss: 0.49757635593414307\n",
      "Epoch 362: train loss: 0.49754366278648376\n",
      "Epoch 363: train loss: 0.4975108206272125\n",
      "Epoch 364: train loss: 0.49747806787490845\n",
      "Epoch 365: train loss: 0.4974451959133148\n",
      "Epoch 366: train loss: 0.4974123239517212\n",
      "Epoch 367: train loss: 0.4973794221878052\n",
      "Epoch 368: train loss: 0.497346431016922\n",
      "Epoch 369: train loss: 0.4973134696483612\n",
      "Epoch 370: train loss: 0.497280478477478\n",
      "Epoch 371: train loss: 0.497247576713562\n",
      "Epoch 372: train loss: 0.49721452593803406\n",
      "Epoch 373: train loss: 0.4971815049648285\n",
      "Epoch 374: train loss: 0.4971484839916229\n",
      "Epoch 375: train loss: 0.4971153438091278\n",
      "Epoch 376: train loss: 0.4970822334289551\n",
      "Epoch 377: train loss: 0.4970490038394928\n",
      "Epoch 378: train loss: 0.49701571464538574\n",
      "Epoch 379: train loss: 0.4969824254512787\n",
      "Epoch 380: train loss: 0.4969491958618164\n",
      "Epoch 381: train loss: 0.4969158470630646\n",
      "Epoch 382: train loss: 0.49688243865966797\n",
      "Epoch 383: train loss: 0.49684906005859375\n",
      "Epoch 384: train loss: 0.49681568145751953\n",
      "Epoch 385: train loss: 0.49678218364715576\n",
      "Epoch 386: train loss: 0.4967487156391144\n",
      "Epoch 387: train loss: 0.4967151880264282\n",
      "Epoch 388: train loss: 0.4966816306114197\n",
      "Epoch 389: train loss: 0.49664807319641113\n",
      "Epoch 390: train loss: 0.4966144859790802\n",
      "Epoch 391: train loss: 0.4965808391571045\n",
      "Epoch 392: train loss: 0.4965471625328064\n",
      "Epoch 393: train loss: 0.4965134859085083\n",
      "Epoch 394: train loss: 0.4964797794818878\n",
      "Epoch 395: train loss: 0.49644607305526733\n",
      "Epoch 396: train loss: 0.4964122474193573\n",
      "Epoch 397: train loss: 0.49637848138809204\n",
      "Epoch 398: train loss: 0.4963446259498596\n",
      "Epoch 399: train loss: 0.4963107705116272\n",
      "Epoch 400: train loss: 0.49627697467803955\n",
      "Epoch 401: train loss: 0.4962430000305176\n",
      "Epoch 402: train loss: 0.4962090849876404\n",
      "Epoch 403: train loss: 0.49617505073547363\n",
      "Epoch 404: train loss: 0.4961409866809845\n",
      "Epoch 405: train loss: 0.49610692262649536\n",
      "Epoch 406: train loss: 0.49607279896736145\n",
      "Epoch 407: train loss: 0.49603867530822754\n",
      "Epoch 408: train loss: 0.49600449204444885\n",
      "Epoch 409: train loss: 0.4959702789783478\n",
      "Epoch 410: train loss: 0.49593615531921387\n",
      "Epoch 411: train loss: 0.49590185284614563\n",
      "Epoch 412: train loss: 0.49586769938468933\n",
      "Epoch 413: train loss: 0.49583345651626587\n",
      "Epoch 414: train loss: 0.4957992732524872\n",
      "Epoch 415: train loss: 0.49576514959335327\n",
      "Epoch 416: train loss: 0.49573102593421936\n",
      "Epoch 417: train loss: 0.49569687247276306\n",
      "Epoch 418: train loss: 0.4956626892089844\n",
      "Epoch 419: train loss: 0.4956285059452057\n",
      "Epoch 420: train loss: 0.4955942928791046\n",
      "Epoch 421: train loss: 0.49556005001068115\n",
      "Epoch 422: train loss: 0.4955257773399353\n",
      "Epoch 423: train loss: 0.49549156427383423\n",
      "Epoch 424: train loss: 0.4954574704170227\n",
      "Epoch 425: train loss: 0.49542340636253357\n",
      "Epoch 426: train loss: 0.4953894019126892\n",
      "Epoch 427: train loss: 0.4953553080558777\n",
      "Epoch 428: train loss: 0.49532124400138855\n",
      "Epoch 429: train loss: 0.49528712034225464\n",
      "Epoch 430: train loss: 0.4952530562877655\n",
      "Epoch 431: train loss: 0.49521908164024353\n",
      "Epoch 432: train loss: 0.4951852262020111\n",
      "Epoch 433: train loss: 0.49515125155448914\n",
      "Epoch 434: train loss: 0.4951173961162567\n",
      "Epoch 435: train loss: 0.49508342146873474\n",
      "Epoch 436: train loss: 0.49504947662353516\n",
      "Epoch 437: train loss: 0.4950155019760132\n",
      "Epoch 438: train loss: 0.4949815273284912\n",
      "Epoch 439: train loss: 0.49494755268096924\n",
      "Epoch 440: train loss: 0.4949135482311249\n",
      "Epoch 441: train loss: 0.49487951397895813\n",
      "Epoch 442: train loss: 0.49484550952911377\n",
      "Epoch 443: train loss: 0.49481144547462463\n",
      "Epoch 444: train loss: 0.4947774112224579\n",
      "Epoch 445: train loss: 0.49474334716796875\n",
      "Epoch 446: train loss: 0.4947093427181244\n",
      "Epoch 447: train loss: 0.49467530846595764\n",
      "Epoch 448: train loss: 0.4946413040161133\n",
      "Epoch 449: train loss: 0.49460721015930176\n",
      "Epoch 450: train loss: 0.4945732057094574\n",
      "Epoch 451: train loss: 0.4945390820503235\n",
      "Epoch 452: train loss: 0.49450504779815674\n",
      "Epoch 453: train loss: 0.4944709837436676\n",
      "Epoch 454: train loss: 0.49443691968917847\n",
      "Epoch 455: train loss: 0.49440279603004456\n",
      "Epoch 456: train loss: 0.4943687319755554\n",
      "Epoch 457: train loss: 0.4943346083164215\n",
      "Epoch 458: train loss: 0.49430051445961\n",
      "Epoch 459: train loss: 0.49426645040512085\n",
      "Epoch 460: train loss: 0.4942323863506317\n",
      "Epoch 461: train loss: 0.494198203086853\n",
      "Epoch 462: train loss: 0.49416404962539673\n",
      "Epoch 463: train loss: 0.49413001537323\n",
      "Epoch 464: train loss: 0.49409595131874084\n",
      "Epoch 465: train loss: 0.4940618872642517\n",
      "Epoch 466: train loss: 0.4940277934074402\n",
      "Epoch 467: train loss: 0.4939936697483063\n",
      "Epoch 468: train loss: 0.49395960569381714\n",
      "Epoch 469: train loss: 0.49392569065093994\n",
      "Epoch 470: train loss: 0.49389177560806274\n",
      "Epoch 471: train loss: 0.4938579201698303\n",
      "Epoch 472: train loss: 0.4938241243362427\n",
      "Epoch 473: train loss: 0.4937902092933655\n",
      "Epoch 474: train loss: 0.49375638365745544\n",
      "Epoch 475: train loss: 0.493722528219223\n",
      "Epoch 476: train loss: 0.4936886727809906\n",
      "Epoch 477: train loss: 0.4936547875404358\n",
      "Epoch 478: train loss: 0.49362099170684814\n",
      "Epoch 479: train loss: 0.4935871362686157\n",
      "Epoch 480: train loss: 0.4935533106327057\n",
      "Epoch 481: train loss: 0.49351945519447327\n",
      "Epoch 482: train loss: 0.4934856593608856\n",
      "Epoch 483: train loss: 0.4934517741203308\n",
      "Epoch 484: train loss: 0.49341803789138794\n",
      "Epoch 485: train loss: 0.49338415265083313\n",
      "Epoch 486: train loss: 0.4933505058288574\n",
      "Epoch 487: train loss: 0.49331679940223694\n",
      "Epoch 488: train loss: 0.49328306317329407\n",
      "Epoch 489: train loss: 0.49324938654899597\n",
      "Epoch 490: train loss: 0.4932156503200531\n",
      "Epoch 491: train loss: 0.4931819438934326\n",
      "Epoch 492: train loss: 0.4931482672691345\n",
      "Epoch 493: train loss: 0.4931148290634155\n",
      "Epoch 494: train loss: 0.4930814504623413\n",
      "Epoch 495: train loss: 0.49304816126823425\n",
      "Epoch 496: train loss: 0.4930148124694824\n",
      "Epoch 497: train loss: 0.4929814338684082\n",
      "Epoch 498: train loss: 0.49294814467430115\n",
      "Epoch 499: train loss: 0.4929147958755493\n",
      "Epoch 500: train loss: 0.4928814470767975\n",
      "Epoch 501: train loss: 0.49284812808036804\n",
      "Epoch 502: train loss: 0.4928149878978729\n",
      "Epoch 503: train loss: 0.4927816390991211\n",
      "Epoch 504: train loss: 0.4927483797073364\n",
      "Epoch 505: train loss: 0.4927152395248413\n",
      "Epoch 506: train loss: 0.49268195033073425\n",
      "Epoch 507: train loss: 0.492648720741272\n",
      "Epoch 508: train loss: 0.49261564016342163\n",
      "Epoch 509: train loss: 0.49258244037628174\n",
      "Epoch 510: train loss: 0.492549329996109\n",
      "Epoch 511: train loss: 0.49251630902290344\n",
      "Epoch 512: train loss: 0.49248331785202026\n",
      "Epoch 513: train loss: 0.49245038628578186\n",
      "Epoch 514: train loss: 0.49241748452186584\n",
      "Epoch 515: train loss: 0.4923846125602722\n",
      "Epoch 516: train loss: 0.4923516809940338\n",
      "Epoch 517: train loss: 0.49231886863708496\n",
      "Epoch 518: train loss: 0.4922860562801361\n",
      "Epoch 519: train loss: 0.49225324392318726\n",
      "Epoch 520: train loss: 0.49222037196159363\n",
      "Epoch 521: train loss: 0.49218761920928955\n",
      "Epoch 522: train loss: 0.4921547770500183\n",
      "Epoch 523: train loss: 0.49212202429771423\n",
      "Epoch 524: train loss: 0.4920893609523773\n",
      "Epoch 525: train loss: 0.49205660820007324\n",
      "Epoch 526: train loss: 0.49202388525009155\n",
      "Epoch 527: train loss: 0.49199116230010986\n",
      "Epoch 528: train loss: 0.4919585585594177\n",
      "Epoch 529: train loss: 0.4919258952140808\n",
      "Epoch 530: train loss: 0.49189329147338867\n",
      "Epoch 531: train loss: 0.49186068773269653\n",
      "Epoch 532: train loss: 0.49182814359664917\n",
      "Epoch 533: train loss: 0.4917956292629242\n",
      "Epoch 534: train loss: 0.491763174533844\n",
      "Epoch 535: train loss: 0.49173077940940857\n",
      "Epoch 536: train loss: 0.4916984438896179\n",
      "Epoch 537: train loss: 0.49166613817214966\n",
      "Epoch 538: train loss: 0.4916338324546814\n",
      "Epoch 539: train loss: 0.4916016161441803\n",
      "Epoch 540: train loss: 0.49156931042671204\n",
      "Epoch 541: train loss: 0.4915371239185333\n",
      "Epoch 542: train loss: 0.4915049970149994\n",
      "Epoch 543: train loss: 0.4914727509021759\n",
      "Epoch 544: train loss: 0.49144065380096436\n",
      "Epoch 545: train loss: 0.4914085566997528\n",
      "Epoch 546: train loss: 0.49137648940086365\n",
      "Epoch 547: train loss: 0.4913443922996521\n",
      "Epoch 548: train loss: 0.4913124144077301\n",
      "Epoch 549: train loss: 0.4912804365158081\n",
      "Epoch 550: train loss: 0.4912485182285309\n",
      "Epoch 551: train loss: 0.4912165403366089\n",
      "Epoch 552: train loss: 0.4911845922470093\n",
      "Epoch 553: train loss: 0.4911527633666992\n",
      "Epoch 554: train loss: 0.49112099409103394\n",
      "Epoch 555: train loss: 0.49108919501304626\n",
      "Epoch 556: train loss: 0.4910573959350586\n",
      "Epoch 557: train loss: 0.4910256266593933\n",
      "Epoch 558: train loss: 0.4909939169883728\n",
      "Epoch 559: train loss: 0.4909622073173523\n",
      "Epoch 560: train loss: 0.490930438041687\n",
      "Epoch 561: train loss: 0.4908986985683441\n",
      "Epoch 562: train loss: 0.4908669888973236\n",
      "Epoch 563: train loss: 0.4908353090286255\n",
      "Epoch 564: train loss: 0.490803599357605\n",
      "Epoch 565: train loss: 0.4907720386981964\n",
      "Epoch 566: train loss: 0.49074047803878784\n",
      "Epoch 567: train loss: 0.4907089173793793\n",
      "Epoch 568: train loss: 0.4906774163246155\n",
      "Epoch 569: train loss: 0.4906459450721741\n",
      "Epoch 570: train loss: 0.49061447381973267\n",
      "Epoch 571: train loss: 0.4905830919742584\n",
      "Epoch 572: train loss: 0.4905516803264618\n",
      "Epoch 573: train loss: 0.49052026867866516\n",
      "Epoch 574: train loss: 0.4904889166355133\n",
      "Epoch 575: train loss: 0.4904576241970062\n",
      "Epoch 576: train loss: 0.4904264211654663\n",
      "Epoch 577: train loss: 0.4903951585292816\n",
      "Epoch 578: train loss: 0.4903639554977417\n",
      "Epoch 579: train loss: 0.4903329312801361\n",
      "Epoch 580: train loss: 0.49030178785324097\n",
      "Epoch 581: train loss: 0.49027085304260254\n",
      "Epoch 582: train loss: 0.4902399778366089\n",
      "Epoch 583: train loss: 0.4902091324329376\n",
      "Epoch 584: train loss: 0.49017831683158875\n",
      "Epoch 585: train loss: 0.49014759063720703\n",
      "Epoch 586: train loss: 0.4901168644428253\n",
      "Epoch 587: train loss: 0.490086168050766\n",
      "Epoch 588: train loss: 0.4900554120540619\n",
      "Epoch 589: train loss: 0.49002471566200256\n",
      "Epoch 590: train loss: 0.48999398946762085\n",
      "Epoch 591: train loss: 0.4899633824825287\n",
      "Epoch 592: train loss: 0.4899328052997589\n",
      "Epoch 593: train loss: 0.4899023473262787\n",
      "Epoch 594: train loss: 0.4898720979690552\n",
      "Epoch 595: train loss: 0.4898418188095093\n",
      "Epoch 596: train loss: 0.48981162905693054\n",
      "Epoch 597: train loss: 0.4897814095020294\n",
      "Epoch 598: train loss: 0.48975127935409546\n",
      "Epoch 599: train loss: 0.4897212088108063\n",
      "Epoch 600: train loss: 0.48969122767448425\n",
      "Epoch 601: train loss: 0.48966121673583984\n",
      "Epoch 602: train loss: 0.48963111639022827\n",
      "Epoch 603: train loss: 0.48960113525390625\n",
      "Epoch 604: train loss: 0.4895711839199066\n",
      "Epoch 605: train loss: 0.48954126238822937\n",
      "Epoch 606: train loss: 0.48951131105422974\n",
      "Epoch 607: train loss: 0.48948150873184204\n",
      "Epoch 608: train loss: 0.48945167660713196\n",
      "Epoch 609: train loss: 0.4894219934940338\n",
      "Epoch 610: train loss: 0.48939231038093567\n",
      "Epoch 611: train loss: 0.4893626272678375\n",
      "Epoch 612: train loss: 0.48933306336402893\n",
      "Epoch 613: train loss: 0.48930343985557556\n",
      "Epoch 614: train loss: 0.48927390575408936\n",
      "Epoch 615: train loss: 0.4892444908618927\n",
      "Epoch 616: train loss: 0.48921507596969604\n",
      "Epoch 617: train loss: 0.4891856908798218\n",
      "Epoch 618: train loss: 0.4891563951969147\n",
      "Epoch 619: train loss: 0.48912715911865234\n",
      "Epoch 620: train loss: 0.4890979528427124\n",
      "Epoch 621: train loss: 0.48906880617141724\n",
      "Epoch 622: train loss: 0.48903974890708923\n",
      "Epoch 623: train loss: 0.48901063203811646\n",
      "Epoch 624: train loss: 0.488981693983078\n",
      "Epoch 625: train loss: 0.48895278573036194\n",
      "Epoch 626: train loss: 0.48892390727996826\n",
      "Epoch 627: train loss: 0.488895058631897\n",
      "Epoch 628: train loss: 0.4888662099838257\n",
      "Epoch 629: train loss: 0.48883751034736633\n",
      "Epoch 630: train loss: 0.48880884051322937\n",
      "Epoch 631: train loss: 0.4887802004814148\n",
      "Epoch 632: train loss: 0.48875176906585693\n",
      "Epoch 633: train loss: 0.4887233376502991\n",
      "Epoch 634: train loss: 0.488694965839386\n",
      "Epoch 635: train loss: 0.4886665940284729\n",
      "Epoch 636: train loss: 0.48863837122917175\n",
      "Epoch 637: train loss: 0.48861008882522583\n",
      "Epoch 638: train loss: 0.48858192563056946\n",
      "Epoch 639: train loss: 0.4885537624359131\n",
      "Epoch 640: train loss: 0.4885258078575134\n",
      "Epoch 641: train loss: 0.4884977340698242\n",
      "Epoch 642: train loss: 0.48846983909606934\n",
      "Epoch 643: train loss: 0.48844200372695923\n",
      "Epoch 644: train loss: 0.48841413855552673\n",
      "Epoch 645: train loss: 0.488386332988739\n",
      "Epoch 646: train loss: 0.48835864663124084\n",
      "Epoch 647: train loss: 0.4883309006690979\n",
      "Epoch 648: train loss: 0.4883032441139221\n",
      "Epoch 649: train loss: 0.4882756471633911\n",
      "Epoch 650: train loss: 0.4882481098175049\n",
      "Epoch 651: train loss: 0.4882206320762634\n",
      "Epoch 652: train loss: 0.48819318413734436\n",
      "Epoch 653: train loss: 0.48816585540771484\n",
      "Epoch 654: train loss: 0.48813849687576294\n",
      "Epoch 655: train loss: 0.4881111979484558\n",
      "Epoch 656: train loss: 0.4880840480327606\n",
      "Epoch 657: train loss: 0.48805686831474304\n",
      "Epoch 658: train loss: 0.48802974820137024\n",
      "Epoch 659: train loss: 0.4880027174949646\n",
      "Epoch 660: train loss: 0.48797571659088135\n",
      "Epoch 661: train loss: 0.48794880509376526\n",
      "Epoch 662: train loss: 0.48792192339897156\n",
      "Epoch 663: train loss: 0.487895131111145\n",
      "Epoch 664: train loss: 0.48786842823028564\n",
      "Epoch 665: train loss: 0.48784172534942627\n",
      "Epoch 666: train loss: 0.48781508207321167\n",
      "Epoch 667: train loss: 0.48778846859931946\n",
      "Epoch 668: train loss: 0.4877621829509735\n",
      "Epoch 669: train loss: 0.48773595690727234\n",
      "Epoch 670: train loss: 0.48770982027053833\n",
      "Epoch 671: train loss: 0.4876837432384491\n",
      "Epoch 672: train loss: 0.48765772581100464\n",
      "Epoch 673: train loss: 0.48763179779052734\n",
      "Epoch 674: train loss: 0.48760589957237244\n",
      "Epoch 675: train loss: 0.4875800311565399\n",
      "Epoch 676: train loss: 0.48755425214767456\n",
      "Epoch 677: train loss: 0.4875285029411316\n",
      "Epoch 678: train loss: 0.4875028431415558\n",
      "Epoch 679: train loss: 0.48747724294662476\n",
      "Epoch 680: train loss: 0.4874516725540161\n",
      "Epoch 681: train loss: 0.48742619156837463\n",
      "Epoch 682: train loss: 0.48740074038505554\n",
      "Epoch 683: train loss: 0.4873753488063812\n",
      "Epoch 684: train loss: 0.4873500466346741\n",
      "Epoch 685: train loss: 0.4873247444629669\n",
      "Epoch 686: train loss: 0.48729947209358215\n",
      "Epoch 687: train loss: 0.4872743785381317\n",
      "Epoch 688: train loss: 0.4872492551803589\n",
      "Epoch 689: train loss: 0.4872242212295532\n",
      "Epoch 690: train loss: 0.4871992766857147\n",
      "Epoch 691: train loss: 0.4871743619441986\n",
      "Epoch 692: train loss: 0.4871494770050049\n",
      "Epoch 693: train loss: 0.4871247708797455\n",
      "Epoch 694: train loss: 0.4871000051498413\n",
      "Epoch 695: train loss: 0.4870753288269043\n",
      "Epoch 696: train loss: 0.4870506823062897\n",
      "Epoch 697: train loss: 0.4870260953903198\n",
      "Epoch 698: train loss: 0.48700159788131714\n",
      "Epoch 699: train loss: 0.48697715997695923\n",
      "Epoch 700: train loss: 0.48695290088653564\n",
      "Epoch 701: train loss: 0.4869288206100464\n",
      "Epoch 702: train loss: 0.4869047701358795\n",
      "Epoch 703: train loss: 0.486880898475647\n",
      "Epoch 704: train loss: 0.48685699701309204\n",
      "Epoch 705: train loss: 0.4868331849575043\n",
      "Epoch 706: train loss: 0.4868094027042389\n",
      "Epoch 707: train loss: 0.48678573966026306\n",
      "Epoch 708: train loss: 0.4867621660232544\n",
      "Epoch 709: train loss: 0.48673880100250244\n",
      "Epoch 710: train loss: 0.48671555519104004\n",
      "Epoch 711: train loss: 0.48669224977493286\n",
      "Epoch 712: train loss: 0.4866690933704376\n",
      "Epoch 713: train loss: 0.48664599657058716\n",
      "Epoch 714: train loss: 0.48662295937538147\n",
      "Epoch 715: train loss: 0.48659995198249817\n",
      "Epoch 716: train loss: 0.48657697439193726\n",
      "Epoch 717: train loss: 0.4865541458129883\n",
      "Epoch 718: train loss: 0.4865312874317169\n",
      "Epoch 719: train loss: 0.4865088164806366\n",
      "Epoch 720: train loss: 0.48648640513420105\n",
      "Epoch 721: train loss: 0.48646408319473267\n",
      "Epoch 722: train loss: 0.4864417314529419\n",
      "Epoch 723: train loss: 0.4864196181297302\n",
      "Epoch 724: train loss: 0.4863976240158081\n",
      "Epoch 725: train loss: 0.48637571930885315\n",
      "Epoch 726: train loss: 0.48635393381118774\n",
      "Epoch 727: train loss: 0.48633208870887756\n",
      "Epoch 728: train loss: 0.48631036281585693\n",
      "Epoch 729: train loss: 0.4862886667251587\n",
      "Epoch 730: train loss: 0.4862670302391052\n",
      "Epoch 731: train loss: 0.48624545335769653\n",
      "Epoch 732: train loss: 0.48622387647628784\n",
      "Epoch 733: train loss: 0.4862024486064911\n",
      "Epoch 734: train loss: 0.4861810803413391\n",
      "Epoch 735: train loss: 0.48615971207618713\n",
      "Epoch 736: train loss: 0.48613837361335754\n",
      "Epoch 737: train loss: 0.4861171245574951\n",
      "Epoch 738: train loss: 0.48609596490859985\n",
      "Epoch 739: train loss: 0.486074835062027\n",
      "Epoch 740: train loss: 0.48605379462242126\n",
      "Epoch 741: train loss: 0.48603278398513794\n",
      "Epoch 742: train loss: 0.4860117733478546\n",
      "Epoch 743: train loss: 0.48599085211753845\n",
      "Epoch 744: train loss: 0.48597002029418945\n",
      "Epoch 745: train loss: 0.48594915866851807\n",
      "Epoch 746: train loss: 0.48592841625213623\n",
      "Epoch 747: train loss: 0.48590776324272156\n",
      "Epoch 748: train loss: 0.4858870804309845\n",
      "Epoch 749: train loss: 0.4858664870262146\n",
      "Epoch 750: train loss: 0.4858459532260895\n",
      "Epoch 751: train loss: 0.48582541942596436\n",
      "Epoch 752: train loss: 0.48580509424209595\n",
      "Epoch 753: train loss: 0.48578470945358276\n",
      "Epoch 754: train loss: 0.48576444387435913\n",
      "Epoch 755: train loss: 0.4857442080974579\n",
      "Epoch 756: train loss: 0.4857240319252014\n",
      "Epoch 757: train loss: 0.48570388555526733\n",
      "Epoch 758: train loss: 0.485683798789978\n",
      "Epoch 759: train loss: 0.4856638014316559\n",
      "Epoch 760: train loss: 0.4856438934803009\n",
      "Epoch 761: train loss: 0.48562392592430115\n",
      "Epoch 762: train loss: 0.48560407757759094\n",
      "Epoch 763: train loss: 0.48558419942855835\n",
      "Epoch 764: train loss: 0.48556435108184814\n",
      "Epoch 765: train loss: 0.48554450273513794\n",
      "Epoch 766: train loss: 0.48552462458610535\n",
      "Epoch 767: train loss: 0.4855048954486847\n",
      "Epoch 768: train loss: 0.48548513650894165\n",
      "Epoch 769: train loss: 0.48546546697616577\n",
      "Epoch 770: train loss: 0.48544588685035706\n",
      "Epoch 771: train loss: 0.4854263961315155\n",
      "Epoch 772: train loss: 0.48540690541267395\n",
      "Epoch 773: train loss: 0.4853874146938324\n",
      "Epoch 774: train loss: 0.4853680729866028\n",
      "Epoch 775: train loss: 0.48534879088401794\n",
      "Epoch 776: train loss: 0.48532962799072266\n",
      "Epoch 777: train loss: 0.485310435295105\n",
      "Epoch 778: train loss: 0.4852912724018097\n",
      "Epoch 779: train loss: 0.48527219891548157\n",
      "Epoch 780: train loss: 0.4852532148361206\n",
      "Epoch 781: train loss: 0.4852343499660492\n",
      "Epoch 782: train loss: 0.48521560430526733\n",
      "Epoch 783: train loss: 0.485196977853775\n",
      "Epoch 784: train loss: 0.4851783812046051\n",
      "Epoch 785: train loss: 0.48515984416007996\n",
      "Epoch 786: train loss: 0.485141396522522\n",
      "Epoch 787: train loss: 0.4851228892803192\n",
      "Epoch 788: train loss: 0.48510444164276123\n",
      "Epoch 789: train loss: 0.4850861132144928\n",
      "Epoch 790: train loss: 0.48506781458854675\n",
      "Epoch 791: train loss: 0.4850495159626007\n",
      "Epoch 792: train loss: 0.4850313365459442\n",
      "Epoch 793: train loss: 0.4850131571292877\n",
      "Epoch 794: train loss: 0.48499518632888794\n",
      "Epoch 795: train loss: 0.4849770963191986\n",
      "Epoch 796: train loss: 0.4849591851234436\n",
      "Epoch 797: train loss: 0.4849412441253662\n",
      "Epoch 798: train loss: 0.48492348194122314\n",
      "Epoch 799: train loss: 0.48490580916404724\n",
      "Epoch 800: train loss: 0.48488810658454895\n",
      "Epoch 801: train loss: 0.48487043380737305\n",
      "Epoch 802: train loss: 0.48485293984413147\n",
      "Epoch 803: train loss: 0.4848354160785675\n",
      "Epoch 804: train loss: 0.4848179221153259\n",
      "Epoch 805: train loss: 0.48480042815208435\n",
      "Epoch 806: train loss: 0.48478296399116516\n",
      "Epoch 807: train loss: 0.48476549983024597\n",
      "Epoch 808: train loss: 0.4847482740879059\n",
      "Epoch 809: train loss: 0.4847310185432434\n",
      "Epoch 810: train loss: 0.48471376299858093\n",
      "Epoch 811: train loss: 0.4846966862678528\n",
      "Epoch 812: train loss: 0.48467960953712463\n",
      "Epoch 813: train loss: 0.48466262221336365\n",
      "Epoch 814: train loss: 0.48464569449424744\n",
      "Epoch 815: train loss: 0.4846287667751312\n",
      "Epoch 816: train loss: 0.4846119284629822\n",
      "Epoch 817: train loss: 0.4845952093601227\n",
      "Epoch 818: train loss: 0.48457857966423035\n",
      "Epoch 819: train loss: 0.4845621883869171\n",
      "Epoch 820: train loss: 0.4845457673072815\n",
      "Epoch 821: train loss: 0.4845294654369354\n",
      "Epoch 822: train loss: 0.4845132827758789\n",
      "Epoch 823: train loss: 0.4844970703125\n",
      "Epoch 824: train loss: 0.48448097705841064\n",
      "Epoch 825: train loss: 0.48446494340896606\n",
      "Epoch 826: train loss: 0.4844488799571991\n",
      "Epoch 827: train loss: 0.48443299531936646\n",
      "Epoch 828: train loss: 0.4844170808792114\n",
      "Epoch 829: train loss: 0.48440128564834595\n",
      "Epoch 830: train loss: 0.48438549041748047\n",
      "Epoch 831: train loss: 0.48436978459358215\n",
      "Epoch 832: train loss: 0.48435404896736145\n",
      "Epoch 833: train loss: 0.4843384325504303\n",
      "Epoch 834: train loss: 0.4843229055404663\n",
      "Epoch 835: train loss: 0.4843073785305023\n",
      "Epoch 836: train loss: 0.4842919111251831\n",
      "Epoch 837: train loss: 0.48427656292915344\n",
      "Epoch 838: train loss: 0.4842611253261566\n",
      "Epoch 839: train loss: 0.48424577713012695\n",
      "Epoch 840: train loss: 0.4842304587364197\n",
      "Epoch 841: train loss: 0.4842151999473572\n",
      "Epoch 842: train loss: 0.48420000076293945\n",
      "Epoch 843: train loss: 0.4841848909854889\n",
      "Epoch 844: train loss: 0.48416975140571594\n",
      "Epoch 845: train loss: 0.4841546416282654\n",
      "Epoch 846: train loss: 0.48413968086242676\n",
      "Epoch 847: train loss: 0.4841247498989105\n",
      "Epoch 848: train loss: 0.4841098487377167\n",
      "Epoch 849: train loss: 0.4840950071811676\n",
      "Epoch 850: train loss: 0.48408016562461853\n",
      "Epoch 851: train loss: 0.484065443277359\n",
      "Epoch 852: train loss: 0.4840506911277771\n",
      "Epoch 853: train loss: 0.48403605818748474\n",
      "Epoch 854: train loss: 0.4840214252471924\n",
      "Epoch 855: train loss: 0.4840069115161896\n",
      "Epoch 856: train loss: 0.4839923679828644\n",
      "Epoch 857: train loss: 0.48397794365882874\n",
      "Epoch 858: train loss: 0.4839635491371155\n",
      "Epoch 859: train loss: 0.483949214220047\n",
      "Epoch 860: train loss: 0.4839349091053009\n",
      "Epoch 861: train loss: 0.4839206337928772\n",
      "Epoch 862: train loss: 0.48390647768974304\n",
      "Epoch 863: train loss: 0.4838922619819641\n",
      "Epoch 864: train loss: 0.48387813568115234\n",
      "Epoch 865: train loss: 0.48386409878730774\n",
      "Epoch 866: train loss: 0.4838500916957855\n",
      "Epoch 867: train loss: 0.4838361442089081\n",
      "Epoch 868: train loss: 0.48382219672203064\n",
      "Epoch 869: train loss: 0.4838082790374756\n",
      "Epoch 870: train loss: 0.4837944805622101\n",
      "Epoch 871: train loss: 0.4837806820869446\n",
      "Epoch 872: train loss: 0.48376694321632385\n",
      "Epoch 873: train loss: 0.4837532639503479\n",
      "Epoch 874: train loss: 0.48373955488204956\n",
      "Epoch 875: train loss: 0.48372602462768555\n",
      "Epoch 876: train loss: 0.48371249437332153\n",
      "Epoch 877: train loss: 0.4836990237236023\n",
      "Epoch 878: train loss: 0.48368552327156067\n",
      "Epoch 879: train loss: 0.4836721122264862\n",
      "Epoch 880: train loss: 0.48365873098373413\n",
      "Epoch 881: train loss: 0.483645498752594\n",
      "Epoch 882: train loss: 0.48363232612609863\n",
      "Epoch 883: train loss: 0.4836193323135376\n",
      "Epoch 884: train loss: 0.4836063086986542\n",
      "Epoch 885: train loss: 0.4835934042930603\n",
      "Epoch 886: train loss: 0.4835805594921112\n",
      "Epoch 887: train loss: 0.48356765508651733\n",
      "Epoch 888: train loss: 0.4835548996925354\n",
      "Epoch 889: train loss: 0.48354220390319824\n",
      "Epoch 890: train loss: 0.4835295081138611\n",
      "Epoch 891: train loss: 0.4835168123245239\n",
      "Epoch 892: train loss: 0.48350420594215393\n",
      "Epoch 893: train loss: 0.4834916591644287\n",
      "Epoch 894: train loss: 0.4834791123867035\n",
      "Epoch 895: train loss: 0.4834666848182678\n",
      "Epoch 896: train loss: 0.48345425724983215\n",
      "Epoch 897: train loss: 0.4834417998790741\n",
      "Epoch 898: train loss: 0.483429491519928\n",
      "Epoch 899: train loss: 0.48341718316078186\n",
      "Epoch 900: train loss: 0.4834049642086029\n",
      "Epoch 901: train loss: 0.4833928048610687\n",
      "Epoch 902: train loss: 0.4833807349205017\n",
      "Epoch 903: train loss: 0.4833686947822571\n",
      "Epoch 904: train loss: 0.48335662484169006\n",
      "Epoch 905: train loss: 0.483344703912735\n",
      "Epoch 906: train loss: 0.4833327531814575\n",
      "Epoch 907: train loss: 0.48332104086875916\n",
      "Epoch 908: train loss: 0.4833092987537384\n",
      "Epoch 909: train loss: 0.48329752683639526\n",
      "Epoch 910: train loss: 0.4832858741283417\n",
      "Epoch 911: train loss: 0.48327428102493286\n",
      "Epoch 912: train loss: 0.4832626283168793\n",
      "Epoch 913: train loss: 0.48325106501579285\n",
      "Epoch 914: train loss: 0.48323947191238403\n",
      "Epoch 915: train loss: 0.4832279682159424\n",
      "Epoch 916: train loss: 0.4832164943218231\n",
      "Epoch 917: train loss: 0.48320505023002625\n",
      "Epoch 918: train loss: 0.48319363594055176\n",
      "Epoch 919: train loss: 0.48318231105804443\n",
      "Epoch 920: train loss: 0.4831709563732147\n",
      "Epoch 921: train loss: 0.4831596910953522\n",
      "Epoch 922: train loss: 0.48314839601516724\n",
      "Epoch 923: train loss: 0.48313722014427185\n",
      "Epoch 924: train loss: 0.4831260144710541\n",
      "Epoch 925: train loss: 0.4831148386001587\n",
      "Epoch 926: train loss: 0.48310375213623047\n",
      "Epoch 927: train loss: 0.48309269547462463\n",
      "Epoch 928: train loss: 0.4830816686153412\n",
      "Epoch 929: train loss: 0.48307058215141296\n",
      "Epoch 930: train loss: 0.48305967450141907\n",
      "Epoch 931: train loss: 0.483048677444458\n",
      "Epoch 932: train loss: 0.4830378293991089\n",
      "Epoch 933: train loss: 0.4830268919467926\n",
      "Epoch 934: train loss: 0.48301607370376587\n",
      "Epoch 935: train loss: 0.4830053150653839\n",
      "Epoch 936: train loss: 0.48299455642700195\n",
      "Epoch 937: train loss: 0.4829838275909424\n",
      "Epoch 938: train loss: 0.48297318816185\n",
      "Epoch 939: train loss: 0.4829625189304352\n",
      "Epoch 940: train loss: 0.48295196890830994\n",
      "Epoch 941: train loss: 0.4829414188861847\n",
      "Epoch 942: train loss: 0.48293083906173706\n",
      "Epoch 943: train loss: 0.48292040824890137\n",
      "Epoch 944: train loss: 0.4829099476337433\n",
      "Epoch 945: train loss: 0.48289954662323\n",
      "Epoch 946: train loss: 0.4828891456127167\n",
      "Epoch 947: train loss: 0.48287880420684814\n",
      "Epoch 948: train loss: 0.4828685522079468\n",
      "Epoch 949: train loss: 0.482858270406723\n",
      "Epoch 950: train loss: 0.4828479588031769\n",
      "Epoch 951: train loss: 0.48283782601356506\n",
      "Epoch 952: train loss: 0.48282763361930847\n",
      "Epoch 953: train loss: 0.4828174412250519\n",
      "Epoch 954: train loss: 0.4828074276447296\n",
      "Epoch 955: train loss: 0.4827972948551178\n",
      "Epoch 956: train loss: 0.48278719186782837\n",
      "Epoch 957: train loss: 0.4827772080898285\n",
      "Epoch 958: train loss: 0.4827671945095062\n",
      "Epoch 959: train loss: 0.48275724053382874\n",
      "Epoch 960: train loss: 0.48274731636047363\n",
      "Epoch 961: train loss: 0.48273733258247375\n",
      "Epoch 962: train loss: 0.4827274680137634\n",
      "Epoch 963: train loss: 0.48271769285202026\n",
      "Epoch 964: train loss: 0.4827079176902771\n",
      "Epoch 965: train loss: 0.48269814252853394\n",
      "Epoch 966: train loss: 0.48268842697143555\n",
      "Epoch 967: train loss: 0.48267877101898193\n",
      "Epoch 968: train loss: 0.48266908526420593\n",
      "Epoch 969: train loss: 0.4826594293117523\n",
      "Epoch 970: train loss: 0.4826498031616211\n",
      "Epoch 971: train loss: 0.48264026641845703\n",
      "Epoch 972: train loss: 0.48263072967529297\n",
      "Epoch 973: train loss: 0.4826211929321289\n",
      "Epoch 974: train loss: 0.48261165618896484\n",
      "Epoch 975: train loss: 0.48260220885276794\n",
      "Epoch 976: train loss: 0.48259276151657104\n",
      "Epoch 977: train loss: 0.48258334398269653\n",
      "Epoch 978: train loss: 0.482573926448822\n",
      "Epoch 979: train loss: 0.48256462812423706\n",
      "Epoch 980: train loss: 0.48255524039268494\n",
      "Epoch 981: train loss: 0.4825459420681\n",
      "Epoch 982: train loss: 0.4825366735458374\n",
      "Epoch 983: train loss: 0.4825274348258972\n",
      "Epoch 984: train loss: 0.48251813650131226\n",
      "Epoch 985: train loss: 0.48250892758369446\n",
      "Epoch 986: train loss: 0.48249977827072144\n",
      "Epoch 987: train loss: 0.4824904203414917\n",
      "Epoch 988: train loss: 0.48248112201690674\n",
      "Epoch 989: train loss: 0.4824718236923218\n",
      "Epoch 990: train loss: 0.4824625849723816\n",
      "Epoch 991: train loss: 0.4824533760547638\n",
      "Epoch 992: train loss: 0.4824441969394684\n",
      "Epoch 993: train loss: 0.4824349880218506\n",
      "Epoch 994: train loss: 0.4824258089065552\n",
      "Epoch 995: train loss: 0.4824167490005493\n",
      "Epoch 996: train loss: 0.4824075996875763\n",
      "Epoch 997: train loss: 0.48239845037460327\n",
      "Epoch 998: train loss: 0.4823894798755646\n",
      "Epoch 999: train loss: 0.4823804497718811\n",
      "Epoch 1000: train loss: 0.48237138986587524\n",
      "Epoch 1001: train loss: 0.48236244916915894\n",
      "Epoch 1002: train loss: 0.482353538274765\n",
      "Epoch 1003: train loss: 0.4823445975780487\n",
      "Epoch 1004: train loss: 0.4823356568813324\n",
      "Epoch 1005: train loss: 0.48232677578926086\n",
      "Epoch 1006: train loss: 0.4823179244995117\n",
      "Epoch 1007: train loss: 0.48230910301208496\n",
      "Epoch 1008: train loss: 0.4823002815246582\n",
      "Epoch 1009: train loss: 0.48229143023490906\n",
      "Epoch 1010: train loss: 0.48228272795677185\n",
      "Epoch 1011: train loss: 0.48227399587631226\n",
      "Epoch 1012: train loss: 0.48226526379585266\n",
      "Epoch 1013: train loss: 0.48225659132003784\n",
      "Epoch 1014: train loss: 0.48224785923957825\n",
      "Epoch 1015: train loss: 0.48223933577537537\n",
      "Epoch 1016: train loss: 0.4822307229042053\n",
      "Epoch 1017: train loss: 0.4822221100330353\n",
      "Epoch 1018: train loss: 0.4822135269641876\n",
      "Epoch 1019: train loss: 0.48220497369766235\n",
      "Epoch 1020: train loss: 0.4821964502334595\n",
      "Epoch 1021: train loss: 0.4821879267692566\n",
      "Epoch 1022: train loss: 0.4821794629096985\n",
      "Epoch 1023: train loss: 0.4821709990501404\n",
      "Epoch 1024: train loss: 0.48216259479522705\n",
      "Epoch 1025: train loss: 0.4821542203426361\n",
      "Epoch 1026: train loss: 0.4821458160877228\n",
      "Epoch 1027: train loss: 0.48213744163513184\n",
      "Epoch 1028: train loss: 0.4821290969848633\n",
      "Epoch 1029: train loss: 0.4821207821369171\n",
      "Epoch 1030: train loss: 0.4821125268936157\n",
      "Epoch 1031: train loss: 0.48210421204566956\n",
      "Epoch 1032: train loss: 0.48209601640701294\n",
      "Epoch 1033: train loss: 0.48208779096603394\n",
      "Epoch 1034: train loss: 0.48207953572273254\n",
      "Epoch 1035: train loss: 0.4820713400840759\n",
      "Epoch 1036: train loss: 0.4820631742477417\n",
      "Epoch 1037: train loss: 0.48205509781837463\n",
      "Epoch 1038: train loss: 0.4820469915866852\n",
      "Epoch 1039: train loss: 0.48203882575035095\n",
      "Epoch 1040: train loss: 0.4820307493209839\n",
      "Epoch 1041: train loss: 0.4820227026939392\n",
      "Epoch 1042: train loss: 0.4820146858692169\n",
      "Epoch 1043: train loss: 0.48200663924217224\n",
      "Epoch 1044: train loss: 0.48199865221977234\n",
      "Epoch 1045: train loss: 0.48199066519737244\n",
      "Epoch 1046: train loss: 0.48198261857032776\n",
      "Epoch 1047: train loss: 0.481974720954895\n",
      "Epoch 1048: train loss: 0.4819668233394623\n",
      "Epoch 1049: train loss: 0.48195889592170715\n",
      "Epoch 1050: train loss: 0.4819509983062744\n",
      "Epoch 1051: train loss: 0.48194316029548645\n",
      "Epoch 1052: train loss: 0.48193538188934326\n",
      "Epoch 1053: train loss: 0.4819275438785553\n",
      "Epoch 1054: train loss: 0.48191970586776733\n",
      "Epoch 1055: train loss: 0.48191189765930176\n",
      "Epoch 1056: train loss: 0.48190411925315857\n",
      "Epoch 1057: train loss: 0.48189640045166016\n",
      "Epoch 1058: train loss: 0.48188865184783936\n",
      "Epoch 1059: train loss: 0.48188093304634094\n",
      "Epoch 1060: train loss: 0.4818733036518097\n",
      "Epoch 1061: train loss: 0.4818655550479889\n",
      "Epoch 1062: train loss: 0.48185792565345764\n",
      "Epoch 1063: train loss: 0.4818502366542816\n",
      "Epoch 1064: train loss: 0.48184266686439514\n",
      "Epoch 1065: train loss: 0.4818350374698639\n",
      "Epoch 1066: train loss: 0.4818274676799774\n",
      "Epoch 1067: train loss: 0.48181989789009094\n",
      "Epoch 1068: train loss: 0.4818122982978821\n",
      "Epoch 1069: train loss: 0.4818049967288971\n",
      "Epoch 1070: train loss: 0.4817976653575897\n",
      "Epoch 1071: train loss: 0.4817904233932495\n",
      "Epoch 1072: train loss: 0.4817832112312317\n",
      "Epoch 1073: train loss: 0.48177602887153625\n",
      "Epoch 1074: train loss: 0.48176881670951843\n",
      "Epoch 1075: train loss: 0.48176151514053345\n",
      "Epoch 1076: train loss: 0.48175424337387085\n",
      "Epoch 1077: train loss: 0.48174700140953064\n",
      "Epoch 1078: train loss: 0.4817397892475128\n",
      "Epoch 1079: train loss: 0.48173245787620544\n",
      "Epoch 1080: train loss: 0.4817253053188324\n",
      "Epoch 1081: train loss: 0.4817180931568146\n",
      "Epoch 1082: train loss: 0.48171088099479675\n",
      "Epoch 1083: train loss: 0.48170366883277893\n",
      "Epoch 1084: train loss: 0.48169654607772827\n",
      "Epoch 1085: train loss: 0.481689453125\n",
      "Epoch 1086: train loss: 0.48168230056762695\n",
      "Epoch 1087: train loss: 0.4816751778125763\n",
      "Epoch 1088: train loss: 0.481668084859848\n",
      "Epoch 1089: train loss: 0.48166102170944214\n",
      "Epoch 1090: train loss: 0.48165401816368103\n",
      "Epoch 1091: train loss: 0.481646865606308\n",
      "Epoch 1092: train loss: 0.48163992166519165\n",
      "Epoch 1093: train loss: 0.48163291811943054\n",
      "Epoch 1094: train loss: 0.48162585496902466\n",
      "Epoch 1095: train loss: 0.4816189110279083\n",
      "Epoch 1096: train loss: 0.4816119074821472\n",
      "Epoch 1097: train loss: 0.4816049635410309\n",
      "Epoch 1098: train loss: 0.48159804940223694\n",
      "Epoch 1099: train loss: 0.4815911054611206\n",
      "Epoch 1100: train loss: 0.48158422112464905\n",
      "Epoch 1101: train loss: 0.4815773665904999\n",
      "Epoch 1102: train loss: 0.48157042264938354\n",
      "Epoch 1103: train loss: 0.4815635681152344\n",
      "Epoch 1104: train loss: 0.4815567135810852\n",
      "Epoch 1105: train loss: 0.4815499186515808\n",
      "Epoch 1106: train loss: 0.48154309391975403\n",
      "Epoch 1107: train loss: 0.48153629899024963\n",
      "Epoch 1108: train loss: 0.48152950406074524\n",
      "Epoch 1109: train loss: 0.4815227687358856\n",
      "Epoch 1110: train loss: 0.4815160036087036\n",
      "Epoch 1111: train loss: 0.481509268283844\n",
      "Epoch 1112: train loss: 0.4815025329589844\n",
      "Epoch 1113: train loss: 0.48149579763412476\n",
      "Epoch 1114: train loss: 0.4814891517162323\n",
      "Epoch 1115: train loss: 0.48148247599601746\n",
      "Epoch 1116: train loss: 0.4814757704734802\n",
      "Epoch 1117: train loss: 0.48146915435791016\n",
      "Epoch 1118: train loss: 0.4814624488353729\n",
      "Epoch 1119: train loss: 0.48145586252212524\n",
      "Epoch 1120: train loss: 0.4814492464065552\n",
      "Epoch 1121: train loss: 0.4814426600933075\n",
      "Epoch 1122: train loss: 0.48143601417541504\n",
      "Epoch 1123: train loss: 0.48142948746681213\n",
      "Epoch 1124: train loss: 0.48142290115356445\n",
      "Epoch 1125: train loss: 0.48141637444496155\n",
      "Epoch 1126: train loss: 0.48140981793403625\n",
      "Epoch 1127: train loss: 0.48140329122543335\n",
      "Epoch 1128: train loss: 0.4813968241214752\n",
      "Epoch 1129: train loss: 0.4813903272151947\n",
      "Epoch 1130: train loss: 0.4813838601112366\n",
      "Epoch 1131: train loss: 0.48137739300727844\n",
      "Epoch 1132: train loss: 0.4813708961009979\n",
      "Epoch 1133: train loss: 0.4813644587993622\n",
      "Epoch 1134: train loss: 0.48135805130004883\n",
      "Epoch 1135: train loss: 0.4813516139984131\n",
      "Epoch 1136: train loss: 0.4813452363014221\n",
      "Epoch 1137: train loss: 0.48133885860443115\n",
      "Epoch 1138: train loss: 0.4813324809074402\n",
      "Epoch 1139: train loss: 0.4813261032104492\n",
      "Epoch 1140: train loss: 0.48131975531578064\n",
      "Epoch 1141: train loss: 0.48131340742111206\n",
      "Epoch 1142: train loss: 0.48130708932876587\n",
      "Epoch 1143: train loss: 0.4813007712364197\n",
      "Epoch 1144: train loss: 0.48129451274871826\n",
      "Epoch 1145: train loss: 0.48128822445869446\n",
      "Epoch 1146: train loss: 0.48128196597099304\n",
      "Epoch 1147: train loss: 0.48127567768096924\n",
      "Epoch 1148: train loss: 0.4812694191932678\n",
      "Epoch 1149: train loss: 0.4812631905078888\n",
      "Epoch 1150: train loss: 0.48125699162483215\n",
      "Epoch 1151: train loss: 0.4812507629394531\n",
      "Epoch 1152: train loss: 0.4812445342540741\n",
      "Epoch 1153: train loss: 0.48123836517333984\n",
      "Epoch 1154: train loss: 0.48123231530189514\n",
      "Epoch 1155: train loss: 0.4812260568141937\n",
      "Epoch 1156: train loss: 0.48121997714042664\n",
      "Epoch 1157: train loss: 0.4812138080596924\n",
      "Epoch 1158: train loss: 0.4812076985836029\n",
      "Epoch 1159: train loss: 0.4812015891075134\n",
      "Epoch 1160: train loss: 0.48119547963142395\n",
      "Epoch 1161: train loss: 0.4811893701553345\n",
      "Epoch 1162: train loss: 0.4811832904815674\n",
      "Epoch 1163: train loss: 0.4811772406101227\n",
      "Epoch 1164: train loss: 0.48117128014564514\n",
      "Epoch 1165: train loss: 0.48116523027420044\n",
      "Epoch 1166: train loss: 0.48115915060043335\n",
      "Epoch 1167: train loss: 0.4811531603336334\n",
      "Epoch 1168: train loss: 0.4811471402645111\n",
      "Epoch 1169: train loss: 0.48114123940467834\n",
      "Epoch 1170: train loss: 0.48113521933555603\n",
      "Epoch 1171: train loss: 0.4811292290687561\n",
      "Epoch 1172: train loss: 0.48112326860427856\n",
      "Epoch 1173: train loss: 0.481117308139801\n",
      "Epoch 1174: train loss: 0.48111140727996826\n",
      "Epoch 1175: train loss: 0.4811054468154907\n",
      "Epoch 1176: train loss: 0.48109957575798035\n",
      "Epoch 1177: train loss: 0.4810936152935028\n",
      "Epoch 1178: train loss: 0.48108771443367004\n",
      "Epoch 1179: train loss: 0.48108187317848206\n",
      "Epoch 1180: train loss: 0.48107603192329407\n",
      "Epoch 1181: train loss: 0.4810701608657837\n",
      "Epoch 1182: train loss: 0.4810643196105957\n",
      "Epoch 1183: train loss: 0.4810584485530853\n",
      "Epoch 1184: train loss: 0.4810526669025421\n",
      "Epoch 1185: train loss: 0.48104679584503174\n",
      "Epoch 1186: train loss: 0.4810410141944885\n",
      "Epoch 1187: train loss: 0.4810352027416229\n",
      "Epoch 1188: train loss: 0.4810294508934021\n",
      "Epoch 1189: train loss: 0.4810236692428589\n",
      "Epoch 1190: train loss: 0.48101791739463806\n",
      "Epoch 1191: train loss: 0.48101210594177246\n",
      "Epoch 1192: train loss: 0.4810064733028412\n",
      "Epoch 1193: train loss: 0.4810006618499756\n",
      "Epoch 1194: train loss: 0.48099496960639954\n",
      "Epoch 1195: train loss: 0.4809892773628235\n",
      "Epoch 1196: train loss: 0.48098352551460266\n",
      "Epoch 1197: train loss: 0.4809778928756714\n",
      "Epoch 1198: train loss: 0.48097217082977295\n",
      "Epoch 1199: train loss: 0.4809665381908417\n",
      "Epoch 1200: train loss: 0.4809608459472656\n",
      "Epoch 1201: train loss: 0.48095524311065674\n",
      "Epoch 1202: train loss: 0.48094961047172546\n",
      "Epoch 1203: train loss: 0.4809439778327942\n",
      "Epoch 1204: train loss: 0.4809383153915405\n",
      "Epoch 1205: train loss: 0.48093274235725403\n",
      "Epoch 1206: train loss: 0.48092713952064514\n",
      "Epoch 1207: train loss: 0.48092159628868103\n",
      "Epoch 1208: train loss: 0.48091596364974976\n",
      "Epoch 1209: train loss: 0.48091042041778564\n",
      "Epoch 1210: train loss: 0.4809049367904663\n",
      "Epoch 1211: train loss: 0.4808993339538574\n",
      "Epoch 1212: train loss: 0.4808937907218933\n",
      "Epoch 1213: train loss: 0.480888307094574\n",
      "Epoch 1214: train loss: 0.48088282346725464\n",
      "Epoch 1215: train loss: 0.4808772802352905\n",
      "Epoch 1216: train loss: 0.4808717966079712\n",
      "Epoch 1217: train loss: 0.4808662533760071\n",
      "Epoch 1218: train loss: 0.4808608293533325\n",
      "Epoch 1219: train loss: 0.4808553457260132\n",
      "Epoch 1220: train loss: 0.48084989190101624\n",
      "Epoch 1221: train loss: 0.4808444082736969\n",
      "Epoch 1222: train loss: 0.48083898425102234\n",
      "Epoch 1223: train loss: 0.4808335602283478\n",
      "Epoch 1224: train loss: 0.4808281660079956\n",
      "Epoch 1225: train loss: 0.48082274198532104\n",
      "Epoch 1226: train loss: 0.48081734776496887\n",
      "Epoch 1227: train loss: 0.4808119535446167\n",
      "Epoch 1228: train loss: 0.4808065891265869\n",
      "Epoch 1229: train loss: 0.48080119490623474\n",
      "Epoch 1230: train loss: 0.4807957708835602\n",
      "Epoch 1231: train loss: 0.48079049587249756\n",
      "Epoch 1232: train loss: 0.48078516125679016\n",
      "Epoch 1233: train loss: 0.48077982664108276\n",
      "Epoch 1234: train loss: 0.480774462223053\n",
      "Epoch 1235: train loss: 0.48076915740966797\n",
      "Epoch 1236: train loss: 0.48076388239860535\n",
      "Epoch 1237: train loss: 0.48075854778289795\n",
      "Epoch 1238: train loss: 0.48075324296951294\n",
      "Epoch 1239: train loss: 0.4807479679584503\n",
      "Epoch 1240: train loss: 0.4807426631450653\n",
      "Epoch 1241: train loss: 0.48073744773864746\n",
      "Epoch 1242: train loss: 0.48073217272758484\n",
      "Epoch 1243: train loss: 0.4807268977165222\n",
      "Epoch 1244: train loss: 0.48072168231010437\n",
      "Epoch 1245: train loss: 0.48071643710136414\n",
      "Epoch 1246: train loss: 0.48071128129959106\n",
      "Epoch 1247: train loss: 0.48070603609085083\n",
      "Epoch 1248: train loss: 0.48070088028907776\n",
      "Epoch 1249: train loss: 0.4806957542896271\n",
      "Epoch 1250: train loss: 0.4806905686855316\n",
      "Epoch 1251: train loss: 0.48068541288375854\n",
      "Epoch 1252: train loss: 0.48068028688430786\n",
      "Epoch 1253: train loss: 0.48067522048950195\n",
      "Epoch 1254: train loss: 0.48067009449005127\n",
      "Epoch 1255: train loss: 0.4806649684906006\n",
      "Epoch 1256: train loss: 0.4806599020957947\n",
      "Epoch 1257: train loss: 0.480654776096344\n",
      "Epoch 1258: train loss: 0.4806497097015381\n",
      "Epoch 1259: train loss: 0.4806446433067322\n",
      "Epoch 1260: train loss: 0.48063957691192627\n",
      "Epoch 1261: train loss: 0.48063451051712036\n",
      "Epoch 1262: train loss: 0.48062950372695923\n",
      "Epoch 1263: train loss: 0.4806244671344757\n",
      "Epoch 1264: train loss: 0.4806194603443146\n",
      "Epoch 1265: train loss: 0.48061445355415344\n",
      "Epoch 1266: train loss: 0.4806094169616699\n",
      "Epoch 1267: train loss: 0.480604350566864\n",
      "Epoch 1268: train loss: 0.4805993437767029\n",
      "Epoch 1269: train loss: 0.48059436678886414\n",
      "Epoch 1270: train loss: 0.4805894196033478\n",
      "Epoch 1271: train loss: 0.48058444261550903\n",
      "Epoch 1272: train loss: 0.48057952523231506\n",
      "Epoch 1273: train loss: 0.48057451844215393\n",
      "Epoch 1274: train loss: 0.48056960105895996\n",
      "Epoch 1275: train loss: 0.4805646538734436\n",
      "Epoch 1276: train loss: 0.480559766292572\n",
      "Epoch 1277: train loss: 0.4805549085140228\n",
      "Epoch 1278: train loss: 0.48055002093315125\n",
      "Epoch 1279: train loss: 0.48054516315460205\n",
      "Epoch 1280: train loss: 0.48054027557373047\n",
      "Epoch 1281: train loss: 0.48053547739982605\n",
      "Epoch 1282: train loss: 0.48053061962127686\n",
      "Epoch 1283: train loss: 0.48052576184272766\n",
      "Epoch 1284: train loss: 0.48052099347114563\n",
      "Epoch 1285: train loss: 0.48051613569259644\n",
      "Epoch 1286: train loss: 0.480511337518692\n",
      "Epoch 1287: train loss: 0.4805065393447876\n",
      "Epoch 1288: train loss: 0.4805017113685608\n",
      "Epoch 1289: train loss: 0.48049697279930115\n",
      "Epoch 1290: train loss: 0.48049217462539673\n",
      "Epoch 1291: train loss: 0.4804873466491699\n",
      "Epoch 1292: train loss: 0.4804826080799103\n",
      "Epoch 1293: train loss: 0.48047783970832825\n",
      "Epoch 1294: train loss: 0.4804731011390686\n",
      "Epoch 1295: train loss: 0.4804683327674866\n",
      "Epoch 1296: train loss: 0.4804636240005493\n",
      "Epoch 1297: train loss: 0.4804588854312897\n",
      "Epoch 1298: train loss: 0.4804541766643524\n",
      "Epoch 1299: train loss: 0.48044946789741516\n",
      "Epoch 1300: train loss: 0.48044469952583313\n",
      "Epoch 1301: train loss: 0.48044002056121826\n",
      "Epoch 1302: train loss: 0.480435311794281\n",
      "Epoch 1303: train loss: 0.48043060302734375\n",
      "Epoch 1304: train loss: 0.4804258942604065\n",
      "Epoch 1305: train loss: 0.4804213047027588\n",
      "Epoch 1306: train loss: 0.48041659593582153\n",
      "Epoch 1307: train loss: 0.48041194677352905\n",
      "Epoch 1308: train loss: 0.4804072380065918\n",
      "Epoch 1309: train loss: 0.4804026484489441\n",
      "Epoch 1310: train loss: 0.4803979992866516\n",
      "Epoch 1311: train loss: 0.4803934097290039\n",
      "Epoch 1312: train loss: 0.4803887605667114\n",
      "Epoch 1313: train loss: 0.4803842008113861\n",
      "Epoch 1314: train loss: 0.48037949204444885\n",
      "Epoch 1315: train loss: 0.48037490248680115\n",
      "Epoch 1316: train loss: 0.48037031292915344\n",
      "Epoch 1317: train loss: 0.48036569356918335\n",
      "Epoch 1318: train loss: 0.48036110401153564\n",
      "Epoch 1319: train loss: 0.4803565442562103\n",
      "Epoch 1320: train loss: 0.4803519546985626\n",
      "Epoch 1321: train loss: 0.4803474247455597\n",
      "Epoch 1322: train loss: 0.48034292459487915\n",
      "Epoch 1323: train loss: 0.48033830523490906\n",
      "Epoch 1324: train loss: 0.48033377528190613\n",
      "Epoch 1325: train loss: 0.4803292751312256\n",
      "Epoch 1326: train loss: 0.48032474517822266\n",
      "Epoch 1327: train loss: 0.4803202450275421\n",
      "Epoch 1328: train loss: 0.4803156852722168\n",
      "Epoch 1329: train loss: 0.48031118512153625\n",
      "Epoch 1330: train loss: 0.4803066551685333\n",
      "Epoch 1331: train loss: 0.48030221462249756\n",
      "Epoch 1332: train loss: 0.480297714471817\n",
      "Epoch 1333: train loss: 0.48029327392578125\n",
      "Epoch 1334: train loss: 0.4802889823913574\n",
      "Epoch 1335: train loss: 0.48028460144996643\n",
      "Epoch 1336: train loss: 0.4802802503108978\n",
      "Epoch 1337: train loss: 0.4802759885787964\n",
      "Epoch 1338: train loss: 0.4802716374397278\n",
      "Epoch 1339: train loss: 0.48026734590530396\n",
      "Epoch 1340: train loss: 0.48026302456855774\n",
      "Epoch 1341: train loss: 0.4802587926387787\n",
      "Epoch 1342: train loss: 0.48025450110435486\n",
      "Epoch 1343: train loss: 0.48025020956993103\n",
      "Epoch 1344: train loss: 0.4802459180355072\n",
      "Epoch 1345: train loss: 0.4802417457103729\n",
      "Epoch 1346: train loss: 0.48023751378059387\n",
      "Epoch 1347: train loss: 0.4802332818508148\n",
      "Epoch 1348: train loss: 0.48022904992103577\n",
      "Epoch 1349: train loss: 0.4802248179912567\n",
      "Epoch 1350: train loss: 0.48022064566612244\n",
      "Epoch 1351: train loss: 0.48021644353866577\n",
      "Epoch 1352: train loss: 0.48021218180656433\n",
      "Epoch 1353: train loss: 0.48020803928375244\n",
      "Epoch 1354: train loss: 0.4802038073539734\n",
      "Epoch 1355: train loss: 0.4801996946334839\n",
      "Epoch 1356: train loss: 0.4801954925060272\n",
      "Epoch 1357: train loss: 0.48019132018089294\n",
      "Epoch 1358: train loss: 0.48018723726272583\n",
      "Epoch 1359: train loss: 0.4801830053329468\n",
      "Epoch 1360: train loss: 0.4801788926124573\n",
      "Epoch 1361: train loss: 0.4801747500896454\n",
      "Epoch 1362: train loss: 0.4801705777645111\n",
      "Epoch 1363: train loss: 0.480166494846344\n",
      "Epoch 1364: train loss: 0.4801623821258545\n",
      "Epoch 1365: train loss: 0.4801582396030426\n",
      "Epoch 1366: train loss: 0.4801541268825531\n",
      "Epoch 1367: train loss: 0.480150043964386\n",
      "Epoch 1368: train loss: 0.4801459014415741\n",
      "Epoch 1369: train loss: 0.48014184832572937\n",
      "Epoch 1370: train loss: 0.4801377058029175\n",
      "Epoch 1371: train loss: 0.48013362288475037\n",
      "Epoch 1372: train loss: 0.48012956976890564\n",
      "Epoch 1373: train loss: 0.4801254868507385\n",
      "Epoch 1374: train loss: 0.4801214039325714\n",
      "Epoch 1375: train loss: 0.4801173806190491\n",
      "Epoch 1376: train loss: 0.48011329770088196\n",
      "Epoch 1377: train loss: 0.4801092743873596\n",
      "Epoch 1378: train loss: 0.4801052510738373\n",
      "Epoch 1379: train loss: 0.48010116815567017\n",
      "Epoch 1380: train loss: 0.48009711503982544\n",
      "Epoch 1381: train loss: 0.4800931215286255\n",
      "Epoch 1382: train loss: 0.48008909821510315\n",
      "Epoch 1383: train loss: 0.4800850749015808\n",
      "Epoch 1384: train loss: 0.48008105158805847\n",
      "Epoch 1385: train loss: 0.4800770878791809\n",
      "Epoch 1386: train loss: 0.48007306456565857\n",
      "Epoch 1387: train loss: 0.480069100856781\n",
      "Epoch 1388: train loss: 0.48006513714790344\n",
      "Epoch 1389: train loss: 0.4800611138343811\n",
      "Epoch 1390: train loss: 0.48005712032318115\n",
      "Epoch 1391: train loss: 0.4800530970096588\n",
      "Epoch 1392: train loss: 0.480049192905426\n",
      "Epoch 1393: train loss: 0.4800451695919037\n",
      "Epoch 1394: train loss: 0.4800412952899933\n",
      "Epoch 1395: train loss: 0.48003727197647095\n",
      "Epoch 1396: train loss: 0.48003339767456055\n",
      "Epoch 1397: train loss: 0.480029433965683\n",
      "Epoch 1398: train loss: 0.4800255000591278\n",
      "Epoch 1399: train loss: 0.480021595954895\n",
      "Epoch 1400: train loss: 0.48001763224601746\n",
      "Epoch 1401: train loss: 0.4800136983394623\n",
      "Epoch 1402: train loss: 0.4800098240375519\n",
      "Epoch 1403: train loss: 0.4800059199333191\n",
      "Epoch 1404: train loss: 0.4800019860267639\n",
      "Epoch 1405: train loss: 0.47999808192253113\n",
      "Epoch 1406: train loss: 0.4799942672252655\n",
      "Epoch 1407: train loss: 0.47999030351638794\n",
      "Epoch 1408: train loss: 0.4799864888191223\n",
      "Epoch 1409: train loss: 0.47998255491256714\n",
      "Epoch 1410: train loss: 0.47997868061065674\n",
      "Epoch 1411: train loss: 0.4799748361110687\n",
      "Epoch 1412: train loss: 0.4799709618091583\n",
      "Epoch 1413: train loss: 0.4799671471118927\n",
      "Epoch 1414: train loss: 0.4799633324146271\n",
      "Epoch 1415: train loss: 0.4799593985080719\n",
      "Epoch 1416: train loss: 0.4799555540084839\n",
      "Epoch 1417: train loss: 0.47995173931121826\n",
      "Epoch 1418: train loss: 0.47994786500930786\n",
      "Epoch 1419: train loss: 0.479944109916687\n",
      "Epoch 1420: train loss: 0.4799402356147766\n",
      "Epoch 1421: train loss: 0.479936420917511\n",
      "Epoch 1422: train loss: 0.47993260622024536\n",
      "Epoch 1423: train loss: 0.4799288511276245\n",
      "Epoch 1424: train loss: 0.4799250364303589\n",
      "Epoch 1425: train loss: 0.47992122173309326\n",
      "Epoch 1426: train loss: 0.47991743683815\n",
      "Epoch 1427: train loss: 0.4799136221408844\n",
      "Epoch 1428: train loss: 0.47990986704826355\n",
      "Epoch 1429: train loss: 0.4799061119556427\n",
      "Epoch 1430: train loss: 0.47990235686302185\n",
      "Epoch 1431: train loss: 0.4798985719680786\n",
      "Epoch 1432: train loss: 0.47989481687545776\n",
      "Epoch 1433: train loss: 0.47989100217819214\n",
      "Epoch 1434: train loss: 0.47988730669021606\n",
      "Epoch 1435: train loss: 0.4798835217952728\n",
      "Epoch 1436: train loss: 0.47987982630729675\n",
      "Epoch 1437: train loss: 0.4798761010169983\n",
      "Epoch 1438: train loss: 0.479872465133667\n",
      "Epoch 1439: train loss: 0.47986868023872375\n",
      "Epoch 1440: train loss: 0.47986504435539246\n",
      "Epoch 1441: train loss: 0.479861319065094\n",
      "Epoch 1442: train loss: 0.4798576533794403\n",
      "Epoch 1443: train loss: 0.479854017496109\n",
      "Epoch 1444: train loss: 0.47985029220581055\n",
      "Epoch 1445: train loss: 0.47984662652015686\n",
      "Epoch 1446: train loss: 0.47984299063682556\n",
      "Epoch 1447: train loss: 0.4798393249511719\n",
      "Epoch 1448: train loss: 0.47983571887016296\n",
      "Epoch 1449: train loss: 0.4798320233821869\n",
      "Epoch 1450: train loss: 0.4798283576965332\n",
      "Epoch 1451: train loss: 0.4798247516155243\n",
      "Epoch 1452: train loss: 0.4798210859298706\n",
      "Epoch 1453: train loss: 0.4798174798488617\n",
      "Epoch 1454: train loss: 0.479813814163208\n",
      "Epoch 1455: train loss: 0.4798102080821991\n",
      "Epoch 1456: train loss: 0.4798066020011902\n",
      "Epoch 1457: train loss: 0.4798029959201813\n",
      "Epoch 1458: train loss: 0.47979936003685\n",
      "Epoch 1459: train loss: 0.47979578375816345\n",
      "Epoch 1460: train loss: 0.47979214787483215\n",
      "Epoch 1461: train loss: 0.47978857159614563\n",
      "Epoch 1462: train loss: 0.4797849655151367\n",
      "Epoch 1463: train loss: 0.4797813594341278\n",
      "Epoch 1464: train loss: 0.47977781295776367\n",
      "Epoch 1465: train loss: 0.47977426648139954\n",
      "Epoch 1466: train loss: 0.4797707200050354\n",
      "Epoch 1467: train loss: 0.47976717352867126\n",
      "Epoch 1468: train loss: 0.47976359724998474\n",
      "Epoch 1469: train loss: 0.4797600507736206\n",
      "Epoch 1470: train loss: 0.4797563850879669\n",
      "Epoch 1471: train loss: 0.47975295782089233\n",
      "Epoch 1472: train loss: 0.4797493815422058\n",
      "Epoch 1473: train loss: 0.4797458350658417\n",
      "Epoch 1474: train loss: 0.47974228858947754\n",
      "Epoch 1475: train loss: 0.4797387421131134\n",
      "Epoch 1476: train loss: 0.47973528504371643\n",
      "Epoch 1477: train loss: 0.4797316789627075\n",
      "Epoch 1478: train loss: 0.47972822189331055\n",
      "Epoch 1479: train loss: 0.4797246754169464\n",
      "Epoch 1480: train loss: 0.47972118854522705\n",
      "Epoch 1481: train loss: 0.4797176718711853\n",
      "Epoch 1482: train loss: 0.47971418499946594\n",
      "Epoch 1483: train loss: 0.4797106683254242\n",
      "Epoch 1484: train loss: 0.4797072112560272\n",
      "Epoch 1485: train loss: 0.47970372438430786\n",
      "Epoch 1486: train loss: 0.4797002673149109\n",
      "Epoch 1487: train loss: 0.47969678044319153\n",
      "Epoch 1488: train loss: 0.47969332337379456\n",
      "Epoch 1489: train loss: 0.4796898663043976\n",
      "Epoch 1490: train loss: 0.4796863794326782\n",
      "Epoch 1491: train loss: 0.47968292236328125\n",
      "Epoch 1492: train loss: 0.4796794652938843\n",
      "Epoch 1493: train loss: 0.4796760678291321\n",
      "Epoch 1494: train loss: 0.4796725809574127\n",
      "Epoch 1495: train loss: 0.47966912388801575\n",
      "Epoch 1496: train loss: 0.47966572642326355\n",
      "Epoch 1497: train loss: 0.4796622693538666\n",
      "Epoch 1498: train loss: 0.4796588718891144\n",
      "Epoch 1499: train loss: 0.4796554446220398\n",
      "Epoch 1500: train loss: 0.4796520471572876\n",
      "Epoch 1501: train loss: 0.4796486496925354\n",
      "Epoch 1502: train loss: 0.4796451926231384\n",
      "Epoch 1503: train loss: 0.47964173555374146\n",
      "Epoch 1504: train loss: 0.47963839769363403\n",
      "Epoch 1505: train loss: 0.47963494062423706\n",
      "Epoch 1506: train loss: 0.47963160276412964\n",
      "Epoch 1507: train loss: 0.47962820529937744\n",
      "Epoch 1508: train loss: 0.47962480783462524\n",
      "Epoch 1509: train loss: 0.4796214699745178\n",
      "Epoch 1510: train loss: 0.47961801290512085\n",
      "Epoch 1511: train loss: 0.4796146750450134\n",
      "Epoch 1512: train loss: 0.479611337184906\n",
      "Epoch 1513: train loss: 0.4796079993247986\n",
      "Epoch 1514: train loss: 0.4796046018600464\n",
      "Epoch 1515: train loss: 0.4796012341976166\n",
      "Epoch 1516: train loss: 0.47959789633750916\n",
      "Epoch 1517: train loss: 0.47959449887275696\n",
      "Epoch 1518: train loss: 0.47959116101264954\n",
      "Epoch 1519: train loss: 0.4795878827571869\n",
      "Epoch 1520: train loss: 0.4795845150947571\n",
      "Epoch 1521: train loss: 0.47958117723464966\n",
      "Epoch 1522: train loss: 0.479577898979187\n",
      "Epoch 1523: train loss: 0.4795745313167572\n",
      "Epoch 1524: train loss: 0.4795711934566498\n",
      "Epoch 1525: train loss: 0.4795679450035095\n",
      "Epoch 1526: train loss: 0.4795646071434021\n",
      "Epoch 1527: train loss: 0.47956132888793945\n",
      "Epoch 1528: train loss: 0.47955799102783203\n",
      "Epoch 1529: train loss: 0.479554682970047\n",
      "Epoch 1530: train loss: 0.4795513451099396\n",
      "Epoch 1531: train loss: 0.4795481562614441\n",
      "Epoch 1532: train loss: 0.47954481840133667\n",
      "Epoch 1533: train loss: 0.4795415699481964\n",
      "Epoch 1534: train loss: 0.47953829169273376\n",
      "Epoch 1535: train loss: 0.47953498363494873\n",
      "Epoch 1536: train loss: 0.4795317053794861\n",
      "Epoch 1537: train loss: 0.4795284569263458\n",
      "Epoch 1538: train loss: 0.47952520847320557\n",
      "Epoch 1539: train loss: 0.4795219302177429\n",
      "Epoch 1540: train loss: 0.4795186221599579\n",
      "Epoch 1541: train loss: 0.47951540350914\n",
      "Epoch 1542: train loss: 0.47951215505599976\n",
      "Epoch 1543: train loss: 0.4795089662075043\n",
      "Epoch 1544: train loss: 0.479505717754364\n",
      "Epoch 1545: train loss: 0.47950243949890137\n",
      "Epoch 1546: train loss: 0.4794992506504059\n",
      "Epoch 1547: train loss: 0.4794960618019104\n",
      "Epoch 1548: train loss: 0.47949281334877014\n",
      "Epoch 1549: train loss: 0.47948962450027466\n",
      "Epoch 1550: train loss: 0.4794863760471344\n",
      "Epoch 1551: train loss: 0.4794831871986389\n",
      "Epoch 1552: train loss: 0.47947996854782104\n",
      "Epoch 1553: train loss: 0.47947677969932556\n",
      "Epoch 1554: train loss: 0.4794735908508301\n",
      "Epoch 1555: train loss: 0.4794704020023346\n",
      "Epoch 1556: train loss: 0.4794672131538391\n",
      "Epoch 1557: train loss: 0.47946396470069885\n",
      "Epoch 1558: train loss: 0.47946083545684814\n",
      "Epoch 1559: train loss: 0.47945764660835266\n",
      "Epoch 1560: train loss: 0.4794544577598572\n",
      "Epoch 1561: train loss: 0.47945132851600647\n",
      "Epoch 1562: train loss: 0.479448139667511\n",
      "Epoch 1563: train loss: 0.4794449508190155\n",
      "Epoch 1564: train loss: 0.4794418215751648\n",
      "Epoch 1565: train loss: 0.4794386625289917\n",
      "Epoch 1566: train loss: 0.479435533285141\n",
      "Epoch 1567: train loss: 0.4794323444366455\n",
      "Epoch 1568: train loss: 0.4794292151927948\n",
      "Epoch 1569: train loss: 0.4794260859489441\n",
      "Epoch 1570: train loss: 0.4794229567050934\n",
      "Epoch 1571: train loss: 0.47941985726356506\n",
      "Epoch 1572: train loss: 0.47941672801971436\n",
      "Epoch 1573: train loss: 0.47941359877586365\n",
      "Epoch 1574: train loss: 0.47941046953201294\n",
      "Epoch 1575: train loss: 0.47940731048583984\n",
      "Epoch 1576: train loss: 0.47940418124198914\n",
      "Epoch 1577: train loss: 0.4794011116027832\n",
      "Epoch 1578: train loss: 0.4793980121612549\n",
      "Epoch 1579: train loss: 0.47939494252204895\n",
      "Epoch 1580: train loss: 0.47939184308052063\n",
      "Epoch 1581: train loss: 0.4793887138366699\n",
      "Epoch 1582: train loss: 0.479385644197464\n",
      "Epoch 1583: train loss: 0.47938254475593567\n",
      "Epoch 1584: train loss: 0.47937947511672974\n",
      "Epoch 1585: train loss: 0.4793763756752014\n",
      "Epoch 1586: train loss: 0.4793733060359955\n",
      "Epoch 1587: train loss: 0.47937023639678955\n",
      "Epoch 1588: train loss: 0.479367196559906\n",
      "Epoch 1589: train loss: 0.4793640971183777\n",
      "Epoch 1590: train loss: 0.47936108708381653\n",
      "Epoch 1591: train loss: 0.479358047246933\n",
      "Epoch 1592: train loss: 0.47935497760772705\n",
      "Epoch 1593: train loss: 0.4793519377708435\n",
      "Epoch 1594: train loss: 0.4793488383293152\n",
      "Epoch 1595: train loss: 0.47934582829475403\n",
      "Epoch 1596: train loss: 0.4793427884578705\n",
      "Epoch 1597: train loss: 0.47933974862098694\n",
      "Epoch 1598: train loss: 0.47933676838874817\n",
      "Epoch 1599: train loss: 0.47933369874954224\n",
      "Epoch 1600: train loss: 0.47933071851730347\n",
      "Epoch 1601: train loss: 0.47932761907577515\n",
      "Epoch 1602: train loss: 0.47932466864585876\n",
      "Epoch 1603: train loss: 0.47932168841362\n",
      "Epoch 1604: train loss: 0.4793187081813812\n",
      "Epoch 1605: train loss: 0.4793156683444977\n",
      "Epoch 1606: train loss: 0.47931262850761414\n",
      "Epoch 1607: train loss: 0.47930964827537537\n",
      "Epoch 1608: train loss: 0.4793066382408142\n",
      "Epoch 1609: train loss: 0.4793036878108978\n",
      "Epoch 1610: train loss: 0.4793006181716919\n",
      "Epoch 1611: train loss: 0.4792976975440979\n",
      "Epoch 1612: train loss: 0.47929465770721436\n",
      "Epoch 1613: train loss: 0.47929173707962036\n",
      "Epoch 1614: train loss: 0.4792887568473816\n",
      "Epoch 1615: train loss: 0.4792858362197876\n",
      "Epoch 1616: train loss: 0.47928285598754883\n",
      "Epoch 1617: train loss: 0.47927987575531006\n",
      "Epoch 1618: train loss: 0.47927695512771606\n",
      "Epoch 1619: train loss: 0.4792739748954773\n",
      "Epoch 1620: train loss: 0.4792709946632385\n",
      "Epoch 1621: train loss: 0.4792681038379669\n",
      "Epoch 1622: train loss: 0.47926509380340576\n",
      "Epoch 1623: train loss: 0.4792621433734894\n",
      "Epoch 1624: train loss: 0.47925928235054016\n",
      "Epoch 1625: train loss: 0.4792563021183014\n",
      "Epoch 1626: train loss: 0.4792533814907074\n",
      "Epoch 1627: train loss: 0.47925040125846863\n",
      "Epoch 1628: train loss: 0.479247510433197\n",
      "Epoch 1629: train loss: 0.479244589805603\n",
      "Epoch 1630: train loss: 0.47924166917800903\n",
      "Epoch 1631: train loss: 0.47923874855041504\n",
      "Epoch 1632: train loss: 0.47923585772514343\n",
      "Epoch 1633: train loss: 0.47923293709754944\n",
      "Epoch 1634: train loss: 0.4792300760746002\n",
      "Epoch 1635: train loss: 0.4792271852493286\n",
      "Epoch 1636: train loss: 0.4792242646217346\n",
      "Epoch 1637: train loss: 0.4792214035987854\n",
      "Epoch 1638: train loss: 0.4792185127735138\n",
      "Epoch 1639: train loss: 0.4792156517505646\n",
      "Epoch 1640: train loss: 0.4792127311229706\n",
      "Epoch 1641: train loss: 0.479209840297699\n",
      "Epoch 1642: train loss: 0.47920697927474976\n",
      "Epoch 1643: train loss: 0.4792041480541229\n",
      "Epoch 1644: train loss: 0.4792012870311737\n",
      "Epoch 1645: train loss: 0.4791983962059021\n",
      "Epoch 1646: train loss: 0.4791954755783081\n",
      "Epoch 1647: train loss: 0.4791926443576813\n",
      "Epoch 1648: train loss: 0.47918984293937683\n",
      "Epoch 1649: train loss: 0.47918701171875\n",
      "Epoch 1650: train loss: 0.479184091091156\n",
      "Epoch 1651: train loss: 0.47918131947517395\n",
      "Epoch 1652: train loss: 0.47917842864990234\n",
      "Epoch 1653: train loss: 0.4791756272315979\n",
      "Epoch 1654: train loss: 0.4791727364063263\n",
      "Epoch 1655: train loss: 0.47916996479034424\n",
      "Epoch 1656: train loss: 0.4791671633720398\n",
      "Epoch 1657: train loss: 0.47916433215141296\n",
      "Epoch 1658: train loss: 0.47916150093078613\n",
      "Epoch 1659: train loss: 0.4791586995124817\n",
      "Epoch 1660: train loss: 0.47915586829185486\n",
      "Epoch 1661: train loss: 0.4791530966758728\n",
      "Epoch 1662: train loss: 0.4791502356529236\n",
      "Epoch 1663: train loss: 0.4791474938392639\n",
      "Epoch 1664: train loss: 0.4791446328163147\n",
      "Epoch 1665: train loss: 0.47914186120033264\n",
      "Epoch 1666: train loss: 0.4791390299797058\n",
      "Epoch 1667: train loss: 0.47913631796836853\n",
      "Epoch 1668: train loss: 0.4791334867477417\n",
      "Epoch 1669: train loss: 0.47913074493408203\n",
      "Epoch 1670: train loss: 0.4791279733181\n",
      "Epoch 1671: train loss: 0.47912514209747314\n",
      "Epoch 1672: train loss: 0.47912243008613586\n",
      "Epoch 1673: train loss: 0.47911959886550903\n",
      "Epoch 1674: train loss: 0.47911688685417175\n",
      "Epoch 1675: train loss: 0.4791140556335449\n",
      "Epoch 1676: train loss: 0.47911134362220764\n",
      "Epoch 1677: train loss: 0.47910863161087036\n",
      "Epoch 1678: train loss: 0.4791058599948883\n",
      "Epoch 1679: train loss: 0.479103147983551\n",
      "Epoch 1680: train loss: 0.47910037636756897\n",
      "Epoch 1681: train loss: 0.4790976047515869\n",
      "Epoch 1682: train loss: 0.479094922542572\n",
      "Epoch 1683: train loss: 0.47909218072891235\n",
      "Epoch 1684: train loss: 0.4790894091129303\n",
      "Epoch 1685: train loss: 0.47908666729927063\n",
      "Epoch 1686: train loss: 0.47908395528793335\n",
      "Epoch 1687: train loss: 0.47908124327659607\n",
      "Epoch 1688: train loss: 0.4790785312652588\n",
      "Epoch 1689: train loss: 0.4790758192539215\n",
      "Epoch 1690: train loss: 0.47907310724258423\n",
      "Epoch 1691: train loss: 0.47907042503356934\n",
      "Epoch 1692: train loss: 0.4790676534175873\n",
      "Epoch 1693: train loss: 0.4790650010108948\n",
      "Epoch 1694: train loss: 0.4790623188018799\n",
      "Epoch 1695: train loss: 0.4790596067905426\n",
      "Epoch 1696: train loss: 0.4790569841861725\n",
      "Epoch 1697: train loss: 0.4790542721748352\n",
      "Epoch 1698: train loss: 0.4790515601634979\n",
      "Epoch 1699: train loss: 0.47904884815216064\n",
      "Epoch 1700: train loss: 0.4790462255477905\n",
      "Epoch 1701: train loss: 0.479043573141098\n",
      "Epoch 1702: train loss: 0.47904089093208313\n",
      "Epoch 1703: train loss: 0.47903817892074585\n",
      "Epoch 1704: train loss: 0.47903552651405334\n",
      "Epoch 1705: train loss: 0.4790329039096832\n",
      "Epoch 1706: train loss: 0.4790302515029907\n",
      "Epoch 1707: train loss: 0.47902756929397583\n",
      "Epoch 1708: train loss: 0.4790249168872833\n",
      "Epoch 1709: train loss: 0.4790222942829132\n",
      "Epoch 1710: train loss: 0.4790197014808655\n",
      "Epoch 1711: train loss: 0.47901707887649536\n",
      "Epoch 1712: train loss: 0.47901439666748047\n",
      "Epoch 1713: train loss: 0.4790118634700775\n",
      "Epoch 1714: train loss: 0.4790091812610626\n",
      "Epoch 1715: train loss: 0.4790065288543701\n",
      "Epoch 1716: train loss: 0.4790039658546448\n",
      "Epoch 1717: train loss: 0.4790012836456299\n",
      "Epoch 1718: train loss: 0.47899869084358215\n",
      "Epoch 1719: train loss: 0.47899606823921204\n",
      "Epoch 1720: train loss: 0.47899341583251953\n",
      "Epoch 1721: train loss: 0.4789908528327942\n",
      "Epoch 1722: train loss: 0.47898828983306885\n",
      "Epoch 1723: train loss: 0.47898566722869873\n",
      "Epoch 1724: train loss: 0.4789830446243286\n",
      "Epoch 1725: train loss: 0.4789804518222809\n",
      "Epoch 1726: train loss: 0.47897788882255554\n",
      "Epoch 1727: train loss: 0.4789752662181854\n",
      "Epoch 1728: train loss: 0.4789727032184601\n",
      "Epoch 1729: train loss: 0.47897011041641235\n",
      "Epoch 1730: train loss: 0.478967547416687\n",
      "Epoch 1731: train loss: 0.47896501421928406\n",
      "Epoch 1732: train loss: 0.47896242141723633\n",
      "Epoch 1733: train loss: 0.478959858417511\n",
      "Epoch 1734: train loss: 0.47895729541778564\n",
      "Epoch 1735: train loss: 0.4789546728134155\n",
      "Epoch 1736: train loss: 0.47895216941833496\n",
      "Epoch 1737: train loss: 0.47894954681396484\n",
      "Epoch 1738: train loss: 0.4789470434188843\n",
      "Epoch 1739: train loss: 0.4789445400238037\n",
      "Epoch 1740: train loss: 0.4789419174194336\n",
      "Epoch 1741: train loss: 0.478939414024353\n",
      "Epoch 1742: train loss: 0.4789368510246277\n",
      "Epoch 1743: train loss: 0.47893428802490234\n",
      "Epoch 1744: train loss: 0.47893184423446655\n",
      "Epoch 1745: train loss: 0.478929340839386\n",
      "Epoch 1746: train loss: 0.47892671823501587\n",
      "Epoch 1747: train loss: 0.4789241552352905\n",
      "Epoch 1748: train loss: 0.47892165184020996\n",
      "Epoch 1749: train loss: 0.4789191484451294\n",
      "Epoch 1750: train loss: 0.4789167046546936\n",
      "Epoch 1751: train loss: 0.47891414165496826\n",
      "Epoch 1752: train loss: 0.4789116084575653\n",
      "Epoch 1753: train loss: 0.47890910506248474\n",
      "Epoch 1754: train loss: 0.4789066016674042\n",
      "Epoch 1755: train loss: 0.4789040982723236\n",
      "Epoch 1756: train loss: 0.47890159487724304\n",
      "Epoch 1757: train loss: 0.4788990914821625\n",
      "Epoch 1758: train loss: 0.4788966178894043\n",
      "Epoch 1759: train loss: 0.4788941740989685\n",
      "Epoch 1760: train loss: 0.47889167070388794\n",
      "Epoch 1761: train loss: 0.478889137506485\n",
      "Epoch 1762: train loss: 0.4788866341114044\n",
      "Epoch 1763: train loss: 0.4788842499256134\n",
      "Epoch 1764: train loss: 0.47888171672821045\n",
      "Epoch 1765: train loss: 0.4788792133331299\n",
      "Epoch 1766: train loss: 0.47887682914733887\n",
      "Epoch 1767: train loss: 0.4788743257522583\n",
      "Epoch 1768: train loss: 0.4788718521595001\n",
      "Epoch 1769: train loss: 0.47886940836906433\n",
      "Epoch 1770: train loss: 0.47886693477630615\n",
      "Epoch 1771: train loss: 0.47886449098587036\n",
      "Epoch 1772: train loss: 0.4788620173931122\n",
      "Epoch 1773: train loss: 0.4788595736026764\n",
      "Epoch 1774: train loss: 0.4788571298122406\n",
      "Epoch 1775: train loss: 0.4788546562194824\n",
      "Epoch 1776: train loss: 0.4788522720336914\n",
      "Epoch 1777: train loss: 0.478849858045578\n",
      "Epoch 1778: train loss: 0.47884735465049744\n",
      "Epoch 1779: train loss: 0.4788450002670288\n",
      "Epoch 1780: train loss: 0.47884252667427063\n",
      "Epoch 1781: train loss: 0.4788401424884796\n",
      "Epoch 1782: train loss: 0.47883766889572144\n",
      "Epoch 1783: train loss: 0.47883522510528564\n",
      "Epoch 1784: train loss: 0.47883281111717224\n",
      "Epoch 1785: train loss: 0.4788304269313812\n",
      "Epoch 1786: train loss: 0.4788280725479126\n",
      "Epoch 1787: train loss: 0.4788255989551544\n",
      "Epoch 1788: train loss: 0.47882315516471863\n",
      "Epoch 1789: train loss: 0.47882080078125\n",
      "Epoch 1790: train loss: 0.4788183867931366\n",
      "Epoch 1791: train loss: 0.4788160026073456\n",
      "Epoch 1792: train loss: 0.4788135886192322\n",
      "Epoch 1793: train loss: 0.4788111746311188\n",
      "Epoch 1794: train loss: 0.47880882024765015\n",
      "Epoch 1795: train loss: 0.47880637645721436\n",
      "Epoch 1796: train loss: 0.4788040220737457\n",
      "Epoch 1797: train loss: 0.4788016676902771\n",
      "Epoch 1798: train loss: 0.4787992835044861\n",
      "Epoch 1799: train loss: 0.4787968695163727\n",
      "Epoch 1800: train loss: 0.47879457473754883\n",
      "Epoch 1801: train loss: 0.4787921607494354\n",
      "Epoch 1802: train loss: 0.4787898063659668\n",
      "Epoch 1803: train loss: 0.47878745198249817\n",
      "Epoch 1804: train loss: 0.47878506779670715\n",
      "Epoch 1805: train loss: 0.4787827134132385\n",
      "Epoch 1806: train loss: 0.4787804186344147\n",
      "Epoch 1807: train loss: 0.47877800464630127\n",
      "Epoch 1808: train loss: 0.47877565026283264\n",
      "Epoch 1809: train loss: 0.4787733554840088\n",
      "Epoch 1810: train loss: 0.4787709414958954\n",
      "Epoch 1811: train loss: 0.47876864671707153\n",
      "Epoch 1812: train loss: 0.4787662923336029\n",
      "Epoch 1813: train loss: 0.4787639379501343\n",
      "Epoch 1814: train loss: 0.4787616431713104\n",
      "Epoch 1815: train loss: 0.4787593483924866\n",
      "Epoch 1816: train loss: 0.47875699400901794\n",
      "Epoch 1817: train loss: 0.4787546396255493\n",
      "Epoch 1818: train loss: 0.47875234484672546\n",
      "Epoch 1819: train loss: 0.4787500500679016\n",
      "Epoch 1820: train loss: 0.478747695684433\n",
      "Epoch 1821: train loss: 0.4787454605102539\n",
      "Epoch 1822: train loss: 0.4787431061267853\n",
      "Epoch 1823: train loss: 0.4787408113479614\n",
      "Epoch 1824: train loss: 0.4787384569644928\n",
      "Epoch 1825: train loss: 0.4787362515926361\n",
      "Epoch 1826: train loss: 0.4787338972091675\n",
      "Epoch 1827: train loss: 0.47873160243034363\n",
      "Epoch 1828: train loss: 0.4787293076515198\n",
      "Epoch 1829: train loss: 0.4787270128726959\n",
      "Epoch 1830: train loss: 0.47872474789619446\n",
      "Epoch 1831: train loss: 0.4787224531173706\n",
      "Epoch 1832: train loss: 0.47872015833854675\n",
      "Epoch 1833: train loss: 0.4787178635597229\n",
      "Epoch 1834: train loss: 0.47871559858322144\n",
      "Epoch 1835: train loss: 0.47871336340904236\n",
      "Epoch 1836: train loss: 0.4787110686302185\n",
      "Epoch 1837: train loss: 0.47870880365371704\n",
      "Epoch 1838: train loss: 0.47870656847953796\n",
      "Epoch 1839: train loss: 0.47870421409606934\n",
      "Epoch 1840: train loss: 0.47870197892189026\n",
      "Epoch 1841: train loss: 0.4786997139453888\n",
      "Epoch 1842: train loss: 0.4786974787712097\n",
      "Epoch 1843: train loss: 0.478695273399353\n",
      "Epoch 1844: train loss: 0.4786929786205292\n",
      "Epoch 1845: train loss: 0.4786907732486725\n",
      "Epoch 1846: train loss: 0.47868847846984863\n",
      "Epoch 1847: train loss: 0.47868630290031433\n",
      "Epoch 1848: train loss: 0.4786840081214905\n",
      "Epoch 1849: train loss: 0.478681743144989\n",
      "Epoch 1850: train loss: 0.47867950797080994\n",
      "Epoch 1851: train loss: 0.47867730259895325\n",
      "Epoch 1852: train loss: 0.47867506742477417\n",
      "Epoch 1853: train loss: 0.4786728024482727\n",
      "Epoch 1854: train loss: 0.4786706566810608\n",
      "Epoch 1855: train loss: 0.4786684215068817\n",
      "Epoch 1856: train loss: 0.478666216135025\n",
      "Epoch 1857: train loss: 0.47866398096084595\n",
      "Epoch 1858: train loss: 0.47866183519363403\n",
      "Epoch 1859: train loss: 0.4786595404148102\n",
      "Epoch 1860: train loss: 0.4786573350429535\n",
      "Epoch 1861: train loss: 0.4786551594734192\n",
      "Epoch 1862: train loss: 0.4786529541015625\n",
      "Epoch 1863: train loss: 0.4786507487297058\n",
      "Epoch 1864: train loss: 0.4786485731601715\n",
      "Epoch 1865: train loss: 0.47864630818367004\n",
      "Epoch 1866: train loss: 0.47864416241645813\n",
      "Epoch 1867: train loss: 0.47864198684692383\n",
      "Epoch 1868: train loss: 0.47863978147506714\n",
      "Epoch 1869: train loss: 0.4786376357078552\n",
      "Epoch 1870: train loss: 0.47863540053367615\n",
      "Epoch 1871: train loss: 0.47863319516181946\n",
      "Epoch 1872: train loss: 0.47863101959228516\n",
      "Epoch 1873: train loss: 0.47862890362739563\n",
      "Epoch 1874: train loss: 0.47862672805786133\n",
      "Epoch 1875: train loss: 0.47862452268600464\n",
      "Epoch 1876: train loss: 0.4786223769187927\n",
      "Epoch 1877: train loss: 0.4786202311515808\n",
      "Epoch 1878: train loss: 0.4786180555820465\n",
      "Epoch 1879: train loss: 0.4786159098148346\n",
      "Epoch 1880: train loss: 0.4786137640476227\n",
      "Epoch 1881: train loss: 0.47861161828041077\n",
      "Epoch 1882: train loss: 0.47860947251319885\n",
      "Epoch 1883: train loss: 0.47860729694366455\n",
      "Epoch 1884: train loss: 0.47860515117645264\n",
      "Epoch 1885: train loss: 0.4786030054092407\n",
      "Epoch 1886: train loss: 0.4786008596420288\n",
      "Epoch 1887: train loss: 0.4785987138748169\n",
      "Epoch 1888: train loss: 0.478596568107605\n",
      "Epoch 1889: train loss: 0.47859448194503784\n",
      "Epoch 1890: train loss: 0.4785923361778259\n",
      "Epoch 1891: train loss: 0.47859013080596924\n",
      "Epoch 1892: train loss: 0.4785880446434021\n",
      "Epoch 1893: train loss: 0.47858595848083496\n",
      "Epoch 1894: train loss: 0.47858375310897827\n",
      "Epoch 1895: train loss: 0.4785817265510559\n",
      "Epoch 1896: train loss: 0.47857964038848877\n",
      "Epoch 1897: train loss: 0.4785774350166321\n",
      "Epoch 1898: train loss: 0.47857534885406494\n",
      "Epoch 1899: train loss: 0.4785732626914978\n",
      "Epoch 1900: train loss: 0.47857117652893066\n",
      "Epoch 1901: train loss: 0.47856903076171875\n",
      "Epoch 1902: train loss: 0.4785669445991516\n",
      "Epoch 1903: train loss: 0.4785648584365845\n",
      "Epoch 1904: train loss: 0.47856277227401733\n",
      "Epoch 1905: train loss: 0.4785606563091278\n",
      "Epoch 1906: train loss: 0.47855857014656067\n",
      "Epoch 1907: train loss: 0.47855648398399353\n",
      "Epoch 1908: train loss: 0.4785543978214264\n",
      "Epoch 1909: train loss: 0.47855231165885925\n",
      "Epoch 1910: train loss: 0.4785502254962921\n",
      "Epoch 1911: train loss: 0.478548139333725\n",
      "Epoch 1912: train loss: 0.4785460829734802\n",
      "Epoch 1913: train loss: 0.4785439968109131\n",
      "Epoch 1914: train loss: 0.47854191064834595\n",
      "Epoch 1915: train loss: 0.4785398840904236\n",
      "Epoch 1916: train loss: 0.47853776812553406\n",
      "Epoch 1917: train loss: 0.4785357415676117\n",
      "Epoch 1918: train loss: 0.47853365540504456\n",
      "Epoch 1919: train loss: 0.4785316288471222\n",
      "Epoch 1920: train loss: 0.47852951288223267\n",
      "Epoch 1921: train loss: 0.4785274267196655\n",
      "Epoch 1922: train loss: 0.47852545976638794\n",
      "Epoch 1923: train loss: 0.4785233736038208\n",
      "Epoch 1924: train loss: 0.4785213768482208\n",
      "Epoch 1925: train loss: 0.4785193204879761\n",
      "Epoch 1926: train loss: 0.47851723432540894\n",
      "Epoch 1927: train loss: 0.4785152077674866\n",
      "Epoch 1928: train loss: 0.4785131812095642\n",
      "Epoch 1929: train loss: 0.47851112484931946\n",
      "Epoch 1930: train loss: 0.4785090982913971\n",
      "Epoch 1931: train loss: 0.4785071015357971\n",
      "Epoch 1932: train loss: 0.47850507497787476\n",
      "Epoch 1933: train loss: 0.47850301861763\n",
      "Epoch 1934: train loss: 0.47850099205970764\n",
      "Epoch 1935: train loss: 0.4784989655017853\n",
      "Epoch 1936: train loss: 0.4784969687461853\n",
      "Epoch 1937: train loss: 0.47849494218826294\n",
      "Epoch 1938: train loss: 0.47849294543266296\n",
      "Epoch 1939: train loss: 0.4784909188747406\n",
      "Epoch 1940: train loss: 0.4784889221191406\n",
      "Epoch 1941: train loss: 0.47848689556121826\n",
      "Epoch 1942: train loss: 0.4784848988056183\n",
      "Epoch 1943: train loss: 0.4784829020500183\n",
      "Epoch 1944: train loss: 0.4784809350967407\n",
      "Epoch 1945: train loss: 0.47847887873649597\n",
      "Epoch 1946: train loss: 0.4784769117832184\n",
      "Epoch 1947: train loss: 0.47847485542297363\n",
      "Epoch 1948: train loss: 0.47847291827201843\n",
      "Epoch 1949: train loss: 0.47847089171409607\n",
      "Epoch 1950: train loss: 0.4784688949584961\n",
      "Epoch 1951: train loss: 0.4784669280052185\n",
      "Epoch 1952: train loss: 0.4784649908542633\n",
      "Epoch 1953: train loss: 0.47846299409866333\n",
      "Epoch 1954: train loss: 0.47846102714538574\n",
      "Epoch 1955: train loss: 0.47845903038978577\n",
      "Epoch 1956: train loss: 0.47845709323883057\n",
      "Epoch 1957: train loss: 0.4784550666809082\n",
      "Epoch 1958: train loss: 0.4784531891345978\n",
      "Epoch 1959: train loss: 0.478451132774353\n",
      "Epoch 1960: train loss: 0.4784492552280426\n",
      "Epoch 1961: train loss: 0.478447288274765\n",
      "Epoch 1962: train loss: 0.47844529151916504\n",
      "Epoch 1963: train loss: 0.47844329476356506\n",
      "Epoch 1964: train loss: 0.47844135761260986\n",
      "Epoch 1965: train loss: 0.4784393906593323\n",
      "Epoch 1966: train loss: 0.47843751311302185\n",
      "Epoch 1967: train loss: 0.4784355163574219\n",
      "Epoch 1968: train loss: 0.4784335792064667\n",
      "Epoch 1969: train loss: 0.4784315824508667\n",
      "Epoch 1970: train loss: 0.4784297049045563\n",
      "Epoch 1971: train loss: 0.4784277677536011\n",
      "Epoch 1972: train loss: 0.47842586040496826\n",
      "Epoch 1973: train loss: 0.47842392325401306\n",
      "Epoch 1974: train loss: 0.47842198610305786\n",
      "Epoch 1975: train loss: 0.47842004895210266\n",
      "Epoch 1976: train loss: 0.47841817140579224\n",
      "Epoch 1977: train loss: 0.47841617465019226\n",
      "Epoch 1978: train loss: 0.47841429710388184\n",
      "Epoch 1979: train loss: 0.47841235995292664\n",
      "Epoch 1980: train loss: 0.47841039299964905\n",
      "Epoch 1981: train loss: 0.478408545255661\n",
      "Epoch 1982: train loss: 0.4784065783023834\n",
      "Epoch 1983: train loss: 0.4784047305583954\n",
      "Epoch 1984: train loss: 0.4784027636051178\n",
      "Epoch 1985: train loss: 0.47840091586112976\n",
      "Epoch 1986: train loss: 0.47839897871017456\n",
      "Epoch 1987: train loss: 0.47839710116386414\n",
      "Epoch 1988: train loss: 0.47839513421058655\n",
      "Epoch 1989: train loss: 0.47839322686195374\n",
      "Epoch 1990: train loss: 0.4783913493156433\n",
      "Epoch 1991: train loss: 0.4783894717693329\n",
      "Epoch 1992: train loss: 0.47838759422302246\n",
      "Epoch 1993: train loss: 0.47838571667671204\n",
      "Epoch 1994: train loss: 0.4783838391304016\n",
      "Epoch 1995: train loss: 0.4783819615840912\n",
      "Epoch 1996: train loss: 0.47838008403778076\n",
      "Epoch 1997: train loss: 0.4783782362937927\n",
      "Epoch 1998: train loss: 0.4783762991428375\n",
      "Epoch 1999: train loss: 0.4783744215965271\n",
      "Epoch 2000: train loss: 0.47837260365486145\n",
      "Epoch 2001: train loss: 0.47837066650390625\n",
      "Epoch 2002: train loss: 0.4783688485622406\n",
      "Epoch 2003: train loss: 0.4783669710159302\n",
      "Epoch 2004: train loss: 0.47836506366729736\n",
      "Epoch 2005: train loss: 0.4783633053302765\n",
      "Epoch 2006: train loss: 0.4783613681793213\n",
      "Epoch 2007: train loss: 0.478359580039978\n",
      "Epoch 2008: train loss: 0.4783577024936676\n",
      "Epoch 2009: train loss: 0.4783558249473572\n",
      "Epoch 2010: train loss: 0.47835394740104675\n",
      "Epoch 2011: train loss: 0.4783521592617035\n",
      "Epoch 2012: train loss: 0.47835028171539307\n",
      "Epoch 2013: train loss: 0.4783484637737274\n",
      "Epoch 2014: train loss: 0.478346586227417\n",
      "Epoch 2015: train loss: 0.47834479808807373\n",
      "Epoch 2016: train loss: 0.4783429205417633\n",
      "Epoch 2017: train loss: 0.47834110260009766\n",
      "Epoch 2018: train loss: 0.47833919525146484\n",
      "Epoch 2019: train loss: 0.47833743691444397\n",
      "Epoch 2020: train loss: 0.47833555936813354\n",
      "Epoch 2021: train loss: 0.4783337116241455\n",
      "Epoch 2022: train loss: 0.47833195328712463\n",
      "Epoch 2023: train loss: 0.4783301055431366\n",
      "Epoch 2024: train loss: 0.47832828760147095\n",
      "Epoch 2025: train loss: 0.4783264696598053\n",
      "Epoch 2026: train loss: 0.47832468152046204\n",
      "Epoch 2027: train loss: 0.4783228933811188\n",
      "Epoch 2028: train loss: 0.47832101583480835\n",
      "Epoch 2029: train loss: 0.4783192574977875\n",
      "Epoch 2030: train loss: 0.4783174693584442\n",
      "Epoch 2031: train loss: 0.47831565141677856\n",
      "Epoch 2032: train loss: 0.4783138036727905\n",
      "Epoch 2033: train loss: 0.47831204533576965\n",
      "Epoch 2034: train loss: 0.4783101975917816\n",
      "Epoch 2035: train loss: 0.47830837965011597\n",
      "Epoch 2036: train loss: 0.4783066511154175\n",
      "Epoch 2037: train loss: 0.4783048629760742\n",
      "Epoch 2038: train loss: 0.47830304503440857\n",
      "Epoch 2039: train loss: 0.4783012568950653\n",
      "Epoch 2040: train loss: 0.47829949855804443\n",
      "Epoch 2041: train loss: 0.47829771041870117\n",
      "Epoch 2042: train loss: 0.4782958924770355\n",
      "Epoch 2043: train loss: 0.47829410433769226\n",
      "Epoch 2044: train loss: 0.4782923460006714\n",
      "Epoch 2045: train loss: 0.47829049825668335\n",
      "Epoch 2046: train loss: 0.47828876972198486\n",
      "Epoch 2047: train loss: 0.478287011384964\n",
      "Epoch 2048: train loss: 0.4782852232456207\n",
      "Epoch 2049: train loss: 0.47828349471092224\n",
      "Epoch 2050: train loss: 0.47828173637390137\n",
      "Epoch 2051: train loss: 0.4782799482345581\n",
      "Epoch 2052: train loss: 0.47827816009521484\n",
      "Epoch 2053: train loss: 0.47827640175819397\n",
      "Epoch 2054: train loss: 0.4782746732234955\n",
      "Epoch 2055: train loss: 0.4782728850841522\n",
      "Epoch 2056: train loss: 0.47827115654945374\n",
      "Epoch 2057: train loss: 0.47826939821243286\n",
      "Epoch 2058: train loss: 0.47826772928237915\n",
      "Epoch 2059: train loss: 0.4782659411430359\n",
      "Epoch 2060: train loss: 0.478264182806015\n",
      "Epoch 2061: train loss: 0.47826239466667175\n",
      "Epoch 2062: train loss: 0.47826072573661804\n",
      "Epoch 2063: train loss: 0.4782589375972748\n",
      "Epoch 2064: train loss: 0.4782571792602539\n",
      "Epoch 2065: train loss: 0.4782554805278778\n",
      "Epoch 2066: train loss: 0.4782537817955017\n",
      "Epoch 2067: train loss: 0.47825199365615845\n",
      "Epoch 2068: train loss: 0.47825032472610474\n",
      "Epoch 2069: train loss: 0.4782485365867615\n",
      "Epoch 2070: train loss: 0.478246808052063\n",
      "Epoch 2071: train loss: 0.4782451093196869\n",
      "Epoch 2072: train loss: 0.4782433807849884\n",
      "Epoch 2073: train loss: 0.4782416522502899\n",
      "Epoch 2074: train loss: 0.47823992371559143\n",
      "Epoch 2075: train loss: 0.4782382547855377\n",
      "Epoch 2076: train loss: 0.47823652625083923\n",
      "Epoch 2077: train loss: 0.4782348573207855\n",
      "Epoch 2078: train loss: 0.47823312878608704\n",
      "Epoch 2079: train loss: 0.4782313406467438\n",
      "Epoch 2080: train loss: 0.47822967171669006\n",
      "Epoch 2081: train loss: 0.47822800278663635\n",
      "Epoch 2082: train loss: 0.47822627425193787\n",
      "Epoch 2083: train loss: 0.4782245457172394\n",
      "Epoch 2084: train loss: 0.4782228469848633\n",
      "Epoch 2085: train loss: 0.47822120785713196\n",
      "Epoch 2086: train loss: 0.47821953892707825\n",
      "Epoch 2087: train loss: 0.47821781039237976\n",
      "Epoch 2088: train loss: 0.4782160818576813\n",
      "Epoch 2089: train loss: 0.47821447253227234\n",
      "Epoch 2090: train loss: 0.47821274399757385\n",
      "Epoch 2091: train loss: 0.47821101546287537\n",
      "Epoch 2092: train loss: 0.47820934653282166\n",
      "Epoch 2093: train loss: 0.4782077372074127\n",
      "Epoch 2094: train loss: 0.478206068277359\n",
      "Epoch 2095: train loss: 0.4782043397426605\n",
      "Epoch 2096: train loss: 0.4782026708126068\n",
      "Epoch 2097: train loss: 0.4782010316848755\n",
      "Epoch 2098: train loss: 0.4781993627548218\n",
      "Epoch 2099: train loss: 0.47819769382476807\n",
      "Epoch 2100: train loss: 0.47819602489471436\n",
      "Epoch 2101: train loss: 0.4781944155693054\n",
      "Epoch 2102: train loss: 0.47819268703460693\n",
      "Epoch 2103: train loss: 0.4781910181045532\n",
      "Epoch 2104: train loss: 0.4781893491744995\n",
      "Epoch 2105: train loss: 0.4781876802444458\n",
      "Epoch 2106: train loss: 0.4781860411167145\n",
      "Epoch 2107: train loss: 0.47818437218666077\n",
      "Epoch 2108: train loss: 0.4781827926635742\n",
      "Epoch 2109: train loss: 0.4781811833381653\n",
      "Epoch 2110: train loss: 0.4781795144081116\n",
      "Epoch 2111: train loss: 0.47817784547805786\n",
      "Epoch 2112: train loss: 0.47817617654800415\n",
      "Epoch 2113: train loss: 0.4781745970249176\n",
      "Epoch 2114: train loss: 0.4781729280948639\n",
      "Epoch 2115: train loss: 0.4781712591648102\n",
      "Epoch 2116: train loss: 0.47816967964172363\n",
      "Epoch 2117: train loss: 0.4781680703163147\n",
      "Epoch 2118: train loss: 0.478166401386261\n",
      "Epoch 2119: train loss: 0.47816479206085205\n",
      "Epoch 2120: train loss: 0.4781632125377655\n",
      "Epoch 2121: train loss: 0.4781615436077118\n",
      "Epoch 2122: train loss: 0.47815996408462524\n",
      "Epoch 2123: train loss: 0.47815829515457153\n",
      "Epoch 2124: train loss: 0.478156715631485\n",
      "Epoch 2125: train loss: 0.4781550467014313\n",
      "Epoch 2126: train loss: 0.47815343737602234\n",
      "Epoch 2127: train loss: 0.4781518578529358\n",
      "Epoch 2128: train loss: 0.47815024852752686\n",
      "Epoch 2129: train loss: 0.4781486392021179\n",
      "Epoch 2130: train loss: 0.4781470000743866\n",
      "Epoch 2131: train loss: 0.47814539074897766\n",
      "Epoch 2132: train loss: 0.4781438112258911\n",
      "Epoch 2133: train loss: 0.4781422019004822\n",
      "Epoch 2134: train loss: 0.47814062237739563\n",
      "Epoch 2135: train loss: 0.47813907265663147\n",
      "Epoch 2136: train loss: 0.47813743352890015\n",
      "Epoch 2137: train loss: 0.478135883808136\n",
      "Epoch 2138: train loss: 0.4781342148780823\n",
      "Epoch 2139: train loss: 0.4781326353549957\n",
      "Epoch 2140: train loss: 0.47813108563423157\n",
      "Epoch 2141: train loss: 0.47812944650650024\n",
      "Epoch 2142: train loss: 0.4781278967857361\n",
      "Epoch 2143: train loss: 0.47812631726264954\n",
      "Epoch 2144: train loss: 0.478124737739563\n",
      "Epoch 2145: train loss: 0.47812318801879883\n",
      "Epoch 2146: train loss: 0.4781216084957123\n",
      "Epoch 2147: train loss: 0.4781200587749481\n",
      "Epoch 2148: train loss: 0.4781184792518616\n",
      "Epoch 2149: train loss: 0.47811686992645264\n",
      "Epoch 2150: train loss: 0.47811535000801086\n",
      "Epoch 2151: train loss: 0.4781137704849243\n",
      "Epoch 2152: train loss: 0.47811222076416016\n",
      "Epoch 2153: train loss: 0.4781106412410736\n",
      "Epoch 2154: train loss: 0.47810906171798706\n",
      "Epoch 2155: train loss: 0.4781075119972229\n",
      "Epoch 2156: train loss: 0.47810599207878113\n",
      "Epoch 2157: train loss: 0.4781044125556946\n",
      "Epoch 2158: train loss: 0.4781028628349304\n",
      "Epoch 2159: train loss: 0.47810128331184387\n",
      "Epoch 2160: train loss: 0.4780997633934021\n",
      "Epoch 2161: train loss: 0.47809815406799316\n",
      "Epoch 2162: train loss: 0.4780966341495514\n",
      "Epoch 2163: train loss: 0.47809505462646484\n",
      "Epoch 2164: train loss: 0.47809356451034546\n",
      "Epoch 2165: train loss: 0.4780919849872589\n",
      "Epoch 2166: train loss: 0.47809046506881714\n",
      "Epoch 2167: train loss: 0.478088915348053\n",
      "Epoch 2168: train loss: 0.4780874252319336\n",
      "Epoch 2169: train loss: 0.47808581590652466\n",
      "Epoch 2170: train loss: 0.4780842959880829\n",
      "Epoch 2171: train loss: 0.4780827760696411\n",
      "Epoch 2172: train loss: 0.47808128595352173\n",
      "Epoch 2173: train loss: 0.47807976603507996\n",
      "Epoch 2174: train loss: 0.4780782461166382\n",
      "Epoch 2175: train loss: 0.47807666659355164\n",
      "Epoch 2176: train loss: 0.47807520627975464\n",
      "Epoch 2177: train loss: 0.4780736565589905\n",
      "Epoch 2178: train loss: 0.4780721366405487\n",
      "Epoch 2179: train loss: 0.47807061672210693\n",
      "Epoch 2180: train loss: 0.47806915640830994\n",
      "Epoch 2181: train loss: 0.4780675768852234\n",
      "Epoch 2182: train loss: 0.4780661165714264\n",
      "Epoch 2183: train loss: 0.47806453704833984\n",
      "Epoch 2184: train loss: 0.47806307673454285\n",
      "Epoch 2185: train loss: 0.4780615270137787\n",
      "Epoch 2186: train loss: 0.4780600666999817\n",
      "Epoch 2187: train loss: 0.4780585467815399\n",
      "Epoch 2188: train loss: 0.47805702686309814\n",
      "Epoch 2189: train loss: 0.47805556654930115\n",
      "Epoch 2190: train loss: 0.4780540466308594\n",
      "Epoch 2191: train loss: 0.4780525863170624\n",
      "Epoch 2192: train loss: 0.4780510663986206\n",
      "Epoch 2193: train loss: 0.47804954648017883\n",
      "Epoch 2194: train loss: 0.4780481457710266\n",
      "Epoch 2195: train loss: 0.47804656624794006\n",
      "Epoch 2196: train loss: 0.47804510593414307\n",
      "Epoch 2197: train loss: 0.4780435860157013\n",
      "Epoch 2198: train loss: 0.4780421257019043\n",
      "Epoch 2199: train loss: 0.4780406057834625\n",
      "Epoch 2200: train loss: 0.4780391454696655\n",
      "Epoch 2201: train loss: 0.47803768515586853\n",
      "Epoch 2202: train loss: 0.47803622484207153\n",
      "Epoch 2203: train loss: 0.47803476452827454\n",
      "Epoch 2204: train loss: 0.47803330421447754\n",
      "Epoch 2205: train loss: 0.47803178429603577\n",
      "Epoch 2206: train loss: 0.47803032398223877\n",
      "Epoch 2207: train loss: 0.4780288636684418\n",
      "Epoch 2208: train loss: 0.47802734375\n",
      "Epoch 2209: train loss: 0.4780259430408478\n",
      "Epoch 2210: train loss: 0.47802451252937317\n",
      "Epoch 2211: train loss: 0.4780229926109314\n",
      "Epoch 2212: train loss: 0.478021502494812\n",
      "Epoch 2213: train loss: 0.4780200719833374\n",
      "Epoch 2214: train loss: 0.47801855206489563\n",
      "Epoch 2215: train loss: 0.4780171513557434\n",
      "Epoch 2216: train loss: 0.4780156910419464\n",
      "Epoch 2217: train loss: 0.4780142903327942\n",
      "Epoch 2218: train loss: 0.4780128300189972\n",
      "Epoch 2219: train loss: 0.4780113995075226\n",
      "Epoch 2220: train loss: 0.4780098795890808\n",
      "Epoch 2221: train loss: 0.47800853848457336\n",
      "Epoch 2222: train loss: 0.4780070185661316\n",
      "Epoch 2223: train loss: 0.47800561785697937\n",
      "Epoch 2224: train loss: 0.47800412774086\n",
      "Epoch 2225: train loss: 0.478002667427063\n",
      "Epoch 2226: train loss: 0.47800126671791077\n",
      "Epoch 2227: train loss: 0.47799986600875854\n",
      "Epoch 2228: train loss: 0.47799840569496155\n",
      "Epoch 2229: train loss: 0.47799697518348694\n",
      "Epoch 2230: train loss: 0.47799545526504517\n",
      "Epoch 2231: train loss: 0.4779941141605377\n",
      "Epoch 2232: train loss: 0.4779926538467407\n",
      "Epoch 2233: train loss: 0.4779912233352661\n",
      "Epoch 2234: train loss: 0.4779898226261139\n",
      "Epoch 2235: train loss: 0.4779883623123169\n",
      "Epoch 2236: train loss: 0.47798699140548706\n",
      "Epoch 2237: train loss: 0.47798553109169006\n",
      "Epoch 2238: train loss: 0.47798413038253784\n",
      "Epoch 2239: train loss: 0.4779827296733856\n",
      "Epoch 2240: train loss: 0.47798123955726624\n",
      "Epoch 2241: train loss: 0.4779798984527588\n",
      "Epoch 2242: train loss: 0.477978378534317\n",
      "Epoch 2243: train loss: 0.47797706723213196\n",
      "Epoch 2244: train loss: 0.47797560691833496\n",
      "Epoch 2245: train loss: 0.47797420620918274\n",
      "Epoch 2246: train loss: 0.47797277569770813\n",
      "Epoch 2247: train loss: 0.4779714345932007\n",
      "Epoch 2248: train loss: 0.4779700040817261\n",
      "Epoch 2249: train loss: 0.47796860337257385\n",
      "Epoch 2250: train loss: 0.47796720266342163\n",
      "Epoch 2251: train loss: 0.4779658317565918\n",
      "Epoch 2252: train loss: 0.4779644310474396\n",
      "Epoch 2253: train loss: 0.47796303033828735\n",
      "Epoch 2254: train loss: 0.47796159982681274\n",
      "Epoch 2255: train loss: 0.4779602587223053\n",
      "Epoch 2256: train loss: 0.47795888781547546\n",
      "Epoch 2257: train loss: 0.47795748710632324\n",
      "Epoch 2258: train loss: 0.47795605659484863\n",
      "Epoch 2259: train loss: 0.4779547154903412\n",
      "Epoch 2260: train loss: 0.4779532849788666\n",
      "Epoch 2261: train loss: 0.47795194387435913\n",
      "Epoch 2262: train loss: 0.4779505729675293\n",
      "Epoch 2263: train loss: 0.47794923186302185\n",
      "Epoch 2264: train loss: 0.47794780135154724\n",
      "Epoch 2265: train loss: 0.4779464602470398\n",
      "Epoch 2266: train loss: 0.4779450297355652\n",
      "Epoch 2267: train loss: 0.47794368863105774\n",
      "Epoch 2268: train loss: 0.4779423773288727\n",
      "Epoch 2269: train loss: 0.4779409170150757\n",
      "Epoch 2270: train loss: 0.47793954610824585\n",
      "Epoch 2271: train loss: 0.4779382348060608\n",
      "Epoch 2272: train loss: 0.47793683409690857\n",
      "Epoch 2273: train loss: 0.47793546319007874\n",
      "Epoch 2274: train loss: 0.47793418169021606\n",
      "Epoch 2275: train loss: 0.47793275117874146\n",
      "Epoch 2276: train loss: 0.477931410074234\n",
      "Epoch 2277: train loss: 0.4779300391674042\n",
      "Epoch 2278: train loss: 0.4779287278652191\n",
      "Epoch 2279: train loss: 0.4779273271560669\n",
      "Epoch 2280: train loss: 0.47792595624923706\n",
      "Epoch 2281: train loss: 0.4779246151447296\n",
      "Epoch 2282: train loss: 0.47792330384254456\n",
      "Epoch 2283: train loss: 0.4779219329357147\n",
      "Epoch 2284: train loss: 0.4779205918312073\n",
      "Epoch 2285: train loss: 0.4779192805290222\n",
      "Epoch 2286: train loss: 0.47791796922683716\n",
      "Epoch 2287: train loss: 0.47791656851768494\n",
      "Epoch 2288: train loss: 0.4779152572154999\n",
      "Epoch 2289: train loss: 0.47791388630867004\n",
      "Epoch 2290: train loss: 0.477912575006485\n",
      "Epoch 2291: train loss: 0.47791123390197754\n",
      "Epoch 2292: train loss: 0.4779099225997925\n",
      "Epoch 2293: train loss: 0.47790855169296265\n",
      "Epoch 2294: train loss: 0.4779072701931\n",
      "Epoch 2295: train loss: 0.47790589928627014\n",
      "Epoch 2296: train loss: 0.4779045879840851\n",
      "Epoch 2297: train loss: 0.47790324687957764\n",
      "Epoch 2298: train loss: 0.47790199518203735\n",
      "Epoch 2299: train loss: 0.47790056467056274\n",
      "Epoch 2300: train loss: 0.4778992533683777\n",
      "Epoch 2301: train loss: 0.477897971868515\n",
      "Epoch 2302: train loss: 0.47789669036865234\n",
      "Epoch 2303: train loss: 0.4778953492641449\n",
      "Epoch 2304: train loss: 0.47789403796195984\n",
      "Epoch 2305: train loss: 0.4778927266597748\n",
      "Epoch 2306: train loss: 0.47789138555526733\n",
      "Epoch 2307: train loss: 0.4778900742530823\n",
      "Epoch 2308: train loss: 0.4778887629508972\n",
      "Epoch 2309: train loss: 0.47788745164871216\n",
      "Epoch 2310: train loss: 0.4778861403465271\n",
      "Epoch 2311: train loss: 0.47788482904434204\n",
      "Epoch 2312: train loss: 0.47788354754447937\n",
      "Epoch 2313: train loss: 0.4778822660446167\n",
      "Epoch 2314: train loss: 0.47788092494010925\n",
      "Epoch 2315: train loss: 0.4778796136379242\n",
      "Epoch 2316: train loss: 0.4778783619403839\n",
      "Epoch 2317: train loss: 0.47787705063819885\n",
      "Epoch 2318: train loss: 0.4778757393360138\n",
      "Epoch 2319: train loss: 0.47787442803382874\n",
      "Epoch 2320: train loss: 0.47787317633628845\n",
      "Epoch 2321: train loss: 0.4778718948364258\n",
      "Epoch 2322: train loss: 0.4778705835342407\n",
      "Epoch 2323: train loss: 0.47786927223205566\n",
      "Epoch 2324: train loss: 0.47786808013916016\n",
      "Epoch 2325: train loss: 0.4778667092323303\n",
      "Epoch 2326: train loss: 0.47786545753479004\n",
      "Epoch 2327: train loss: 0.477864146232605\n",
      "Epoch 2328: train loss: 0.4778628945350647\n",
      "Epoch 2329: train loss: 0.47786158323287964\n",
      "Epoch 2330: train loss: 0.47786033153533936\n",
      "Epoch 2331: train loss: 0.4778590202331543\n",
      "Epoch 2332: train loss: 0.477857768535614\n",
      "Epoch 2333: train loss: 0.47785645723342896\n",
      "Epoch 2334: train loss: 0.47785520553588867\n",
      "Epoch 2335: train loss: 0.4778539538383484\n",
      "Epoch 2336: train loss: 0.47785264253616333\n",
      "Epoch 2337: train loss: 0.47785139083862305\n",
      "Epoch 2338: train loss: 0.47785013914108276\n",
      "Epoch 2339: train loss: 0.47784894704818726\n",
      "Epoch 2340: train loss: 0.4778476357460022\n",
      "Epoch 2341: train loss: 0.47784632444381714\n",
      "Epoch 2342: train loss: 0.47784513235092163\n",
      "Epoch 2343: train loss: 0.4778438210487366\n",
      "Epoch 2344: train loss: 0.4778425693511963\n",
      "Epoch 2345: train loss: 0.477841317653656\n",
      "Epoch 2346: train loss: 0.4778400659561157\n",
      "Epoch 2347: train loss: 0.47783881425857544\n",
      "Epoch 2348: train loss: 0.47783759236335754\n",
      "Epoch 2349: train loss: 0.4778362810611725\n",
      "Epoch 2350: train loss: 0.4778350293636322\n",
      "Epoch 2351: train loss: 0.4778337776660919\n",
      "Epoch 2352: train loss: 0.47783252596855164\n",
      "Epoch 2353: train loss: 0.47783127427101135\n",
      "Epoch 2354: train loss: 0.47783002257347107\n",
      "Epoch 2355: train loss: 0.47782883048057556\n",
      "Epoch 2356: train loss: 0.4778275787830353\n",
      "Epoch 2357: train loss: 0.4778263568878174\n",
      "Epoch 2358: train loss: 0.4778250455856323\n",
      "Epoch 2359: train loss: 0.4778238534927368\n",
      "Epoch 2360: train loss: 0.47782260179519653\n",
      "Epoch 2361: train loss: 0.47782135009765625\n",
      "Epoch 2362: train loss: 0.47782015800476074\n",
      "Epoch 2363: train loss: 0.4778188467025757\n",
      "Epoch 2364: train loss: 0.4778176546096802\n",
      "Epoch 2365: train loss: 0.4778164327144623\n",
      "Epoch 2366: train loss: 0.4778152406215668\n",
      "Epoch 2367: train loss: 0.4778139889240265\n",
      "Epoch 2368: train loss: 0.4778127372264862\n",
      "Epoch 2369: train loss: 0.4778115749359131\n",
      "Epoch 2370: train loss: 0.4778103232383728\n",
      "Epoch 2371: train loss: 0.4778090715408325\n",
      "Epoch 2372: train loss: 0.477807879447937\n",
      "Epoch 2373: train loss: 0.47780662775039673\n",
      "Epoch 2374: train loss: 0.47780540585517883\n",
      "Epoch 2375: train loss: 0.4778042137622833\n",
      "Epoch 2376: train loss: 0.47780296206474304\n",
      "Epoch 2377: train loss: 0.4778017997741699\n",
      "Epoch 2378: train loss: 0.47780054807662964\n",
      "Epoch 2379: train loss: 0.47779935598373413\n",
      "Epoch 2380: train loss: 0.47779813408851624\n",
      "Epoch 2381: train loss: 0.4777969419956207\n",
      "Epoch 2382: train loss: 0.4777957499027252\n",
      "Epoch 2383: train loss: 0.4777945280075073\n",
      "Epoch 2384: train loss: 0.4777933359146118\n",
      "Epoch 2385: train loss: 0.47779208421707153\n",
      "Epoch 2386: train loss: 0.477790892124176\n",
      "Epoch 2387: train loss: 0.47778967022895813\n",
      "Epoch 2388: train loss: 0.4777885377407074\n",
      "Epoch 2389: train loss: 0.4777873456478119\n",
      "Epoch 2390: train loss: 0.477786123752594\n",
      "Epoch 2391: train loss: 0.4777848720550537\n",
      "Epoch 2392: train loss: 0.477783739566803\n",
      "Epoch 2393: train loss: 0.4777825176715851\n",
      "Epoch 2394: train loss: 0.47778138518333435\n",
      "Epoch 2395: train loss: 0.47778016328811646\n",
      "Epoch 2396: train loss: 0.47777897119522095\n",
      "Epoch 2397: train loss: 0.47777777910232544\n",
      "Epoch 2398: train loss: 0.47777655720710754\n",
      "Epoch 2399: train loss: 0.47777536511421204\n",
      "Epoch 2400: train loss: 0.4777742624282837\n",
      "Epoch 2401: train loss: 0.4777730703353882\n",
      "Epoch 2402: train loss: 0.4777718782424927\n",
      "Epoch 2403: train loss: 0.4777706563472748\n",
      "Epoch 2404: train loss: 0.47776952385902405\n",
      "Epoch 2405: train loss: 0.4777683615684509\n",
      "Epoch 2406: train loss: 0.47776710987091064\n",
      "Epoch 2407: train loss: 0.4777659475803375\n",
      "Epoch 2408: train loss: 0.4777648150920868\n",
      "Epoch 2409: train loss: 0.4777635931968689\n",
      "Epoch 2410: train loss: 0.4777624011039734\n",
      "Epoch 2411: train loss: 0.47776126861572266\n",
      "Epoch 2412: train loss: 0.47776010632514954\n",
      "Epoch 2413: train loss: 0.4777589440345764\n",
      "Epoch 2414: train loss: 0.4777578115463257\n",
      "Epoch 2415: train loss: 0.4777566194534302\n",
      "Epoch 2416: train loss: 0.47775545716285706\n",
      "Epoch 2417: train loss: 0.47775429487228394\n",
      "Epoch 2418: train loss: 0.4777531623840332\n",
      "Epoch 2419: train loss: 0.4777519404888153\n",
      "Epoch 2420: train loss: 0.4777508080005646\n",
      "Epoch 2421: train loss: 0.47774964570999146\n",
      "Epoch 2422: train loss: 0.4777485132217407\n",
      "Epoch 2423: train loss: 0.4777472913265228\n",
      "Epoch 2424: train loss: 0.4777461588382721\n",
      "Epoch 2425: train loss: 0.47774505615234375\n",
      "Epoch 2426: train loss: 0.47774386405944824\n",
      "Epoch 2427: train loss: 0.4777427017688751\n",
      "Epoch 2428: train loss: 0.4777415990829468\n",
      "Epoch 2429: train loss: 0.47774046659469604\n",
      "Epoch 2430: train loss: 0.47773924469947815\n",
      "Epoch 2431: train loss: 0.4777381718158722\n",
      "Epoch 2432: train loss: 0.4777370095252991\n",
      "Epoch 2433: train loss: 0.47773581743240356\n",
      "Epoch 2434: train loss: 0.4777347147464752\n",
      "Epoch 2435: train loss: 0.4777335524559021\n",
      "Epoch 2436: train loss: 0.47773244976997375\n",
      "Epoch 2437: train loss: 0.477731317281723\n",
      "Epoch 2438: train loss: 0.4777301549911499\n",
      "Epoch 2439: train loss: 0.47772902250289917\n",
      "Epoch 2440: train loss: 0.47772786021232605\n",
      "Epoch 2441: train loss: 0.4777267277240753\n",
      "Epoch 2442: train loss: 0.477725625038147\n",
      "Epoch 2443: train loss: 0.47772446274757385\n",
      "Epoch 2444: train loss: 0.4777233600616455\n",
      "Epoch 2445: train loss: 0.4777222275733948\n",
      "Epoch 2446: train loss: 0.47772112488746643\n",
      "Epoch 2447: train loss: 0.4777199625968933\n",
      "Epoch 2448: train loss: 0.4777188301086426\n",
      "Epoch 2449: train loss: 0.47771772742271423\n",
      "Epoch 2450: train loss: 0.4777166247367859\n",
      "Epoch 2451: train loss: 0.47771549224853516\n",
      "Epoch 2452: train loss: 0.47771432995796204\n",
      "Epoch 2453: train loss: 0.47771328687667847\n",
      "Epoch 2454: train loss: 0.47771212458610535\n",
      "Epoch 2455: train loss: 0.4777110517024994\n",
      "Epoch 2456: train loss: 0.47770994901657104\n",
      "Epoch 2457: train loss: 0.4777087867259979\n",
      "Epoch 2458: train loss: 0.4777076542377472\n",
      "Epoch 2459: train loss: 0.47770658135414124\n",
      "Epoch 2460: train loss: 0.4777054488658905\n",
      "Epoch 2461: train loss: 0.47770440578460693\n",
      "Epoch 2462: train loss: 0.4777032434940338\n",
      "Epoch 2463: train loss: 0.4777021110057831\n",
      "Epoch 2464: train loss: 0.47770100831985474\n",
      "Epoch 2465: train loss: 0.47769996523857117\n",
      "Epoch 2466: train loss: 0.47769880294799805\n",
      "Epoch 2467: train loss: 0.4776977002620697\n",
      "Epoch 2468: train loss: 0.47769665718078613\n",
      "Epoch 2469: train loss: 0.4776955246925354\n",
      "Epoch 2470: train loss: 0.47769442200660706\n",
      "Epoch 2471: train loss: 0.4776933193206787\n",
      "Epoch 2472: train loss: 0.47769227623939514\n",
      "Epoch 2473: train loss: 0.477691113948822\n",
      "Epoch 2474: train loss: 0.47769007086753845\n",
      "Epoch 2475: train loss: 0.4776889383792877\n",
      "Epoch 2476: train loss: 0.47768789529800415\n",
      "Epoch 2477: train loss: 0.47768673300743103\n",
      "Epoch 2478: train loss: 0.47768568992614746\n",
      "Epoch 2479: train loss: 0.4776845872402191\n",
      "Epoch 2480: train loss: 0.47768354415893555\n",
      "Epoch 2481: train loss: 0.4776824414730072\n",
      "Epoch 2482: train loss: 0.47768136858940125\n",
      "Epoch 2483: train loss: 0.4776802659034729\n",
      "Epoch 2484: train loss: 0.47767922282218933\n",
      "Epoch 2485: train loss: 0.47767817974090576\n",
      "Epoch 2486: train loss: 0.4776770770549774\n",
      "Epoch 2487: train loss: 0.4776759743690491\n",
      "Epoch 2488: train loss: 0.4776748716831207\n",
      "Epoch 2489: train loss: 0.4776737689971924\n",
      "Epoch 2490: train loss: 0.4776727259159088\n",
      "Epoch 2491: train loss: 0.47767162322998047\n",
      "Epoch 2492: train loss: 0.4776705801486969\n",
      "Epoch 2493: train loss: 0.47766950726509094\n",
      "Epoch 2494: train loss: 0.477668434381485\n",
      "Epoch 2495: train loss: 0.4776673913002014\n",
      "Epoch 2496: train loss: 0.47766634821891785\n",
      "Epoch 2497: train loss: 0.4776652753353119\n",
      "Epoch 2498: train loss: 0.47766420245170593\n",
      "Epoch 2499: train loss: 0.4776631295681\n",
      "Epoch 2500: train loss: 0.4776620864868164\n",
      "Epoch 2501: train loss: 0.47766101360321045\n",
      "Epoch 2502: train loss: 0.4776598811149597\n",
      "Epoch 2503: train loss: 0.4776588976383209\n",
      "Epoch 2504: train loss: 0.47765785455703735\n",
      "Epoch 2505: train loss: 0.477656751871109\n",
      "Epoch 2506: train loss: 0.47765570878982544\n",
      "Epoch 2507: train loss: 0.47765466570854187\n",
      "Epoch 2508: train loss: 0.4776536226272583\n",
      "Epoch 2509: train loss: 0.47765251994132996\n",
      "Epoch 2510: train loss: 0.4776514768600464\n",
      "Epoch 2511: train loss: 0.4776504337787628\n",
      "Epoch 2512: train loss: 0.47764936089515686\n",
      "Epoch 2513: train loss: 0.4776483178138733\n",
      "Epoch 2514: train loss: 0.47764724493026733\n",
      "Epoch 2515: train loss: 0.47764623165130615\n",
      "Epoch 2516: train loss: 0.4776451885700226\n",
      "Epoch 2517: train loss: 0.477644145488739\n",
      "Epoch 2518: train loss: 0.47764310240745544\n",
      "Epoch 2519: train loss: 0.4776420593261719\n",
      "Epoch 2520: train loss: 0.4776410162448883\n",
      "Epoch 2521: train loss: 0.47763997316360474\n",
      "Epoch 2522: train loss: 0.47763898968696594\n",
      "Epoch 2523: train loss: 0.47763791680336\n",
      "Epoch 2524: train loss: 0.4776369333267212\n",
      "Epoch 2525: train loss: 0.47763583064079285\n",
      "Epoch 2526: train loss: 0.4776347875595093\n",
      "Epoch 2527: train loss: 0.4776337444782257\n",
      "Epoch 2528: train loss: 0.4776327610015869\n",
      "Epoch 2529: train loss: 0.47763171792030334\n",
      "Epoch 2530: train loss: 0.4776306748390198\n",
      "Epoch 2531: train loss: 0.4776296615600586\n",
      "Epoch 2532: train loss: 0.477628618478775\n",
      "Epoch 2533: train loss: 0.47762757539749146\n",
      "Epoch 2534: train loss: 0.47762659192085266\n",
      "Epoch 2535: train loss: 0.4776255488395691\n",
      "Epoch 2536: train loss: 0.4776245057582855\n",
      "Epoch 2537: train loss: 0.47762346267700195\n",
      "Epoch 2538: train loss: 0.47762244939804077\n",
      "Epoch 2539: train loss: 0.477621465921402\n",
      "Epoch 2540: train loss: 0.4776204228401184\n",
      "Epoch 2541: train loss: 0.4776194393634796\n",
      "Epoch 2542: train loss: 0.47761842608451843\n",
      "Epoch 2543: train loss: 0.47761738300323486\n",
      "Epoch 2544: train loss: 0.47761639952659607\n",
      "Epoch 2545: train loss: 0.4776153564453125\n",
      "Epoch 2546: train loss: 0.4776143729686737\n",
      "Epoch 2547: train loss: 0.47761332988739014\n",
      "Epoch 2548: train loss: 0.47761231660842896\n",
      "Epoch 2549: train loss: 0.4776112735271454\n",
      "Epoch 2550: train loss: 0.4776102900505066\n",
      "Epoch 2551: train loss: 0.4776093065738678\n",
      "Epoch 2552: train loss: 0.4776082932949066\n",
      "Epoch 2553: train loss: 0.47760725021362305\n",
      "Epoch 2554: train loss: 0.47760626673698425\n",
      "Epoch 2555: train loss: 0.4776052236557007\n",
      "Epoch 2556: train loss: 0.4776042401790619\n",
      "Epoch 2557: train loss: 0.47760316729545593\n",
      "Epoch 2558: train loss: 0.4776023030281067\n",
      "Epoch 2559: train loss: 0.4776012599468231\n",
      "Epoch 2560: train loss: 0.47760024666786194\n",
      "Epoch 2561: train loss: 0.47759926319122314\n",
      "Epoch 2562: train loss: 0.47759827971458435\n",
      "Epoch 2563: train loss: 0.47759726643562317\n",
      "Epoch 2564: train loss: 0.4775962233543396\n",
      "Epoch 2565: train loss: 0.4775952398777008\n",
      "Epoch 2566: train loss: 0.4775942862033844\n",
      "Epoch 2567: train loss: 0.4775933027267456\n",
      "Epoch 2568: train loss: 0.4775923192501068\n",
      "Epoch 2569: train loss: 0.47759130597114563\n",
      "Epoch 2570: train loss: 0.47759026288986206\n",
      "Epoch 2571: train loss: 0.47758933901786804\n",
      "Epoch 2572: train loss: 0.47758838534355164\n",
      "Epoch 2573: train loss: 0.47758740186691284\n",
      "Epoch 2574: train loss: 0.47758641839027405\n",
      "Epoch 2575: train loss: 0.4775853753089905\n",
      "Epoch 2576: train loss: 0.4775843620300293\n",
      "Epoch 2577: train loss: 0.4775834381580353\n",
      "Epoch 2578: train loss: 0.4775824248790741\n",
      "Epoch 2579: train loss: 0.4775815010070801\n",
      "Epoch 2580: train loss: 0.4775804877281189\n",
      "Epoch 2581: train loss: 0.4775795042514801\n",
      "Epoch 2582: train loss: 0.4775785207748413\n",
      "Epoch 2583: train loss: 0.4775775372982025\n",
      "Epoch 2584: train loss: 0.4775765836238861\n",
      "Epoch 2585: train loss: 0.4775756001472473\n",
      "Epoch 2586: train loss: 0.4775746464729309\n",
      "Epoch 2587: train loss: 0.4775736629962921\n",
      "Epoch 2588: train loss: 0.47757264971733093\n",
      "Epoch 2589: train loss: 0.4775717258453369\n",
      "Epoch 2590: train loss: 0.4775707423686981\n",
      "Epoch 2591: train loss: 0.4775697886943817\n",
      "Epoch 2592: train loss: 0.4775688350200653\n",
      "Epoch 2593: train loss: 0.47756779193878174\n",
      "Epoch 2594: train loss: 0.4775668680667877\n",
      "Epoch 2595: train loss: 0.4775659143924713\n",
      "Epoch 2596: train loss: 0.4775649309158325\n",
      "Epoch 2597: train loss: 0.4775639772415161\n",
      "Epoch 2598: train loss: 0.47756311297416687\n",
      "Epoch 2599: train loss: 0.4775620698928833\n",
      "Epoch 2600: train loss: 0.4775611162185669\n",
      "Epoch 2601: train loss: 0.4775601327419281\n",
      "Epoch 2602: train loss: 0.47755923867225647\n",
      "Epoch 2603: train loss: 0.4775582551956177\n",
      "Epoch 2604: train loss: 0.47755730152130127\n",
      "Epoch 2605: train loss: 0.47755634784698486\n",
      "Epoch 2606: train loss: 0.47755536437034607\n",
      "Epoch 2607: train loss: 0.47755444049835205\n",
      "Epoch 2608: train loss: 0.47755348682403564\n",
      "Epoch 2609: train loss: 0.477552592754364\n",
      "Epoch 2610: train loss: 0.4775516092777252\n",
      "Epoch 2611: train loss: 0.4775506258010864\n",
      "Epoch 2612: train loss: 0.47754967212677\n",
      "Epoch 2613: train loss: 0.4775487780570984\n",
      "Epoch 2614: train loss: 0.47754785418510437\n",
      "Epoch 2615: train loss: 0.4775468111038208\n",
      "Epoch 2616: train loss: 0.47754591703414917\n",
      "Epoch 2617: train loss: 0.47754496335983276\n",
      "Epoch 2618: train loss: 0.47754403948783875\n",
      "Epoch 2619: train loss: 0.47754308581352234\n",
      "Epoch 2620: train loss: 0.47754213213920593\n",
      "Epoch 2621: train loss: 0.4775412082672119\n",
      "Epoch 2622: train loss: 0.4775402545928955\n",
      "Epoch 2623: train loss: 0.4775393307209015\n",
      "Epoch 2624: train loss: 0.4775383770465851\n",
      "Epoch 2625: train loss: 0.47753745317459106\n",
      "Epoch 2626: train loss: 0.47753649950027466\n",
      "Epoch 2627: train loss: 0.47753557562828064\n",
      "Epoch 2628: train loss: 0.477534681558609\n",
      "Epoch 2629: train loss: 0.4775336682796478\n",
      "Epoch 2630: train loss: 0.4775328040122986\n",
      "Epoch 2631: train loss: 0.4775318503379822\n",
      "Epoch 2632: train loss: 0.47753089666366577\n",
      "Epoch 2633: train loss: 0.47753003239631653\n",
      "Epoch 2634: train loss: 0.4775290787220001\n",
      "Epoch 2635: train loss: 0.47752809524536133\n",
      "Epoch 2636: train loss: 0.4775272011756897\n",
      "Epoch 2637: train loss: 0.47752630710601807\n",
      "Epoch 2638: train loss: 0.47752538323402405\n",
      "Epoch 2639: train loss: 0.47752442955970764\n",
      "Epoch 2640: train loss: 0.477523535490036\n",
      "Epoch 2641: train loss: 0.477522611618042\n",
      "Epoch 2642: train loss: 0.4775216579437256\n",
      "Epoch 2643: train loss: 0.47752076387405396\n",
      "Epoch 2644: train loss: 0.4775198698043823\n",
      "Epoch 2645: train loss: 0.4775189459323883\n",
      "Epoch 2646: train loss: 0.4775180518627167\n",
      "Epoch 2647: train loss: 0.4775170683860779\n",
      "Epoch 2648: train loss: 0.47751617431640625\n",
      "Epoch 2649: train loss: 0.47751522064208984\n",
      "Epoch 2650: train loss: 0.4775143563747406\n",
      "Epoch 2651: train loss: 0.47751346230506897\n",
      "Epoch 2652: train loss: 0.4775124788284302\n",
      "Epoch 2653: train loss: 0.47751161456108093\n",
      "Epoch 2654: train loss: 0.4775106906890869\n",
      "Epoch 2655: train loss: 0.4775097966194153\n",
      "Epoch 2656: train loss: 0.47750890254974365\n",
      "Epoch 2657: train loss: 0.47750797867774963\n",
      "Epoch 2658: train loss: 0.477507084608078\n",
      "Epoch 2659: train loss: 0.47750619053840637\n",
      "Epoch 2660: train loss: 0.47750529646873474\n",
      "Epoch 2661: train loss: 0.4775043725967407\n",
      "Epoch 2662: train loss: 0.4775034785270691\n",
      "Epoch 2663: train loss: 0.47750258445739746\n",
      "Epoch 2664: train loss: 0.47750169038772583\n",
      "Epoch 2665: train loss: 0.4775007665157318\n",
      "Epoch 2666: train loss: 0.4774998724460602\n",
      "Epoch 2667: train loss: 0.47749897837638855\n",
      "Epoch 2668: train loss: 0.4774980843067169\n",
      "Epoch 2669: train loss: 0.4774971604347229\n",
      "Epoch 2670: train loss: 0.47749626636505127\n",
      "Epoch 2671: train loss: 0.47749531269073486\n",
      "Epoch 2672: train loss: 0.477494478225708\n",
      "Epoch 2673: train loss: 0.47749361395835876\n",
      "Epoch 2674: train loss: 0.47749266028404236\n",
      "Epoch 2675: train loss: 0.4774918258190155\n",
      "Epoch 2676: train loss: 0.47749093174934387\n",
      "Epoch 2677: train loss: 0.47749000787734985\n",
      "Epoch 2678: train loss: 0.4774891138076782\n",
      "Epoch 2679: train loss: 0.47748827934265137\n",
      "Epoch 2680: train loss: 0.47748738527297974\n",
      "Epoch 2681: train loss: 0.4774864912033081\n",
      "Epoch 2682: train loss: 0.4774855971336365\n",
      "Epoch 2683: train loss: 0.47748470306396484\n",
      "Epoch 2684: train loss: 0.4774837791919708\n",
      "Epoch 2685: train loss: 0.47748294472694397\n",
      "Epoch 2686: train loss: 0.47748205065727234\n",
      "Epoch 2687: train loss: 0.4774811565876007\n",
      "Epoch 2688: train loss: 0.47748032212257385\n",
      "Epoch 2689: train loss: 0.47747939825057983\n",
      "Epoch 2690: train loss: 0.477478563785553\n",
      "Epoch 2691: train loss: 0.47747766971588135\n",
      "Epoch 2692: train loss: 0.4774767756462097\n",
      "Epoch 2693: train loss: 0.4774758815765381\n",
      "Epoch 2694: train loss: 0.47747498750686646\n",
      "Epoch 2695: train loss: 0.4774741530418396\n",
      "Epoch 2696: train loss: 0.47747331857681274\n",
      "Epoch 2697: train loss: 0.4774723947048187\n",
      "Epoch 2698: train loss: 0.47747156023979187\n",
      "Epoch 2699: train loss: 0.47747060656547546\n",
      "Epoch 2700: train loss: 0.4774697721004486\n",
      "Epoch 2701: train loss: 0.47746893763542175\n",
      "Epoch 2702: train loss: 0.4774680435657501\n",
      "Epoch 2703: train loss: 0.47746720910072327\n",
      "Epoch 2704: train loss: 0.47746631503105164\n",
      "Epoch 2705: train loss: 0.4774654805660248\n",
      "Epoch 2706: train loss: 0.47746458649635315\n",
      "Epoch 2707: train loss: 0.4774637222290039\n",
      "Epoch 2708: train loss: 0.47746288776397705\n",
      "Epoch 2709: train loss: 0.4774620234966278\n",
      "Epoch 2710: train loss: 0.4774610996246338\n",
      "Epoch 2711: train loss: 0.47746026515960693\n",
      "Epoch 2712: train loss: 0.4774594306945801\n",
      "Epoch 2713: train loss: 0.4774585962295532\n",
      "Epoch 2714: train loss: 0.47745776176452637\n",
      "Epoch 2715: train loss: 0.47745686769485474\n",
      "Epoch 2716: train loss: 0.4774559736251831\n",
      "Epoch 2717: train loss: 0.4774550795555115\n",
      "Epoch 2718: train loss: 0.4774543046951294\n",
      "Epoch 2719: train loss: 0.47745341062545776\n",
      "Epoch 2720: train loss: 0.4774525761604309\n",
      "Epoch 2721: train loss: 0.4774516820907593\n",
      "Epoch 2722: train loss: 0.47745078802108765\n",
      "Epoch 2723: train loss: 0.47745001316070557\n",
      "Epoch 2724: train loss: 0.47744911909103394\n",
      "Epoch 2725: train loss: 0.4774482846260071\n",
      "Epoch 2726: train loss: 0.47744739055633545\n",
      "Epoch 2727: train loss: 0.4774465560913086\n",
      "Epoch 2728: train loss: 0.4774457812309265\n",
      "Epoch 2729: train loss: 0.4774448871612549\n",
      "Epoch 2730: train loss: 0.477444052696228\n",
      "Epoch 2731: train loss: 0.47744321823120117\n",
      "Epoch 2732: train loss: 0.4774423837661743\n",
      "Epoch 2733: train loss: 0.47744154930114746\n",
      "Epoch 2734: train loss: 0.4774407148361206\n",
      "Epoch 2735: train loss: 0.477439820766449\n",
      "Epoch 2736: train loss: 0.4774389863014221\n",
      "Epoch 2737: train loss: 0.47743815183639526\n",
      "Epoch 2738: train loss: 0.47743725776672363\n",
      "Epoch 2739: train loss: 0.47743648290634155\n",
      "Epoch 2740: train loss: 0.4774356484413147\n",
      "Epoch 2741: train loss: 0.47743481397628784\n",
      "Epoch 2742: train loss: 0.477433979511261\n",
      "Epoch 2743: train loss: 0.47743314504623413\n",
      "Epoch 2744: train loss: 0.4774323105812073\n",
      "Epoch 2745: train loss: 0.4774315059185028\n",
      "Epoch 2746: train loss: 0.47743064165115356\n",
      "Epoch 2747: train loss: 0.4774298071861267\n",
      "Epoch 2748: train loss: 0.47742900252342224\n",
      "Epoch 2749: train loss: 0.4774281680583954\n",
      "Epoch 2750: train loss: 0.47742733359336853\n",
      "Epoch 2751: train loss: 0.4774264991283417\n",
      "Epoch 2752: train loss: 0.4774256646633148\n",
      "Epoch 2753: train loss: 0.47742483019828796\n",
      "Epoch 2754: train loss: 0.47742408514022827\n",
      "Epoch 2755: train loss: 0.47742322087287903\n",
      "Epoch 2756: train loss: 0.47742241621017456\n",
      "Epoch 2757: train loss: 0.4774215519428253\n",
      "Epoch 2758: train loss: 0.4774206876754761\n",
      "Epoch 2759: train loss: 0.477419912815094\n",
      "Epoch 2760: train loss: 0.47741907835006714\n",
      "Epoch 2761: train loss: 0.4774182438850403\n",
      "Epoch 2762: train loss: 0.4774174094200134\n",
      "Epoch 2763: train loss: 0.47741663455963135\n",
      "Epoch 2764: train loss: 0.4774158298969269\n",
      "Epoch 2765: train loss: 0.47741496562957764\n",
      "Epoch 2766: train loss: 0.47741416096687317\n",
      "Epoch 2767: train loss: 0.4774133265018463\n",
      "Epoch 2768: train loss: 0.47741255164146423\n",
      "Epoch 2769: train loss: 0.47741177678108215\n",
      "Epoch 2770: train loss: 0.4774109423160553\n",
      "Epoch 2771: train loss: 0.47741013765335083\n",
      "Epoch 2772: train loss: 0.4774092435836792\n",
      "Epoch 2773: train loss: 0.4774085283279419\n",
      "Epoch 2774: train loss: 0.47740769386291504\n",
      "Epoch 2775: train loss: 0.4774068593978882\n",
      "Epoch 2776: train loss: 0.4774061143398285\n",
      "Epoch 2777: train loss: 0.47740527987480164\n",
      "Epoch 2778: train loss: 0.4774044454097748\n",
      "Epoch 2779: train loss: 0.4774036109447479\n",
      "Epoch 2780: train loss: 0.47740283608436584\n",
      "Epoch 2781: train loss: 0.477402001619339\n",
      "Epoch 2782: train loss: 0.4774011969566345\n",
      "Epoch 2783: train loss: 0.47740042209625244\n",
      "Epoch 2784: train loss: 0.47739964723587036\n",
      "Epoch 2785: train loss: 0.4773988127708435\n",
      "Epoch 2786: train loss: 0.4773980379104614\n",
      "Epoch 2787: train loss: 0.4773971736431122\n",
      "Epoch 2788: train loss: 0.4773963987827301\n",
      "Epoch 2789: train loss: 0.477395623922348\n",
      "Epoch 2790: train loss: 0.47739484906196594\n",
      "Epoch 2791: train loss: 0.4773939847946167\n",
      "Epoch 2792: train loss: 0.4773932099342346\n",
      "Epoch 2793: train loss: 0.47739243507385254\n",
      "Epoch 2794: train loss: 0.47739166021347046\n",
      "Epoch 2795: train loss: 0.477390855550766\n",
      "Epoch 2796: train loss: 0.4773900806903839\n",
      "Epoch 2797: train loss: 0.47738924622535706\n",
      "Epoch 2798: train loss: 0.477388471364975\n",
      "Epoch 2799: train loss: 0.4773876667022705\n",
      "Epoch 2800: train loss: 0.4773868918418884\n",
      "Epoch 2801: train loss: 0.47738611698150635\n",
      "Epoch 2802: train loss: 0.4773853123188019\n",
      "Epoch 2803: train loss: 0.477384477853775\n",
      "Epoch 2804: train loss: 0.4773837625980377\n",
      "Epoch 2805: train loss: 0.47738292813301086\n",
      "Epoch 2806: train loss: 0.47738218307495117\n",
      "Epoch 2807: train loss: 0.4773813486099243\n",
      "Epoch 2808: train loss: 0.47738057374954224\n",
      "Epoch 2809: train loss: 0.47737979888916016\n",
      "Epoch 2810: train loss: 0.47737905383110046\n",
      "Epoch 2811: train loss: 0.4773782193660736\n",
      "Epoch 2812: train loss: 0.4773774743080139\n",
      "Epoch 2813: train loss: 0.47737663984298706\n",
      "Epoch 2814: train loss: 0.47737592458724976\n",
      "Epoch 2815: train loss: 0.4773751497268677\n",
      "Epoch 2816: train loss: 0.47737428545951843\n",
      "Epoch 2817: train loss: 0.47737357020378113\n",
      "Epoch 2818: train loss: 0.47737279534339905\n",
      "Epoch 2819: train loss: 0.4773719906806946\n",
      "Epoch 2820: train loss: 0.4773712158203125\n",
      "Epoch 2821: train loss: 0.4773704409599304\n",
      "Epoch 2822: train loss: 0.4773696959018707\n",
      "Epoch 2823: train loss: 0.47736886143684387\n",
      "Epoch 2824: train loss: 0.47736814618110657\n",
      "Epoch 2825: train loss: 0.4773673415184021\n",
      "Epoch 2826: train loss: 0.47736656665802\n",
      "Epoch 2827: train loss: 0.4773658215999603\n",
      "Epoch 2828: train loss: 0.47736504673957825\n",
      "Epoch 2829: train loss: 0.47736427187919617\n",
      "Epoch 2830: train loss: 0.4773635268211365\n",
      "Epoch 2831: train loss: 0.4773626923561096\n",
      "Epoch 2832: train loss: 0.4773619771003723\n",
      "Epoch 2833: train loss: 0.47736117243766785\n",
      "Epoch 2834: train loss: 0.47736045718193054\n",
      "Epoch 2835: train loss: 0.4773596227169037\n",
      "Epoch 2836: train loss: 0.477358877658844\n",
      "Epoch 2837: train loss: 0.4773581624031067\n",
      "Epoch 2838: train loss: 0.4773573577404022\n",
      "Epoch 2839: train loss: 0.4773566424846649\n",
      "Epoch 2840: train loss: 0.47735583782196045\n",
      "Epoch 2841: train loss: 0.47735512256622314\n",
      "Epoch 2842: train loss: 0.47735434770584106\n",
      "Epoch 2843: train loss: 0.4773535430431366\n",
      "Epoch 2844: train loss: 0.4773528277873993\n",
      "Epoch 2845: train loss: 0.4773520231246948\n",
      "Epoch 2846: train loss: 0.47735124826431274\n",
      "Epoch 2847: train loss: 0.47735053300857544\n",
      "Epoch 2848: train loss: 0.47734972834587097\n",
      "Epoch 2849: train loss: 0.47734907269477844\n",
      "Epoch 2850: train loss: 0.4773482084274292\n",
      "Epoch 2851: train loss: 0.4773474931716919\n",
      "Epoch 2852: train loss: 0.4773467481136322\n",
      "Epoch 2853: train loss: 0.4773460328578949\n",
      "Epoch 2854: train loss: 0.4773452579975128\n",
      "Epoch 2855: train loss: 0.4773445129394531\n",
      "Epoch 2856: train loss: 0.47734373807907104\n",
      "Epoch 2857: train loss: 0.47734299302101135\n",
      "Epoch 2858: train loss: 0.47734227776527405\n",
      "Epoch 2859: train loss: 0.47734153270721436\n",
      "Epoch 2860: train loss: 0.4773407578468323\n",
      "Epoch 2861: train loss: 0.4773399531841278\n",
      "Epoch 2862: train loss: 0.4773392379283905\n",
      "Epoch 2863: train loss: 0.4773384928703308\n",
      "Epoch 2864: train loss: 0.4773377776145935\n",
      "Epoch 2865: train loss: 0.4773370027542114\n",
      "Epoch 2866: train loss: 0.47733625769615173\n",
      "Epoch 2867: train loss: 0.47733551263809204\n",
      "Epoch 2868: train loss: 0.47733479738235474\n",
      "Epoch 2869: train loss: 0.47733405232429504\n",
      "Epoch 2870: train loss: 0.47733327746391296\n",
      "Epoch 2871: train loss: 0.47733253240585327\n",
      "Epoch 2872: train loss: 0.47733181715011597\n",
      "Epoch 2873: train loss: 0.4773310720920563\n",
      "Epoch 2874: train loss: 0.47733035683631897\n",
      "Epoch 2875: train loss: 0.4773296117782593\n",
      "Epoch 2876: train loss: 0.477328896522522\n",
      "Epoch 2877: train loss: 0.4773281514644623\n",
      "Epoch 2878: train loss: 0.4773273766040802\n",
      "Epoch 2879: train loss: 0.4773266017436981\n",
      "Epoch 2880: train loss: 0.477325975894928\n",
      "Epoch 2881: train loss: 0.4773251712322235\n",
      "Epoch 2882: train loss: 0.4773244559764862\n",
      "Epoch 2883: train loss: 0.47732365131378174\n",
      "Epoch 2884: train loss: 0.4773229956626892\n",
      "Epoch 2885: train loss: 0.4773223102092743\n",
      "Epoch 2886: train loss: 0.47732147574424744\n",
      "Epoch 2887: train loss: 0.47732073068618774\n",
      "Epoch 2888: train loss: 0.47732001543045044\n",
      "Epoch 2889: train loss: 0.4773193299770355\n",
      "Epoch 2890: train loss: 0.47731858491897583\n",
      "Epoch 2891: train loss: 0.47731781005859375\n",
      "Epoch 2892: train loss: 0.47731712460517883\n",
      "Epoch 2893: train loss: 0.47731640934944153\n",
      "Epoch 2894: train loss: 0.47731566429138184\n",
      "Epoch 2895: train loss: 0.47731494903564453\n",
      "Epoch 2896: train loss: 0.47731420397758484\n",
      "Epoch 2897: train loss: 0.4773135185241699\n",
      "Epoch 2898: train loss: 0.4773128032684326\n",
      "Epoch 2899: train loss: 0.4773120582103729\n",
      "Epoch 2900: train loss: 0.4773113429546356\n",
      "Epoch 2901: train loss: 0.4773105978965759\n",
      "Epoch 2902: train loss: 0.47730985283851624\n",
      "Epoch 2903: train loss: 0.4773091971874237\n",
      "Epoch 2904: train loss: 0.477308452129364\n",
      "Epoch 2905: train loss: 0.47730767726898193\n",
      "Epoch 2906: train loss: 0.4773070514202118\n",
      "Epoch 2907: train loss: 0.4773063063621521\n",
      "Epoch 2908: train loss: 0.4773055911064148\n",
      "Epoch 2909: train loss: 0.4773048460483551\n",
      "Epoch 2910: train loss: 0.4773041307926178\n",
      "Epoch 2911: train loss: 0.4773033857345581\n",
      "Epoch 2912: train loss: 0.4773027002811432\n",
      "Epoch 2913: train loss: 0.4773019850254059\n",
      "Epoch 2914: train loss: 0.47730129957199097\n",
      "Epoch 2915: train loss: 0.4773005545139313\n",
      "Epoch 2916: train loss: 0.47729983925819397\n",
      "Epoch 2917: train loss: 0.47729915380477905\n",
      "Epoch 2918: train loss: 0.477298378944397\n",
      "Epoch 2919: train loss: 0.47729769349098206\n",
      "Epoch 2920: train loss: 0.47729700803756714\n",
      "Epoch 2921: train loss: 0.47729629278182983\n",
      "Epoch 2922: train loss: 0.47729554772377014\n",
      "Epoch 2923: train loss: 0.47729480266571045\n",
      "Epoch 2924: train loss: 0.4772941470146179\n",
      "Epoch 2925: train loss: 0.477293461561203\n",
      "Epoch 2926: train loss: 0.4772927165031433\n",
      "Epoch 2927: train loss: 0.477292001247406\n",
      "Epoch 2928: train loss: 0.4772913157939911\n",
      "Epoch 2929: train loss: 0.47729063034057617\n",
      "Epoch 2930: train loss: 0.47728991508483887\n",
      "Epoch 2931: train loss: 0.47728922963142395\n",
      "Epoch 2932: train loss: 0.47728848457336426\n",
      "Epoch 2933: train loss: 0.47728776931762695\n",
      "Epoch 2934: train loss: 0.47728708386421204\n",
      "Epoch 2935: train loss: 0.4772863984107971\n",
      "Epoch 2936: train loss: 0.4772856831550598\n",
      "Epoch 2937: train loss: 0.4772849977016449\n",
      "Epoch 2938: train loss: 0.47728431224823\n",
      "Epoch 2939: train loss: 0.4772835969924927\n",
      "Epoch 2940: train loss: 0.47728291153907776\n",
      "Epoch 2941: train loss: 0.47728222608566284\n",
      "Epoch 2942: train loss: 0.47728148102760315\n",
      "Epoch 2943: train loss: 0.4772808253765106\n",
      "Epoch 2944: train loss: 0.4772800803184509\n",
      "Epoch 2945: train loss: 0.477279394865036\n",
      "Epoch 2946: train loss: 0.4772787094116211\n",
      "Epoch 2947: train loss: 0.47727805376052856\n",
      "Epoch 2948: train loss: 0.47727736830711365\n",
      "Epoch 2949: train loss: 0.47727662324905396\n",
      "Epoch 2950: train loss: 0.4772759675979614\n",
      "Epoch 2951: train loss: 0.47727522253990173\n",
      "Epoch 2952: train loss: 0.4772745370864868\n",
      "Epoch 2953: train loss: 0.4772738218307495\n",
      "Epoch 2954: train loss: 0.47727319598197937\n",
      "Epoch 2955: train loss: 0.47727251052856445\n",
      "Epoch 2956: train loss: 0.47727176547050476\n",
      "Epoch 2957: train loss: 0.47727108001708984\n",
      "Epoch 2958: train loss: 0.47727036476135254\n",
      "Epoch 2959: train loss: 0.4772696793079376\n",
      "Epoch 2960: train loss: 0.4772689938545227\n",
      "Epoch 2961: train loss: 0.4772683382034302\n",
      "Epoch 2962: train loss: 0.47726765275001526\n",
      "Epoch 2963: train loss: 0.47726690769195557\n",
      "Epoch 2964: train loss: 0.4772662818431854\n",
      "Epoch 2965: train loss: 0.4772655963897705\n",
      "Epoch 2966: train loss: 0.477264940738678\n",
      "Epoch 2967: train loss: 0.4772641956806183\n",
      "Epoch 2968: train loss: 0.47726351022720337\n",
      "Epoch 2969: train loss: 0.47726282477378845\n",
      "Epoch 2970: train loss: 0.4772621691226959\n",
      "Epoch 2971: train loss: 0.477261483669281\n",
      "Epoch 2972: train loss: 0.47726085782051086\n",
      "Epoch 2973: train loss: 0.47726011276245117\n",
      "Epoch 2974: train loss: 0.47725948691368103\n",
      "Epoch 2975: train loss: 0.4772587716579437\n",
      "Epoch 2976: train loss: 0.4772580862045288\n",
      "Epoch 2977: train loss: 0.47725746035575867\n",
      "Epoch 2978: train loss: 0.477256715297699\n",
      "Epoch 2979: train loss: 0.47725602984428406\n",
      "Epoch 2980: train loss: 0.47725537419319153\n",
      "Epoch 2981: train loss: 0.4772546887397766\n",
      "Epoch 2982: train loss: 0.4772540032863617\n",
      "Epoch 2983: train loss: 0.4772533178329468\n",
      "Epoch 2984: train loss: 0.47725263237953186\n",
      "Epoch 2985: train loss: 0.4772520065307617\n",
      "Epoch 2986: train loss: 0.4772512912750244\n",
      "Epoch 2987: train loss: 0.4772506654262543\n",
      "Epoch 2988: train loss: 0.47724997997283936\n",
      "Epoch 2989: train loss: 0.4772493541240692\n",
      "Epoch 2990: train loss: 0.4772486686706543\n",
      "Epoch 2991: train loss: 0.4772479832172394\n",
      "Epoch 2992: train loss: 0.47724732756614685\n",
      "Epoch 2993: train loss: 0.47724664211273193\n",
      "Epoch 2994: train loss: 0.47724589705467224\n",
      "Epoch 2995: train loss: 0.4772452712059021\n",
      "Epoch 2996: train loss: 0.4772445857524872\n",
      "Epoch 2997: train loss: 0.47724393010139465\n",
      "Epoch 2998: train loss: 0.4772433042526245\n",
      "Epoch 2999: train loss: 0.4772426187992096\n",
      "Epoch 3000: train loss: 0.4772419333457947\n",
      "Epoch 3001: train loss: 0.47724130749702454\n",
      "Epoch 3002: train loss: 0.47724056243896484\n",
      "Epoch 3003: train loss: 0.4772399365901947\n",
      "Epoch 3004: train loss: 0.47723931074142456\n",
      "Epoch 3005: train loss: 0.47723865509033203\n",
      "Epoch 3006: train loss: 0.4772379696369171\n",
      "Epoch 3007: train loss: 0.4772372245788574\n",
      "Epoch 3008: train loss: 0.4772365987300873\n",
      "Epoch 3009: train loss: 0.47723597288131714\n",
      "Epoch 3010: train loss: 0.4772352874279022\n",
      "Epoch 3011: train loss: 0.4772346317768097\n",
      "Epoch 3012: train loss: 0.47723403573036194\n",
      "Epoch 3013: train loss: 0.477233350276947\n",
      "Epoch 3014: train loss: 0.4772326350212097\n",
      "Epoch 3015: train loss: 0.4772320091724396\n",
      "Epoch 3016: train loss: 0.47723132371902466\n",
      "Epoch 3017: train loss: 0.4772306978702545\n",
      "Epoch 3018: train loss: 0.4772300720214844\n",
      "Epoch 3019: train loss: 0.47722938656806946\n",
      "Epoch 3020: train loss: 0.4772287607192993\n",
      "Epoch 3021: train loss: 0.477228045463562\n",
      "Epoch 3022: train loss: 0.47722741961479187\n",
      "Epoch 3023: train loss: 0.47722679376602173\n",
      "Epoch 3024: train loss: 0.4772261381149292\n",
      "Epoch 3025: train loss: 0.4772254228591919\n",
      "Epoch 3026: train loss: 0.47722479701042175\n",
      "Epoch 3027: train loss: 0.4772241711616516\n",
      "Epoch 3028: train loss: 0.4772234857082367\n",
      "Epoch 3029: train loss: 0.47722285985946655\n",
      "Epoch 3030: train loss: 0.47722217440605164\n",
      "Epoch 3031: train loss: 0.4772215485572815\n",
      "Epoch 3032: train loss: 0.47722092270851135\n",
      "Epoch 3033: train loss: 0.47722023725509644\n",
      "Epoch 3034: train loss: 0.4772196114063263\n",
      "Epoch 3035: train loss: 0.47721895575523376\n",
      "Epoch 3036: train loss: 0.47721830010414124\n",
      "Epoch 3037: train loss: 0.4772176444530487\n",
      "Epoch 3038: train loss: 0.4772169589996338\n",
      "Epoch 3039: train loss: 0.4772163927555084\n",
      "Epoch 3040: train loss: 0.47721564769744873\n",
      "Epoch 3041: train loss: 0.47721514105796814\n",
      "Epoch 3042: train loss: 0.4772144556045532\n",
      "Epoch 3043: train loss: 0.4772138297557831\n",
      "Epoch 3044: train loss: 0.47721314430236816\n",
      "Epoch 3045: train loss: 0.47721245884895325\n",
      "Epoch 3046: train loss: 0.4772118330001831\n",
      "Epoch 3047: train loss: 0.47721120715141296\n",
      "Epoch 3048: train loss: 0.4772105813026428\n",
      "Epoch 3049: train loss: 0.4772098958492279\n",
      "Epoch 3050: train loss: 0.47720927000045776\n",
      "Epoch 3051: train loss: 0.4772086441516876\n",
      "Epoch 3052: train loss: 0.4772080183029175\n",
      "Epoch 3053: train loss: 0.47720733284950256\n",
      "Epoch 3054: train loss: 0.4772067070007324\n",
      "Epoch 3055: train loss: 0.4772060811519623\n",
      "Epoch 3056: train loss: 0.47720545530319214\n",
      "Epoch 3057: train loss: 0.477204829454422\n",
      "Epoch 3058: train loss: 0.47720420360565186\n",
      "Epoch 3059: train loss: 0.47720351815223694\n",
      "Epoch 3060: train loss: 0.4772028923034668\n",
      "Epoch 3061: train loss: 0.47720226645469666\n",
      "Epoch 3062: train loss: 0.4772016406059265\n",
      "Epoch 3063: train loss: 0.4772009551525116\n",
      "Epoch 3064: train loss: 0.47720032930374146\n",
      "Epoch 3065: train loss: 0.4771997034549713\n",
      "Epoch 3066: train loss: 0.47719913721084595\n",
      "Epoch 3067: train loss: 0.47719845175743103\n",
      "Epoch 3068: train loss: 0.4771977663040161\n",
      "Epoch 3069: train loss: 0.47719714045524597\n",
      "Epoch 3070: train loss: 0.47719651460647583\n",
      "Epoch 3071: train loss: 0.47719594836235046\n",
      "Epoch 3072: train loss: 0.4771953225135803\n",
      "Epoch 3073: train loss: 0.4771946370601654\n",
      "Epoch 3074: train loss: 0.47719407081604004\n",
      "Epoch 3075: train loss: 0.4771933853626251\n",
      "Epoch 3076: train loss: 0.477192759513855\n",
      "Epoch 3077: train loss: 0.47719213366508484\n",
      "Epoch 3078: train loss: 0.4771915078163147\n",
      "Epoch 3079: train loss: 0.47719088196754456\n",
      "Epoch 3080: train loss: 0.4771903157234192\n",
      "Epoch 3081: train loss: 0.4771896302700043\n",
      "Epoch 3082: train loss: 0.47718900442123413\n",
      "Epoch 3083: train loss: 0.47718843817710876\n",
      "Epoch 3084: train loss: 0.4771876931190491\n",
      "Epoch 3085: train loss: 0.4771871268749237\n",
      "Epoch 3086: train loss: 0.47718653082847595\n",
      "Epoch 3087: train loss: 0.4771858751773834\n",
      "Epoch 3088: train loss: 0.47718527913093567\n",
      "Epoch 3089: train loss: 0.4771846532821655\n",
      "Epoch 3090: train loss: 0.4771840274333954\n",
      "Epoch 3091: train loss: 0.47718340158462524\n",
      "Epoch 3092: train loss: 0.4771827757358551\n",
      "Epoch 3093: train loss: 0.47718220949172974\n",
      "Epoch 3094: train loss: 0.4771815836429596\n",
      "Epoch 3095: train loss: 0.4771808981895447\n",
      "Epoch 3096: train loss: 0.4771803319454193\n",
      "Epoch 3097: train loss: 0.4771796464920044\n",
      "Epoch 3098: train loss: 0.47717902064323425\n",
      "Epoch 3099: train loss: 0.4771784543991089\n",
      "Epoch 3100: train loss: 0.47717782855033875\n",
      "Epoch 3101: train loss: 0.477177232503891\n",
      "Epoch 3102: train loss: 0.47717660665512085\n",
      "Epoch 3103: train loss: 0.4771759808063507\n",
      "Epoch 3104: train loss: 0.47717535495758057\n",
      "Epoch 3105: train loss: 0.4771747887134552\n",
      "Epoch 3106: train loss: 0.4771741032600403\n",
      "Epoch 3107: train loss: 0.4771735966205597\n",
      "Epoch 3108: train loss: 0.4771729111671448\n",
      "Epoch 3109: train loss: 0.477172315120697\n",
      "Epoch 3110: train loss: 0.4771716892719269\n",
      "Epoch 3111: train loss: 0.47717106342315674\n",
      "Epoch 3112: train loss: 0.4771704375743866\n",
      "Epoch 3113: train loss: 0.47716987133026123\n",
      "Epoch 3114: train loss: 0.4771692454814911\n",
      "Epoch 3115: train loss: 0.4771686792373657\n",
      "Epoch 3116: train loss: 0.4771679937839508\n",
      "Epoch 3117: train loss: 0.47716742753982544\n",
      "Epoch 3118: train loss: 0.4771668016910553\n",
      "Epoch 3119: train loss: 0.47716617584228516\n",
      "Epoch 3120: train loss: 0.477165549993515\n",
      "Epoch 3121: train loss: 0.47716495394706726\n",
      "Epoch 3122: train loss: 0.4771643877029419\n",
      "Epoch 3123: train loss: 0.477163702249527\n",
      "Epoch 3124: train loss: 0.4771631360054016\n",
      "Epoch 3125: train loss: 0.47716251015663147\n",
      "Epoch 3126: train loss: 0.4771619439125061\n",
      "Epoch 3127: train loss: 0.47716134786605835\n",
      "Epoch 3128: train loss: 0.4771607220172882\n",
      "Epoch 3129: train loss: 0.47716009616851807\n",
      "Epoch 3130: train loss: 0.4771594703197479\n",
      "Epoch 3131: train loss: 0.47715890407562256\n",
      "Epoch 3132: train loss: 0.4771583378314972\n",
      "Epoch 3133: train loss: 0.47715771198272705\n",
      "Epoch 3134: train loss: 0.4771570563316345\n",
      "Epoch 3135: train loss: 0.47715649008750916\n",
      "Epoch 3136: train loss: 0.477155864238739\n",
      "Epoch 3137: train loss: 0.47715529799461365\n",
      "Epoch 3138: train loss: 0.4771546721458435\n",
      "Epoch 3139: train loss: 0.4771541357040405\n",
      "Epoch 3140: train loss: 0.4771535098552704\n",
      "Epoch 3141: train loss: 0.47715288400650024\n",
      "Epoch 3142: train loss: 0.4771522581577301\n",
      "Epoch 3143: train loss: 0.47715169191360474\n",
      "Epoch 3144: train loss: 0.4771510660648346\n",
      "Epoch 3145: train loss: 0.47715049982070923\n",
      "Epoch 3146: train loss: 0.4771498441696167\n",
      "Epoch 3147: train loss: 0.47714927792549133\n",
      "Epoch 3148: train loss: 0.47714871168136597\n",
      "Epoch 3149: train loss: 0.4771480858325958\n",
      "Epoch 3150: train loss: 0.4771474599838257\n",
      "Epoch 3151: train loss: 0.4771468937397003\n",
      "Epoch 3152: train loss: 0.47714629769325256\n",
      "Epoch 3153: train loss: 0.4771457314491272\n",
      "Epoch 3154: train loss: 0.47714516520500183\n",
      "Epoch 3155: train loss: 0.4771445393562317\n",
      "Epoch 3156: train loss: 0.47714394330978394\n",
      "Epoch 3157: train loss: 0.4771433174610138\n",
      "Epoch 3158: train loss: 0.4771427512168884\n",
      "Epoch 3159: train loss: 0.4771421253681183\n",
      "Epoch 3160: train loss: 0.4771415591239929\n",
      "Epoch 3161: train loss: 0.47714096307754517\n",
      "Epoch 3162: train loss: 0.4771403968334198\n",
      "Epoch 3163: train loss: 0.47713977098464966\n",
      "Epoch 3164: train loss: 0.4771392047405243\n",
      "Epoch 3165: train loss: 0.4771386384963989\n",
      "Epoch 3166: train loss: 0.47713804244995117\n",
      "Epoch 3167: train loss: 0.4771374762058258\n",
      "Epoch 3168: train loss: 0.47713685035705566\n",
      "Epoch 3169: train loss: 0.4771362841129303\n",
      "Epoch 3170: train loss: 0.47713568806648254\n",
      "Epoch 3171: train loss: 0.4771350622177124\n",
      "Epoch 3172: train loss: 0.47713449597358704\n",
      "Epoch 3173: train loss: 0.47713392972946167\n",
      "Epoch 3174: train loss: 0.4771333336830139\n",
      "Epoch 3175: train loss: 0.4771327078342438\n",
      "Epoch 3176: train loss: 0.47713208198547363\n",
      "Epoch 3177: train loss: 0.47713157534599304\n",
      "Epoch 3178: train loss: 0.4771309494972229\n",
      "Epoch 3179: train loss: 0.47713035345077515\n",
      "Epoch 3180: train loss: 0.4771297872066498\n",
      "Epoch 3181: train loss: 0.4771292805671692\n",
      "Epoch 3182: train loss: 0.47712868452072144\n",
      "Epoch 3183: train loss: 0.4771280586719513\n",
      "Epoch 3184: train loss: 0.4771274924278259\n",
      "Epoch 3185: train loss: 0.47712692618370056\n",
      "Epoch 3186: train loss: 0.4771263301372528\n",
      "Epoch 3187: train loss: 0.47712576389312744\n",
      "Epoch 3188: train loss: 0.4771251976490021\n",
      "Epoch 3189: train loss: 0.47712457180023193\n",
      "Epoch 3190: train loss: 0.47712403535842896\n",
      "Epoch 3191: train loss: 0.4771234095096588\n",
      "Epoch 3192: train loss: 0.47712284326553345\n",
      "Epoch 3193: train loss: 0.4771222174167633\n",
      "Epoch 3194: train loss: 0.4771216809749603\n",
      "Epoch 3195: train loss: 0.4771210551261902\n",
      "Epoch 3196: train loss: 0.4771205484867096\n",
      "Epoch 3197: train loss: 0.47711992263793945\n",
      "Epoch 3198: train loss: 0.4771193265914917\n",
      "Epoch 3199: train loss: 0.4771188199520111\n",
      "Epoch 3200: train loss: 0.47711819410324097\n",
      "Epoch 3201: train loss: 0.477117657661438\n",
      "Epoch 3202: train loss: 0.47711703181266785\n",
      "Epoch 3203: train loss: 0.47711652517318726\n",
      "Epoch 3204: train loss: 0.4771158993244171\n",
      "Epoch 3205: train loss: 0.47711530327796936\n",
      "Epoch 3206: train loss: 0.47711479663848877\n",
      "Epoch 3207: train loss: 0.4771142303943634\n",
      "Epoch 3208: train loss: 0.47711363434791565\n",
      "Epoch 3209: train loss: 0.4771130084991455\n",
      "Epoch 3210: train loss: 0.4771125018596649\n",
      "Epoch 3211: train loss: 0.47711193561553955\n",
      "Epoch 3212: train loss: 0.4771113395690918\n",
      "Epoch 3213: train loss: 0.4771108329296112\n",
      "Epoch 3214: train loss: 0.47711020708084106\n",
      "Epoch 3215: train loss: 0.4771096110343933\n",
      "Epoch 3216: train loss: 0.4771091043949127\n",
      "Epoch 3217: train loss: 0.4771084785461426\n",
      "Epoch 3218: train loss: 0.4771078824996948\n",
      "Epoch 3219: train loss: 0.47710731625556946\n",
      "Epoch 3220: train loss: 0.47710680961608887\n",
      "Epoch 3221: train loss: 0.4771061837673187\n",
      "Epoch 3222: train loss: 0.47710564732551575\n",
      "Epoch 3223: train loss: 0.4771050810813904\n",
      "Epoch 3224: train loss: 0.477104514837265\n",
      "Epoch 3225: train loss: 0.47710397839546204\n",
      "Epoch 3226: train loss: 0.47710341215133667\n",
      "Epoch 3227: train loss: 0.4771028161048889\n",
      "Epoch 3228: train loss: 0.4771021902561188\n",
      "Epoch 3229: train loss: 0.4771016836166382\n",
      "Epoch 3230: train loss: 0.4771011471748352\n",
      "Epoch 3231: train loss: 0.47710058093070984\n",
      "Epoch 3232: train loss: 0.4771000146865845\n",
      "Epoch 3233: train loss: 0.4770994186401367\n",
      "Epoch 3234: train loss: 0.47709885239601135\n",
      "Epoch 3235: train loss: 0.477098286151886\n",
      "Epoch 3236: train loss: 0.4770977199077606\n",
      "Epoch 3237: train loss: 0.47709718346595764\n",
      "Epoch 3238: train loss: 0.4770966172218323\n",
      "Epoch 3239: train loss: 0.4770960211753845\n",
      "Epoch 3240: train loss: 0.47709545493125916\n",
      "Epoch 3241: train loss: 0.47709494829177856\n",
      "Epoch 3242: train loss: 0.4770943522453308\n",
      "Epoch 3243: train loss: 0.47709378600120544\n",
      "Epoch 3244: train loss: 0.4770932197570801\n",
      "Epoch 3245: train loss: 0.4770926833152771\n",
      "Epoch 3246: train loss: 0.4770921766757965\n",
      "Epoch 3247: train loss: 0.47709155082702637\n",
      "Epoch 3248: train loss: 0.4770910143852234\n",
      "Epoch 3249: train loss: 0.4770905077457428\n",
      "Epoch 3250: train loss: 0.47708991169929504\n",
      "Epoch 3251: train loss: 0.4770893454551697\n",
      "Epoch 3252: train loss: 0.4770888090133667\n",
      "Epoch 3253: train loss: 0.47708818316459656\n",
      "Epoch 3254: train loss: 0.47708767652511597\n",
      "Epoch 3255: train loss: 0.4770871102809906\n",
      "Epoch 3256: train loss: 0.47708651423454285\n",
      "Epoch 3257: train loss: 0.47708600759506226\n",
      "Epoch 3258: train loss: 0.4770854413509369\n",
      "Epoch 3259: train loss: 0.4770849049091339\n",
      "Epoch 3260: train loss: 0.47708433866500854\n",
      "Epoch 3261: train loss: 0.47708380222320557\n",
      "Epoch 3262: train loss: 0.4770832359790802\n",
      "Epoch 3263: train loss: 0.47708266973495483\n",
      "Epoch 3264: train loss: 0.47708213329315186\n",
      "Epoch 3265: train loss: 0.4770815670490265\n",
      "Epoch 3266: train loss: 0.4770810306072235\n",
      "Epoch 3267: train loss: 0.47708046436309814\n",
      "Epoch 3268: train loss: 0.47707992792129517\n",
      "Epoch 3269: train loss: 0.4770793616771698\n",
      "Epoch 3270: train loss: 0.4770788550376892\n",
      "Epoch 3271: train loss: 0.47707825899124146\n",
      "Epoch 3272: train loss: 0.47707775235176086\n",
      "Epoch 3273: train loss: 0.4770772159099579\n",
      "Epoch 3274: train loss: 0.4770767092704773\n",
      "Epoch 3275: train loss: 0.47707608342170715\n",
      "Epoch 3276: train loss: 0.4770755469799042\n",
      "Epoch 3277: train loss: 0.4770750403404236\n",
      "Epoch 3278: train loss: 0.47707444429397583\n",
      "Epoch 3279: train loss: 0.47707387804985046\n",
      "Epoch 3280: train loss: 0.4770733416080475\n",
      "Epoch 3281: train loss: 0.4770727753639221\n",
      "Epoch 3282: train loss: 0.4770722985267639\n",
      "Epoch 3283: train loss: 0.47707173228263855\n",
      "Epoch 3284: train loss: 0.4770711660385132\n",
      "Epoch 3285: train loss: 0.477070689201355\n",
      "Epoch 3286: train loss: 0.4770701825618744\n",
      "Epoch 3287: train loss: 0.47706952691078186\n",
      "Epoch 3288: train loss: 0.47706902027130127\n",
      "Epoch 3289: train loss: 0.4770684838294983\n",
      "Epoch 3290: train loss: 0.4770679175853729\n",
      "Epoch 3291: train loss: 0.47706741094589233\n",
      "Epoch 3292: train loss: 0.47706687450408936\n",
      "Epoch 3293: train loss: 0.477066308259964\n",
      "Epoch 3294: train loss: 0.477065771818161\n",
      "Epoch 3295: train loss: 0.4770652651786804\n",
      "Epoch 3296: train loss: 0.47706472873687744\n",
      "Epoch 3297: train loss: 0.4770641624927521\n",
      "Epoch 3298: train loss: 0.4770636260509491\n",
      "Epoch 3299: train loss: 0.47706305980682373\n",
      "Epoch 3300: train loss: 0.47706255316734314\n",
      "Epoch 3301: train loss: 0.47706201672554016\n",
      "Epoch 3302: train loss: 0.4770614504814148\n",
      "Epoch 3303: train loss: 0.4770609140396118\n",
      "Epoch 3304: train loss: 0.4770604074001312\n",
      "Epoch 3305: train loss: 0.47705981135368347\n",
      "Epoch 3306: train loss: 0.4770593047142029\n",
      "Epoch 3307: train loss: 0.4770587384700775\n",
      "Epoch 3308: train loss: 0.4770582616329193\n",
      "Epoch 3309: train loss: 0.47705766558647156\n",
      "Epoch 3310: train loss: 0.47705715894699097\n",
      "Epoch 3311: train loss: 0.477056622505188\n",
      "Epoch 3312: train loss: 0.4770561158657074\n",
      "Epoch 3313: train loss: 0.4770555794239044\n",
      "Epoch 3314: train loss: 0.47705501317977905\n",
      "Epoch 3315: train loss: 0.4770544767379761\n",
      "Epoch 3316: train loss: 0.4770539700984955\n",
      "Epoch 3317: train loss: 0.4770534038543701\n",
      "Epoch 3318: train loss: 0.47705286741256714\n",
      "Epoch 3319: train loss: 0.47705236077308655\n",
      "Epoch 3320: train loss: 0.47705182433128357\n",
      "Epoch 3321: train loss: 0.477051317691803\n",
      "Epoch 3322: train loss: 0.47705078125\n",
      "Epoch 3323: train loss: 0.47705021500587463\n",
      "Epoch 3324: train loss: 0.47704967856407166\n",
      "Epoch 3325: train loss: 0.47704920172691345\n",
      "Epoch 3326: train loss: 0.4770485758781433\n",
      "Epoch 3327: train loss: 0.4770481288433075\n",
      "Epoch 3328: train loss: 0.4770475924015045\n",
      "Epoch 3329: train loss: 0.4770470857620239\n",
      "Epoch 3330: train loss: 0.47704654932022095\n",
      "Epoch 3331: train loss: 0.47704601287841797\n",
      "Epoch 3332: train loss: 0.4770455062389374\n",
      "Epoch 3333: train loss: 0.477044939994812\n",
      "Epoch 3334: train loss: 0.4770444631576538\n",
      "Epoch 3335: train loss: 0.47704389691352844\n",
      "Epoch 3336: train loss: 0.47704336047172546\n",
      "Epoch 3337: train loss: 0.47704288363456726\n",
      "Epoch 3338: train loss: 0.4770423173904419\n",
      "Epoch 3339: train loss: 0.4770417809486389\n",
      "Epoch 3340: train loss: 0.4770412743091583\n",
      "Epoch 3341: train loss: 0.47704073786735535\n",
      "Epoch 3342: train loss: 0.47704023122787476\n",
      "Epoch 3343: train loss: 0.4770396947860718\n",
      "Epoch 3344: train loss: 0.4770391881465912\n",
      "Epoch 3345: train loss: 0.4770386517047882\n",
      "Epoch 3346: train loss: 0.4770381450653076\n",
      "Epoch 3347: train loss: 0.47703760862350464\n",
      "Epoch 3348: train loss: 0.47703710198402405\n",
      "Epoch 3349: train loss: 0.47703656554222107\n",
      "Epoch 3350: train loss: 0.4770360589027405\n",
      "Epoch 3351: train loss: 0.4770355224609375\n",
      "Epoch 3352: train loss: 0.4770350456237793\n",
      "Epoch 3353: train loss: 0.47703447937965393\n",
      "Epoch 3354: train loss: 0.47703394293785095\n",
      "Epoch 3355: train loss: 0.47703349590301514\n",
      "Epoch 3356: train loss: 0.47703295946121216\n",
      "Epoch 3357: train loss: 0.4770323932170868\n",
      "Epoch 3358: train loss: 0.4770319163799286\n",
      "Epoch 3359: train loss: 0.4770313799381256\n",
      "Epoch 3360: train loss: 0.477030873298645\n",
      "Epoch 3361: train loss: 0.47703033685684204\n",
      "Epoch 3362: train loss: 0.4770298898220062\n",
      "Epoch 3363: train loss: 0.47702929377555847\n",
      "Epoch 3364: train loss: 0.47702884674072266\n",
      "Epoch 3365: train loss: 0.4770283102989197\n",
      "Epoch 3366: train loss: 0.47702768445014954\n",
      "Epoch 3367: train loss: 0.4770272672176361\n",
      "Epoch 3368: train loss: 0.47702673077583313\n",
      "Epoch 3369: train loss: 0.47702622413635254\n",
      "Epoch 3370: train loss: 0.47702568769454956\n",
      "Epoch 3371: train loss: 0.47702518105506897\n",
      "Epoch 3372: train loss: 0.477024644613266\n",
      "Epoch 3373: train loss: 0.4770241379737854\n",
      "Epoch 3374: train loss: 0.4770236611366272\n",
      "Epoch 3375: train loss: 0.47702309489250183\n",
      "Epoch 3376: train loss: 0.4770226776599884\n",
      "Epoch 3377: train loss: 0.47702208161354065\n",
      "Epoch 3378: train loss: 0.47702163457870483\n",
      "Epoch 3379: train loss: 0.47702109813690186\n",
      "Epoch 3380: train loss: 0.47702059149742126\n",
      "Epoch 3381: train loss: 0.47702011466026306\n",
      "Epoch 3382: train loss: 0.4770195186138153\n",
      "Epoch 3383: train loss: 0.4770190119743347\n",
      "Epoch 3384: train loss: 0.4770185351371765\n",
      "Epoch 3385: train loss: 0.4770180284976959\n",
      "Epoch 3386: train loss: 0.47701749205589294\n",
      "Epoch 3387: train loss: 0.47701698541641235\n",
      "Epoch 3388: train loss: 0.4770164489746094\n",
      "Epoch 3389: train loss: 0.47701597213745117\n",
      "Epoch 3390: train loss: 0.4770154654979706\n",
      "Epoch 3391: train loss: 0.4770149290561676\n",
      "Epoch 3392: train loss: 0.477014422416687\n",
      "Epoch 3393: train loss: 0.4770139455795288\n",
      "Epoch 3394: train loss: 0.4770134687423706\n",
      "Epoch 3395: train loss: 0.47701296210289\n",
      "Epoch 3396: train loss: 0.47701242566108704\n",
      "Epoch 3397: train loss: 0.47701188921928406\n",
      "Epoch 3398: train loss: 0.47701144218444824\n",
      "Epoch 3399: train loss: 0.47701090574264526\n",
      "Epoch 3400: train loss: 0.4770103991031647\n",
      "Epoch 3401: train loss: 0.47700992226600647\n",
      "Epoch 3402: train loss: 0.4770093858242035\n",
      "Epoch 3403: train loss: 0.4770089387893677\n",
      "Epoch 3404: train loss: 0.4770083427429199\n",
      "Epoch 3405: train loss: 0.4770079255104065\n",
      "Epoch 3406: train loss: 0.4770074188709259\n",
      "Epoch 3407: train loss: 0.4770068824291229\n",
      "Epoch 3408: train loss: 0.47700637578964233\n",
      "Epoch 3409: train loss: 0.47700589895248413\n",
      "Epoch 3410: train loss: 0.47700533270835876\n",
      "Epoch 3411: train loss: 0.47700488567352295\n",
      "Epoch 3412: train loss: 0.47700437903404236\n",
      "Epoch 3413: train loss: 0.4770038425922394\n",
      "Epoch 3414: train loss: 0.4770033359527588\n",
      "Epoch 3415: train loss: 0.4770027995109558\n",
      "Epoch 3416: train loss: 0.47700235247612\n",
      "Epoch 3417: train loss: 0.4770018756389618\n",
      "Epoch 3418: train loss: 0.4770013391971588\n",
      "Epoch 3419: train loss: 0.4770008623600006\n",
      "Epoch 3420: train loss: 0.47700035572052\n",
      "Epoch 3421: train loss: 0.47699981927871704\n",
      "Epoch 3422: train loss: 0.4769993722438812\n",
      "Epoch 3423: train loss: 0.476998895406723\n",
      "Epoch 3424: train loss: 0.47699835896492004\n",
      "Epoch 3425: train loss: 0.47699785232543945\n",
      "Epoch 3426: train loss: 0.47699737548828125\n",
      "Epoch 3427: train loss: 0.47699683904647827\n",
      "Epoch 3428: train loss: 0.47699639201164246\n",
      "Epoch 3429: train loss: 0.4769958555698395\n",
      "Epoch 3430: train loss: 0.4769953489303589\n",
      "Epoch 3431: train loss: 0.4769948720932007\n",
      "Epoch 3432: train loss: 0.4769943952560425\n",
      "Epoch 3433: train loss: 0.4769938886165619\n",
      "Epoch 3434: train loss: 0.4769934117794037\n",
      "Epoch 3435: train loss: 0.4769928753376007\n",
      "Epoch 3436: train loss: 0.4769923985004425\n",
      "Epoch 3437: train loss: 0.4769918918609619\n",
      "Epoch 3438: train loss: 0.4769914150238037\n",
      "Epoch 3439: train loss: 0.4769909083843231\n",
      "Epoch 3440: train loss: 0.4769904315471649\n",
      "Epoch 3441: train loss: 0.4769899547100067\n",
      "Epoch 3442: train loss: 0.47698941826820374\n",
      "Epoch 3443: train loss: 0.47698891162872314\n",
      "Epoch 3444: train loss: 0.4769884943962097\n",
      "Epoch 3445: train loss: 0.47698795795440674\n",
      "Epoch 3446: train loss: 0.47698745131492615\n",
      "Epoch 3447: train loss: 0.47698697447776794\n",
      "Epoch 3448: train loss: 0.47698649764060974\n",
      "Epoch 3449: train loss: 0.47698599100112915\n",
      "Epoch 3450: train loss: 0.47698551416397095\n",
      "Epoch 3451: train loss: 0.47698503732681274\n",
      "Epoch 3452: train loss: 0.47698453068733215\n",
      "Epoch 3453: train loss: 0.4769839942455292\n",
      "Epoch 3454: train loss: 0.47698351740837097\n",
      "Epoch 3455: train loss: 0.47698307037353516\n",
      "Epoch 3456: train loss: 0.47698259353637695\n",
      "Epoch 3457: train loss: 0.47698211669921875\n",
      "Epoch 3458: train loss: 0.4769815504550934\n",
      "Epoch 3459: train loss: 0.47698113322257996\n",
      "Epoch 3460: train loss: 0.476980596780777\n",
      "Epoch 3461: train loss: 0.4769800901412964\n",
      "Epoch 3462: train loss: 0.4769796133041382\n",
      "Epoch 3463: train loss: 0.47697913646698\n",
      "Epoch 3464: train loss: 0.4769786596298218\n",
      "Epoch 3465: train loss: 0.4769781827926636\n",
      "Epoch 3466: train loss: 0.476977676153183\n",
      "Epoch 3467: train loss: 0.4769771993160248\n",
      "Epoch 3468: train loss: 0.4769767224788666\n",
      "Epoch 3469: train loss: 0.476976215839386\n",
      "Epoch 3470: train loss: 0.4769757390022278\n",
      "Epoch 3471: train loss: 0.4769752621650696\n",
      "Epoch 3472: train loss: 0.47697481513023376\n",
      "Epoch 3473: train loss: 0.4769742786884308\n",
      "Epoch 3474: train loss: 0.4769738018512726\n",
      "Epoch 3475: train loss: 0.4769733250141144\n",
      "Epoch 3476: train loss: 0.4769728183746338\n",
      "Epoch 3477: train loss: 0.4769723415374756\n",
      "Epoch 3478: train loss: 0.4769718647003174\n",
      "Epoch 3479: train loss: 0.4769713580608368\n",
      "Epoch 3480: train loss: 0.47697094082832336\n",
      "Epoch 3481: train loss: 0.4769704043865204\n",
      "Epoch 3482: train loss: 0.4769698977470398\n",
      "Epoch 3483: train loss: 0.47696948051452637\n",
      "Epoch 3484: train loss: 0.4769689440727234\n",
      "Epoch 3485: train loss: 0.4769684672355652\n",
      "Epoch 3486: train loss: 0.47696802020072937\n",
      "Epoch 3487: train loss: 0.47696754336357117\n",
      "Epoch 3488: train loss: 0.4769670069217682\n",
      "Epoch 3489: train loss: 0.47696658968925476\n",
      "Epoch 3490: train loss: 0.47696608304977417\n",
      "Epoch 3491: train loss: 0.47696560621261597\n",
      "Epoch 3492: train loss: 0.47696512937545776\n",
      "Epoch 3493: train loss: 0.47696465253829956\n",
      "Epoch 3494: train loss: 0.47696420550346375\n",
      "Epoch 3495: train loss: 0.47696366906166077\n",
      "Epoch 3496: train loss: 0.47696325182914734\n",
      "Epoch 3497: train loss: 0.47696271538734436\n",
      "Epoch 3498: train loss: 0.47696226835250854\n",
      "Epoch 3499: train loss: 0.47696179151535034\n",
      "Epoch 3500: train loss: 0.47696131467819214\n",
      "Epoch 3501: train loss: 0.47696083784103394\n",
      "Epoch 3502: train loss: 0.47696033120155334\n",
      "Epoch 3503: train loss: 0.47695985436439514\n",
      "Epoch 3504: train loss: 0.47695937752723694\n",
      "Epoch 3505: train loss: 0.47695890069007874\n",
      "Epoch 3506: train loss: 0.4769584536552429\n",
      "Epoch 3507: train loss: 0.4769579768180847\n",
      "Epoch 3508: train loss: 0.4769575595855713\n",
      "Epoch 3509: train loss: 0.4769570231437683\n",
      "Epoch 3510: train loss: 0.4769565761089325\n",
      "Epoch 3511: train loss: 0.4769560992717743\n",
      "Epoch 3512: train loss: 0.4769555628299713\n",
      "Epoch 3513: train loss: 0.4769550859928131\n",
      "Epoch 3514: train loss: 0.4769546687602997\n",
      "Epoch 3515: train loss: 0.4769541621208191\n",
      "Epoch 3516: train loss: 0.4769536852836609\n",
      "Epoch 3517: train loss: 0.4769532084465027\n",
      "Epoch 3518: train loss: 0.47695276141166687\n",
      "Epoch 3519: train loss: 0.47695228457450867\n",
      "Epoch 3520: train loss: 0.47695180773735046\n",
      "Epoch 3521: train loss: 0.47695133090019226\n",
      "Epoch 3522: train loss: 0.47695085406303406\n",
      "Epoch 3523: train loss: 0.47695043683052063\n",
      "Epoch 3524: train loss: 0.47694993019104004\n",
      "Epoch 3525: train loss: 0.47694945335388184\n",
      "Epoch 3526: train loss: 0.4769490361213684\n",
      "Epoch 3527: train loss: 0.47694846987724304\n",
      "Epoch 3528: train loss: 0.4769480526447296\n",
      "Epoch 3529: train loss: 0.4769475758075714\n",
      "Epoch 3530: train loss: 0.4769470989704132\n",
      "Epoch 3531: train loss: 0.476946622133255\n",
      "Epoch 3532: train loss: 0.4769461750984192\n",
      "Epoch 3533: train loss: 0.47694575786590576\n",
      "Epoch 3534: train loss: 0.4769452214241028\n",
      "Epoch 3535: train loss: 0.4769447445869446\n",
      "Epoch 3536: train loss: 0.47694429755210876\n",
      "Epoch 3537: train loss: 0.47694382071495056\n",
      "Epoch 3538: train loss: 0.47694340348243713\n",
      "Epoch 3539: train loss: 0.47694286704063416\n",
      "Epoch 3540: train loss: 0.47694239020347595\n",
      "Epoch 3541: train loss: 0.4769419729709625\n",
      "Epoch 3542: train loss: 0.4769415259361267\n",
      "Epoch 3543: train loss: 0.47694098949432373\n",
      "Epoch 3544: train loss: 0.4769405126571655\n",
      "Epoch 3545: train loss: 0.4769400358200073\n",
      "Epoch 3546: train loss: 0.4769396185874939\n",
      "Epoch 3547: train loss: 0.4769391715526581\n",
      "Epoch 3548: train loss: 0.4769386947154999\n",
      "Epoch 3549: train loss: 0.47693827748298645\n",
      "Epoch 3550: train loss: 0.47693774104118347\n",
      "Epoch 3551: train loss: 0.47693732380867004\n",
      "Epoch 3552: train loss: 0.47693687677383423\n",
      "Epoch 3553: train loss: 0.476936399936676\n",
      "Epoch 3554: train loss: 0.4769359230995178\n",
      "Epoch 3555: train loss: 0.4769354462623596\n",
      "Epoch 3556: train loss: 0.4769349694252014\n",
      "Epoch 3557: train loss: 0.476934552192688\n",
      "Epoch 3558: train loss: 0.4769341051578522\n",
      "Epoch 3559: train loss: 0.47693362832069397\n",
      "Epoch 3560: train loss: 0.47693315148353577\n",
      "Epoch 3561: train loss: 0.47693267464637756\n",
      "Epoch 3562: train loss: 0.476932168006897\n",
      "Epoch 3563: train loss: 0.47693175077438354\n",
      "Epoch 3564: train loss: 0.4769313335418701\n",
      "Epoch 3565: train loss: 0.4769308567047119\n",
      "Epoch 3566: train loss: 0.4769303798675537\n",
      "Epoch 3567: train loss: 0.4769299030303955\n",
      "Epoch 3568: train loss: 0.4769294559955597\n",
      "Epoch 3569: train loss: 0.47692903876304626\n",
      "Epoch 3570: train loss: 0.47692856192588806\n",
      "Epoch 3571: train loss: 0.47692808508872986\n",
      "Epoch 3572: train loss: 0.47692760825157166\n",
      "Epoch 3573: train loss: 0.47692713141441345\n",
      "Epoch 3574: train loss: 0.47692668437957764\n",
      "Epoch 3575: train loss: 0.47692620754241943\n",
      "Epoch 3576: train loss: 0.476925790309906\n",
      "Epoch 3577: train loss: 0.4769253134727478\n",
      "Epoch 3578: train loss: 0.4769248366355896\n",
      "Epoch 3579: train loss: 0.47692441940307617\n",
      "Epoch 3580: train loss: 0.47692394256591797\n",
      "Epoch 3581: train loss: 0.47692349553108215\n",
      "Epoch 3582: train loss: 0.4769230782985687\n",
      "Epoch 3583: train loss: 0.4769226014614105\n",
      "Epoch 3584: train loss: 0.4769221246242523\n",
      "Epoch 3585: train loss: 0.4769217073917389\n",
      "Epoch 3586: train loss: 0.4769212305545807\n",
      "Epoch 3587: train loss: 0.4769207537174225\n",
      "Epoch 3588: train loss: 0.47692030668258667\n",
      "Epoch 3589: train loss: 0.47691982984542847\n",
      "Epoch 3590: train loss: 0.47691941261291504\n",
      "Epoch 3591: train loss: 0.47691893577575684\n",
      "Epoch 3592: train loss: 0.47691845893859863\n",
      "Epoch 3593: train loss: 0.4769180119037628\n",
      "Epoch 3594: train loss: 0.4769175946712494\n",
      "Epoch 3595: train loss: 0.4769171178340912\n",
      "Epoch 3596: train loss: 0.47691670060157776\n",
      "Epoch 3597: train loss: 0.47691622376441956\n",
      "Epoch 3598: train loss: 0.47691580653190613\n",
      "Epoch 3599: train loss: 0.4769153296947479\n",
      "Epoch 3600: train loss: 0.4769148528575897\n",
      "Epoch 3601: train loss: 0.4769144058227539\n",
      "Epoch 3602: train loss: 0.4769139885902405\n",
      "Epoch 3603: train loss: 0.4769134521484375\n",
      "Epoch 3604: train loss: 0.4769130349159241\n",
      "Epoch 3605: train loss: 0.47691261768341064\n",
      "Epoch 3606: train loss: 0.47691214084625244\n",
      "Epoch 3607: train loss: 0.476911723613739\n",
      "Epoch 3608: train loss: 0.4769112467765808\n",
      "Epoch 3609: train loss: 0.476910799741745\n",
      "Epoch 3610: train loss: 0.47691038250923157\n",
      "Epoch 3611: train loss: 0.47690990567207336\n",
      "Epoch 3612: train loss: 0.47690942883491516\n",
      "Epoch 3613: train loss: 0.47690901160240173\n",
      "Epoch 3614: train loss: 0.47690853476524353\n",
      "Epoch 3615: train loss: 0.4769081175327301\n",
      "Epoch 3616: train loss: 0.4769077003002167\n",
      "Epoch 3617: train loss: 0.47690722346305847\n",
      "Epoch 3618: train loss: 0.47690677642822266\n",
      "Epoch 3619: train loss: 0.47690635919570923\n",
      "Epoch 3620: train loss: 0.476905882358551\n",
      "Epoch 3621: train loss: 0.4769054055213928\n",
      "Epoch 3622: train loss: 0.4769049882888794\n",
      "Epoch 3623: train loss: 0.4769045114517212\n",
      "Epoch 3624: train loss: 0.47690409421920776\n",
      "Epoch 3625: train loss: 0.47690361738204956\n",
      "Epoch 3626: train loss: 0.47690317034721375\n",
      "Epoch 3627: train loss: 0.4769027531147003\n",
      "Epoch 3628: train loss: 0.4769022762775421\n",
      "Epoch 3629: train loss: 0.4769018590450287\n",
      "Epoch 3630: train loss: 0.47690144181251526\n",
      "Epoch 3631: train loss: 0.47690096497535706\n",
      "Epoch 3632: train loss: 0.47690048813819885\n",
      "Epoch 3633: train loss: 0.4769000709056854\n",
      "Epoch 3634: train loss: 0.47689956426620483\n",
      "Epoch 3635: train loss: 0.4768991768360138\n",
      "Epoch 3636: train loss: 0.4768986701965332\n",
      "Epoch 3637: train loss: 0.47689831256866455\n",
      "Epoch 3638: train loss: 0.47689783573150635\n",
      "Epoch 3639: train loss: 0.47689735889434814\n",
      "Epoch 3640: train loss: 0.4768969416618347\n",
      "Epoch 3641: train loss: 0.4768965244293213\n",
      "Epoch 3642: train loss: 0.47689610719680786\n",
      "Epoch 3643: train loss: 0.47689563035964966\n",
      "Epoch 3644: train loss: 0.47689521312713623\n",
      "Epoch 3645: train loss: 0.476894736289978\n",
      "Epoch 3646: train loss: 0.4768943190574646\n",
      "Epoch 3647: train loss: 0.4768938422203064\n",
      "Epoch 3648: train loss: 0.4768933951854706\n",
      "Epoch 3649: train loss: 0.47689297795295715\n",
      "Epoch 3650: train loss: 0.4768925607204437\n",
      "Epoch 3651: train loss: 0.4768920838832855\n",
      "Epoch 3652: train loss: 0.4768916666507721\n",
      "Epoch 3653: train loss: 0.4768911898136139\n",
      "Epoch 3654: train loss: 0.47689077258110046\n",
      "Epoch 3655: train loss: 0.47689035534858704\n",
      "Epoch 3656: train loss: 0.47688987851142883\n",
      "Epoch 3657: train loss: 0.4768894612789154\n",
      "Epoch 3658: train loss: 0.4768889844417572\n",
      "Epoch 3659: train loss: 0.4768885672092438\n",
      "Epoch 3660: train loss: 0.47688814997673035\n",
      "Epoch 3661: train loss: 0.4768877327442169\n",
      "Epoch 3662: train loss: 0.47688722610473633\n",
      "Epoch 3663: train loss: 0.4768868088722229\n",
      "Epoch 3664: train loss: 0.4768863916397095\n",
      "Epoch 3665: train loss: 0.47688591480255127\n",
      "Epoch 3666: train loss: 0.47688549757003784\n",
      "Epoch 3667: train loss: 0.4768850803375244\n",
      "Epoch 3668: train loss: 0.476884663105011\n",
      "Epoch 3669: train loss: 0.4768841862678528\n",
      "Epoch 3670: train loss: 0.47688376903533936\n",
      "Epoch 3671: train loss: 0.47688329219818115\n",
      "Epoch 3672: train loss: 0.4768828749656677\n",
      "Epoch 3673: train loss: 0.4768824577331543\n",
      "Epoch 3674: train loss: 0.47688204050064087\n",
      "Epoch 3675: train loss: 0.47688156366348267\n",
      "Epoch 3676: train loss: 0.47688114643096924\n",
      "Epoch 3677: train loss: 0.4768806993961334\n",
      "Epoch 3678: train loss: 0.47688028216362\n",
      "Epoch 3679: train loss: 0.4768798053264618\n",
      "Epoch 3680: train loss: 0.47687938809394836\n",
      "Epoch 3681: train loss: 0.47687897086143494\n",
      "Epoch 3682: train loss: 0.47687849402427673\n",
      "Epoch 3683: train loss: 0.4768781363964081\n",
      "Epoch 3684: train loss: 0.47687771916389465\n",
      "Epoch 3685: train loss: 0.47687724232673645\n",
      "Epoch 3686: train loss: 0.476876825094223\n",
      "Epoch 3687: train loss: 0.4768764078617096\n",
      "Epoch 3688: train loss: 0.4768759310245514\n",
      "Epoch 3689: train loss: 0.47687551379203796\n",
      "Epoch 3690: train loss: 0.47687509655952454\n",
      "Epoch 3691: train loss: 0.4768746793270111\n",
      "Epoch 3692: train loss: 0.4768742024898529\n",
      "Epoch 3693: train loss: 0.4768737852573395\n",
      "Epoch 3694: train loss: 0.4768733084201813\n",
      "Epoch 3695: train loss: 0.4768729507923126\n",
      "Epoch 3696: train loss: 0.4768724739551544\n",
      "Epoch 3697: train loss: 0.476872056722641\n",
      "Epoch 3698: train loss: 0.47687163949012756\n",
      "Epoch 3699: train loss: 0.47687116265296936\n",
      "Epoch 3700: train loss: 0.47687074542045593\n",
      "Epoch 3701: train loss: 0.4768703281879425\n",
      "Epoch 3702: train loss: 0.4768699109554291\n",
      "Epoch 3703: train loss: 0.47686949372291565\n",
      "Epoch 3704: train loss: 0.47686904668807983\n",
      "Epoch 3705: train loss: 0.4768686294555664\n",
      "Epoch 3706: train loss: 0.476868212223053\n",
      "Epoch 3707: train loss: 0.4768677353858948\n",
      "Epoch 3708: train loss: 0.47686734795570374\n",
      "Epoch 3709: train loss: 0.4768669307231903\n",
      "Epoch 3710: train loss: 0.4768664836883545\n",
      "Epoch 3711: train loss: 0.4768660068511963\n",
      "Epoch 3712: train loss: 0.47686564922332764\n",
      "Epoch 3713: train loss: 0.4768652319908142\n",
      "Epoch 3714: train loss: 0.476864755153656\n",
      "Epoch 3715: train loss: 0.4768643379211426\n",
      "Epoch 3716: train loss: 0.47686392068862915\n",
      "Epoch 3717: train loss: 0.4768635034561157\n",
      "Epoch 3718: train loss: 0.4768630862236023\n",
      "Epoch 3719: train loss: 0.4768626093864441\n",
      "Epoch 3720: train loss: 0.47686225175857544\n",
      "Epoch 3721: train loss: 0.47686177492141724\n",
      "Epoch 3722: train loss: 0.4768613576889038\n",
      "Epoch 3723: train loss: 0.4768609404563904\n",
      "Epoch 3724: train loss: 0.4768604636192322\n",
      "Epoch 3725: train loss: 0.47686004638671875\n",
      "Epoch 3726: train loss: 0.4768596887588501\n",
      "Epoch 3727: train loss: 0.4768592119216919\n",
      "Epoch 3728: train loss: 0.47685885429382324\n",
      "Epoch 3729: train loss: 0.47685837745666504\n",
      "Epoch 3730: train loss: 0.4768579602241516\n",
      "Epoch 3731: train loss: 0.4768575429916382\n",
      "Epoch 3732: train loss: 0.47685712575912476\n",
      "Epoch 3733: train loss: 0.47685670852661133\n",
      "Epoch 3734: train loss: 0.4768562316894531\n",
      "Epoch 3735: train loss: 0.4768558740615845\n",
      "Epoch 3736: train loss: 0.47685539722442627\n",
      "Epoch 3737: train loss: 0.47685497999191284\n",
      "Epoch 3738: train loss: 0.4768546223640442\n",
      "Epoch 3739: train loss: 0.476854145526886\n",
      "Epoch 3740: train loss: 0.47685372829437256\n",
      "Epoch 3741: train loss: 0.47685331106185913\n",
      "Epoch 3742: train loss: 0.4768528938293457\n",
      "Epoch 3743: train loss: 0.4768524765968323\n",
      "Epoch 3744: train loss: 0.47685205936431885\n",
      "Epoch 3745: train loss: 0.47685158252716064\n",
      "Epoch 3746: train loss: 0.476851224899292\n",
      "Epoch 3747: train loss: 0.47685080766677856\n",
      "Epoch 3748: train loss: 0.47685039043426514\n",
      "Epoch 3749: train loss: 0.4768499732017517\n",
      "Epoch 3750: train loss: 0.4768495559692383\n",
      "Epoch 3751: train loss: 0.47684913873672485\n",
      "Epoch 3752: train loss: 0.47684866189956665\n",
      "Epoch 3753: train loss: 0.4768482446670532\n",
      "Epoch 3754: train loss: 0.4768478274345398\n",
      "Epoch 3755: train loss: 0.47684744000434875\n",
      "Epoch 3756: train loss: 0.47684699296951294\n",
      "Epoch 3757: train loss: 0.4768466055393219\n",
      "Epoch 3758: train loss: 0.47684618830680847\n",
      "Epoch 3759: train loss: 0.47684577107429504\n",
      "Epoch 3760: train loss: 0.47684532403945923\n",
      "Epoch 3761: train loss: 0.4768449068069458\n",
      "Epoch 3762: train loss: 0.47684445977211\n",
      "Epoch 3763: train loss: 0.47684410214424133\n",
      "Epoch 3764: train loss: 0.47684362530708313\n",
      "Epoch 3765: train loss: 0.4768432676792145\n",
      "Epoch 3766: train loss: 0.4768427908420563\n",
      "Epoch 3767: train loss: 0.47684237360954285\n",
      "Epoch 3768: train loss: 0.4768420159816742\n",
      "Epoch 3769: train loss: 0.476841539144516\n",
      "Epoch 3770: train loss: 0.47684112191200256\n",
      "Epoch 3771: train loss: 0.47684070467948914\n",
      "Epoch 3772: train loss: 0.4768402874469757\n",
      "Epoch 3773: train loss: 0.47683992981910706\n",
      "Epoch 3774: train loss: 0.47683951258659363\n",
      "Epoch 3775: train loss: 0.4768390953540802\n",
      "Epoch 3776: train loss: 0.4768386781215668\n",
      "Epoch 3777: train loss: 0.47683826088905334\n",
      "Epoch 3778: train loss: 0.4768378436565399\n",
      "Epoch 3779: train loss: 0.4768374264240265\n",
      "Epoch 3780: train loss: 0.47683700919151306\n",
      "Epoch 3781: train loss: 0.476836621761322\n",
      "Epoch 3782: train loss: 0.4768361747264862\n",
      "Epoch 3783: train loss: 0.47683578729629517\n",
      "Epoch 3784: train loss: 0.47683537006378174\n",
      "Epoch 3785: train loss: 0.4768349528312683\n",
      "Epoch 3786: train loss: 0.4768345355987549\n",
      "Epoch 3787: train loss: 0.47683411836624146\n",
      "Epoch 3788: train loss: 0.476833701133728\n",
      "Epoch 3789: train loss: 0.4768332839012146\n",
      "Epoch 3790: train loss: 0.47683286666870117\n",
      "Epoch 3791: train loss: 0.4768325090408325\n",
      "Epoch 3792: train loss: 0.4768320322036743\n",
      "Epoch 3793: train loss: 0.4768316149711609\n",
      "Epoch 3794: train loss: 0.47683125734329224\n",
      "Epoch 3795: train loss: 0.4768308401107788\n",
      "Epoch 3796: train loss: 0.47683048248291016\n",
      "Epoch 3797: train loss: 0.47683006525039673\n",
      "Epoch 3798: train loss: 0.4768296480178833\n",
      "Epoch 3799: train loss: 0.4768292009830475\n",
      "Epoch 3800: train loss: 0.47682878375053406\n",
      "Epoch 3801: train loss: 0.47682833671569824\n",
      "Epoch 3802: train loss: 0.4768279492855072\n",
      "Epoch 3803: train loss: 0.47682759165763855\n",
      "Epoch 3804: train loss: 0.4768271744251251\n",
      "Epoch 3805: train loss: 0.4768266975879669\n",
      "Epoch 3806: train loss: 0.47682633996009827\n",
      "Epoch 3807: train loss: 0.4768259823322296\n",
      "Epoch 3808: train loss: 0.4768255054950714\n",
      "Epoch 3809: train loss: 0.47682514786720276\n",
      "Epoch 3810: train loss: 0.47682473063468933\n",
      "Epoch 3811: train loss: 0.4768243134021759\n",
      "Epoch 3812: train loss: 0.4768238961696625\n",
      "Epoch 3813: train loss: 0.47682350873947144\n",
      "Epoch 3814: train loss: 0.476823091506958\n",
      "Epoch 3815: train loss: 0.4768226742744446\n",
      "Epoch 3816: train loss: 0.47682225704193115\n",
      "Epoch 3817: train loss: 0.4768218994140625\n",
      "Epoch 3818: train loss: 0.4768214821815491\n",
      "Epoch 3819: train loss: 0.47682106494903564\n",
      "Epoch 3820: train loss: 0.4768206477165222\n",
      "Epoch 3821: train loss: 0.4768202304840088\n",
      "Epoch 3822: train loss: 0.47681987285614014\n",
      "Epoch 3823: train loss: 0.47681939601898193\n",
      "Epoch 3824: train loss: 0.4768190383911133\n",
      "Epoch 3825: train loss: 0.47681862115859985\n",
      "Epoch 3826: train loss: 0.4768182039260864\n",
      "Epoch 3827: train loss: 0.4768178164958954\n",
      "Epoch 3828: train loss: 0.47681739926338196\n",
      "Epoch 3829: train loss: 0.47681698203086853\n",
      "Epoch 3830: train loss: 0.4768166244029999\n",
      "Epoch 3831: train loss: 0.47681620717048645\n",
      "Epoch 3832: train loss: 0.476815789937973\n",
      "Epoch 3833: train loss: 0.47681543231010437\n",
      "Epoch 3834: train loss: 0.47681501507759094\n",
      "Epoch 3835: train loss: 0.4768145978450775\n",
      "Epoch 3836: train loss: 0.4768141508102417\n",
      "Epoch 3837: train loss: 0.47681379318237305\n",
      "Epoch 3838: train loss: 0.4768133759498596\n",
      "Epoch 3839: train loss: 0.4768129587173462\n",
      "Epoch 3840: train loss: 0.47681260108947754\n",
      "Epoch 3841: train loss: 0.4768121838569641\n",
      "Epoch 3842: train loss: 0.4768117666244507\n",
      "Epoch 3843: train loss: 0.47681140899658203\n",
      "Epoch 3844: train loss: 0.4768109917640686\n",
      "Epoch 3845: train loss: 0.47681060433387756\n",
      "Epoch 3846: train loss: 0.47681018710136414\n",
      "Epoch 3847: train loss: 0.4768097698688507\n",
      "Epoch 3848: train loss: 0.4768093526363373\n",
      "Epoch 3849: train loss: 0.47680899500846863\n",
      "Epoch 3850: train loss: 0.4768085777759552\n",
      "Epoch 3851: train loss: 0.4768081605434418\n",
      "Epoch 3852: train loss: 0.47680774331092834\n",
      "Epoch 3853: train loss: 0.4768073856830597\n",
      "Epoch 3854: train loss: 0.47680696845054626\n",
      "Epoch 3855: train loss: 0.4768065810203552\n",
      "Epoch 3856: train loss: 0.476806104183197\n",
      "Epoch 3857: train loss: 0.47680574655532837\n",
      "Epoch 3858: train loss: 0.4768053889274597\n",
      "Epoch 3859: train loss: 0.4768049716949463\n",
      "Epoch 3860: train loss: 0.47680455446243286\n",
      "Epoch 3861: train loss: 0.4768041968345642\n",
      "Epoch 3862: train loss: 0.47680380940437317\n",
      "Epoch 3863: train loss: 0.47680339217185974\n",
      "Epoch 3864: train loss: 0.4768029749393463\n",
      "Epoch 3865: train loss: 0.4768025577068329\n",
      "Epoch 3866: train loss: 0.47680214047431946\n",
      "Epoch 3867: train loss: 0.4768017828464508\n",
      "Epoch 3868: train loss: 0.47680142521858215\n",
      "Epoch 3869: train loss: 0.4768010079860687\n",
      "Epoch 3870: train loss: 0.4768005907535553\n",
      "Epoch 3871: train loss: 0.47680017352104187\n",
      "Epoch 3872: train loss: 0.47679978609085083\n",
      "Epoch 3873: train loss: 0.4767994284629822\n",
      "Epoch 3874: train loss: 0.47679901123046875\n",
      "Epoch 3875: train loss: 0.4767986536026001\n",
      "Epoch 3876: train loss: 0.4767981767654419\n",
      "Epoch 3877: train loss: 0.47679784893989563\n",
      "Epoch 3878: train loss: 0.4767974317073822\n",
      "Epoch 3879: train loss: 0.47679707407951355\n",
      "Epoch 3880: train loss: 0.4767966568470001\n",
      "Epoch 3881: train loss: 0.4767962396144867\n",
      "Epoch 3882: train loss: 0.47679588198661804\n",
      "Epoch 3883: train loss: 0.47679540514945984\n",
      "Epoch 3884: train loss: 0.4767950773239136\n",
      "Epoch 3885: train loss: 0.47679463028907776\n",
      "Epoch 3886: train loss: 0.4767943024635315\n",
      "Epoch 3887: train loss: 0.47679394483566284\n",
      "Epoch 3888: train loss: 0.47679346799850464\n",
      "Epoch 3889: train loss: 0.47679316997528076\n",
      "Epoch 3890: train loss: 0.47679275274276733\n",
      "Epoch 3891: train loss: 0.4767923355102539\n",
      "Epoch 3892: train loss: 0.4767919182777405\n",
      "Epoch 3893: train loss: 0.47679153084754944\n",
      "Epoch 3894: train loss: 0.4767911732196808\n",
      "Epoch 3895: train loss: 0.4767906963825226\n",
      "Epoch 3896: train loss: 0.47679033875465393\n",
      "Epoch 3897: train loss: 0.47679001092910767\n",
      "Epoch 3898: train loss: 0.47678956389427185\n",
      "Epoch 3899: train loss: 0.4767891764640808\n",
      "Epoch 3900: train loss: 0.4767887592315674\n",
      "Epoch 3901: train loss: 0.47678840160369873\n",
      "Epoch 3902: train loss: 0.4767879843711853\n",
      "Epoch 3903: train loss: 0.47678762674331665\n",
      "Epoch 3904: train loss: 0.4767872095108032\n",
      "Epoch 3905: train loss: 0.4767868220806122\n",
      "Epoch 3906: train loss: 0.47678646445274353\n",
      "Epoch 3907: train loss: 0.4767860472202301\n",
      "Epoch 3908: train loss: 0.4767856299877167\n",
      "Epoch 3909: train loss: 0.476785272359848\n",
      "Epoch 3910: train loss: 0.47678491473197937\n",
      "Epoch 3911: train loss: 0.47678449749946594\n",
      "Epoch 3912: train loss: 0.4767841100692749\n",
      "Epoch 3913: train loss: 0.4767836928367615\n",
      "Epoch 3914: train loss: 0.4767833352088928\n",
      "Epoch 3915: train loss: 0.4767829179763794\n",
      "Epoch 3916: train loss: 0.47678256034851074\n",
      "Epoch 3917: train loss: 0.4767821729183197\n",
      "Epoch 3918: train loss: 0.4767817556858063\n",
      "Epoch 3919: train loss: 0.47678133845329285\n",
      "Epoch 3920: train loss: 0.47678104043006897\n",
      "Epoch 3921: train loss: 0.47678062319755554\n",
      "Epoch 3922: train loss: 0.4767802059650421\n",
      "Epoch 3923: train loss: 0.4767798185348511\n",
      "Epoch 3924: train loss: 0.47677940130233765\n",
      "Epoch 3925: train loss: 0.476779043674469\n",
      "Epoch 3926: train loss: 0.47677868604660034\n",
      "Epoch 3927: train loss: 0.4767782688140869\n",
      "Epoch 3928: train loss: 0.47677791118621826\n",
      "Epoch 3929: train loss: 0.47677749395370483\n",
      "Epoch 3930: train loss: 0.4767771065235138\n",
      "Epoch 3931: train loss: 0.47677674889564514\n",
      "Epoch 3932: train loss: 0.4767763316631317\n",
      "Epoch 3933: train loss: 0.47677597403526306\n",
      "Epoch 3934: train loss: 0.47677555680274963\n",
      "Epoch 3935: train loss: 0.4767751693725586\n",
      "Epoch 3936: train loss: 0.47677481174468994\n",
      "Epoch 3937: train loss: 0.4767743945121765\n",
      "Epoch 3938: train loss: 0.47677403688430786\n",
      "Epoch 3939: train loss: 0.4767736792564392\n",
      "Epoch 3940: train loss: 0.4767732620239258\n",
      "Epoch 3941: train loss: 0.47677287459373474\n",
      "Epoch 3942: train loss: 0.4767725169658661\n",
      "Epoch 3943: train loss: 0.47677215933799744\n",
      "Epoch 3944: train loss: 0.476771742105484\n",
      "Epoch 3945: train loss: 0.4767713248729706\n",
      "Epoch 3946: train loss: 0.47677093744277954\n",
      "Epoch 3947: train loss: 0.4767705202102661\n",
      "Epoch 3948: train loss: 0.47677022218704224\n",
      "Epoch 3949: train loss: 0.4767698049545288\n",
      "Epoch 3950: train loss: 0.47676944732666016\n",
      "Epoch 3951: train loss: 0.4767690598964691\n",
      "Epoch 3952: train loss: 0.47676870226860046\n",
      "Epoch 3953: train loss: 0.47676828503608704\n",
      "Epoch 3954: train loss: 0.4767678678035736\n",
      "Epoch 3955: train loss: 0.47676753997802734\n",
      "Epoch 3956: train loss: 0.4767671227455139\n",
      "Epoch 3957: train loss: 0.47676676511764526\n",
      "Epoch 3958: train loss: 0.4767664074897766\n",
      "Epoch 3959: train loss: 0.4767659902572632\n",
      "Epoch 3960: train loss: 0.47676557302474976\n",
      "Epoch 3961: train loss: 0.4767652451992035\n",
      "Epoch 3962: train loss: 0.47676482796669006\n",
      "Epoch 3963: train loss: 0.4767644703388214\n",
      "Epoch 3964: train loss: 0.476764053106308\n",
      "Epoch 3965: train loss: 0.47676369547843933\n",
      "Epoch 3966: train loss: 0.4767633080482483\n",
      "Epoch 3967: train loss: 0.47676289081573486\n",
      "Epoch 3968: train loss: 0.4767625331878662\n",
      "Epoch 3969: train loss: 0.47676223516464233\n",
      "Epoch 3970: train loss: 0.47676175832748413\n",
      "Epoch 3971: train loss: 0.47676143050193787\n",
      "Epoch 3972: train loss: 0.4767610728740692\n",
      "Epoch 3973: train loss: 0.4767606556415558\n",
      "Epoch 3974: train loss: 0.47676029801368713\n",
      "Epoch 3975: train loss: 0.4767599105834961\n",
      "Epoch 3976: train loss: 0.47675949335098267\n",
      "Epoch 3977: train loss: 0.476759135723114\n",
      "Epoch 3978: train loss: 0.47675877809524536\n",
      "Epoch 3979: train loss: 0.47675836086273193\n",
      "Epoch 3980: train loss: 0.47675803303718567\n",
      "Epoch 3981: train loss: 0.47675761580467224\n",
      "Epoch 3982: train loss: 0.4767572581768036\n",
      "Epoch 3983: train loss: 0.47675690054893494\n",
      "Epoch 3984: train loss: 0.4767564833164215\n",
      "Epoch 3985: train loss: 0.47675609588623047\n",
      "Epoch 3986: train loss: 0.4767557382583618\n",
      "Epoch 3987: train loss: 0.4767553210258484\n",
      "Epoch 3988: train loss: 0.4767550230026245\n",
      "Epoch 3989: train loss: 0.4767546057701111\n",
      "Epoch 3990: train loss: 0.47675421833992004\n",
      "Epoch 3991: train loss: 0.4767538607120514\n",
      "Epoch 3992: train loss: 0.47675350308418274\n",
      "Epoch 3993: train loss: 0.4767531156539917\n",
      "Epoch 3994: train loss: 0.47675269842147827\n",
      "Epoch 3995: train loss: 0.4767523407936096\n",
      "Epoch 3996: train loss: 0.47675198316574097\n",
      "Epoch 3997: train loss: 0.4767516255378723\n",
      "Epoch 3998: train loss: 0.47675129771232605\n",
      "Epoch 3999: train loss: 0.4767508804798126\n",
      "Epoch 4000: train loss: 0.4767504632472992\n",
      "Epoch 4001: train loss: 0.47675010561943054\n",
      "Epoch 4002: train loss: 0.4767497479915619\n",
      "Epoch 4003: train loss: 0.47674936056137085\n",
      "Epoch 4004: train loss: 0.4767490029335022\n",
      "Epoch 4005: train loss: 0.47674864530563354\n",
      "Epoch 4006: train loss: 0.4767482280731201\n",
      "Epoch 4007: train loss: 0.4767478406429291\n",
      "Epoch 4008: train loss: 0.4767474830150604\n",
      "Epoch 4009: train loss: 0.47674718499183655\n",
      "Epoch 4010: train loss: 0.4767467677593231\n",
      "Epoch 4011: train loss: 0.4767463803291321\n",
      "Epoch 4012: train loss: 0.47674596309661865\n",
      "Epoch 4013: train loss: 0.47674560546875\n",
      "Epoch 4014: train loss: 0.47674524784088135\n",
      "Epoch 4015: train loss: 0.4767448604106903\n",
      "Epoch 4016: train loss: 0.47674456238746643\n",
      "Epoch 4017: train loss: 0.476744145154953\n",
      "Epoch 4018: train loss: 0.47674378752708435\n",
      "Epoch 4019: train loss: 0.4767433702945709\n",
      "Epoch 4020: train loss: 0.4767429828643799\n",
      "Epoch 4021: train loss: 0.47674262523651123\n",
      "Epoch 4022: train loss: 0.4767422676086426\n",
      "Epoch 4023: train loss: 0.4767419099807739\n",
      "Epoch 4024: train loss: 0.4767415225505829\n",
      "Epoch 4025: train loss: 0.47674116492271423\n",
      "Epoch 4026: train loss: 0.4767408072948456\n",
      "Epoch 4027: train loss: 0.47674039006233215\n",
      "Epoch 4028: train loss: 0.4767400622367859\n",
      "Epoch 4029: train loss: 0.47673970460891724\n",
      "Epoch 4030: train loss: 0.4767392873764038\n",
      "Epoch 4031: train loss: 0.47673892974853516\n",
      "Epoch 4032: train loss: 0.4767386019229889\n",
      "Epoch 4033: train loss: 0.47673818469047546\n",
      "Epoch 4034: train loss: 0.4767378270626068\n",
      "Epoch 4035: train loss: 0.47673743963241577\n",
      "Epoch 4036: train loss: 0.47673702239990234\n",
      "Epoch 4037: train loss: 0.47673672437667847\n",
      "Epoch 4038: train loss: 0.4767363667488098\n",
      "Epoch 4039: train loss: 0.4767359793186188\n",
      "Epoch 4040: train loss: 0.4767356216907501\n",
      "Epoch 4041: train loss: 0.47673526406288147\n",
      "Epoch 4042: train loss: 0.47673484683036804\n",
      "Epoch 4043: train loss: 0.4767344892024994\n",
      "Epoch 4044: train loss: 0.47673410177230835\n",
      "Epoch 4045: train loss: 0.4767337441444397\n",
      "Epoch 4046: train loss: 0.47673341631889343\n",
      "Epoch 4047: train loss: 0.4767329692840576\n",
      "Epoch 4048: train loss: 0.47673264145851135\n",
      "Epoch 4049: train loss: 0.4767323434352875\n",
      "Epoch 4050: train loss: 0.47673192620277405\n",
      "Epoch 4051: train loss: 0.476731538772583\n",
      "Epoch 4052: train loss: 0.47673118114471436\n",
      "Epoch 4053: train loss: 0.4767307639122009\n",
      "Epoch 4054: train loss: 0.47673046588897705\n",
      "Epoch 4055: train loss: 0.476730078458786\n",
      "Epoch 4056: train loss: 0.47672972083091736\n",
      "Epoch 4057: train loss: 0.4767293632030487\n",
      "Epoch 4058: train loss: 0.47672897577285767\n",
      "Epoch 4059: train loss: 0.476728618144989\n",
      "Epoch 4060: train loss: 0.47672832012176514\n",
      "Epoch 4061: train loss: 0.4767279028892517\n",
      "Epoch 4062: train loss: 0.47672751545906067\n",
      "Epoch 4063: train loss: 0.476727157831192\n",
      "Epoch 4064: train loss: 0.47672680020332336\n",
      "Epoch 4065: train loss: 0.4767264127731323\n",
      "Epoch 4066: train loss: 0.47672605514526367\n",
      "Epoch 4067: train loss: 0.4767257571220398\n",
      "Epoch 4068: train loss: 0.47672533988952637\n",
      "Epoch 4069: train loss: 0.4767249524593353\n",
      "Epoch 4070: train loss: 0.47672465443611145\n",
      "Epoch 4071: train loss: 0.476724237203598\n",
      "Epoch 4072: train loss: 0.47672387957572937\n",
      "Epoch 4073: train loss: 0.47672349214553833\n",
      "Epoch 4074: train loss: 0.4767231345176697\n",
      "Epoch 4075: train loss: 0.4767228364944458\n",
      "Epoch 4076: train loss: 0.47672238945961\n",
      "Epoch 4077: train loss: 0.4767220914363861\n",
      "Epoch 4078: train loss: 0.4767216742038727\n",
      "Epoch 4079: train loss: 0.4767213463783264\n",
      "Epoch 4080: train loss: 0.47672098875045776\n",
      "Epoch 4081: train loss: 0.4767206311225891\n",
      "Epoch 4082: train loss: 0.47672027349472046\n",
      "Epoch 4083: train loss: 0.4767198860645294\n",
      "Epoch 4084: train loss: 0.47671952843666077\n",
      "Epoch 4085: train loss: 0.4767191708087921\n",
      "Epoch 4086: train loss: 0.4767187833786011\n",
      "Epoch 4087: train loss: 0.4767184257507324\n",
      "Epoch 4088: train loss: 0.47671806812286377\n",
      "Epoch 4089: train loss: 0.4767177104949951\n",
      "Epoch 4090: train loss: 0.47671738266944885\n",
      "Epoch 4091: train loss: 0.4767170250415802\n",
      "Epoch 4092: train loss: 0.4767166078090668\n",
      "Epoch 4093: train loss: 0.4767162799835205\n",
      "Epoch 4094: train loss: 0.4767158627510071\n",
      "Epoch 4095: train loss: 0.4767155647277832\n",
      "Epoch 4096: train loss: 0.47671520709991455\n",
      "Epoch 4097: train loss: 0.4767148196697235\n",
      "Epoch 4098: train loss: 0.47671446204185486\n",
      "Epoch 4099: train loss: 0.4767141044139862\n",
      "Epoch 4100: train loss: 0.47671377658843994\n",
      "Epoch 4101: train loss: 0.4767133593559265\n",
      "Epoch 4102: train loss: 0.47671300172805786\n",
      "Epoch 4103: train loss: 0.4767126441001892\n",
      "Epoch 4104: train loss: 0.47671231627464294\n",
      "Epoch 4105: train loss: 0.4767119586467743\n",
      "Epoch 4106: train loss: 0.47671154141426086\n",
      "Epoch 4107: train loss: 0.4767112135887146\n",
      "Epoch 4108: train loss: 0.47671085596084595\n",
      "Epoch 4109: train loss: 0.4767104983329773\n",
      "Epoch 4110: train loss: 0.47671011090278625\n",
      "Epoch 4111: train loss: 0.4767097532749176\n",
      "Epoch 4112: train loss: 0.4767094552516937\n",
      "Epoch 4113: train loss: 0.4767090678215027\n",
      "Epoch 4114: train loss: 0.47670871019363403\n",
      "Epoch 4115: train loss: 0.4767083525657654\n",
      "Epoch 4116: train loss: 0.47670799493789673\n",
      "Epoch 4117: train loss: 0.4767076075077057\n",
      "Epoch 4118: train loss: 0.4767073094844818\n",
      "Epoch 4119: train loss: 0.47670692205429077\n",
      "Epoch 4120: train loss: 0.4767065644264221\n",
      "Epoch 4121: train loss: 0.47670626640319824\n",
      "Epoch 4122: train loss: 0.4767058491706848\n",
      "Epoch 4123: train loss: 0.4767054617404938\n",
      "Epoch 4124: train loss: 0.4767051637172699\n",
      "Epoch 4125: train loss: 0.47670480608940125\n",
      "Epoch 4126: train loss: 0.4767044186592102\n",
      "Epoch 4127: train loss: 0.47670406103134155\n",
      "Epoch 4128: train loss: 0.4767037034034729\n",
      "Epoch 4129: train loss: 0.47670337557792664\n",
      "Epoch 4130: train loss: 0.476703017950058\n",
      "Epoch 4131: train loss: 0.4767026901245117\n",
      "Epoch 4132: train loss: 0.4767022430896759\n",
      "Epoch 4133: train loss: 0.47670191526412964\n",
      "Epoch 4134: train loss: 0.476701557636261\n",
      "Epoch 4135: train loss: 0.4767012298107147\n",
      "Epoch 4136: train loss: 0.47670087218284607\n",
      "Epoch 4137: train loss: 0.4767005741596222\n",
      "Epoch 4138: train loss: 0.47670015692710876\n",
      "Epoch 4139: train loss: 0.4766998291015625\n",
      "Epoch 4140: train loss: 0.47669947147369385\n",
      "Epoch 4141: train loss: 0.4766990840435028\n",
      "Epoch 4142: train loss: 0.47669878602027893\n",
      "Epoch 4143: train loss: 0.4766984283924103\n",
      "Epoch 4144: train loss: 0.47669804096221924\n",
      "Epoch 4145: train loss: 0.47669774293899536\n",
      "Epoch 4146: train loss: 0.4766973853111267\n",
      "Epoch 4147: train loss: 0.47669699788093567\n",
      "Epoch 4148: train loss: 0.476696640253067\n",
      "Epoch 4149: train loss: 0.47669628262519836\n",
      "Epoch 4150: train loss: 0.4766958951950073\n",
      "Epoch 4151: train loss: 0.47669559717178345\n",
      "Epoch 4152: train loss: 0.4766952395439148\n",
      "Epoch 4153: train loss: 0.47669485211372375\n",
      "Epoch 4154: train loss: 0.4766945540904999\n",
      "Epoch 4155: train loss: 0.4766941964626312\n",
      "Epoch 4156: train loss: 0.4766938090324402\n",
      "Epoch 4157: train loss: 0.47669345140457153\n",
      "Epoch 4158: train loss: 0.47669315338134766\n",
      "Epoch 4159: train loss: 0.4766927659511566\n",
      "Epoch 4160: train loss: 0.47669246792793274\n",
      "Epoch 4161: train loss: 0.4766920804977417\n",
      "Epoch 4162: train loss: 0.47669172286987305\n",
      "Epoch 4163: train loss: 0.4766913652420044\n",
      "Epoch 4164: train loss: 0.47669103741645813\n",
      "Epoch 4165: train loss: 0.4766906797885895\n",
      "Epoch 4166: train loss: 0.4766903221607208\n",
      "Epoch 4167: train loss: 0.4766899645328522\n",
      "Epoch 4168: train loss: 0.47668957710266113\n",
      "Epoch 4169: train loss: 0.4766892194747925\n",
      "Epoch 4170: train loss: 0.4766889214515686\n",
      "Epoch 4171: train loss: 0.47668859362602234\n",
      "Epoch 4172: train loss: 0.4766882359981537\n",
      "Epoch 4173: train loss: 0.47668784856796265\n",
      "Epoch 4174: train loss: 0.476687490940094\n",
      "Epoch 4175: train loss: 0.4766871929168701\n",
      "Epoch 4176: train loss: 0.4766868054866791\n",
      "Epoch 4177: train loss: 0.4766864478588104\n",
      "Epoch 4178: train loss: 0.47668617963790894\n",
      "Epoch 4179: train loss: 0.4766858220100403\n",
      "Epoch 4180: train loss: 0.47668540477752686\n",
      "Epoch 4181: train loss: 0.476685106754303\n",
      "Epoch 4182: train loss: 0.47668471932411194\n",
      "Epoch 4183: train loss: 0.47668442130088806\n",
      "Epoch 4184: train loss: 0.476684033870697\n",
      "Epoch 4185: train loss: 0.47668373584747314\n",
      "Epoch 4186: train loss: 0.4766833186149597\n",
      "Epoch 4187: train loss: 0.47668299078941345\n",
      "Epoch 4188: train loss: 0.4766826927661896\n",
      "Epoch 4189: train loss: 0.4766823351383209\n",
      "Epoch 4190: train loss: 0.4766819477081299\n",
      "Epoch 4191: train loss: 0.47668159008026123\n",
      "Epoch 4192: train loss: 0.47668129205703735\n",
      "Epoch 4193: train loss: 0.4766809046268463\n",
      "Epoch 4194: train loss: 0.47668060660362244\n",
      "Epoch 4195: train loss: 0.4766802191734314\n",
      "Epoch 4196: train loss: 0.47667986154556274\n",
      "Epoch 4197: train loss: 0.47667956352233887\n",
      "Epoch 4198: train loss: 0.4766792356967926\n",
      "Epoch 4199: train loss: 0.4766788184642792\n",
      "Epoch 4200: train loss: 0.4766785204410553\n",
      "Epoch 4201: train loss: 0.47667819261550903\n",
      "Epoch 4202: train loss: 0.47667789459228516\n",
      "Epoch 4203: train loss: 0.4766775071620941\n",
      "Epoch 4204: train loss: 0.47667714953422546\n",
      "Epoch 4205: train loss: 0.4766767919063568\n",
      "Epoch 4206: train loss: 0.47667640447616577\n",
      "Epoch 4207: train loss: 0.4766761064529419\n",
      "Epoch 4208: train loss: 0.47667577862739563\n",
      "Epoch 4209: train loss: 0.476675420999527\n",
      "Epoch 4210: train loss: 0.4766750633716583\n",
      "Epoch 4211: train loss: 0.47667473554611206\n",
      "Epoch 4212: train loss: 0.4766743779182434\n",
      "Epoch 4213: train loss: 0.47667402029037476\n",
      "Epoch 4214: train loss: 0.4766736924648285\n",
      "Epoch 4215: train loss: 0.4766733944416046\n",
      "Epoch 4216: train loss: 0.4766729772090912\n",
      "Epoch 4217: train loss: 0.4766726493835449\n",
      "Epoch 4218: train loss: 0.47667235136032104\n",
      "Epoch 4219: train loss: 0.47667196393013\n",
      "Epoch 4220: train loss: 0.47667160630226135\n",
      "Epoch 4221: train loss: 0.4766713082790375\n",
      "Epoch 4222: train loss: 0.4766709804534912\n",
      "Epoch 4223: train loss: 0.4766705632209778\n",
      "Epoch 4224: train loss: 0.4766702651977539\n",
      "Epoch 4225: train loss: 0.47666987776756287\n",
      "Epoch 4226: train loss: 0.4766695201396942\n",
      "Epoch 4227: train loss: 0.4766692519187927\n",
      "Epoch 4228: train loss: 0.4766688346862793\n",
      "Epoch 4229: train loss: 0.4766685366630554\n",
      "Epoch 4230: train loss: 0.47666820883750916\n",
      "Epoch 4231: train loss: 0.4766678512096405\n",
      "Epoch 4232: train loss: 0.47666752338409424\n",
      "Epoch 4233: train loss: 0.47666722536087036\n",
      "Epoch 4234: train loss: 0.47666680812835693\n",
      "Epoch 4235: train loss: 0.47666648030281067\n",
      "Epoch 4236: train loss: 0.4766661822795868\n",
      "Epoch 4237: train loss: 0.47666582465171814\n",
      "Epoch 4238: train loss: 0.4766654372215271\n",
      "Epoch 4239: train loss: 0.4766651391983032\n",
      "Epoch 4240: train loss: 0.47666481137275696\n",
      "Epoch 4241: train loss: 0.47666439414024353\n",
      "Epoch 4242: train loss: 0.47666409611701965\n",
      "Epoch 4243: train loss: 0.4766637682914734\n",
      "Epoch 4244: train loss: 0.47666341066360474\n",
      "Epoch 4245: train loss: 0.47666308283805847\n",
      "Epoch 4246: train loss: 0.4766627848148346\n",
      "Epoch 4247: train loss: 0.47666242718696594\n",
      "Epoch 4248: train loss: 0.4766620993614197\n",
      "Epoch 4249: train loss: 0.4766618013381958\n",
      "Epoch 4250: train loss: 0.47666141390800476\n",
      "Epoch 4251: train loss: 0.4766610562801361\n",
      "Epoch 4252: train loss: 0.47666072845458984\n",
      "Epoch 4253: train loss: 0.47666043043136597\n",
      "Epoch 4254: train loss: 0.4766600728034973\n",
      "Epoch 4255: train loss: 0.4766596853733063\n",
      "Epoch 4256: train loss: 0.4766593873500824\n",
      "Epoch 4257: train loss: 0.47665902972221375\n",
      "Epoch 4258: train loss: 0.4766587018966675\n",
      "Epoch 4259: train loss: 0.4766584038734436\n",
      "Epoch 4260: train loss: 0.47665801644325256\n",
      "Epoch 4261: train loss: 0.4766576588153839\n",
      "Epoch 4262: train loss: 0.47665730118751526\n",
      "Epoch 4263: train loss: 0.476656973361969\n",
      "Epoch 4264: train loss: 0.4766567051410675\n",
      "Epoch 4265: train loss: 0.4766562879085541\n",
      "Epoch 4266: train loss: 0.476656049489975\n",
      "Epoch 4267: train loss: 0.47665566205978394\n",
      "Epoch 4268: train loss: 0.47665536403656006\n",
      "Epoch 4269: train loss: 0.47665494680404663\n",
      "Epoch 4270: train loss: 0.47665467858314514\n",
      "Epoch 4271: train loss: 0.4766543209552765\n",
      "Epoch 4272: train loss: 0.4766539931297302\n",
      "Epoch 4273: train loss: 0.4766536355018616\n",
      "Epoch 4274: train loss: 0.4766533076763153\n",
      "Epoch 4275: train loss: 0.47665300965309143\n",
      "Epoch 4276: train loss: 0.4766526520252228\n",
      "Epoch 4277: train loss: 0.47665226459503174\n",
      "Epoch 4278: train loss: 0.47665202617645264\n",
      "Epoch 4279: train loss: 0.4766516089439392\n",
      "Epoch 4280: train loss: 0.4766513407230377\n",
      "Epoch 4281: train loss: 0.47665098309516907\n",
      "Epoch 4282: train loss: 0.476650595664978\n",
      "Epoch 4283: train loss: 0.47665029764175415\n",
      "Epoch 4284: train loss: 0.4766499698162079\n",
      "Epoch 4285: train loss: 0.47664955258369446\n",
      "Epoch 4286: train loss: 0.47664928436279297\n",
      "Epoch 4287: train loss: 0.4766489267349243\n",
      "Epoch 4288: train loss: 0.47664862871170044\n",
      "Epoch 4289: train loss: 0.4766483008861542\n",
      "Epoch 4290: train loss: 0.4766479432582855\n",
      "Epoch 4291: train loss: 0.47664758563041687\n",
      "Epoch 4292: train loss: 0.4766473174095154\n",
      "Epoch 4293: train loss: 0.47664695978164673\n",
      "Epoch 4294: train loss: 0.4766465723514557\n",
      "Epoch 4295: train loss: 0.4766462743282318\n",
      "Epoch 4296: train loss: 0.47664588689804077\n",
      "Epoch 4297: train loss: 0.47664564847946167\n",
      "Epoch 4298: train loss: 0.47664526104927063\n",
      "Epoch 4299: train loss: 0.47664496302604675\n",
      "Epoch 4300: train loss: 0.4766446053981781\n",
      "Epoch 4301: train loss: 0.4766443371772766\n",
      "Epoch 4302: train loss: 0.47664397954940796\n",
      "Epoch 4303: train loss: 0.4766435921192169\n",
      "Epoch 4304: train loss: 0.47664329409599304\n",
      "Epoch 4305: train loss: 0.4766429662704468\n",
      "Epoch 4306: train loss: 0.4766426086425781\n",
      "Epoch 4307: train loss: 0.47664228081703186\n",
      "Epoch 4308: train loss: 0.476641982793808\n",
      "Epoch 4309: train loss: 0.47664162516593933\n",
      "Epoch 4310: train loss: 0.47664129734039307\n",
      "Epoch 4311: train loss: 0.4766409993171692\n",
      "Epoch 4312: train loss: 0.47664061188697815\n",
      "Epoch 4313: train loss: 0.4766403138637543\n",
      "Epoch 4314: train loss: 0.4766399562358856\n",
      "Epoch 4315: train loss: 0.47663968801498413\n",
      "Epoch 4316: train loss: 0.4766393303871155\n",
      "Epoch 4317: train loss: 0.47663894295692444\n",
      "Epoch 4318: train loss: 0.47663864493370056\n",
      "Epoch 4319: train loss: 0.4766382575035095\n",
      "Epoch 4320: train loss: 0.47663795948028564\n",
      "Epoch 4321: train loss: 0.4766376316547394\n",
      "Epoch 4322: train loss: 0.4766372740268707\n",
      "Epoch 4323: train loss: 0.47663700580596924\n",
      "Epoch 4324: train loss: 0.47663670778274536\n",
      "Epoch 4325: train loss: 0.4766363501548767\n",
      "Epoch 4326: train loss: 0.47663596272468567\n",
      "Epoch 4327: train loss: 0.4766356647014618\n",
      "Epoch 4328: train loss: 0.4766353368759155\n",
      "Epoch 4329: train loss: 0.4766349792480469\n",
      "Epoch 4330: train loss: 0.4766346514225006\n",
      "Epoch 4331: train loss: 0.47663435339927673\n",
      "Epoch 4332: train loss: 0.47663402557373047\n",
      "Epoch 4333: train loss: 0.4766336679458618\n",
      "Epoch 4334: train loss: 0.47663336992263794\n",
      "Epoch 4335: train loss: 0.4766329824924469\n",
      "Epoch 4336: train loss: 0.476632684469223\n",
      "Epoch 4337: train loss: 0.47663235664367676\n",
      "Epoch 4338: train loss: 0.4766320586204529\n",
      "Epoch 4339: train loss: 0.47663170099258423\n",
      "Epoch 4340: train loss: 0.47663137316703796\n",
      "Epoch 4341: train loss: 0.4766310453414917\n",
      "Epoch 4342: train loss: 0.4766307473182678\n",
      "Epoch 4343: train loss: 0.47663038969039917\n",
      "Epoch 4344: train loss: 0.4766300618648529\n",
      "Epoch 4345: train loss: 0.47662976384162903\n",
      "Epoch 4346: train loss: 0.476629376411438\n",
      "Epoch 4347: train loss: 0.4766290783882141\n",
      "Epoch 4348: train loss: 0.47662875056266785\n",
      "Epoch 4349: train loss: 0.47662845253944397\n",
      "Epoch 4350: train loss: 0.4766280949115753\n",
      "Epoch 4351: train loss: 0.47662776708602905\n",
      "Epoch 4352: train loss: 0.4766274094581604\n",
      "Epoch 4353: train loss: 0.4766271412372589\n",
      "Epoch 4354: train loss: 0.47662678360939026\n",
      "Epoch 4355: train loss: 0.476626455783844\n",
      "Epoch 4356: train loss: 0.4766261577606201\n",
      "Epoch 4357: train loss: 0.47662582993507385\n",
      "Epoch 4358: train loss: 0.47662553191185\n",
      "Epoch 4359: train loss: 0.47662514448165894\n",
      "Epoch 4360: train loss: 0.4766247868537903\n",
      "Epoch 4361: train loss: 0.4766245186328888\n",
      "Epoch 4362: train loss: 0.4766242206096649\n",
      "Epoch 4363: train loss: 0.47662386298179626\n",
      "Epoch 4364: train loss: 0.4766235947608948\n",
      "Epoch 4365: train loss: 0.47662320733070374\n",
      "Epoch 4366: train loss: 0.47662290930747986\n",
      "Epoch 4367: train loss: 0.4766225516796112\n",
      "Epoch 4368: train loss: 0.47662222385406494\n",
      "Epoch 4369: train loss: 0.47662192583084106\n",
      "Epoch 4370: train loss: 0.47662153840065\n",
      "Epoch 4371: train loss: 0.4766212999820709\n",
      "Epoch 4372: train loss: 0.4766209125518799\n",
      "Epoch 4373: train loss: 0.47662055492401123\n",
      "Epoch 4374: train loss: 0.47662028670310974\n",
      "Epoch 4375: train loss: 0.4766199290752411\n",
      "Epoch 4376: train loss: 0.4766196012496948\n",
      "Epoch 4377: train loss: 0.47661930322647095\n",
      "Epoch 4378: train loss: 0.4766189754009247\n",
      "Epoch 4379: train loss: 0.4766186773777008\n",
      "Epoch 4380: train loss: 0.47661834955215454\n",
      "Epoch 4381: train loss: 0.4766179919242859\n",
      "Epoch 4382: train loss: 0.476617693901062\n",
      "Epoch 4383: train loss: 0.47661736607551575\n",
      "Epoch 4384: train loss: 0.47661706805229187\n",
      "Epoch 4385: train loss: 0.4766167402267456\n",
      "Epoch 4386: train loss: 0.47661638259887695\n",
      "Epoch 4387: train loss: 0.4766160547733307\n",
      "Epoch 4388: train loss: 0.4766157567501068\n",
      "Epoch 4389: train loss: 0.47661542892456055\n",
      "Epoch 4390: train loss: 0.47661513090133667\n",
      "Epoch 4391: train loss: 0.4766148030757904\n",
      "Epoch 4392: train loss: 0.47661444544792175\n",
      "Epoch 4393: train loss: 0.4766141176223755\n",
      "Epoch 4394: train loss: 0.4766138195991516\n",
      "Epoch 4395: train loss: 0.47661349177360535\n",
      "Epoch 4396: train loss: 0.47661319375038147\n",
      "Epoch 4397: train loss: 0.4766128659248352\n",
      "Epoch 4398: train loss: 0.47661256790161133\n",
      "Epoch 4399: train loss: 0.4766122102737427\n",
      "Epoch 4400: train loss: 0.4766118824481964\n",
      "Epoch 4401: train loss: 0.47661152482032776\n",
      "Epoch 4402: train loss: 0.47661125659942627\n",
      "Epoch 4403: train loss: 0.47661092877388\n",
      "Epoch 4404: train loss: 0.47661057114601135\n",
      "Epoch 4405: train loss: 0.4766102731227875\n",
      "Epoch 4406: train loss: 0.4766099452972412\n",
      "Epoch 4407: train loss: 0.47660964727401733\n",
      "Epoch 4408: train loss: 0.47660931944847107\n",
      "Epoch 4409: train loss: 0.4766090214252472\n",
      "Epoch 4410: train loss: 0.4766086935997009\n",
      "Epoch 4411: train loss: 0.47660839557647705\n",
      "Epoch 4412: train loss: 0.4766080677509308\n",
      "Epoch 4413: train loss: 0.47660771012306213\n",
      "Epoch 4414: train loss: 0.47660738229751587\n",
      "Epoch 4415: train loss: 0.4766071140766144\n",
      "Epoch 4416: train loss: 0.4766067564487457\n",
      "Epoch 4417: train loss: 0.47660645842552185\n",
      "Epoch 4418: train loss: 0.4766061305999756\n",
      "Epoch 4419: train loss: 0.4766058325767517\n",
      "Epoch 4420: train loss: 0.47660550475120544\n",
      "Epoch 4421: train loss: 0.4766051471233368\n",
      "Epoch 4422: train loss: 0.4766048789024353\n",
      "Epoch 4423: train loss: 0.47660452127456665\n",
      "Epoch 4424: train loss: 0.4766041934490204\n",
      "Epoch 4425: train loss: 0.4766038954257965\n",
      "Epoch 4426: train loss: 0.47660356760025024\n",
      "Epoch 4427: train loss: 0.47660326957702637\n",
      "Epoch 4428: train loss: 0.4766028821468353\n",
      "Epoch 4429: train loss: 0.4766026437282562\n",
      "Epoch 4430: train loss: 0.47660231590270996\n",
      "Epoch 4431: train loss: 0.4766020178794861\n",
      "Epoch 4432: train loss: 0.4766016900539398\n",
      "Epoch 4433: train loss: 0.47660139203071594\n",
      "Epoch 4434: train loss: 0.4766010642051697\n",
      "Epoch 4435: train loss: 0.4766007661819458\n",
      "Epoch 4436: train loss: 0.47660037875175476\n",
      "Epoch 4437: train loss: 0.4766000807285309\n",
      "Epoch 4438: train loss: 0.47659969329833984\n",
      "Epoch 4439: train loss: 0.47659945487976074\n",
      "Epoch 4440: train loss: 0.4765990674495697\n",
      "Epoch 4441: train loss: 0.4765987694263458\n",
      "Epoch 4442: train loss: 0.47659850120544434\n",
      "Epoch 4443: train loss: 0.4765981435775757\n",
      "Epoch 4444: train loss: 0.4765978157520294\n",
      "Epoch 4445: train loss: 0.47659751772880554\n",
      "Epoch 4446: train loss: 0.47659724950790405\n",
      "Epoch 4447: train loss: 0.4765968918800354\n",
      "Epoch 4448: train loss: 0.47659656405448914\n",
      "Epoch 4449: train loss: 0.47659626603126526\n",
      "Epoch 4450: train loss: 0.476595938205719\n",
      "Epoch 4451: train loss: 0.4765956401824951\n",
      "Epoch 4452: train loss: 0.47659531235694885\n",
      "Epoch 4453: train loss: 0.476595014333725\n",
      "Epoch 4454: train loss: 0.4765947461128235\n",
      "Epoch 4455: train loss: 0.47659438848495483\n",
      "Epoch 4456: train loss: 0.4765940010547638\n",
      "Epoch 4457: train loss: 0.4765937626361847\n",
      "Epoch 4458: train loss: 0.47659337520599365\n",
      "Epoch 4459: train loss: 0.4765930771827698\n",
      "Epoch 4460: train loss: 0.4765928089618683\n",
      "Epoch 4461: train loss: 0.476592481136322\n",
      "Epoch 4462: train loss: 0.47659218311309814\n",
      "Epoch 4463: train loss: 0.4765918254852295\n",
      "Epoch 4464: train loss: 0.4765914976596832\n",
      "Epoch 4465: train loss: 0.47659122943878174\n",
      "Epoch 4466: train loss: 0.47659093141555786\n",
      "Epoch 4467: train loss: 0.4765906035900116\n",
      "Epoch 4468: train loss: 0.4765903055667877\n",
      "Epoch 4469: train loss: 0.47658994793891907\n",
      "Epoch 4470: train loss: 0.4765896797180176\n",
      "Epoch 4471: train loss: 0.4765893220901489\n",
      "Epoch 4472: train loss: 0.47658905386924744\n",
      "Epoch 4473: train loss: 0.47658872604370117\n",
      "Epoch 4474: train loss: 0.4765883684158325\n",
      "Epoch 4475: train loss: 0.47658810019493103\n",
      "Epoch 4476: train loss: 0.47658780217170715\n",
      "Epoch 4477: train loss: 0.4765874743461609\n",
      "Epoch 4478: train loss: 0.47658711671829224\n",
      "Epoch 4479: train loss: 0.47658684849739075\n",
      "Epoch 4480: train loss: 0.47658655047416687\n",
      "Epoch 4481: train loss: 0.47658616304397583\n",
      "Epoch 4482: train loss: 0.47658592462539673\n",
      "Epoch 4483: train loss: 0.47658559679985046\n",
      "Epoch 4484: train loss: 0.4765852391719818\n",
      "Epoch 4485: train loss: 0.4765850305557251\n",
      "Epoch 4486: train loss: 0.47658464312553406\n",
      "Epoch 4487: train loss: 0.4765843451023102\n",
      "Epoch 4488: train loss: 0.4765840172767639\n",
      "Epoch 4489: train loss: 0.47658365964889526\n",
      "Epoch 4490: train loss: 0.4765833616256714\n",
      "Epoch 4491: train loss: 0.4765830934047699\n",
      "Epoch 4492: train loss: 0.47658273577690125\n",
      "Epoch 4493: train loss: 0.47658246755599976\n",
      "Epoch 4494: train loss: 0.4765821099281311\n",
      "Epoch 4495: train loss: 0.4765818417072296\n",
      "Epoch 4496: train loss: 0.47658151388168335\n",
      "Epoch 4497: train loss: 0.47658124566078186\n",
      "Epoch 4498: train loss: 0.476580947637558\n",
      "Epoch 4499: train loss: 0.47658059000968933\n",
      "Epoch 4500: train loss: 0.47658026218414307\n",
      "Epoch 4501: train loss: 0.4765799939632416\n",
      "Epoch 4502: train loss: 0.4765796363353729\n",
      "Epoch 4503: train loss: 0.47657933831214905\n",
      "Epoch 4504: train loss: 0.4765790104866028\n",
      "Epoch 4505: train loss: 0.4765787124633789\n",
      "Epoch 4506: train loss: 0.47657838463783264\n",
      "Epoch 4507: train loss: 0.47657808661460876\n",
      "Epoch 4508: train loss: 0.4765778183937073\n",
      "Epoch 4509: train loss: 0.47657743096351624\n",
      "Epoch 4510: train loss: 0.47657719254493713\n",
      "Epoch 4511: train loss: 0.47657686471939087\n",
      "Epoch 4512: train loss: 0.476576566696167\n",
      "Epoch 4513: train loss: 0.4765762984752655\n",
      "Epoch 4514: train loss: 0.47657597064971924\n",
      "Epoch 4515: train loss: 0.4765756130218506\n",
      "Epoch 4516: train loss: 0.4765753149986267\n",
      "Epoch 4517: train loss: 0.47657498717308044\n",
      "Epoch 4518: train loss: 0.47657468914985657\n",
      "Epoch 4519: train loss: 0.4765743613243103\n",
      "Epoch 4520: train loss: 0.4765740931034088\n",
      "Epoch 4521: train loss: 0.47657373547554016\n",
      "Epoch 4522: train loss: 0.47657346725463867\n",
      "Epoch 4523: train loss: 0.4765731692314148\n",
      "Epoch 4524: train loss: 0.47657284140586853\n",
      "Epoch 4525: train loss: 0.47657254338264465\n",
      "Epoch 4526: train loss: 0.4765722155570984\n",
      "Epoch 4527: train loss: 0.4765719175338745\n",
      "Epoch 4528: train loss: 0.47657158970832825\n",
      "Epoch 4529: train loss: 0.47657129168510437\n",
      "Epoch 4530: train loss: 0.4765710234642029\n",
      "Epoch 4531: train loss: 0.4765706956386566\n",
      "Epoch 4532: train loss: 0.47657039761543274\n",
      "Epoch 4533: train loss: 0.4765700697898865\n",
      "Epoch 4534: train loss: 0.4765697717666626\n",
      "Epoch 4535: train loss: 0.47656944394111633\n",
      "Epoch 4536: train loss: 0.47656914591789246\n",
      "Epoch 4537: train loss: 0.47656887769699097\n",
      "Epoch 4538: train loss: 0.4765685200691223\n",
      "Epoch 4539: train loss: 0.4765682518482208\n",
      "Epoch 4540: train loss: 0.47656792402267456\n",
      "Epoch 4541: train loss: 0.4765676259994507\n",
      "Epoch 4542: train loss: 0.4765672981739044\n",
      "Epoch 4543: train loss: 0.47656700015068054\n",
      "Epoch 4544: train loss: 0.4765666723251343\n",
      "Epoch 4545: train loss: 0.4765664339065552\n",
      "Epoch 4546: train loss: 0.47656604647636414\n",
      "Epoch 4547: train loss: 0.47656577825546265\n",
      "Epoch 4548: train loss: 0.476565420627594\n",
      "Epoch 4549: train loss: 0.4765651524066925\n",
      "Epoch 4550: train loss: 0.47656485438346863\n",
      "Epoch 4551: train loss: 0.4765644967556\n",
      "Epoch 4552: train loss: 0.4765642285346985\n",
      "Epoch 4553: train loss: 0.476563960313797\n",
      "Epoch 4554: train loss: 0.4765636622905731\n",
      "Epoch 4555: train loss: 0.47656333446502686\n",
      "Epoch 4556: train loss: 0.476563036441803\n",
      "Epoch 4557: train loss: 0.4765627086162567\n",
      "Epoch 4558: train loss: 0.47656238079071045\n",
      "Epoch 4559: train loss: 0.4765620827674866\n",
      "Epoch 4560: train loss: 0.4765618145465851\n",
      "Epoch 4561: train loss: 0.4765615165233612\n",
      "Epoch 4562: train loss: 0.47656112909317017\n",
      "Epoch 4563: train loss: 0.4765608310699463\n",
      "Epoch 4564: train loss: 0.4765605628490448\n",
      "Epoch 4565: train loss: 0.4765602648258209\n",
      "Epoch 4566: train loss: 0.47655999660491943\n",
      "Epoch 4567: train loss: 0.47655966877937317\n",
      "Epoch 4568: train loss: 0.4765593707561493\n",
      "Epoch 4569: train loss: 0.476559042930603\n",
      "Epoch 4570: train loss: 0.47655874490737915\n",
      "Epoch 4571: train loss: 0.4765584170818329\n",
      "Epoch 4572: train loss: 0.4765581488609314\n",
      "Epoch 4573: train loss: 0.47655779123306274\n",
      "Epoch 4574: train loss: 0.47655752301216125\n",
      "Epoch 4575: train loss: 0.4765572249889374\n",
      "Epoch 4576: train loss: 0.4765568971633911\n",
      "Epoch 4577: train loss: 0.476556658744812\n",
      "Epoch 4578: train loss: 0.47655633091926575\n",
      "Epoch 4579: train loss: 0.47655603289604187\n",
      "Epoch 4580: train loss: 0.4765557646751404\n",
      "Epoch 4581: train loss: 0.47655540704727173\n",
      "Epoch 4582: train loss: 0.47655513882637024\n",
      "Epoch 4583: train loss: 0.4765547513961792\n",
      "Epoch 4584: train loss: 0.4765545129776001\n",
      "Epoch 4585: train loss: 0.4765542447566986\n",
      "Epoch 4586: train loss: 0.47655388712882996\n",
      "Epoch 4587: train loss: 0.47655361890792847\n",
      "Epoch 4588: train loss: 0.4765532910823822\n",
      "Epoch 4589: train loss: 0.4765529930591583\n",
      "Epoch 4590: train loss: 0.47655266523361206\n",
      "Epoch 4591: train loss: 0.4765523672103882\n",
      "Epoch 4592: train loss: 0.4765520989894867\n",
      "Epoch 4593: train loss: 0.4765518009662628\n",
      "Epoch 4594: train loss: 0.47655147314071655\n",
      "Epoch 4595: train loss: 0.47655120491981506\n",
      "Epoch 4596: train loss: 0.4765509068965912\n",
      "Epoch 4597: train loss: 0.47655051946640015\n",
      "Epoch 4598: train loss: 0.47655028104782104\n",
      "Epoch 4599: train loss: 0.4765499532222748\n",
      "Epoch 4600: train loss: 0.4765496850013733\n",
      "Epoch 4601: train loss: 0.47654932737350464\n",
      "Epoch 4602: train loss: 0.47654902935028076\n",
      "Epoch 4603: train loss: 0.4765487611293793\n",
      "Epoch 4604: train loss: 0.476548433303833\n",
      "Epoch 4605: train loss: 0.4765481948852539\n",
      "Epoch 4606: train loss: 0.47654786705970764\n",
      "Epoch 4607: train loss: 0.47654756903648376\n",
      "Epoch 4608: train loss: 0.4765472412109375\n",
      "Epoch 4609: train loss: 0.476546972990036\n",
      "Epoch 4610: train loss: 0.47654667496681213\n",
      "Epoch 4611: train loss: 0.47654640674591064\n",
      "Epoch 4612: train loss: 0.4765460789203644\n",
      "Epoch 4613: train loss: 0.4765457212924957\n",
      "Epoch 4614: train loss: 0.47654545307159424\n",
      "Epoch 4615: train loss: 0.47654521465301514\n",
      "Epoch 4616: train loss: 0.4765448272228241\n",
      "Epoch 4617: train loss: 0.476544588804245\n",
      "Epoch 4618: train loss: 0.47654426097869873\n",
      "Epoch 4619: train loss: 0.47654399275779724\n",
      "Epoch 4620: train loss: 0.47654369473457336\n",
      "Epoch 4621: train loss: 0.4765433669090271\n",
      "Epoch 4622: train loss: 0.4765430688858032\n",
      "Epoch 4623: train loss: 0.47654280066490173\n",
      "Epoch 4624: train loss: 0.47654247283935547\n",
      "Epoch 4625: train loss: 0.4765421748161316\n",
      "Epoch 4626: train loss: 0.4765418469905853\n",
      "Epoch 4627: train loss: 0.4765416085720062\n",
      "Epoch 4628: train loss: 0.47654134035110474\n",
      "Epoch 4629: train loss: 0.4765409827232361\n",
      "Epoch 4630: train loss: 0.4765407145023346\n",
      "Epoch 4631: train loss: 0.47654038667678833\n",
      "Epoch 4632: train loss: 0.47654008865356445\n",
      "Epoch 4633: train loss: 0.4765397608280182\n",
      "Epoch 4634: train loss: 0.4765394926071167\n",
      "Epoch 4635: train loss: 0.4765391945838928\n",
      "Epoch 4636: train loss: 0.47653892636299133\n",
      "Epoch 4637: train loss: 0.4765385687351227\n",
      "Epoch 4638: train loss: 0.47653836011886597\n",
      "Epoch 4639: train loss: 0.4765380024909973\n",
      "Epoch 4640: train loss: 0.47653767466545105\n",
      "Epoch 4641: train loss: 0.47653740644454956\n",
      "Epoch 4642: train loss: 0.4765371084213257\n",
      "Epoch 4643: train loss: 0.4765367805957794\n",
      "Epoch 4644: train loss: 0.4765365719795227\n",
      "Epoch 4645: train loss: 0.47653621435165405\n",
      "Epoch 4646: train loss: 0.4765359163284302\n",
      "Epoch 4647: train loss: 0.4765356481075287\n",
      "Epoch 4648: train loss: 0.4765353202819824\n",
      "Epoch 4649: train loss: 0.47653505206108093\n",
      "Epoch 4650: train loss: 0.47653475403785706\n",
      "Epoch 4651: train loss: 0.47653448581695557\n",
      "Epoch 4652: train loss: 0.4765341281890869\n",
      "Epoch 4653: train loss: 0.4765338599681854\n",
      "Epoch 4654: train loss: 0.47653359174728394\n",
      "Epoch 4655: train loss: 0.4765332341194153\n",
      "Epoch 4656: train loss: 0.4765329360961914\n",
      "Epoch 4657: train loss: 0.4765327274799347\n",
      "Epoch 4658: train loss: 0.47653234004974365\n",
      "Epoch 4659: train loss: 0.47653210163116455\n",
      "Epoch 4660: train loss: 0.4765317738056183\n",
      "Epoch 4661: train loss: 0.4765315055847168\n",
      "Epoch 4662: train loss: 0.4765312075614929\n",
      "Epoch 4663: train loss: 0.4765308201313019\n",
      "Epoch 4664: train loss: 0.47653061151504517\n",
      "Epoch 4665: train loss: 0.4765303134918213\n",
      "Epoch 4666: train loss: 0.476529985666275\n",
      "Epoch 4667: train loss: 0.4765297472476959\n",
      "Epoch 4668: train loss: 0.4765293598175049\n",
      "Epoch 4669: train loss: 0.47652915120124817\n",
      "Epoch 4670: train loss: 0.4765287935733795\n",
      "Epoch 4671: train loss: 0.476528525352478\n",
      "Epoch 4672: train loss: 0.47652822732925415\n",
      "Epoch 4673: train loss: 0.47652795910835266\n",
      "Epoch 4674: train loss: 0.4765276312828064\n",
      "Epoch 4675: train loss: 0.4765273332595825\n",
      "Epoch 4676: train loss: 0.47652706503868103\n",
      "Epoch 4677: train loss: 0.47652679681777954\n",
      "Epoch 4678: train loss: 0.47652649879455566\n",
      "Epoch 4679: train loss: 0.4765261709690094\n",
      "Epoch 4680: train loss: 0.4765258729457855\n",
      "Epoch 4681: train loss: 0.47652560472488403\n",
      "Epoch 4682: train loss: 0.47652530670166016\n",
      "Epoch 4683: train loss: 0.47652503848075867\n",
      "Epoch 4684: train loss: 0.4765247106552124\n",
      "Epoch 4685: train loss: 0.4765244722366333\n",
      "Epoch 4686: train loss: 0.47652408480644226\n",
      "Epoch 4687: train loss: 0.47652381658554077\n",
      "Epoch 4688: train loss: 0.4765235185623169\n",
      "Epoch 4689: train loss: 0.4765233099460602\n",
      "Epoch 4690: train loss: 0.47652289271354675\n",
      "Epoch 4691: train loss: 0.47652268409729004\n",
      "Epoch 4692: train loss: 0.4765223562717438\n",
      "Epoch 4693: train loss: 0.4765220582485199\n",
      "Epoch 4694: train loss: 0.47652173042297363\n",
      "Epoch 4695: train loss: 0.47652149200439453\n",
      "Epoch 4696: train loss: 0.47652122378349304\n",
      "Epoch 4697: train loss: 0.4765208959579468\n",
      "Epoch 4698: train loss: 0.4765205979347229\n",
      "Epoch 4699: train loss: 0.4765203297138214\n",
      "Epoch 4700: train loss: 0.47652000188827515\n",
      "Epoch 4701: train loss: 0.47651976346969604\n",
      "Epoch 4702: train loss: 0.4765194356441498\n",
      "Epoch 4703: train loss: 0.4765191376209259\n",
      "Epoch 4704: train loss: 0.4765188694000244\n",
      "Epoch 4705: train loss: 0.4765186011791229\n",
      "Epoch 4706: train loss: 0.47651830315589905\n",
      "Epoch 4707: train loss: 0.4765179753303528\n",
      "Epoch 4708: train loss: 0.4765176773071289\n",
      "Epoch 4709: train loss: 0.4765174090862274\n",
      "Epoch 4710: train loss: 0.4765171408653259\n",
      "Epoch 4711: train loss: 0.47651684284210205\n",
      "Epoch 4712: train loss: 0.4765165150165558\n",
      "Epoch 4713: train loss: 0.4765162467956543\n",
      "Epoch 4714: train loss: 0.47651588916778564\n",
      "Epoch 4715: train loss: 0.47651568055152893\n",
      "Epoch 4716: train loss: 0.47651535272598267\n",
      "Epoch 4717: train loss: 0.4765150547027588\n",
      "Epoch 4718: train loss: 0.4765147864818573\n",
      "Epoch 4719: train loss: 0.4765144884586334\n",
      "Epoch 4720: train loss: 0.47651422023773193\n",
      "Epoch 4721: train loss: 0.47651389241218567\n",
      "Epoch 4722: train loss: 0.4765135943889618\n",
      "Epoch 4723: train loss: 0.4765133261680603\n",
      "Epoch 4724: train loss: 0.4765130579471588\n",
      "Epoch 4725: train loss: 0.47651275992393494\n",
      "Epoch 4726: train loss: 0.47651243209838867\n",
      "Epoch 4727: train loss: 0.4765121638774872\n",
      "Epoch 4728: train loss: 0.4765119254589081\n",
      "Epoch 4729: train loss: 0.47651153802871704\n",
      "Epoch 4730: train loss: 0.47651129961013794\n",
      "Epoch 4731: train loss: 0.4765109717845917\n",
      "Epoch 4732: train loss: 0.4765107035636902\n",
      "Epoch 4733: train loss: 0.4765104651451111\n",
      "Epoch 4734: train loss: 0.4765101373195648\n",
      "Epoch 4735: train loss: 0.47650983929634094\n",
      "Epoch 4736: train loss: 0.47650957107543945\n",
      "Epoch 4737: train loss: 0.47650930285453796\n",
      "Epoch 4738: train loss: 0.4765089750289917\n",
      "Epoch 4739: train loss: 0.4765086770057678\n",
      "Epoch 4740: train loss: 0.47650840878486633\n",
      "Epoch 4741: train loss: 0.47650814056396484\n",
      "Epoch 4742: train loss: 0.47650784254074097\n",
      "Epoch 4743: train loss: 0.4765075147151947\n",
      "Epoch 4744: train loss: 0.4765072762966156\n",
      "Epoch 4745: train loss: 0.4765070080757141\n",
      "Epoch 4746: train loss: 0.47650668025016785\n",
      "Epoch 4747: train loss: 0.47650638222694397\n",
      "Epoch 4748: train loss: 0.47650617361068726\n",
      "Epoch 4749: train loss: 0.476505845785141\n",
      "Epoch 4750: train loss: 0.4765055477619171\n",
      "Epoch 4751: train loss: 0.4765052795410156\n",
      "Epoch 4752: train loss: 0.47650495171546936\n",
      "Epoch 4753: train loss: 0.4765046536922455\n",
      "Epoch 4754: train loss: 0.476504385471344\n",
      "Epoch 4755: train loss: 0.4765041172504425\n",
      "Epoch 4756: train loss: 0.47650381922721863\n",
      "Epoch 4757: train loss: 0.47650355100631714\n",
      "Epoch 4758: train loss: 0.47650325298309326\n",
      "Epoch 4759: train loss: 0.476502925157547\n",
      "Epoch 4760: train loss: 0.4765026569366455\n",
      "Epoch 4761: train loss: 0.4765024185180664\n",
      "Epoch 4762: train loss: 0.47650209069252014\n",
      "Epoch 4763: train loss: 0.47650182247161865\n",
      "Epoch 4764: train loss: 0.4765015244483948\n",
      "Epoch 4765: train loss: 0.4765012562274933\n",
      "Epoch 4766: train loss: 0.476500928401947\n",
      "Epoch 4767: train loss: 0.4765007197856903\n",
      "Epoch 4768: train loss: 0.47650036215782166\n",
      "Epoch 4769: train loss: 0.4765000641345978\n",
      "Epoch 4770: train loss: 0.47649985551834106\n",
      "Epoch 4771: train loss: 0.4764995276927948\n",
      "Epoch 4772: train loss: 0.4764992296695709\n",
      "Epoch 4773: train loss: 0.47649890184402466\n",
      "Epoch 4774: train loss: 0.47649869322776794\n",
      "Epoch 4775: train loss: 0.47649839520454407\n",
      "Epoch 4776: train loss: 0.4764980673789978\n",
      "Epoch 4777: train loss: 0.4764977991580963\n",
      "Epoch 4778: train loss: 0.47649750113487244\n",
      "Epoch 4779: train loss: 0.47649723291397095\n",
      "Epoch 4780: train loss: 0.4764969050884247\n",
      "Epoch 4781: train loss: 0.4764966666698456\n",
      "Epoch 4782: train loss: 0.4764963984489441\n",
      "Epoch 4783: train loss: 0.4764961302280426\n",
      "Epoch 4784: train loss: 0.4764958322048187\n",
      "Epoch 4785: train loss: 0.47649556398391724\n",
      "Epoch 4786: train loss: 0.47649523615837097\n",
      "Epoch 4787: train loss: 0.4764949381351471\n",
      "Epoch 4788: train loss: 0.4764946699142456\n",
      "Epoch 4789: train loss: 0.4764944016933441\n",
      "Epoch 4790: train loss: 0.47649410367012024\n",
      "Epoch 4791: train loss: 0.47649383544921875\n",
      "Epoch 4792: train loss: 0.4764935076236725\n",
      "Epoch 4793: train loss: 0.4764932096004486\n",
      "Epoch 4794: train loss: 0.4764930009841919\n",
      "Epoch 4795: train loss: 0.47649267315864563\n",
      "Epoch 4796: train loss: 0.47649243474006653\n",
      "Epoch 4797: train loss: 0.47649210691452026\n",
      "Epoch 4798: train loss: 0.4764918386936188\n",
      "Epoch 4799: train loss: 0.4764915406703949\n",
      "Epoch 4800: train loss: 0.4764912724494934\n",
      "Epoch 4801: train loss: 0.47649097442626953\n",
      "Epoch 4802: train loss: 0.47649070620536804\n",
      "Epoch 4803: train loss: 0.4764903783798218\n",
      "Epoch 4804: train loss: 0.47649016976356506\n",
      "Epoch 4805: train loss: 0.4764898717403412\n",
      "Epoch 4806: train loss: 0.4764895439147949\n",
      "Epoch 4807: train loss: 0.47648927569389343\n",
      "Epoch 4808: train loss: 0.47648903727531433\n",
      "Epoch 4809: train loss: 0.47648870944976807\n",
      "Epoch 4810: train loss: 0.4764884412288666\n",
      "Epoch 4811: train loss: 0.4764881432056427\n",
      "Epoch 4812: train loss: 0.4764878749847412\n",
      "Epoch 4813: train loss: 0.4764876067638397\n",
      "Epoch 4814: train loss: 0.47648730874061584\n",
      "Epoch 4815: train loss: 0.4764869809150696\n",
      "Epoch 4816: train loss: 0.47648677229881287\n",
      "Epoch 4817: train loss: 0.476486474275589\n",
      "Epoch 4818: train loss: 0.4764861464500427\n",
      "Epoch 4819: train loss: 0.47648587822914124\n",
      "Epoch 4820: train loss: 0.47648558020591736\n",
      "Epoch 4821: train loss: 0.47648537158966064\n",
      "Epoch 4822: train loss: 0.4764850437641144\n",
      "Epoch 4823: train loss: 0.4764847457408905\n",
      "Epoch 4824: train loss: 0.4764845371246338\n",
      "Epoch 4825: train loss: 0.4764842092990875\n",
      "Epoch 4826: train loss: 0.47648391127586365\n",
      "Epoch 4827: train loss: 0.4764835834503174\n",
      "Epoch 4828: train loss: 0.47648337483406067\n",
      "Epoch 4829: train loss: 0.47648313641548157\n",
      "Epoch 4830: train loss: 0.4764827489852905\n",
      "Epoch 4831: train loss: 0.4764825403690338\n",
      "Epoch 4832: train loss: 0.47648224234580994\n",
      "Epoch 4833: train loss: 0.4764820337295532\n",
      "Epoch 4834: train loss: 0.47648170590400696\n",
      "Epoch 4835: train loss: 0.4764814078807831\n",
      "Epoch 4836: train loss: 0.4764810800552368\n",
      "Epoch 4837: train loss: 0.4764808714389801\n",
      "Epoch 4838: train loss: 0.4764805734157562\n",
      "Epoch 4839: train loss: 0.47648030519485474\n",
      "Epoch 4840: train loss: 0.47647997736930847\n",
      "Epoch 4841: train loss: 0.47647973895072937\n",
      "Epoch 4842: train loss: 0.4764794707298279\n",
      "Epoch 4843: train loss: 0.4764792025089264\n",
      "Epoch 4844: train loss: 0.4764789044857025\n",
      "Epoch 4845: train loss: 0.476478636264801\n",
      "Epoch 4846: train loss: 0.47647830843925476\n",
      "Epoch 4847: train loss: 0.47647804021835327\n",
      "Epoch 4848: train loss: 0.4764777421951294\n",
      "Epoch 4849: train loss: 0.4764774739742279\n",
      "Epoch 4850: train loss: 0.4764772057533264\n",
      "Epoch 4851: train loss: 0.47647690773010254\n",
      "Epoch 4852: train loss: 0.47647663950920105\n",
      "Epoch 4853: train loss: 0.47647637128829956\n",
      "Epoch 4854: train loss: 0.47647613286972046\n",
      "Epoch 4855: train loss: 0.4764757454395294\n",
      "Epoch 4856: train loss: 0.4764755368232727\n",
      "Epoch 4857: train loss: 0.47647523880004883\n",
      "Epoch 4858: train loss: 0.47647497057914734\n",
      "Epoch 4859: train loss: 0.47647470235824585\n",
      "Epoch 4860: train loss: 0.476474404335022\n",
      "Epoch 4861: train loss: 0.4764741361141205\n",
      "Epoch 4862: train loss: 0.476473867893219\n",
      "Epoch 4863: train loss: 0.4764735996723175\n",
      "Epoch 4864: train loss: 0.4764733612537384\n",
      "Epoch 4865: train loss: 0.47647303342819214\n",
      "Epoch 4866: train loss: 0.47647276520729065\n",
      "Epoch 4867: train loss: 0.47647252678871155\n",
      "Epoch 4868: train loss: 0.4764721989631653\n",
      "Epoch 4869: train loss: 0.4764719009399414\n",
      "Epoch 4870: train loss: 0.4764716327190399\n",
      "Epoch 4871: train loss: 0.4764713644981384\n",
      "Epoch 4872: train loss: 0.47647106647491455\n",
      "Epoch 4873: train loss: 0.47647082805633545\n",
      "Epoch 4874: train loss: 0.4764705300331116\n",
      "Epoch 4875: train loss: 0.4764702022075653\n",
      "Epoch 4876: train loss: 0.4764699935913086\n",
      "Epoch 4877: train loss: 0.4764696955680847\n",
      "Epoch 4878: train loss: 0.4764694273471832\n",
      "Epoch 4879: train loss: 0.47646912932395935\n",
      "Epoch 4880: train loss: 0.47646886110305786\n",
      "Epoch 4881: train loss: 0.47646859288215637\n",
      "Epoch 4882: train loss: 0.4764683246612549\n",
      "Epoch 4883: train loss: 0.476468026638031\n",
      "Epoch 4884: train loss: 0.4764678180217743\n",
      "Epoch 4885: train loss: 0.476467490196228\n",
      "Epoch 4886: train loss: 0.47646719217300415\n",
      "Epoch 4887: train loss: 0.47646698355674744\n",
      "Epoch 4888: train loss: 0.47646665573120117\n",
      "Epoch 4889: train loss: 0.4764663875102997\n",
      "Epoch 4890: train loss: 0.4764660894870758\n",
      "Epoch 4891: train loss: 0.4764658212661743\n",
      "Epoch 4892: train loss: 0.4764655530452728\n",
      "Epoch 4893: train loss: 0.4764653146266937\n",
      "Epoch 4894: train loss: 0.47646498680114746\n",
      "Epoch 4895: train loss: 0.4764646887779236\n",
      "Epoch 4896: train loss: 0.47646448016166687\n",
      "Epoch 4897: train loss: 0.4764642119407654\n",
      "Epoch 4898: train loss: 0.4764639437198639\n",
      "Epoch 4899: train loss: 0.47646364569664\n",
      "Epoch 4900: train loss: 0.4764633774757385\n",
      "Epoch 4901: train loss: 0.47646304965019226\n",
      "Epoch 4902: train loss: 0.47646278142929077\n",
      "Epoch 4903: train loss: 0.47646254301071167\n",
      "Epoch 4904: train loss: 0.4764622151851654\n",
      "Epoch 4905: train loss: 0.4764620065689087\n",
      "Epoch 4906: train loss: 0.4764617383480072\n",
      "Epoch 4907: train loss: 0.47646138072013855\n",
      "Epoch 4908: train loss: 0.47646117210388184\n",
      "Epoch 4909: train loss: 0.47646090388298035\n",
      "Epoch 4910: train loss: 0.47646060585975647\n",
      "Epoch 4911: train loss: 0.4764602780342102\n",
      "Epoch 4912: train loss: 0.4764600694179535\n",
      "Epoch 4913: train loss: 0.4764597713947296\n",
      "Epoch 4914: train loss: 0.4764595031738281\n",
      "Epoch 4915: train loss: 0.47645923495292664\n",
      "Epoch 4916: train loss: 0.47645893692970276\n",
      "Epoch 4917: train loss: 0.47645872831344604\n",
      "Epoch 4918: train loss: 0.4764584004878998\n",
      "Epoch 4919: train loss: 0.4764581024646759\n",
      "Epoch 4920: train loss: 0.4764578342437744\n",
      "Epoch 4921: train loss: 0.4764575660228729\n",
      "Epoch 4922: train loss: 0.4764573574066162\n",
      "Epoch 4923: train loss: 0.47645705938339233\n",
      "Epoch 4924: train loss: 0.47645679116249084\n",
      "Epoch 4925: train loss: 0.4764564633369446\n",
      "Epoch 4926: train loss: 0.4764562249183655\n",
      "Epoch 4927: train loss: 0.476455956697464\n",
      "Epoch 4928: train loss: 0.4764556884765625\n",
      "Epoch 4929: train loss: 0.476455420255661\n",
      "Epoch 4930: train loss: 0.4764551520347595\n",
      "Epoch 4931: train loss: 0.4764549136161804\n",
      "Epoch 4932: train loss: 0.47645458579063416\n",
      "Epoch 4933: train loss: 0.47645431756973267\n",
      "Epoch 4934: train loss: 0.4764540195465088\n",
      "Epoch 4935: train loss: 0.4764536917209625\n",
      "Epoch 4936: train loss: 0.4764534831047058\n",
      "Epoch 4937: train loss: 0.4764532446861267\n",
      "Epoch 4938: train loss: 0.4764529764652252\n",
      "Epoch 4939: train loss: 0.47645270824432373\n",
      "Epoch 4940: train loss: 0.47645244002342224\n",
      "Epoch 4941: train loss: 0.47645214200019836\n",
      "Epoch 4942: train loss: 0.4764518737792969\n",
      "Epoch 4943: train loss: 0.4764516055583954\n",
      "Epoch 4944: train loss: 0.4764513373374939\n",
      "Epoch 4945: train loss: 0.47645103931427\n",
      "Epoch 4946: train loss: 0.47645077109336853\n",
      "Epoch 4947: train loss: 0.47645050287246704\n",
      "Epoch 4948: train loss: 0.47645026445388794\n",
      "Epoch 4949: train loss: 0.47644999623298645\n",
      "Epoch 4950: train loss: 0.4764496684074402\n",
      "Epoch 4951: train loss: 0.4764494299888611\n",
      "Epoch 4952: train loss: 0.47644922137260437\n",
      "Epoch 4953: train loss: 0.4764488935470581\n",
      "Epoch 4954: train loss: 0.4764486253261566\n",
      "Epoch 4955: train loss: 0.47644832730293274\n",
      "Epoch 4956: train loss: 0.47644805908203125\n",
      "Epoch 4957: train loss: 0.47644779086112976\n",
      "Epoch 4958: train loss: 0.47644752264022827\n",
      "Epoch 4959: train loss: 0.4764472246170044\n",
      "Epoch 4960: train loss: 0.4764470160007477\n",
      "Epoch 4961: train loss: 0.4764466881752014\n",
      "Epoch 4962: train loss: 0.4764464795589447\n",
      "Epoch 4963: train loss: 0.4764461815357208\n",
      "Epoch 4964: train loss: 0.47644591331481934\n",
      "Epoch 4965: train loss: 0.47644564509391785\n",
      "Epoch 4966: train loss: 0.47644540667533875\n",
      "Epoch 4967: train loss: 0.4764450788497925\n",
      "Epoch 4968: train loss: 0.476444810628891\n",
      "Epoch 4969: train loss: 0.4764445722103119\n",
      "Epoch 4970: train loss: 0.4764443039894104\n",
      "Epoch 4971: train loss: 0.4764440357685089\n",
      "Epoch 4972: train loss: 0.4764437675476074\n",
      "Epoch 4973: train loss: 0.47644349932670593\n",
      "Epoch 4974: train loss: 0.47644320130348206\n",
      "Epoch 4975: train loss: 0.47644293308258057\n",
      "Epoch 4976: train loss: 0.4764426648616791\n",
      "Epoch 4977: train loss: 0.4764424264431\n",
      "Epoch 4978: train loss: 0.4764421582221985\n",
      "Epoch 4979: train loss: 0.476441890001297\n",
      "Epoch 4980: train loss: 0.4764415919780731\n",
      "Epoch 4981: train loss: 0.47644132375717163\n",
      "Epoch 4982: train loss: 0.47644105553627014\n",
      "Epoch 4983: train loss: 0.47644078731536865\n",
      "Epoch 4984: train loss: 0.47644054889678955\n",
      "Epoch 4985: train loss: 0.47644028067588806\n",
      "Epoch 4986: train loss: 0.4764400124549866\n",
      "Epoch 4987: train loss: 0.4764396846294403\n",
      "Epoch 4988: train loss: 0.4764394462108612\n",
      "Epoch 4989: train loss: 0.4764391779899597\n",
      "Epoch 4990: train loss: 0.4764389097690582\n",
      "Epoch 4991: train loss: 0.47643864154815674\n",
      "Epoch 4992: train loss: 0.47643840312957764\n",
      "Epoch 4993: train loss: 0.47643813490867615\n",
      "Epoch 4994: train loss: 0.47643786668777466\n",
      "Epoch 4995: train loss: 0.47643759846687317\n",
      "Epoch 4996: train loss: 0.47643736004829407\n",
      "Epoch 4997: train loss: 0.4764370918273926\n",
      "Epoch 4998: train loss: 0.4764367640018463\n",
      "Epoch 4999: train loss: 0.4764364957809448\n",
      "Epoch 5000: train loss: 0.4764362573623657\n",
      "Epoch 5001: train loss: 0.47643592953681946\n",
      "Epoch 5002: train loss: 0.47643566131591797\n",
      "Epoch 5003: train loss: 0.47643542289733887\n",
      "Epoch 5004: train loss: 0.47643521428108215\n",
      "Epoch 5005: train loss: 0.4764348864555359\n",
      "Epoch 5006: train loss: 0.4764346182346344\n",
      "Epoch 5007: train loss: 0.4764343798160553\n",
      "Epoch 5008: train loss: 0.4764341115951538\n",
      "Epoch 5009: train loss: 0.4764338433742523\n",
      "Epoch 5010: train loss: 0.47643357515335083\n",
      "Epoch 5011: train loss: 0.47643333673477173\n",
      "Epoch 5012: train loss: 0.47643306851387024\n",
      "Epoch 5013: train loss: 0.476432740688324\n",
      "Epoch 5014: train loss: 0.4764324724674225\n",
      "Epoch 5015: train loss: 0.47643226385116577\n",
      "Epoch 5016: train loss: 0.4764319658279419\n",
      "Epoch 5017: train loss: 0.4764316976070404\n",
      "Epoch 5018: train loss: 0.4764314293861389\n",
      "Epoch 5019: train loss: 0.47643113136291504\n",
      "Epoch 5020: train loss: 0.4764309227466583\n",
      "Epoch 5021: train loss: 0.4764307141304016\n",
      "Epoch 5022: train loss: 0.47643038630485535\n",
      "Epoch 5023: train loss: 0.47643014788627625\n",
      "Epoch 5024: train loss: 0.47642987966537476\n",
      "Epoch 5025: train loss: 0.4764295518398285\n",
      "Epoch 5026: train loss: 0.4764293432235718\n",
      "Epoch 5027: train loss: 0.4764290452003479\n",
      "Epoch 5028: train loss: 0.47642871737480164\n",
      "Epoch 5029: train loss: 0.4764285087585449\n",
      "Epoch 5030: train loss: 0.47642824053764343\n",
      "Epoch 5031: train loss: 0.47642800211906433\n",
      "Epoch 5032: train loss: 0.47642773389816284\n",
      "Epoch 5033: train loss: 0.4764274060726166\n",
      "Epoch 5034: train loss: 0.47642719745635986\n",
      "Epoch 5035: train loss: 0.47642695903778076\n",
      "Epoch 5036: train loss: 0.4764266312122345\n",
      "Epoch 5037: train loss: 0.4764264225959778\n",
      "Epoch 5038: train loss: 0.4764261245727539\n",
      "Epoch 5039: train loss: 0.4764259159564972\n",
      "Epoch 5040: train loss: 0.4764255881309509\n",
      "Epoch 5041: train loss: 0.47642531991004944\n",
      "Epoch 5042: train loss: 0.47642505168914795\n",
      "Epoch 5043: train loss: 0.47642481327056885\n",
      "Epoch 5044: train loss: 0.47642454504966736\n",
      "Epoch 5045: train loss: 0.47642427682876587\n",
      "Epoch 5046: train loss: 0.4764240086078644\n",
      "Epoch 5047: train loss: 0.4764237701892853\n",
      "Epoch 5048: train loss: 0.476423442363739\n",
      "Epoch 5049: train loss: 0.4764231741428375\n",
      "Epoch 5050: train loss: 0.4764229655265808\n",
      "Epoch 5051: train loss: 0.47642266750335693\n",
      "Epoch 5052: train loss: 0.47642239928245544\n",
      "Epoch 5053: train loss: 0.47642213106155396\n",
      "Epoch 5054: train loss: 0.47642192244529724\n",
      "Epoch 5055: train loss: 0.47642168402671814\n",
      "Epoch 5056: train loss: 0.47642141580581665\n",
      "Epoch 5057: train loss: 0.4764210879802704\n",
      "Epoch 5058: train loss: 0.47642087936401367\n",
      "Epoch 5059: train loss: 0.4764205813407898\n",
      "Epoch 5060: train loss: 0.4764203727245331\n",
      "Epoch 5061: train loss: 0.4764200448989868\n",
      "Epoch 5062: train loss: 0.4764197766780853\n",
      "Epoch 5063: train loss: 0.4764195382595062\n",
      "Epoch 5064: train loss: 0.47641927003860474\n",
      "Epoch 5065: train loss: 0.47641900181770325\n",
      "Epoch 5066: train loss: 0.47641873359680176\n",
      "Epoch 5067: train loss: 0.47641852498054504\n",
      "Epoch 5068: train loss: 0.47641828656196594\n",
      "Epoch 5069: train loss: 0.4764179587364197\n",
      "Epoch 5070: train loss: 0.47641775012016296\n",
      "Epoch 5071: train loss: 0.4764174222946167\n",
      "Epoch 5072: train loss: 0.4764171838760376\n",
      "Epoch 5073: train loss: 0.4764169156551361\n",
      "Epoch 5074: train loss: 0.4764166474342346\n",
      "Epoch 5075: train loss: 0.47641637921333313\n",
      "Epoch 5076: train loss: 0.47641608119010925\n",
      "Epoch 5077: train loss: 0.47641587257385254\n",
      "Epoch 5078: train loss: 0.47641560435295105\n",
      "Epoch 5079: train loss: 0.47641533613204956\n",
      "Epoch 5080: train loss: 0.47641509771347046\n",
      "Epoch 5081: train loss: 0.47641482949256897\n",
      "Epoch 5082: train loss: 0.4764145612716675\n",
      "Epoch 5083: train loss: 0.476414293050766\n",
      "Epoch 5084: train loss: 0.4764140546321869\n",
      "Epoch 5085: train loss: 0.4764137864112854\n",
      "Epoch 5086: train loss: 0.4764135181903839\n",
      "Epoch 5087: train loss: 0.4764132499694824\n",
      "Epoch 5088: train loss: 0.47641298174858093\n",
      "Epoch 5089: train loss: 0.47641274333000183\n",
      "Epoch 5090: train loss: 0.47641247510910034\n",
      "Epoch 5091: train loss: 0.47641220688819885\n",
      "Epoch 5092: train loss: 0.47641193866729736\n",
      "Epoch 5093: train loss: 0.47641170024871826\n",
      "Epoch 5094: train loss: 0.4764114320278168\n",
      "Epoch 5095: train loss: 0.4764111638069153\n",
      "Epoch 5096: train loss: 0.47641095519065857\n",
      "Epoch 5097: train loss: 0.4764106571674347\n",
      "Epoch 5098: train loss: 0.4764103889465332\n",
      "Epoch 5099: train loss: 0.4764101207256317\n",
      "Epoch 5100: train loss: 0.476409912109375\n",
      "Epoch 5101: train loss: 0.4764096438884735\n",
      "Epoch 5102: train loss: 0.476409375667572\n",
      "Epoch 5103: train loss: 0.47640907764434814\n",
      "Epoch 5104: train loss: 0.47640880942344666\n",
      "Epoch 5105: train loss: 0.47640854120254517\n",
      "Epoch 5106: train loss: 0.47640830278396606\n",
      "Epoch 5107: train loss: 0.4764080345630646\n",
      "Epoch 5108: train loss: 0.4764077663421631\n",
      "Epoch 5109: train loss: 0.47640755772590637\n",
      "Epoch 5110: train loss: 0.4764072895050049\n",
      "Epoch 5111: train loss: 0.476406991481781\n",
      "Epoch 5112: train loss: 0.4764067828655243\n",
      "Epoch 5113: train loss: 0.4764065146446228\n",
      "Epoch 5114: train loss: 0.4764062464237213\n",
      "Epoch 5115: train loss: 0.4764059782028198\n",
      "Epoch 5116: train loss: 0.4764057397842407\n",
      "Epoch 5117: train loss: 0.47640547156333923\n",
      "Epoch 5118: train loss: 0.47640520334243774\n",
      "Epoch 5119: train loss: 0.47640499472618103\n",
      "Epoch 5120: train loss: 0.47640469670295715\n",
      "Epoch 5121: train loss: 0.47640448808670044\n",
      "Epoch 5122: train loss: 0.47640421986579895\n",
      "Epoch 5123: train loss: 0.4764038920402527\n",
      "Epoch 5124: train loss: 0.4764036536216736\n",
      "Epoch 5125: train loss: 0.47640344500541687\n",
      "Epoch 5126: train loss: 0.4764031171798706\n",
      "Epoch 5127: train loss: 0.4764029085636139\n",
      "Epoch 5128: train loss: 0.4764026403427124\n",
      "Epoch 5129: train loss: 0.4764024019241333\n",
      "Epoch 5130: train loss: 0.4764021337032318\n",
      "Epoch 5131: train loss: 0.4764018654823303\n",
      "Epoch 5132: train loss: 0.47640159726142883\n",
      "Epoch 5133: train loss: 0.47640132904052734\n",
      "Epoch 5134: train loss: 0.47640109062194824\n",
      "Epoch 5135: train loss: 0.47640082240104675\n",
      "Epoch 5136: train loss: 0.47640055418014526\n",
      "Epoch 5137: train loss: 0.4764002859592438\n",
      "Epoch 5138: train loss: 0.4764000475406647\n",
      "Epoch 5139: train loss: 0.4763997793197632\n",
      "Epoch 5140: train loss: 0.4763995110988617\n",
      "Epoch 5141: train loss: 0.476399302482605\n",
      "Epoch 5142: train loss: 0.4763990342617035\n",
      "Epoch 5143: train loss: 0.4763987362384796\n",
      "Epoch 5144: train loss: 0.4763985276222229\n",
      "Epoch 5145: train loss: 0.4763982594013214\n",
      "Epoch 5146: train loss: 0.4763979911804199\n",
      "Epoch 5147: train loss: 0.47639772295951843\n",
      "Epoch 5148: train loss: 0.47639748454093933\n",
      "Epoch 5149: train loss: 0.47639721632003784\n",
      "Epoch 5150: train loss: 0.47639694809913635\n",
      "Epoch 5151: train loss: 0.47639673948287964\n",
      "Epoch 5152: train loss: 0.47639644145965576\n",
      "Epoch 5153: train loss: 0.47639623284339905\n",
      "Epoch 5154: train loss: 0.47639596462249756\n",
      "Epoch 5155: train loss: 0.47639569640159607\n",
      "Epoch 5156: train loss: 0.4763954281806946\n",
      "Epoch 5157: train loss: 0.47639521956443787\n",
      "Epoch 5158: train loss: 0.476394921541214\n",
      "Epoch 5159: train loss: 0.4763946533203125\n",
      "Epoch 5160: train loss: 0.4763944447040558\n",
      "Epoch 5161: train loss: 0.4763941168785095\n",
      "Epoch 5162: train loss: 0.4763938784599304\n",
      "Epoch 5163: train loss: 0.47639361023902893\n",
      "Epoch 5164: train loss: 0.4763934016227722\n",
      "Epoch 5165: train loss: 0.4763931334018707\n",
      "Epoch 5166: train loss: 0.47639286518096924\n",
      "Epoch 5167: train loss: 0.4763926565647125\n",
      "Epoch 5168: train loss: 0.47639235854148865\n",
      "Epoch 5169: train loss: 0.47639214992523193\n",
      "Epoch 5170: train loss: 0.47639188170433044\n",
      "Epoch 5171: train loss: 0.47639161348342896\n",
      "Epoch 5172: train loss: 0.47639140486717224\n",
      "Epoch 5173: train loss: 0.4763910472393036\n",
      "Epoch 5174: train loss: 0.4763908386230469\n",
      "Epoch 5175: train loss: 0.4763905704021454\n",
      "Epoch 5176: train loss: 0.47639036178588867\n",
      "Epoch 5177: train loss: 0.4763900637626648\n",
      "Epoch 5178: train loss: 0.4763897955417633\n",
      "Epoch 5179: train loss: 0.4763895273208618\n",
      "Epoch 5180: train loss: 0.4763893187046051\n",
      "Epoch 5181: train loss: 0.4763890504837036\n",
      "Epoch 5182: train loss: 0.4763888120651245\n",
      "Epoch 5183: train loss: 0.4763886034488678\n",
      "Epoch 5184: train loss: 0.47638827562332153\n",
      "Epoch 5185: train loss: 0.4763880670070648\n",
      "Epoch 5186: train loss: 0.47638776898384094\n",
      "Epoch 5187: train loss: 0.47638750076293945\n",
      "Epoch 5188: train loss: 0.47638729214668274\n",
      "Epoch 5189: train loss: 0.47638702392578125\n",
      "Epoch 5190: train loss: 0.47638681530952454\n",
      "Epoch 5191: train loss: 0.47638648748397827\n",
      "Epoch 5192: train loss: 0.47638624906539917\n",
      "Epoch 5193: train loss: 0.4763859808444977\n",
      "Epoch 5194: train loss: 0.47638577222824097\n",
      "Epoch 5195: train loss: 0.4763855040073395\n",
      "Epoch 5196: train loss: 0.476385235786438\n",
      "Epoch 5197: train loss: 0.4763849973678589\n",
      "Epoch 5198: train loss: 0.4763847887516022\n",
      "Epoch 5199: train loss: 0.4763845205307007\n",
      "Epoch 5200: train loss: 0.4763842523097992\n",
      "Epoch 5201: train loss: 0.4763839840888977\n",
      "Epoch 5202: train loss: 0.4763837456703186\n",
      "Epoch 5203: train loss: 0.4763834774494171\n",
      "Epoch 5204: train loss: 0.4763832092285156\n",
      "Epoch 5205: train loss: 0.47638294100761414\n",
      "Epoch 5206: train loss: 0.4763827323913574\n",
      "Epoch 5207: train loss: 0.47638246417045593\n",
      "Epoch 5208: train loss: 0.47638222575187683\n",
      "Epoch 5209: train loss: 0.47638195753097534\n",
      "Epoch 5210: train loss: 0.47638168931007385\n",
      "Epoch 5211: train loss: 0.47638148069381714\n",
      "Epoch 5212: train loss: 0.47638121247291565\n",
      "Epoch 5213: train loss: 0.47638097405433655\n",
      "Epoch 5214: train loss: 0.47638070583343506\n",
      "Epoch 5215: train loss: 0.47638043761253357\n",
      "Epoch 5216: train loss: 0.4763801693916321\n",
      "Epoch 5217: train loss: 0.476379930973053\n",
      "Epoch 5218: train loss: 0.47637972235679626\n",
      "Epoch 5219: train loss: 0.4763794541358948\n",
      "Epoch 5220: train loss: 0.4763791859149933\n",
      "Epoch 5221: train loss: 0.4763789176940918\n",
      "Epoch 5222: train loss: 0.4763786494731903\n",
      "Epoch 5223: train loss: 0.4763784408569336\n",
      "Epoch 5224: train loss: 0.4763782024383545\n",
      "Epoch 5225: train loss: 0.476377934217453\n",
      "Epoch 5226: train loss: 0.4763776659965515\n",
      "Epoch 5227: train loss: 0.4763774573802948\n",
      "Epoch 5228: train loss: 0.4763771593570709\n",
      "Epoch 5229: train loss: 0.47637689113616943\n",
      "Epoch 5230: train loss: 0.4763766825199127\n",
      "Epoch 5231: train loss: 0.476376473903656\n",
      "Epoch 5232: train loss: 0.47637614607810974\n",
      "Epoch 5233: train loss: 0.476375937461853\n",
      "Epoch 5234: train loss: 0.4763756990432739\n",
      "Epoch 5235: train loss: 0.47637543082237244\n",
      "Epoch 5236: train loss: 0.4763752222061157\n",
      "Epoch 5237: train loss: 0.47637495398521423\n",
      "Epoch 5238: train loss: 0.47637468576431274\n",
      "Epoch 5239: train loss: 0.47637438774108887\n",
      "Epoch 5240: train loss: 0.47637417912483215\n",
      "Epoch 5241: train loss: 0.47637391090393066\n",
      "Epoch 5242: train loss: 0.4763736426830292\n",
      "Epoch 5243: train loss: 0.47637343406677246\n",
      "Epoch 5244: train loss: 0.47637316584587097\n",
      "Epoch 5245: train loss: 0.47637292742729187\n",
      "Epoch 5246: train loss: 0.47637271881103516\n",
      "Epoch 5247: train loss: 0.47637245059013367\n",
      "Epoch 5248: train loss: 0.4763721823692322\n",
      "Epoch 5249: train loss: 0.4763719141483307\n",
      "Epoch 5250: train loss: 0.4763716459274292\n",
      "Epoch 5251: train loss: 0.4763714373111725\n",
      "Epoch 5252: train loss: 0.4763711988925934\n",
      "Epoch 5253: train loss: 0.4763709306716919\n",
      "Epoch 5254: train loss: 0.4763707220554352\n",
      "Epoch 5255: train loss: 0.4763703942298889\n",
      "Epoch 5256: train loss: 0.4763701856136322\n",
      "Epoch 5257: train loss: 0.4763699471950531\n",
      "Epoch 5258: train loss: 0.4763696789741516\n",
      "Epoch 5259: train loss: 0.4763694703578949\n",
      "Epoch 5260: train loss: 0.4763692021369934\n",
      "Epoch 5261: train loss: 0.4763689339160919\n",
      "Epoch 5262: train loss: 0.4763686954975128\n",
      "Epoch 5263: train loss: 0.4763684868812561\n",
      "Epoch 5264: train loss: 0.4763682186603546\n",
      "Epoch 5265: train loss: 0.47636789083480835\n",
      "Epoch 5266: train loss: 0.4763677418231964\n",
      "Epoch 5267: train loss: 0.4763674736022949\n",
      "Epoch 5268: train loss: 0.47636720538139343\n",
      "Epoch 5269: train loss: 0.4763669967651367\n",
      "Epoch 5270: train loss: 0.47636669874191284\n",
      "Epoch 5271: train loss: 0.47636643052101135\n",
      "Epoch 5272: train loss: 0.47636622190475464\n",
      "Epoch 5273: train loss: 0.4763660132884979\n",
      "Epoch 5274: train loss: 0.47636571526527405\n",
      "Epoch 5275: train loss: 0.47636544704437256\n",
      "Epoch 5276: train loss: 0.47636517882347107\n",
      "Epoch 5277: train loss: 0.47636497020721436\n",
      "Epoch 5278: train loss: 0.47636470198631287\n",
      "Epoch 5279: train loss: 0.47636449337005615\n",
      "Epoch 5280: train loss: 0.4763641953468323\n",
      "Epoch 5281: train loss: 0.47636398673057556\n",
      "Epoch 5282: train loss: 0.47636377811431885\n",
      "Epoch 5283: train loss: 0.47636350989341736\n",
      "Epoch 5284: train loss: 0.4763631820678711\n",
      "Epoch 5285: train loss: 0.47636303305625916\n",
      "Epoch 5286: train loss: 0.4763627350330353\n",
      "Epoch 5287: train loss: 0.4763624668121338\n",
      "Epoch 5288: train loss: 0.4763622581958771\n",
      "Epoch 5289: train loss: 0.4763619899749756\n",
      "Epoch 5290: train loss: 0.47636178135871887\n",
      "Epoch 5291: train loss: 0.4763615131378174\n",
      "Epoch 5292: train loss: 0.4763612151145935\n",
      "Epoch 5293: train loss: 0.47636106610298157\n",
      "Epoch 5294: train loss: 0.4763607978820801\n",
      "Epoch 5295: train loss: 0.47636058926582336\n",
      "Epoch 5296: train loss: 0.4763602614402771\n",
      "Epoch 5297: train loss: 0.4763600528240204\n",
      "Epoch 5298: train loss: 0.47635984420776367\n",
      "Epoch 5299: train loss: 0.4763595461845398\n",
      "Epoch 5300: train loss: 0.4763592779636383\n",
      "Epoch 5301: train loss: 0.4763590693473816\n",
      "Epoch 5302: train loss: 0.4763588011264801\n",
      "Epoch 5303: train loss: 0.4763585329055786\n",
      "Epoch 5304: train loss: 0.4763582944869995\n",
      "Epoch 5305: train loss: 0.4763580858707428\n",
      "Epoch 5306: train loss: 0.4763578176498413\n",
      "Epoch 5307: train loss: 0.4763576090335846\n",
      "Epoch 5308: train loss: 0.4763573408126831\n",
      "Epoch 5309: train loss: 0.4763570725917816\n",
      "Epoch 5310: train loss: 0.4763568639755249\n",
      "Epoch 5311: train loss: 0.476356565952301\n",
      "Epoch 5312: train loss: 0.47635629773139954\n",
      "Epoch 5313: train loss: 0.4763560891151428\n",
      "Epoch 5314: train loss: 0.4763558804988861\n",
      "Epoch 5315: train loss: 0.47635555267333984\n",
      "Epoch 5316: train loss: 0.47635534405708313\n",
      "Epoch 5317: train loss: 0.47635510563850403\n",
      "Epoch 5318: train loss: 0.4763548970222473\n",
      "Epoch 5319: train loss: 0.4763546288013458\n",
      "Epoch 5320: train loss: 0.4763544201850891\n",
      "Epoch 5321: train loss: 0.4763541519641876\n",
      "Epoch 5322: train loss: 0.47635388374328613\n",
      "Epoch 5323: train loss: 0.47635364532470703\n",
      "Epoch 5324: train loss: 0.4763534367084503\n",
      "Epoch 5325: train loss: 0.4763532280921936\n",
      "Epoch 5326: train loss: 0.47635290026664734\n",
      "Epoch 5327: train loss: 0.47635263204574585\n",
      "Epoch 5328: train loss: 0.47635242342948914\n",
      "Epoch 5329: train loss: 0.4763522148132324\n",
      "Epoch 5330: train loss: 0.47635194659233093\n",
      "Epoch 5331: train loss: 0.47635170817375183\n",
      "Epoch 5332: train loss: 0.47635143995285034\n",
      "Epoch 5333: train loss: 0.47635117173194885\n",
      "Epoch 5334: train loss: 0.47635096311569214\n",
      "Epoch 5335: train loss: 0.47635069489479065\n",
      "Epoch 5336: train loss: 0.47635045647621155\n",
      "Epoch 5337: train loss: 0.47635024785995483\n",
      "Epoch 5338: train loss: 0.47634992003440857\n",
      "Epoch 5339: train loss: 0.47634977102279663\n",
      "Epoch 5340: train loss: 0.47634950280189514\n",
      "Epoch 5341: train loss: 0.47634923458099365\n",
      "Epoch 5342: train loss: 0.47634902596473694\n",
      "Epoch 5343: train loss: 0.4763488173484802\n",
      "Epoch 5344: train loss: 0.47634854912757874\n",
      "Epoch 5345: train loss: 0.47634825110435486\n",
      "Epoch 5346: train loss: 0.4763481020927429\n",
      "Epoch 5347: train loss: 0.47634777426719666\n",
      "Epoch 5348: train loss: 0.47634750604629517\n",
      "Epoch 5349: train loss: 0.47634729743003845\n",
      "Epoch 5350: train loss: 0.47634708881378174\n",
      "Epoch 5351: train loss: 0.476346880197525\n",
      "Epoch 5352: train loss: 0.47634658217430115\n",
      "Epoch 5353: train loss: 0.47634631395339966\n",
      "Epoch 5354: train loss: 0.47634610533714294\n",
      "Epoch 5355: train loss: 0.47634589672088623\n",
      "Epoch 5356: train loss: 0.47634562849998474\n",
      "Epoch 5357: train loss: 0.47634533047676086\n",
      "Epoch 5358: train loss: 0.47634512186050415\n",
      "Epoch 5359: train loss: 0.47634491324424744\n",
      "Epoch 5360: train loss: 0.47634464502334595\n",
      "Epoch 5361: train loss: 0.47634437680244446\n",
      "Epoch 5362: train loss: 0.47634410858154297\n",
      "Epoch 5363: train loss: 0.47634389996528625\n",
      "Epoch 5364: train loss: 0.47634366154670715\n",
      "Epoch 5365: train loss: 0.47634339332580566\n",
      "Epoch 5366: train loss: 0.4763432443141937\n",
      "Epoch 5367: train loss: 0.47634291648864746\n",
      "Epoch 5368: train loss: 0.47634270787239075\n",
      "Epoch 5369: train loss: 0.47634243965148926\n",
      "Epoch 5370: train loss: 0.47634223103523254\n",
      "Epoch 5371: train loss: 0.47634202241897583\n",
      "Epoch 5372: train loss: 0.47634172439575195\n",
      "Epoch 5373: train loss: 0.47634151577949524\n",
      "Epoch 5374: train loss: 0.47634124755859375\n",
      "Epoch 5375: train loss: 0.47634097933769226\n",
      "Epoch 5376: train loss: 0.47634077072143555\n",
      "Epoch 5377: train loss: 0.47634050250053406\n",
      "Epoch 5378: train loss: 0.47634029388427734\n",
      "Epoch 5379: train loss: 0.47634008526802063\n",
      "Epoch 5380: train loss: 0.47633984684944153\n",
      "Epoch 5381: train loss: 0.47633957862854004\n",
      "Epoch 5382: train loss: 0.4763393700122833\n",
      "Epoch 5383: train loss: 0.47633910179138184\n",
      "Epoch 5384: train loss: 0.47633883357048035\n",
      "Epoch 5385: train loss: 0.47633862495422363\n",
      "Epoch 5386: train loss: 0.47633832693099976\n",
      "Epoch 5387: train loss: 0.47633811831474304\n",
      "Epoch 5388: train loss: 0.47633790969848633\n",
      "Epoch 5389: train loss: 0.47633764147758484\n",
      "Epoch 5390: train loss: 0.47633737325668335\n",
      "Epoch 5391: train loss: 0.47633716464042664\n",
      "Epoch 5392: train loss: 0.47633689641952515\n",
      "Epoch 5393: train loss: 0.47633665800094604\n",
      "Epoch 5394: train loss: 0.47633644938468933\n",
      "Epoch 5395: train loss: 0.4763362407684326\n",
      "Epoch 5396: train loss: 0.47633597254753113\n",
      "Epoch 5397: train loss: 0.47633570432662964\n",
      "Epoch 5398: train loss: 0.4763354957103729\n",
      "Epoch 5399: train loss: 0.4763352870941162\n",
      "Epoch 5400: train loss: 0.47633498907089233\n",
      "Epoch 5401: train loss: 0.4763347804546356\n",
      "Epoch 5402: train loss: 0.4763345718383789\n",
      "Epoch 5403: train loss: 0.4763343036174774\n",
      "Epoch 5404: train loss: 0.4763340353965759\n",
      "Epoch 5405: train loss: 0.4763338267803192\n",
      "Epoch 5406: train loss: 0.4763335585594177\n",
      "Epoch 5407: train loss: 0.476333349943161\n",
      "Epoch 5408: train loss: 0.4763330817222595\n",
      "Epoch 5409: train loss: 0.4763328731060028\n",
      "Epoch 5410: train loss: 0.47633257508277893\n",
      "Epoch 5411: train loss: 0.4763323664665222\n",
      "Epoch 5412: train loss: 0.4763321578502655\n",
      "Epoch 5413: train loss: 0.4763319492340088\n",
      "Epoch 5414: train loss: 0.4763316214084625\n",
      "Epoch 5415: train loss: 0.4763314127922058\n",
      "Epoch 5416: train loss: 0.4763311743736267\n",
      "Epoch 5417: train loss: 0.47633096575737\n",
      "Epoch 5418: train loss: 0.4763306975364685\n",
      "Epoch 5419: train loss: 0.4763304889202118\n",
      "Epoch 5420: train loss: 0.4763302206993103\n",
      "Epoch 5421: train loss: 0.4763299524784088\n",
      "Epoch 5422: train loss: 0.4763298034667969\n",
      "Epoch 5423: train loss: 0.4763295352458954\n",
      "Epoch 5424: train loss: 0.4763292372226715\n",
      "Epoch 5425: train loss: 0.4763290286064148\n",
      "Epoch 5426: train loss: 0.4763288199901581\n",
      "Epoch 5427: train loss: 0.47632861137390137\n",
      "Epoch 5428: train loss: 0.4763283431529999\n",
      "Epoch 5429: train loss: 0.4763280749320984\n",
      "Epoch 5430: train loss: 0.4763278067111969\n",
      "Epoch 5431: train loss: 0.47632765769958496\n",
      "Epoch 5432: train loss: 0.47632744908332825\n",
      "Epoch 5433: train loss: 0.47632715106010437\n",
      "Epoch 5434: train loss: 0.47632694244384766\n",
      "Epoch 5435: train loss: 0.47632667422294617\n",
      "Epoch 5436: train loss: 0.4763264060020447\n",
      "Epoch 5437: train loss: 0.47632619738578796\n",
      "Epoch 5438: train loss: 0.4763259291648865\n",
      "Epoch 5439: train loss: 0.47632572054862976\n",
      "Epoch 5440: train loss: 0.47632545232772827\n",
      "Epoch 5441: train loss: 0.47632521390914917\n",
      "Epoch 5442: train loss: 0.47632500529289246\n",
      "Epoch 5443: train loss: 0.47632479667663574\n",
      "Epoch 5444: train loss: 0.47632452845573425\n",
      "Epoch 5445: train loss: 0.47632426023483276\n",
      "Epoch 5446: train loss: 0.47632405161857605\n",
      "Epoch 5447: train loss: 0.47632378339767456\n",
      "Epoch 5448: train loss: 0.4763236343860626\n",
      "Epoch 5449: train loss: 0.47632333636283875\n",
      "Epoch 5450: train loss: 0.47632306814193726\n",
      "Epoch 5451: train loss: 0.4763229191303253\n",
      "Epoch 5452: train loss: 0.47632265090942383\n",
      "Epoch 5453: train loss: 0.4763224422931671\n",
      "Epoch 5454: train loss: 0.4763221740722656\n",
      "Epoch 5455: train loss: 0.47632190585136414\n",
      "Epoch 5456: train loss: 0.4763216972351074\n",
      "Epoch 5457: train loss: 0.4763214886188507\n",
      "Epoch 5458: train loss: 0.4763212203979492\n",
      "Epoch 5459: train loss: 0.4763209819793701\n",
      "Epoch 5460: train loss: 0.4763207733631134\n",
      "Epoch 5461: train loss: 0.4763205647468567\n",
      "Epoch 5462: train loss: 0.4763202369213104\n",
      "Epoch 5463: train loss: 0.4763200283050537\n",
      "Epoch 5464: train loss: 0.476319819688797\n",
      "Epoch 5465: train loss: 0.4763195514678955\n",
      "Epoch 5466: train loss: 0.4763193130493164\n",
      "Epoch 5467: train loss: 0.4763191044330597\n",
      "Epoch 5468: train loss: 0.4763188362121582\n",
      "Epoch 5469: train loss: 0.47631868720054626\n",
      "Epoch 5470: train loss: 0.476318359375\n",
      "Epoch 5471: train loss: 0.4763180911540985\n",
      "Epoch 5472: train loss: 0.4763179421424866\n",
      "Epoch 5473: train loss: 0.4763176739215851\n",
      "Epoch 5474: train loss: 0.47631746530532837\n",
      "Epoch 5475: train loss: 0.4763171970844269\n",
      "Epoch 5476: train loss: 0.4763169586658478\n",
      "Epoch 5477: train loss: 0.47631675004959106\n",
      "Epoch 5478: train loss: 0.4763164818286896\n",
      "Epoch 5479: train loss: 0.47631627321243286\n",
      "Epoch 5480: train loss: 0.47631606459617615\n",
      "Epoch 5481: train loss: 0.4763157367706299\n",
      "Epoch 5482: train loss: 0.47631552815437317\n",
      "Epoch 5483: train loss: 0.47631531953811646\n",
      "Epoch 5484: train loss: 0.47631508111953735\n",
      "Epoch 5485: train loss: 0.47631487250328064\n",
      "Epoch 5486: train loss: 0.47631460428237915\n",
      "Epoch 5487: train loss: 0.47631439566612244\n",
      "Epoch 5488: train loss: 0.4763141870498657\n",
      "Epoch 5489: train loss: 0.47631385922431946\n",
      "Epoch 5490: train loss: 0.47631365060806274\n",
      "Epoch 5491: train loss: 0.47631344199180603\n",
      "Epoch 5492: train loss: 0.4763132333755493\n",
      "Epoch 5493: train loss: 0.4763129651546478\n",
      "Epoch 5494: train loss: 0.4763127565383911\n",
      "Epoch 5495: train loss: 0.476312518119812\n",
      "Epoch 5496: train loss: 0.4763123095035553\n",
      "Epoch 5497: train loss: 0.4763121008872986\n",
      "Epoch 5498: train loss: 0.4763118326663971\n",
      "Epoch 5499: train loss: 0.4763115644454956\n",
      "Epoch 5500: train loss: 0.4763112962245941\n",
      "Epoch 5501: train loss: 0.4763110876083374\n",
      "Epoch 5502: train loss: 0.4763108789920807\n",
      "Epoch 5503: train loss: 0.4763106107711792\n",
      "Epoch 5504: train loss: 0.4763104021549225\n",
      "Epoch 5505: train loss: 0.47631019353866577\n",
      "Epoch 5506: train loss: 0.4763098955154419\n",
      "Epoch 5507: train loss: 0.4763096868991852\n",
      "Epoch 5508: train loss: 0.47630947828292847\n",
      "Epoch 5509: train loss: 0.47630926966667175\n",
      "Epoch 5510: train loss: 0.47630906105041504\n",
      "Epoch 5511: train loss: 0.47630879282951355\n",
      "Epoch 5512: train loss: 0.47630852460861206\n",
      "Epoch 5513: train loss: 0.47630831599235535\n",
      "Epoch 5514: train loss: 0.47630807757377625\n",
      "Epoch 5515: train loss: 0.47630786895751953\n",
      "Epoch 5516: train loss: 0.47630760073661804\n",
      "Epoch 5517: train loss: 0.47630739212036133\n",
      "Epoch 5518: train loss: 0.47630712389945984\n",
      "Epoch 5519: train loss: 0.4763069748878479\n",
      "Epoch 5520: train loss: 0.4763067066669464\n",
      "Epoch 5521: train loss: 0.4763064980506897\n",
      "Epoch 5522: train loss: 0.4763062298297882\n",
      "Epoch 5523: train loss: 0.4763059616088867\n",
      "Epoch 5524: train loss: 0.4763058125972748\n",
      "Epoch 5525: train loss: 0.4763055443763733\n",
      "Epoch 5526: train loss: 0.4763053059577942\n",
      "Epoch 5527: train loss: 0.4763050973415375\n",
      "Epoch 5528: train loss: 0.47630488872528076\n",
      "Epoch 5529: train loss: 0.4763045608997345\n",
      "Epoch 5530: train loss: 0.4763043522834778\n",
      "Epoch 5531: train loss: 0.47630414366722107\n",
      "Epoch 5532: train loss: 0.47630393505096436\n",
      "Epoch 5533: train loss: 0.47630366683006287\n",
      "Epoch 5534: train loss: 0.47630345821380615\n",
      "Epoch 5535: train loss: 0.47630324959754944\n",
      "Epoch 5536: train loss: 0.47630298137664795\n",
      "Epoch 5537: train loss: 0.47630274295806885\n",
      "Epoch 5538: train loss: 0.47630253434181213\n",
      "Epoch 5539: train loss: 0.47630226612091064\n",
      "Epoch 5540: train loss: 0.47630205750465393\n",
      "Epoch 5541: train loss: 0.47630178928375244\n",
      "Epoch 5542: train loss: 0.4763015806674957\n",
      "Epoch 5543: train loss: 0.476301372051239\n",
      "Epoch 5544: train loss: 0.4763011038303375\n",
      "Epoch 5545: train loss: 0.4763008952140808\n",
      "Epoch 5546: train loss: 0.4763006865978241\n",
      "Epoch 5547: train loss: 0.476300448179245\n",
      "Epoch 5548: train loss: 0.4763001799583435\n",
      "Epoch 5549: train loss: 0.4762999713420868\n",
      "Epoch 5550: train loss: 0.4762997627258301\n",
      "Epoch 5551: train loss: 0.4762994945049286\n",
      "Epoch 5552: train loss: 0.4762992858886719\n",
      "Epoch 5553: train loss: 0.47629907727241516\n",
      "Epoch 5554: train loss: 0.47629886865615845\n",
      "Epoch 5555: train loss: 0.4762985408306122\n",
      "Epoch 5556: train loss: 0.47629839181900024\n",
      "Epoch 5557: train loss: 0.47629812359809875\n",
      "Epoch 5558: train loss: 0.47629791498184204\n",
      "Epoch 5559: train loss: 0.47629767656326294\n",
      "Epoch 5560: train loss: 0.47629740834236145\n",
      "Epoch 5561: train loss: 0.47629719972610474\n",
      "Epoch 5562: train loss: 0.476296991109848\n",
      "Epoch 5563: train loss: 0.4762967824935913\n",
      "Epoch 5564: train loss: 0.4762965142726898\n",
      "Epoch 5565: train loss: 0.4762963056564331\n",
      "Epoch 5566: train loss: 0.4762960374355316\n",
      "Epoch 5567: train loss: 0.4762958288192749\n",
      "Epoch 5568: train loss: 0.4762955904006958\n",
      "Epoch 5569: train loss: 0.4762953519821167\n",
      "Epoch 5570: train loss: 0.47629514336586\n",
      "Epoch 5571: train loss: 0.4762949049472809\n",
      "Epoch 5572: train loss: 0.47629469633102417\n",
      "Epoch 5573: train loss: 0.47629451751708984\n",
      "Epoch 5574: train loss: 0.47629421949386597\n",
      "Epoch 5575: train loss: 0.47629401087760925\n",
      "Epoch 5576: train loss: 0.47629374265670776\n",
      "Epoch 5577: train loss: 0.47629353404045105\n",
      "Epoch 5578: train loss: 0.47629326581954956\n",
      "Epoch 5579: train loss: 0.47629305720329285\n",
      "Epoch 5580: train loss: 0.47629284858703613\n",
      "Epoch 5581: train loss: 0.47629261016845703\n",
      "Epoch 5582: train loss: 0.4762924313545227\n",
      "Epoch 5583: train loss: 0.4762921929359436\n",
      "Epoch 5584: train loss: 0.4762919247150421\n",
      "Epoch 5585: train loss: 0.4762917160987854\n",
      "Epoch 5586: train loss: 0.4762915074825287\n",
      "Epoch 5587: train loss: 0.476291298866272\n",
      "Epoch 5588: train loss: 0.47629109025001526\n",
      "Epoch 5589: train loss: 0.476290762424469\n",
      "Epoch 5590: train loss: 0.4762905538082123\n",
      "Epoch 5591: train loss: 0.47629034519195557\n",
      "Epoch 5592: train loss: 0.47629013657569885\n",
      "Epoch 5593: train loss: 0.47628992795944214\n",
      "Epoch 5594: train loss: 0.47628965973854065\n",
      "Epoch 5595: train loss: 0.47628942131996155\n",
      "Epoch 5596: train loss: 0.47628921270370483\n",
      "Epoch 5597: train loss: 0.47628894448280334\n",
      "Epoch 5598: train loss: 0.4762887954711914\n",
      "Epoch 5599: train loss: 0.4762885272502899\n",
      "Epoch 5600: train loss: 0.4762882590293884\n",
      "Epoch 5601: train loss: 0.4762880504131317\n",
      "Epoch 5602: train loss: 0.4762877821922302\n",
      "Epoch 5603: train loss: 0.4762875735759735\n",
      "Epoch 5604: train loss: 0.4762873649597168\n",
      "Epoch 5605: train loss: 0.4762871563434601\n",
      "Epoch 5606: train loss: 0.4762868881225586\n",
      "Epoch 5607: train loss: 0.4762866795063019\n",
      "Epoch 5608: train loss: 0.47628647089004517\n",
      "Epoch 5609: train loss: 0.47628623247146606\n",
      "Epoch 5610: train loss: 0.47628602385520935\n",
      "Epoch 5611: train loss: 0.47628581523895264\n",
      "Epoch 5612: train loss: 0.47628554701805115\n",
      "Epoch 5613: train loss: 0.47628533840179443\n",
      "Epoch 5614: train loss: 0.4762851297855377\n",
      "Epoch 5615: train loss: 0.47628486156463623\n",
      "Epoch 5616: train loss: 0.4762847125530243\n",
      "Epoch 5617: train loss: 0.476284384727478\n",
      "Epoch 5618: train loss: 0.4762842357158661\n",
      "Epoch 5619: train loss: 0.4762839674949646\n",
      "Epoch 5620: train loss: 0.4762837588787079\n",
      "Epoch 5621: train loss: 0.47628355026245117\n",
      "Epoch 5622: train loss: 0.4762832522392273\n",
      "Epoch 5623: train loss: 0.47628307342529297\n",
      "Epoch 5624: train loss: 0.47628283500671387\n",
      "Epoch 5625: train loss: 0.47628262639045715\n",
      "Epoch 5626: train loss: 0.47628241777420044\n",
      "Epoch 5627: train loss: 0.4762822091579437\n",
      "Epoch 5628: train loss: 0.47628194093704224\n",
      "Epoch 5629: train loss: 0.4762817323207855\n",
      "Epoch 5630: train loss: 0.47628146409988403\n",
      "Epoch 5631: train loss: 0.4762812554836273\n",
      "Epoch 5632: train loss: 0.4762810468673706\n",
      "Epoch 5633: train loss: 0.4762807786464691\n",
      "Epoch 5634: train loss: 0.4762805700302124\n",
      "Epoch 5635: train loss: 0.4762803614139557\n",
      "Epoch 5636: train loss: 0.4762800931930542\n",
      "Epoch 5637: train loss: 0.4762798845767975\n",
      "Epoch 5638: train loss: 0.47627967596054077\n",
      "Epoch 5639: train loss: 0.47627946734428406\n",
      "Epoch 5640: train loss: 0.47627922892570496\n",
      "Epoch 5641: train loss: 0.47627896070480347\n",
      "Epoch 5642: train loss: 0.47627881169319153\n",
      "Epoch 5643: train loss: 0.47627854347229004\n",
      "Epoch 5644: train loss: 0.4762783348560333\n",
      "Epoch 5645: train loss: 0.4762781262397766\n",
      "Epoch 5646: train loss: 0.4762779176235199\n",
      "Epoch 5647: train loss: 0.47627758979797363\n",
      "Epoch 5648: train loss: 0.4762774407863617\n",
      "Epoch 5649: train loss: 0.476277232170105\n",
      "Epoch 5650: train loss: 0.4762769639492035\n",
      "Epoch 5651: train loss: 0.4762767255306244\n",
      "Epoch 5652: train loss: 0.4762765169143677\n",
      "Epoch 5653: train loss: 0.47627633810043335\n",
      "Epoch 5654: train loss: 0.47627612948417664\n",
      "Epoch 5655: train loss: 0.47627586126327515\n",
      "Epoch 5656: train loss: 0.47627562284469604\n",
      "Epoch 5657: train loss: 0.47627541422843933\n",
      "Epoch 5658: train loss: 0.4762752056121826\n",
      "Epoch 5659: train loss: 0.47627493739128113\n",
      "Epoch 5660: train loss: 0.4762747287750244\n",
      "Epoch 5661: train loss: 0.4762745201587677\n",
      "Epoch 5662: train loss: 0.476274311542511\n",
      "Epoch 5663: train loss: 0.4762741029262543\n",
      "Epoch 5664: train loss: 0.4762738347053528\n",
      "Epoch 5665: train loss: 0.4762735664844513\n",
      "Epoch 5666: train loss: 0.4762733578681946\n",
      "Epoch 5667: train loss: 0.47627314925193787\n",
      "Epoch 5668: train loss: 0.47627294063568115\n",
      "Epoch 5669: train loss: 0.47627273201942444\n",
      "Epoch 5670: train loss: 0.4762725234031677\n",
      "Epoch 5671: train loss: 0.476272314786911\n",
      "Epoch 5672: train loss: 0.4762720465660095\n",
      "Epoch 5673: train loss: 0.4762718379497528\n",
      "Epoch 5674: train loss: 0.4762715995311737\n",
      "Epoch 5675: train loss: 0.476271390914917\n",
      "Epoch 5676: train loss: 0.4762711226940155\n",
      "Epoch 5677: train loss: 0.4762709140777588\n",
      "Epoch 5678: train loss: 0.47627076506614685\n",
      "Epoch 5679: train loss: 0.47627049684524536\n",
      "Epoch 5680: train loss: 0.47627028822898865\n",
      "Epoch 5681: train loss: 0.47627002000808716\n",
      "Epoch 5682: train loss: 0.4762698709964752\n",
      "Epoch 5683: train loss: 0.47626960277557373\n",
      "Epoch 5684: train loss: 0.476269394159317\n",
      "Epoch 5685: train loss: 0.4762691259384155\n",
      "Epoch 5686: train loss: 0.4762688875198364\n",
      "Epoch 5687: train loss: 0.4762687087059021\n",
      "Epoch 5688: train loss: 0.4762685000896454\n",
      "Epoch 5689: train loss: 0.4762682318687439\n",
      "Epoch 5690: train loss: 0.4762680232524872\n",
      "Epoch 5691: train loss: 0.47626781463623047\n",
      "Epoch 5692: train loss: 0.47626757621765137\n",
      "Epoch 5693: train loss: 0.47626739740371704\n",
      "Epoch 5694: train loss: 0.47626715898513794\n",
      "Epoch 5695: train loss: 0.4762669503688812\n",
      "Epoch 5696: train loss: 0.47626668214797974\n",
      "Epoch 5697: train loss: 0.47626641392707825\n",
      "Epoch 5698: train loss: 0.4762662649154663\n",
      "Epoch 5699: train loss: 0.4762660562992096\n",
      "Epoch 5700: train loss: 0.4762657880783081\n",
      "Epoch 5701: train loss: 0.4762655794620514\n",
      "Epoch 5702: train loss: 0.4762653708457947\n",
      "Epoch 5703: train loss: 0.4762651026248932\n",
      "Epoch 5704: train loss: 0.47626495361328125\n",
      "Epoch 5705: train loss: 0.47626468539237976\n",
      "Epoch 5706: train loss: 0.47626447677612305\n",
      "Epoch 5707: train loss: 0.47626426815986633\n",
      "Epoch 5708: train loss: 0.4762640595436096\n",
      "Epoch 5709: train loss: 0.47626379132270813\n",
      "Epoch 5710: train loss: 0.47626355290412903\n",
      "Epoch 5711: train loss: 0.4762633740901947\n",
      "Epoch 5712: train loss: 0.4762631356716156\n",
      "Epoch 5713: train loss: 0.4762629568576813\n",
      "Epoch 5714: train loss: 0.4762627184391022\n",
      "Epoch 5715: train loss: 0.47626250982284546\n",
      "Epoch 5716: train loss: 0.47626230120658875\n",
      "Epoch 5717: train loss: 0.47626209259033203\n",
      "Epoch 5718: train loss: 0.47626182436943054\n",
      "Epoch 5719: train loss: 0.47626161575317383\n",
      "Epoch 5720: train loss: 0.4762614071369171\n",
      "Epoch 5721: train loss: 0.4762611389160156\n",
      "Epoch 5722: train loss: 0.47626087069511414\n",
      "Epoch 5723: train loss: 0.4762607216835022\n",
      "Epoch 5724: train loss: 0.4762604534626007\n",
      "Epoch 5725: train loss: 0.47626030445098877\n",
      "Epoch 5726: train loss: 0.4762600362300873\n",
      "Epoch 5727: train loss: 0.47625982761383057\n",
      "Epoch 5728: train loss: 0.47625961899757385\n",
      "Epoch 5729: train loss: 0.47625941038131714\n",
      "Epoch 5730: train loss: 0.4762592017650604\n",
      "Epoch 5731: train loss: 0.47625893354415894\n",
      "Epoch 5732: train loss: 0.4762587249279022\n",
      "Epoch 5733: train loss: 0.4762584865093231\n",
      "Epoch 5734: train loss: 0.4762582778930664\n",
      "Epoch 5735: train loss: 0.4762580692768097\n",
      "Epoch 5736: train loss: 0.476257860660553\n",
      "Epoch 5737: train loss: 0.47625765204429626\n",
      "Epoch 5738: train loss: 0.47625744342803955\n",
      "Epoch 5739: train loss: 0.47625717520713806\n",
      "Epoch 5740: train loss: 0.47625696659088135\n",
      "Epoch 5741: train loss: 0.47625675797462463\n",
      "Epoch 5742: train loss: 0.4762565493583679\n",
      "Epoch 5743: train loss: 0.47625628113746643\n",
      "Epoch 5744: train loss: 0.4762561321258545\n",
      "Epoch 5745: train loss: 0.476255863904953\n",
      "Epoch 5746: train loss: 0.4762556552886963\n",
      "Epoch 5747: train loss: 0.4762554466724396\n",
      "Epoch 5748: train loss: 0.4762551784515381\n",
      "Epoch 5749: train loss: 0.47625496983528137\n",
      "Epoch 5750: train loss: 0.47625482082366943\n",
      "Epoch 5751: train loss: 0.47625455260276794\n",
      "Epoch 5752: train loss: 0.47625428438186646\n",
      "Epoch 5753: train loss: 0.47625407576560974\n",
      "Epoch 5754: train loss: 0.4762539267539978\n",
      "Epoch 5755: train loss: 0.4762536585330963\n",
      "Epoch 5756: train loss: 0.4762534499168396\n",
      "Epoch 5757: train loss: 0.4762532413005829\n",
      "Epoch 5758: train loss: 0.47625303268432617\n",
      "Epoch 5759: train loss: 0.47625282406806946\n",
      "Epoch 5760: train loss: 0.47625255584716797\n",
      "Epoch 5761: train loss: 0.47625240683555603\n",
      "Epoch 5762: train loss: 0.47625210881233215\n",
      "Epoch 5763: train loss: 0.4762519299983978\n",
      "Epoch 5764: train loss: 0.4762516915798187\n",
      "Epoch 5765: train loss: 0.4762515127658844\n",
      "Epoch 5766: train loss: 0.4762512743473053\n",
      "Epoch 5767: train loss: 0.4762510657310486\n",
      "Epoch 5768: train loss: 0.47625085711479187\n",
      "Epoch 5769: train loss: 0.47625064849853516\n",
      "Epoch 5770: train loss: 0.4762503206729889\n",
      "Epoch 5771: train loss: 0.47625017166137695\n",
      "Epoch 5772: train loss: 0.47624996304512024\n",
      "Epoch 5773: train loss: 0.4762498140335083\n",
      "Epoch 5774: train loss: 0.4762495458126068\n",
      "Epoch 5775: train loss: 0.4762492775917053\n",
      "Epoch 5776: train loss: 0.4762490689754486\n",
      "Epoch 5777: train loss: 0.4762488603591919\n",
      "Epoch 5778: train loss: 0.4762486517429352\n",
      "Epoch 5779: train loss: 0.47624844312667847\n",
      "Epoch 5780: train loss: 0.47624823451042175\n",
      "Epoch 5781: train loss: 0.47624796628952026\n",
      "Epoch 5782: train loss: 0.47624775767326355\n",
      "Epoch 5783: train loss: 0.47624754905700684\n",
      "Epoch 5784: train loss: 0.4762473404407501\n",
      "Epoch 5785: train loss: 0.4762471318244934\n",
      "Epoch 5786: train loss: 0.4762469232082367\n",
      "Epoch 5787: train loss: 0.47624671459198\n",
      "Epoch 5788: train loss: 0.47624650597572327\n",
      "Epoch 5789: train loss: 0.4762462377548218\n",
      "Epoch 5790: train loss: 0.47624602913856506\n",
      "Epoch 5791: train loss: 0.47624582052230835\n",
      "Epoch 5792: train loss: 0.47624561190605164\n",
      "Epoch 5793: train loss: 0.4762454032897949\n",
      "Epoch 5794: train loss: 0.4762451946735382\n",
      "Epoch 5795: train loss: 0.4762449860572815\n",
      "Epoch 5796: train loss: 0.47624471783638\n",
      "Epoch 5797: train loss: 0.4762445092201233\n",
      "Epoch 5798: train loss: 0.47624436020851135\n",
      "Epoch 5799: train loss: 0.47624409198760986\n",
      "Epoch 5800: train loss: 0.4762439429759979\n",
      "Epoch 5801: train loss: 0.47624364495277405\n",
      "Epoch 5802: train loss: 0.47624343633651733\n",
      "Epoch 5803: train loss: 0.476243257522583\n",
      "Epoch 5804: train loss: 0.4762430191040039\n",
      "Epoch 5805: train loss: 0.4762428104877472\n",
      "Epoch 5806: train loss: 0.4762426018714905\n",
      "Epoch 5807: train loss: 0.476242333650589\n",
      "Epoch 5808: train loss: 0.47624218463897705\n",
      "Epoch 5809: train loss: 0.47624194622039795\n",
      "Epoch 5810: train loss: 0.47624170780181885\n",
      "Epoch 5811: train loss: 0.47624149918556213\n",
      "Epoch 5812: train loss: 0.4762412905693054\n",
      "Epoch 5813: train loss: 0.4762410819530487\n",
      "Epoch 5814: train loss: 0.476240873336792\n",
      "Epoch 5815: train loss: 0.4762406051158905\n",
      "Epoch 5816: train loss: 0.4762403964996338\n",
      "Epoch 5817: train loss: 0.4762401878833771\n",
      "Epoch 5818: train loss: 0.47623997926712036\n",
      "Epoch 5819: train loss: 0.47623977065086365\n",
      "Epoch 5820: train loss: 0.47623956203460693\n",
      "Epoch 5821: train loss: 0.4762393534183502\n",
      "Epoch 5822: train loss: 0.4762391448020935\n",
      "Epoch 5823: train loss: 0.4762389361858368\n",
      "Epoch 5824: train loss: 0.4762386679649353\n",
      "Epoch 5825: train loss: 0.4762384593486786\n",
      "Epoch 5826: train loss: 0.47623831033706665\n",
      "Epoch 5827: train loss: 0.47623810172080994\n",
      "Epoch 5828: train loss: 0.47623783349990845\n",
      "Epoch 5829: train loss: 0.47623762488365173\n",
      "Epoch 5830: train loss: 0.476237416267395\n",
      "Epoch 5831: train loss: 0.4762372672557831\n",
      "Epoch 5832: train loss: 0.4762369394302368\n",
      "Epoch 5833: train loss: 0.4762367308139801\n",
      "Epoch 5834: train loss: 0.4762365221977234\n",
      "Epoch 5835: train loss: 0.4762363135814667\n",
      "Epoch 5836: train loss: 0.47623610496520996\n",
      "Epoch 5837: train loss: 0.47623589634895325\n",
      "Epoch 5838: train loss: 0.47623568773269653\n",
      "Epoch 5839: train loss: 0.4762354791164398\n",
      "Epoch 5840: train loss: 0.4762352705001831\n",
      "Epoch 5841: train loss: 0.4762350022792816\n",
      "Epoch 5842: train loss: 0.4762348532676697\n",
      "Epoch 5843: train loss: 0.4762345850467682\n",
      "Epoch 5844: train loss: 0.4762343764305115\n",
      "Epoch 5845: train loss: 0.47623416781425476\n",
      "Epoch 5846: train loss: 0.47623395919799805\n",
      "Epoch 5847: train loss: 0.47623369097709656\n",
      "Epoch 5848: train loss: 0.4762335419654846\n",
      "Epoch 5849: train loss: 0.4762333333492279\n",
      "Epoch 5850: train loss: 0.4762331247329712\n",
      "Epoch 5851: train loss: 0.4762329161167145\n",
      "Epoch 5852: train loss: 0.476232647895813\n",
      "Epoch 5853: train loss: 0.4762324392795563\n",
      "Epoch 5854: train loss: 0.47623223066329956\n",
      "Epoch 5855: train loss: 0.47623202204704285\n",
      "Epoch 5856: train loss: 0.47623181343078613\n",
      "Epoch 5857: train loss: 0.4762316048145294\n",
      "Epoch 5858: train loss: 0.4762313961982727\n",
      "Epoch 5859: train loss: 0.476231187582016\n",
      "Epoch 5860: train loss: 0.4762309789657593\n",
      "Epoch 5861: train loss: 0.47623077034950256\n",
      "Epoch 5862: train loss: 0.4762305021286011\n",
      "Epoch 5863: train loss: 0.47623029351234436\n",
      "Epoch 5864: train loss: 0.47623008489608765\n",
      "Epoch 5865: train loss: 0.4762299358844757\n",
      "Epoch 5866: train loss: 0.4762296676635742\n",
      "Epoch 5867: train loss: 0.4762294590473175\n",
      "Epoch 5868: train loss: 0.4762292504310608\n",
      "Epoch 5869: train loss: 0.4762290418148041\n",
      "Epoch 5870: train loss: 0.47622883319854736\n",
      "Epoch 5871: train loss: 0.47622859477996826\n",
      "Epoch 5872: train loss: 0.4762284755706787\n",
      "Epoch 5873: train loss: 0.47622817754745483\n",
      "Epoch 5874: train loss: 0.4762279987335205\n",
      "Epoch 5875: train loss: 0.4762277603149414\n",
      "Epoch 5876: train loss: 0.4762275516986847\n",
      "Epoch 5877: train loss: 0.476227343082428\n",
      "Epoch 5878: train loss: 0.47622713446617126\n",
      "Epoch 5879: train loss: 0.47622692584991455\n",
      "Epoch 5880: train loss: 0.47622668743133545\n",
      "Epoch 5881: train loss: 0.47622647881507874\n",
      "Epoch 5882: train loss: 0.4762263298034668\n",
      "Epoch 5883: train loss: 0.4762261211872101\n",
      "Epoch 5884: train loss: 0.4762258529663086\n",
      "Epoch 5885: train loss: 0.47622570395469666\n",
      "Epoch 5886: train loss: 0.47622543573379517\n",
      "Epoch 5887: train loss: 0.47622522711753845\n",
      "Epoch 5888: train loss: 0.47622501850128174\n",
      "Epoch 5889: train loss: 0.476224809885025\n",
      "Epoch 5890: train loss: 0.4762246012687683\n",
      "Epoch 5891: train loss: 0.4762243926525116\n",
      "Epoch 5892: train loss: 0.4762241840362549\n",
      "Epoch 5893: train loss: 0.47622397541999817\n",
      "Epoch 5894: train loss: 0.47622373700141907\n",
      "Epoch 5895: train loss: 0.47622352838516235\n",
      "Epoch 5896: train loss: 0.47622331976890564\n",
      "Epoch 5897: train loss: 0.4762231111526489\n",
      "Epoch 5898: train loss: 0.4762229323387146\n",
      "Epoch 5899: train loss: 0.4762226641178131\n",
      "Epoch 5900: train loss: 0.4762224555015564\n",
      "Epoch 5901: train loss: 0.4762222468852997\n",
      "Epoch 5902: train loss: 0.4762220084667206\n",
      "Epoch 5903: train loss: 0.47622179985046387\n",
      "Epoch 5904: train loss: 0.47622162103652954\n",
      "Epoch 5905: train loss: 0.4762214124202728\n",
      "Epoch 5906: train loss: 0.4762212038040161\n",
      "Epoch 5907: train loss: 0.4762209951877594\n",
      "Epoch 5908: train loss: 0.4762207865715027\n",
      "Epoch 5909: train loss: 0.47622057795524597\n",
      "Epoch 5910: train loss: 0.47622033953666687\n",
      "Epoch 5911: train loss: 0.47622016072273254\n",
      "Epoch 5912: train loss: 0.47621995210647583\n",
      "Epoch 5913: train loss: 0.4762197434902191\n",
      "Epoch 5914: train loss: 0.47621950507164\n",
      "Epoch 5915: train loss: 0.4762193262577057\n",
      "Epoch 5916: train loss: 0.4762190580368042\n",
      "Epoch 5917: train loss: 0.4762188494205475\n",
      "Epoch 5918: train loss: 0.47621864080429077\n",
      "Epoch 5919: train loss: 0.47621843218803406\n",
      "Epoch 5920: train loss: 0.47621822357177734\n",
      "Epoch 5921: train loss: 0.4762180745601654\n",
      "Epoch 5922: train loss: 0.4762178063392639\n",
      "Epoch 5923: train loss: 0.4762175977230072\n",
      "Epoch 5924: train loss: 0.47621744871139526\n",
      "Epoch 5925: train loss: 0.4762171804904938\n",
      "Epoch 5926: train loss: 0.47621697187423706\n",
      "Epoch 5927: train loss: 0.47621676325798035\n",
      "Epoch 5928: train loss: 0.47621652483940125\n",
      "Epoch 5929: train loss: 0.4762164056301117\n",
      "Epoch 5930: train loss: 0.4762161374092102\n",
      "Epoch 5931: train loss: 0.4762159287929535\n",
      "Epoch 5932: train loss: 0.4762157201766968\n",
      "Epoch 5933: train loss: 0.47621551156044006\n",
      "Epoch 5934: train loss: 0.47621530294418335\n",
      "Epoch 5935: train loss: 0.47621509432792664\n",
      "Epoch 5936: train loss: 0.4762148857116699\n",
      "Epoch 5937: train loss: 0.4762146770954132\n",
      "Epoch 5938: train loss: 0.47621452808380127\n",
      "Epoch 5939: train loss: 0.4762142598628998\n",
      "Epoch 5940: train loss: 0.47621405124664307\n",
      "Epoch 5941: train loss: 0.47621384263038635\n",
      "Epoch 5942: train loss: 0.47621363401412964\n",
      "Epoch 5943: train loss: 0.4762134253978729\n",
      "Epoch 5944: train loss: 0.4762132167816162\n",
      "Epoch 5945: train loss: 0.4762130081653595\n",
      "Epoch 5946: train loss: 0.4762127995491028\n",
      "Epoch 5947: train loss: 0.47621259093284607\n",
      "Epoch 5948: train loss: 0.47621238231658936\n",
      "Epoch 5949: train loss: 0.47621217370033264\n",
      "Epoch 5950: train loss: 0.4762119650840759\n",
      "Epoch 5951: train loss: 0.4762117564678192\n",
      "Epoch 5952: train loss: 0.4762115478515625\n",
      "Epoch 5953: train loss: 0.4762113392353058\n",
      "Epoch 5954: train loss: 0.47621119022369385\n",
      "Epoch 5955: train loss: 0.47621092200279236\n",
      "Epoch 5956: train loss: 0.47621065378189087\n",
      "Epoch 5957: train loss: 0.47621050477027893\n",
      "Epoch 5958: train loss: 0.4762102961540222\n",
      "Epoch 5959: train loss: 0.4762100279331207\n",
      "Epoch 5960: train loss: 0.4762098789215088\n",
      "Epoch 5961: train loss: 0.4762096703052521\n",
      "Epoch 5962: train loss: 0.47620946168899536\n",
      "Epoch 5963: train loss: 0.47620925307273865\n",
      "Epoch 5964: train loss: 0.47620904445648193\n",
      "Epoch 5965: train loss: 0.4762088358402252\n",
      "Epoch 5966: train loss: 0.4762086272239685\n",
      "Epoch 5967: train loss: 0.4762084186077118\n",
      "Epoch 5968: train loss: 0.4762082099914551\n",
      "Epoch 5969: train loss: 0.47620800137519836\n",
      "Epoch 5970: train loss: 0.4762078523635864\n",
      "Epoch 5971: train loss: 0.47620758414268494\n",
      "Epoch 5972: train loss: 0.4762073755264282\n",
      "Epoch 5973: train loss: 0.4762071669101715\n",
      "Epoch 5974: train loss: 0.4762069582939148\n",
      "Epoch 5975: train loss: 0.47620677947998047\n",
      "Epoch 5976: train loss: 0.47620657086372375\n",
      "Epoch 5977: train loss: 0.47620633244514465\n",
      "Epoch 5978: train loss: 0.47620612382888794\n",
      "Epoch 5979: train loss: 0.4762059152126312\n",
      "Epoch 5980: train loss: 0.4762057065963745\n",
      "Epoch 5981: train loss: 0.4762055277824402\n",
      "Epoch 5982: train loss: 0.4762052893638611\n",
      "Epoch 5983: train loss: 0.47620508074760437\n",
      "Epoch 5984: train loss: 0.47620490193367004\n",
      "Epoch 5985: train loss: 0.47620469331741333\n",
      "Epoch 5986: train loss: 0.47620445489883423\n",
      "Epoch 5987: train loss: 0.4762042760848999\n",
      "Epoch 5988: train loss: 0.4762040674686432\n",
      "Epoch 5989: train loss: 0.4762037992477417\n",
      "Epoch 5990: train loss: 0.47620365023612976\n",
      "Epoch 5991: train loss: 0.47620344161987305\n",
      "Epoch 5992: train loss: 0.47620323300361633\n",
      "Epoch 5993: train loss: 0.4762030243873596\n",
      "Epoch 5994: train loss: 0.4762028753757477\n",
      "Epoch 5995: train loss: 0.4762026071548462\n",
      "Epoch 5996: train loss: 0.4762023985385895\n",
      "Epoch 5997: train loss: 0.47620224952697754\n",
      "Epoch 5998: train loss: 0.4762019217014313\n",
      "Epoch 5999: train loss: 0.47620177268981934\n",
      "Epoch 6000: train loss: 0.4762015640735626\n",
      "Epoch 6001: train loss: 0.4762014150619507\n",
      "Epoch 6002: train loss: 0.4762011468410492\n",
      "Epoch 6003: train loss: 0.47620099782943726\n",
      "Epoch 6004: train loss: 0.47620072960853577\n",
      "Epoch 6005: train loss: 0.47620052099227905\n",
      "Epoch 6006: train loss: 0.4762003719806671\n",
      "Epoch 6007: train loss: 0.4762001037597656\n",
      "Epoch 6008: train loss: 0.4761999547481537\n",
      "Epoch 6009: train loss: 0.476199746131897\n",
      "Epoch 6010: train loss: 0.4761994779109955\n",
      "Epoch 6011: train loss: 0.47619932889938354\n",
      "Epoch 6012: train loss: 0.47619912028312683\n",
      "Epoch 6013: train loss: 0.4761989116668701\n",
      "Epoch 6014: train loss: 0.4761987030506134\n",
      "Epoch 6015: train loss: 0.4761984944343567\n",
      "Epoch 6016: train loss: 0.4761982858181\n",
      "Epoch 6017: train loss: 0.47619807720184326\n",
      "Epoch 6018: train loss: 0.47619786858558655\n",
      "Epoch 6019: train loss: 0.4761976897716522\n",
      "Epoch 6020: train loss: 0.4761974811553955\n",
      "Epoch 6021: train loss: 0.4761972725391388\n",
      "Epoch 6022: train loss: 0.4761970639228821\n",
      "Epoch 6023: train loss: 0.47619685530662537\n",
      "Epoch 6024: train loss: 0.4761967062950134\n",
      "Epoch 6025: train loss: 0.47619643807411194\n",
      "Epoch 6026: train loss: 0.47619616985321045\n",
      "Epoch 6027: train loss: 0.4761960208415985\n",
      "Epoch 6028: train loss: 0.4761958122253418\n",
      "Epoch 6029: train loss: 0.4761956036090851\n",
      "Epoch 6030: train loss: 0.4761953353881836\n",
      "Epoch 6031: train loss: 0.4761951267719269\n",
      "Epoch 6032: train loss: 0.47619497776031494\n",
      "Epoch 6033: train loss: 0.4761947691440582\n",
      "Epoch 6034: train loss: 0.4761945605278015\n",
      "Epoch 6035: train loss: 0.4761943519115448\n",
      "Epoch 6036: train loss: 0.47619420289993286\n",
      "Epoch 6037: train loss: 0.47619399428367615\n",
      "Epoch 6038: train loss: 0.47619378566741943\n",
      "Epoch 6039: train loss: 0.4761935770511627\n",
      "Epoch 6040: train loss: 0.476193368434906\n",
      "Epoch 6041: train loss: 0.4761931598186493\n",
      "Epoch 6042: train loss: 0.4761929512023926\n",
      "Epoch 6043: train loss: 0.47619274258613586\n",
      "Epoch 6044: train loss: 0.47619253396987915\n",
      "Epoch 6045: train loss: 0.47619232535362244\n",
      "Epoch 6046: train loss: 0.4761921465396881\n",
      "Epoch 6047: train loss: 0.476191908121109\n",
      "Epoch 6048: train loss: 0.4761917293071747\n",
      "Epoch 6049: train loss: 0.4761914908885956\n",
      "Epoch 6050: train loss: 0.47619128227233887\n",
      "Epoch 6051: train loss: 0.47619110345840454\n",
      "Epoch 6052: train loss: 0.4761908948421478\n",
      "Epoch 6053: train loss: 0.4761906862258911\n",
      "Epoch 6054: train loss: 0.476190447807312\n",
      "Epoch 6055: train loss: 0.47619032859802246\n",
      "Epoch 6056: train loss: 0.47619011998176575\n",
      "Epoch 6057: train loss: 0.47618991136550903\n",
      "Epoch 6058: train loss: 0.4761897027492523\n",
      "Epoch 6059: train loss: 0.4761894941329956\n",
      "Epoch 6060: train loss: 0.4761892855167389\n",
      "Epoch 6061: train loss: 0.4761890769004822\n",
      "Epoch 6062: train loss: 0.47618886828422546\n",
      "Epoch 6063: train loss: 0.47618865966796875\n",
      "Epoch 6064: train loss: 0.4761885106563568\n",
      "Epoch 6065: train loss: 0.4761882424354553\n",
      "Epoch 6066: train loss: 0.4761880338191986\n",
      "Epoch 6067: train loss: 0.47618788480758667\n",
      "Epoch 6068: train loss: 0.4761876165866852\n",
      "Epoch 6069: train loss: 0.47618740797042847\n",
      "Epoch 6070: train loss: 0.4761872887611389\n",
      "Epoch 6071: train loss: 0.4761870503425598\n",
      "Epoch 6072: train loss: 0.4761868417263031\n",
      "Epoch 6073: train loss: 0.4761866629123688\n",
      "Epoch 6074: train loss: 0.4761864244937897\n",
      "Epoch 6075: train loss: 0.47618624567985535\n",
      "Epoch 6076: train loss: 0.47618600726127625\n",
      "Epoch 6077: train loss: 0.47618579864501953\n",
      "Epoch 6078: train loss: 0.4761856198310852\n",
      "Epoch 6079: train loss: 0.4761854112148285\n",
      "Epoch 6080: train loss: 0.4761852025985718\n",
      "Epoch 6081: train loss: 0.47618499398231506\n",
      "Epoch 6082: train loss: 0.4761848449707031\n",
      "Epoch 6083: train loss: 0.47618457674980164\n",
      "Epoch 6084: train loss: 0.4761843681335449\n",
      "Epoch 6085: train loss: 0.476184219121933\n",
      "Epoch 6086: train loss: 0.4761839509010315\n",
      "Epoch 6087: train loss: 0.47618380188941956\n",
      "Epoch 6088: train loss: 0.47618359327316284\n",
      "Epoch 6089: train loss: 0.47618338465690613\n",
      "Epoch 6090: train loss: 0.4761832356452942\n",
      "Epoch 6091: train loss: 0.4761829674243927\n",
      "Epoch 6092: train loss: 0.47618281841278076\n",
      "Epoch 6093: train loss: 0.47618260979652405\n",
      "Epoch 6094: train loss: 0.47618240118026733\n",
      "Epoch 6095: train loss: 0.4761821925640106\n",
      "Epoch 6096: train loss: 0.4761819839477539\n",
      "Epoch 6097: train loss: 0.4761817753314972\n",
      "Epoch 6098: train loss: 0.47618159651756287\n",
      "Epoch 6099: train loss: 0.47618138790130615\n",
      "Epoch 6100: train loss: 0.47618117928504944\n",
      "Epoch 6101: train loss: 0.47618091106414795\n",
      "Epoch 6102: train loss: 0.476180762052536\n",
      "Epoch 6103: train loss: 0.4761806130409241\n",
      "Epoch 6104: train loss: 0.47618040442466736\n",
      "Epoch 6105: train loss: 0.47618013620376587\n",
      "Epoch 6106: train loss: 0.47617998719215393\n",
      "Epoch 6107: train loss: 0.4761797785758972\n",
      "Epoch 6108: train loss: 0.4761795103549957\n",
      "Epoch 6109: train loss: 0.4761793613433838\n",
      "Epoch 6110: train loss: 0.4761791527271271\n",
      "Epoch 6111: train loss: 0.47617894411087036\n",
      "Epoch 6112: train loss: 0.47617873549461365\n",
      "Epoch 6113: train loss: 0.4761785864830017\n",
      "Epoch 6114: train loss: 0.4761783182621002\n",
      "Epoch 6115: train loss: 0.4761781692504883\n",
      "Epoch 6116: train loss: 0.47617796063423157\n",
      "Epoch 6117: train loss: 0.47617775201797485\n",
      "Epoch 6118: train loss: 0.47617754340171814\n",
      "Epoch 6119: train loss: 0.4761773645877838\n",
      "Epoch 6120: train loss: 0.4761771559715271\n",
      "Epoch 6121: train loss: 0.4761769473552704\n",
      "Epoch 6122: train loss: 0.47617673873901367\n",
      "Epoch 6123: train loss: 0.47617653012275696\n",
      "Epoch 6124: train loss: 0.47617632150650024\n",
      "Epoch 6125: train loss: 0.47617611289024353\n",
      "Epoch 6126: train loss: 0.4761759638786316\n",
      "Epoch 6127: train loss: 0.4761756956577301\n",
      "Epoch 6128: train loss: 0.47617554664611816\n",
      "Epoch 6129: train loss: 0.47617533802986145\n",
      "Epoch 6130: train loss: 0.47617512941360474\n",
      "Epoch 6131: train loss: 0.476174920797348\n",
      "Epoch 6132: train loss: 0.4761747717857361\n",
      "Epoch 6133: train loss: 0.47617456316947937\n",
      "Epoch 6134: train loss: 0.47617435455322266\n",
      "Epoch 6135: train loss: 0.47617414593696594\n",
      "Epoch 6136: train loss: 0.47617393732070923\n",
      "Epoch 6137: train loss: 0.4761737585067749\n",
      "Epoch 6138: train loss: 0.4761735200881958\n",
      "Epoch 6139: train loss: 0.4761733412742615\n",
      "Epoch 6140: train loss: 0.47617313265800476\n",
      "Epoch 6141: train loss: 0.4761729836463928\n",
      "Epoch 6142: train loss: 0.4761727750301361\n",
      "Epoch 6143: train loss: 0.4761725664138794\n",
      "Epoch 6144: train loss: 0.4761723577976227\n",
      "Epoch 6145: train loss: 0.4761720895767212\n",
      "Epoch 6146: train loss: 0.47617194056510925\n",
      "Epoch 6147: train loss: 0.47617173194885254\n",
      "Epoch 6148: train loss: 0.4761715829372406\n",
      "Epoch 6149: train loss: 0.4761713743209839\n",
      "Epoch 6150: train loss: 0.4761711657047272\n",
      "Epoch 6151: train loss: 0.47617095708847046\n",
      "Epoch 6152: train loss: 0.47617074847221375\n",
      "Epoch 6153: train loss: 0.4761705696582794\n",
      "Epoch 6154: train loss: 0.4761703610420227\n",
      "Epoch 6155: train loss: 0.476170152425766\n",
      "Epoch 6156: train loss: 0.4761699438095093\n",
      "Epoch 6157: train loss: 0.47616973519325256\n",
      "Epoch 6158: train loss: 0.47616952657699585\n",
      "Epoch 6159: train loss: 0.4761693775653839\n",
      "Epoch 6160: train loss: 0.4761691689491272\n",
      "Epoch 6161: train loss: 0.4761689603328705\n",
      "Epoch 6162: train loss: 0.47616881132125854\n",
      "Epoch 6163: train loss: 0.47616860270500183\n",
      "Epoch 6164: train loss: 0.4761683940887451\n",
      "Epoch 6165: train loss: 0.4761681854724884\n",
      "Epoch 6166: train loss: 0.4761679768562317\n",
      "Epoch 6167: train loss: 0.47616779804229736\n",
      "Epoch 6168: train loss: 0.47616758942604065\n",
      "Epoch 6169: train loss: 0.47616738080978394\n",
      "Epoch 6170: train loss: 0.4761671721935272\n",
      "Epoch 6171: train loss: 0.4761669635772705\n",
      "Epoch 6172: train loss: 0.4761667251586914\n",
      "Epoch 6173: train loss: 0.4761665463447571\n",
      "Epoch 6174: train loss: 0.47616639733314514\n",
      "Epoch 6175: train loss: 0.4761661887168884\n",
      "Epoch 6176: train loss: 0.4761659801006317\n",
      "Epoch 6177: train loss: 0.476165771484375\n",
      "Epoch 6178: train loss: 0.47616562247276306\n",
      "Epoch 6179: train loss: 0.47616541385650635\n",
      "Epoch 6180: train loss: 0.47616514563560486\n",
      "Epoch 6181: train loss: 0.4761649966239929\n",
      "Epoch 6182: train loss: 0.4761647880077362\n",
      "Epoch 6183: train loss: 0.4761646091938019\n",
      "Epoch 6184: train loss: 0.47616440057754517\n",
      "Epoch 6185: train loss: 0.47616419196128845\n",
      "Epoch 6186: train loss: 0.47616398334503174\n",
      "Epoch 6187: train loss: 0.476163774728775\n",
      "Epoch 6188: train loss: 0.4761635661125183\n",
      "Epoch 6189: train loss: 0.47616341710090637\n",
      "Epoch 6190: train loss: 0.47616320848464966\n",
      "Epoch 6191: train loss: 0.47616299986839294\n",
      "Epoch 6192: train loss: 0.476162850856781\n",
      "Epoch 6193: train loss: 0.4761625826358795\n",
      "Epoch 6194: train loss: 0.4761624336242676\n",
      "Epoch 6195: train loss: 0.47616222500801086\n",
      "Epoch 6196: train loss: 0.47616201639175415\n",
      "Epoch 6197: train loss: 0.4761618375778198\n",
      "Epoch 6198: train loss: 0.4761616885662079\n",
      "Epoch 6199: train loss: 0.476161390542984\n",
      "Epoch 6200: train loss: 0.4761612117290497\n",
      "Epoch 6201: train loss: 0.47616106271743774\n",
      "Epoch 6202: train loss: 0.47616085410118103\n",
      "Epoch 6203: train loss: 0.47616058588027954\n",
      "Epoch 6204: train loss: 0.4761604368686676\n",
      "Epoch 6205: train loss: 0.47616028785705566\n",
      "Epoch 6206: train loss: 0.47616007924079895\n",
      "Epoch 6207: train loss: 0.47615987062454224\n",
      "Epoch 6208: train loss: 0.4761597216129303\n",
      "Epoch 6209: train loss: 0.4761594533920288\n",
      "Epoch 6210: train loss: 0.47615930438041687\n",
      "Epoch 6211: train loss: 0.47615909576416016\n",
      "Epoch 6212: train loss: 0.47615888714790344\n",
      "Epoch 6213: train loss: 0.47615867853164673\n",
      "Epoch 6214: train loss: 0.47615846991539\n",
      "Epoch 6215: train loss: 0.4761582911014557\n",
      "Epoch 6216: train loss: 0.47615814208984375\n",
      "Epoch 6217: train loss: 0.47615787386894226\n",
      "Epoch 6218: train loss: 0.4761577248573303\n",
      "Epoch 6219: train loss: 0.4761575162410736\n",
      "Epoch 6220: train loss: 0.4761573076248169\n",
      "Epoch 6221: train loss: 0.4761570990085602\n",
      "Epoch 6222: train loss: 0.47615689039230347\n",
      "Epoch 6223: train loss: 0.47615674138069153\n",
      "Epoch 6224: train loss: 0.47615647315979004\n",
      "Epoch 6225: train loss: 0.4761563539505005\n",
      "Epoch 6226: train loss: 0.4761561453342438\n",
      "Epoch 6227: train loss: 0.47615593671798706\n",
      "Epoch 6228: train loss: 0.47615572810173035\n",
      "Epoch 6229: train loss: 0.47615551948547363\n",
      "Epoch 6230: train loss: 0.4761553704738617\n",
      "Epoch 6231: train loss: 0.476155161857605\n",
      "Epoch 6232: train loss: 0.47615495324134827\n",
      "Epoch 6233: train loss: 0.47615480422973633\n",
      "Epoch 6234: train loss: 0.47615453600883484\n",
      "Epoch 6235: train loss: 0.4761543869972229\n",
      "Epoch 6236: train loss: 0.4761541783809662\n",
      "Epoch 6237: train loss: 0.4761539697647095\n",
      "Epoch 6238: train loss: 0.47615379095077515\n",
      "Epoch 6239: train loss: 0.47615358233451843\n",
      "Epoch 6240: train loss: 0.4761534333229065\n",
      "Epoch 6241: train loss: 0.4761532247066498\n",
      "Epoch 6242: train loss: 0.47615301609039307\n",
      "Epoch 6243: train loss: 0.47615280747413635\n",
      "Epoch 6244: train loss: 0.47615259885787964\n",
      "Epoch 6245: train loss: 0.4761524498462677\n",
      "Epoch 6246: train loss: 0.476152241230011\n",
      "Epoch 6247: train loss: 0.4761520326137543\n",
      "Epoch 6248: train loss: 0.47615182399749756\n",
      "Epoch 6249: train loss: 0.4761516749858856\n",
      "Epoch 6250: train loss: 0.4761514663696289\n",
      "Epoch 6251: train loss: 0.4761512577533722\n",
      "Epoch 6252: train loss: 0.4761510491371155\n",
      "Epoch 6253: train loss: 0.47615087032318115\n",
      "Epoch 6254: train loss: 0.47615066170692444\n",
      "Epoch 6255: train loss: 0.4761505126953125\n",
      "Epoch 6256: train loss: 0.476150244474411\n",
      "Epoch 6257: train loss: 0.4761500954627991\n",
      "Epoch 6258: train loss: 0.47614988684654236\n",
      "Epoch 6259: train loss: 0.47614961862564087\n",
      "Epoch 6260: train loss: 0.4761495292186737\n",
      "Epoch 6261: train loss: 0.476149320602417\n",
      "Epoch 6262: train loss: 0.47614914178848267\n",
      "Epoch 6263: train loss: 0.47614893317222595\n",
      "Epoch 6264: train loss: 0.47614872455596924\n",
      "Epoch 6265: train loss: 0.4761485159397125\n",
      "Epoch 6266: train loss: 0.4761483073234558\n",
      "Epoch 6267: train loss: 0.47614815831184387\n",
      "Epoch 6268: train loss: 0.47614794969558716\n",
      "Epoch 6269: train loss: 0.47614774107933044\n",
      "Epoch 6270: train loss: 0.4761475920677185\n",
      "Epoch 6271: train loss: 0.4761473834514618\n",
      "Epoch 6272: train loss: 0.4761471748352051\n",
      "Epoch 6273: train loss: 0.47614696621894836\n",
      "Epoch 6274: train loss: 0.4761468470096588\n",
      "Epoch 6275: train loss: 0.4761465787887573\n",
      "Epoch 6276: train loss: 0.4761464297771454\n",
      "Epoch 6277: train loss: 0.47614622116088867\n",
      "Epoch 6278: train loss: 0.47614601254463196\n",
      "Epoch 6279: train loss: 0.47614580392837524\n",
      "Epoch 6280: train loss: 0.47614559531211853\n",
      "Epoch 6281: train loss: 0.4761453866958618\n",
      "Epoch 6282: train loss: 0.4761452376842499\n",
      "Epoch 6283: train loss: 0.47614502906799316\n",
      "Epoch 6284: train loss: 0.47614482045173645\n",
      "Epoch 6285: train loss: 0.4761446714401245\n",
      "Epoch 6286: train loss: 0.4761444628238678\n",
      "Epoch 6287: train loss: 0.47614428400993347\n",
      "Epoch 6288: train loss: 0.47614407539367676\n",
      "Epoch 6289: train loss: 0.4761439263820648\n",
      "Epoch 6290: train loss: 0.47614365816116333\n",
      "Epoch 6291: train loss: 0.4761435091495514\n",
      "Epoch 6292: train loss: 0.4761433005332947\n",
      "Epoch 6293: train loss: 0.47614309191703796\n",
      "Epoch 6294: train loss: 0.476142942905426\n",
      "Epoch 6295: train loss: 0.4761427342891693\n",
      "Epoch 6296: train loss: 0.476142555475235\n",
      "Epoch 6297: train loss: 0.47614234685897827\n",
      "Epoch 6298: train loss: 0.47614213824272156\n",
      "Epoch 6299: train loss: 0.47614192962646484\n",
      "Epoch 6300: train loss: 0.4761417806148529\n",
      "Epoch 6301: train loss: 0.47614163160324097\n",
      "Epoch 6302: train loss: 0.4761413633823395\n",
      "Epoch 6303: train loss: 0.47614121437072754\n",
      "Epoch 6304: train loss: 0.4761410057544708\n",
      "Epoch 6305: train loss: 0.4761407971382141\n",
      "Epoch 6306: train loss: 0.4761405885219574\n",
      "Epoch 6307: train loss: 0.47614043951034546\n",
      "Epoch 6308: train loss: 0.47614023089408875\n",
      "Epoch 6309: train loss: 0.4761400520801544\n",
      "Epoch 6310: train loss: 0.4761398434638977\n",
      "Epoch 6311: train loss: 0.47613969445228577\n",
      "Epoch 6312: train loss: 0.47613948583602905\n",
      "Epoch 6313: train loss: 0.47613927721977234\n",
      "Epoch 6314: train loss: 0.4761391282081604\n",
      "Epoch 6315: train loss: 0.4761389195919037\n",
      "Epoch 6316: train loss: 0.476138710975647\n",
      "Epoch 6317: train loss: 0.47613850235939026\n",
      "Epoch 6318: train loss: 0.47613829374313354\n",
      "Epoch 6319: train loss: 0.4761381149291992\n",
      "Epoch 6320: train loss: 0.4761379063129425\n",
      "Epoch 6321: train loss: 0.4761376976966858\n",
      "Epoch 6322: train loss: 0.47613754868507385\n",
      "Epoch 6323: train loss: 0.47613734006881714\n",
      "Epoch 6324: train loss: 0.4761371910572052\n",
      "Epoch 6325: train loss: 0.47613704204559326\n",
      "Epoch 6326: train loss: 0.47613683342933655\n",
      "Epoch 6327: train loss: 0.47613656520843506\n",
      "Epoch 6328: train loss: 0.4761364161968231\n",
      "Epoch 6329: train loss: 0.4761362075805664\n",
      "Epoch 6330: train loss: 0.4761360287666321\n",
      "Epoch 6331: train loss: 0.47613582015037537\n",
      "Epoch 6332: train loss: 0.47613561153411865\n",
      "Epoch 6333: train loss: 0.47613540291786194\n",
      "Epoch 6334: train loss: 0.47613525390625\n",
      "Epoch 6335: train loss: 0.4761350452899933\n",
      "Epoch 6336: train loss: 0.4761348366737366\n",
      "Epoch 6337: train loss: 0.47613468766212463\n",
      "Epoch 6338: train loss: 0.4761344790458679\n",
      "Epoch 6339: train loss: 0.4761343002319336\n",
      "Epoch 6340: train loss: 0.4761340916156769\n",
      "Epoch 6341: train loss: 0.47613388299942017\n",
      "Epoch 6342: train loss: 0.4761337339878082\n",
      "Epoch 6343: train loss: 0.4761335253715515\n",
      "Epoch 6344: train loss: 0.4761333763599396\n",
      "Epoch 6345: train loss: 0.47613316774368286\n",
      "Epoch 6346: train loss: 0.4761330187320709\n",
      "Epoch 6347: train loss: 0.4761328101158142\n",
      "Epoch 6348: train loss: 0.4761326014995575\n",
      "Epoch 6349: train loss: 0.4761323928833008\n",
      "Epoch 6350: train loss: 0.47613221406936646\n",
      "Epoch 6351: train loss: 0.47613200545310974\n",
      "Epoch 6352: train loss: 0.476131796836853\n",
      "Epoch 6353: train loss: 0.47613170742988586\n",
      "Epoch 6354: train loss: 0.4761314392089844\n",
      "Epoch 6355: train loss: 0.47613129019737244\n",
      "Epoch 6356: train loss: 0.4761310815811157\n",
      "Epoch 6357: train loss: 0.4761309027671814\n",
      "Epoch 6358: train loss: 0.4761306941509247\n",
      "Epoch 6359: train loss: 0.47613048553466797\n",
      "Epoch 6360: train loss: 0.47613027691841125\n",
      "Epoch 6361: train loss: 0.4761301279067993\n",
      "Epoch 6362: train loss: 0.4761299192905426\n",
      "Epoch 6363: train loss: 0.4761297106742859\n",
      "Epoch 6364: train loss: 0.47612956166267395\n",
      "Epoch 6365: train loss: 0.476129412651062\n",
      "Epoch 6366: train loss: 0.4761292040348053\n",
      "Epoch 6367: train loss: 0.4761289954185486\n",
      "Epoch 6368: train loss: 0.47612878680229187\n",
      "Epoch 6369: train loss: 0.47612857818603516\n",
      "Epoch 6370: train loss: 0.47612839937210083\n",
      "Epoch 6371: train loss: 0.4761281907558441\n",
      "Epoch 6372: train loss: 0.4761279821395874\n",
      "Epoch 6373: train loss: 0.47612783312797546\n",
      "Epoch 6374: train loss: 0.47612762451171875\n",
      "Epoch 6375: train loss: 0.4761274755001068\n",
      "Epoch 6376: train loss: 0.4761272966861725\n",
      "Epoch 6377: train loss: 0.4761270582675934\n",
      "Epoch 6378: train loss: 0.47612687945365906\n",
      "Epoch 6379: train loss: 0.47612667083740234\n",
      "Epoch 6380: train loss: 0.4761265218257904\n",
      "Epoch 6381: train loss: 0.4761263132095337\n",
      "Epoch 6382: train loss: 0.47612616419792175\n",
      "Epoch 6383: train loss: 0.47612595558166504\n",
      "Epoch 6384: train loss: 0.4761258065700531\n",
      "Epoch 6385: train loss: 0.4761255979537964\n",
      "Epoch 6386: train loss: 0.4761253893375397\n",
      "Epoch 6387: train loss: 0.47612521052360535\n",
      "Epoch 6388: train loss: 0.47612500190734863\n",
      "Epoch 6389: train loss: 0.4761247932910919\n",
      "Epoch 6390: train loss: 0.47612464427948\n",
      "Epoch 6391: train loss: 0.47612443566322327\n",
      "Epoch 6392: train loss: 0.47612428665161133\n",
      "Epoch 6393: train loss: 0.4761240780353546\n",
      "Epoch 6394: train loss: 0.4761238694190979\n",
      "Epoch 6395: train loss: 0.4761236608028412\n",
      "Epoch 6396: train loss: 0.47612348198890686\n",
      "Epoch 6397: train loss: 0.4761233329772949\n",
      "Epoch 6398: train loss: 0.4761231243610382\n",
      "Epoch 6399: train loss: 0.4761229157447815\n",
      "Epoch 6400: train loss: 0.47612276673316956\n",
      "Epoch 6401: train loss: 0.47612255811691284\n",
      "Epoch 6402: train loss: 0.4761224091053009\n",
      "Epoch 6403: train loss: 0.4761222004890442\n",
      "Epoch 6404: train loss: 0.47612202167510986\n",
      "Epoch 6405: train loss: 0.47612181305885315\n",
      "Epoch 6406: train loss: 0.47612160444259644\n",
      "Epoch 6407: train loss: 0.4761214554309845\n",
      "Epoch 6408: train loss: 0.4761212468147278\n",
      "Epoch 6409: train loss: 0.47612109780311584\n",
      "Epoch 6410: train loss: 0.47612088918685913\n",
      "Epoch 6411: train loss: 0.4761207401752472\n",
      "Epoch 6412: train loss: 0.4761205315589905\n",
      "Epoch 6413: train loss: 0.47612032294273376\n",
      "Epoch 6414: train loss: 0.47612011432647705\n",
      "Epoch 6415: train loss: 0.4761199355125427\n",
      "Epoch 6416: train loss: 0.476119726896286\n",
      "Epoch 6417: train loss: 0.4761195778846741\n",
      "Epoch 6418: train loss: 0.47611936926841736\n",
      "Epoch 6419: train loss: 0.4761192202568054\n",
      "Epoch 6420: train loss: 0.4761190414428711\n",
      "Epoch 6421: train loss: 0.476118803024292\n",
      "Epoch 6422: train loss: 0.4761185944080353\n",
      "Epoch 6423: train loss: 0.47611841559410095\n",
      "Epoch 6424: train loss: 0.47611820697784424\n",
      "Epoch 6425: train loss: 0.4761180579662323\n",
      "Epoch 6426: train loss: 0.4761177897453308\n",
      "Epoch 6427: train loss: 0.47611770033836365\n",
      "Epoch 6428: train loss: 0.47611749172210693\n",
      "Epoch 6429: train loss: 0.476117342710495\n",
      "Epoch 6430: train loss: 0.4761170744895935\n",
      "Epoch 6431: train loss: 0.47611695528030396\n",
      "Epoch 6432: train loss: 0.47611674666404724\n",
      "Epoch 6433: train loss: 0.4761165976524353\n",
      "Epoch 6434: train loss: 0.47611644864082336\n",
      "Epoch 6435: train loss: 0.47611624002456665\n",
      "Epoch 6436: train loss: 0.47611603140830994\n",
      "Epoch 6437: train loss: 0.4761158525943756\n",
      "Epoch 6438: train loss: 0.4761156141757965\n",
      "Epoch 6439: train loss: 0.4761154353618622\n",
      "Epoch 6440: train loss: 0.47611528635025024\n",
      "Epoch 6441: train loss: 0.47611507773399353\n",
      "Epoch 6442: train loss: 0.4761148691177368\n",
      "Epoch 6443: train loss: 0.4761147201061249\n",
      "Epoch 6444: train loss: 0.47611451148986816\n",
      "Epoch 6445: train loss: 0.4761143624782562\n",
      "Epoch 6446: train loss: 0.4761141538619995\n",
      "Epoch 6447: train loss: 0.4761139452457428\n",
      "Epoch 6448: train loss: 0.47611376643180847\n",
      "Epoch 6449: train loss: 0.47611361742019653\n",
      "Epoch 6450: train loss: 0.4761134088039398\n",
      "Epoch 6451: train loss: 0.4761132001876831\n",
      "Epoch 6452: train loss: 0.47611305117607117\n",
      "Epoch 6453: train loss: 0.47611284255981445\n",
      "Epoch 6454: train loss: 0.47611263394355774\n",
      "Epoch 6455: train loss: 0.4761124849319458\n",
      "Epoch 6456: train loss: 0.4761122465133667\n",
      "Epoch 6457: train loss: 0.47611209750175476\n",
      "Epoch 6458: train loss: 0.47611188888549805\n",
      "Epoch 6459: train loss: 0.4761117398738861\n",
      "Epoch 6460: train loss: 0.4761115312576294\n",
      "Epoch 6461: train loss: 0.4761113226413727\n",
      "Epoch 6462: train loss: 0.47611117362976074\n",
      "Epoch 6463: train loss: 0.4761109948158264\n",
      "Epoch 6464: train loss: 0.4761107861995697\n",
      "Epoch 6465: train loss: 0.476110577583313\n",
      "Epoch 6466: train loss: 0.47611042857170105\n",
      "Epoch 6467: train loss: 0.47611016035079956\n",
      "Epoch 6468: train loss: 0.4761100709438324\n",
      "Epoch 6469: train loss: 0.4761098623275757\n",
      "Epoch 6470: train loss: 0.47610971331596375\n",
      "Epoch 6471: train loss: 0.47610950469970703\n",
      "Epoch 6472: train loss: 0.4761092960834503\n",
      "Epoch 6473: train loss: 0.476109117269516\n",
      "Epoch 6474: train loss: 0.4761089086532593\n",
      "Epoch 6475: train loss: 0.4761088192462921\n",
      "Epoch 6476: train loss: 0.4761086106300354\n",
      "Epoch 6477: train loss: 0.4761084020137787\n",
      "Epoch 6478: train loss: 0.476108193397522\n",
      "Epoch 6479: train loss: 0.47610798478126526\n",
      "Epoch 6480: train loss: 0.4761078655719757\n",
      "Epoch 6481: train loss: 0.476107656955719\n",
      "Epoch 6482: train loss: 0.47610750794410706\n",
      "Epoch 6483: train loss: 0.47610729932785034\n",
      "Epoch 6484: train loss: 0.47610709071159363\n",
      "Epoch 6485: train loss: 0.4761068820953369\n",
      "Epoch 6486: train loss: 0.476106733083725\n",
      "Epoch 6487: train loss: 0.47610655426979065\n",
      "Epoch 6488: train loss: 0.47610634565353394\n",
      "Epoch 6489: train loss: 0.4761061370372772\n",
      "Epoch 6490: train loss: 0.4761059880256653\n",
      "Epoch 6491: train loss: 0.47610577940940857\n",
      "Epoch 6492: train loss: 0.47610563039779663\n",
      "Epoch 6493: train loss: 0.4761054217815399\n",
      "Epoch 6494: train loss: 0.476105272769928\n",
      "Epoch 6495: train loss: 0.47610509395599365\n",
      "Epoch 6496: train loss: 0.47610488533973694\n",
      "Epoch 6497: train loss: 0.4761046767234802\n",
      "Epoch 6498: train loss: 0.4761045277118683\n",
      "Epoch 6499: train loss: 0.4761043190956116\n",
      "Epoch 6500: train loss: 0.47610417008399963\n",
      "Epoch 6501: train loss: 0.47610390186309814\n",
      "Epoch 6502: train loss: 0.4761037826538086\n",
      "Epoch 6503: train loss: 0.4761035740375519\n",
      "Epoch 6504: train loss: 0.47610336542129517\n",
      "Epoch 6505: train loss: 0.4761032164096832\n",
      "Epoch 6506: train loss: 0.4761030077934265\n",
      "Epoch 6507: train loss: 0.4761028587818146\n",
      "Epoch 6508: train loss: 0.47610270977020264\n",
      "Epoch 6509: train loss: 0.4761025011539459\n",
      "Epoch 6510: train loss: 0.4761023223400116\n",
      "Epoch 6511: train loss: 0.4761021137237549\n",
      "Epoch 6512: train loss: 0.47610190510749817\n",
      "Epoch 6513: train loss: 0.47610175609588623\n",
      "Epoch 6514: train loss: 0.4761016070842743\n",
      "Epoch 6515: train loss: 0.4761013984680176\n",
      "Epoch 6516: train loss: 0.47610118985176086\n",
      "Epoch 6517: train loss: 0.4761010706424713\n",
      "Epoch 6518: train loss: 0.4761008024215698\n",
      "Epoch 6519: train loss: 0.4761006534099579\n",
      "Epoch 6520: train loss: 0.47610044479370117\n",
      "Epoch 6521: train loss: 0.47610029578208923\n",
      "Epoch 6522: train loss: 0.4761000871658325\n",
      "Epoch 6523: train loss: 0.4760999381542206\n",
      "Epoch 6524: train loss: 0.47609972953796387\n",
      "Epoch 6525: train loss: 0.47609952092170715\n",
      "Epoch 6526: train loss: 0.4760994017124176\n",
      "Epoch 6527: train loss: 0.4760991930961609\n",
      "Epoch 6528: train loss: 0.47609904408454895\n",
      "Epoch 6529: train loss: 0.47609883546829224\n",
      "Epoch 6530: train loss: 0.4760986268520355\n",
      "Epoch 6531: train loss: 0.4760984778404236\n",
      "Epoch 6532: train loss: 0.47609826922416687\n",
      "Epoch 6533: train loss: 0.47609809041023254\n",
      "Epoch 6534: train loss: 0.47609788179397583\n",
      "Epoch 6535: train loss: 0.4760977327823639\n",
      "Epoch 6536: train loss: 0.4760975241661072\n",
      "Epoch 6537: train loss: 0.47609731554985046\n",
      "Epoch 6538: train loss: 0.4760972261428833\n",
      "Epoch 6539: train loss: 0.4760969877243042\n",
      "Epoch 6540: train loss: 0.47609683871269226\n",
      "Epoch 6541: train loss: 0.47609663009643555\n",
      "Epoch 6542: train loss: 0.4760964810848236\n",
      "Epoch 6543: train loss: 0.4760962724685669\n",
      "Epoch 6544: train loss: 0.4760960638523102\n",
      "Epoch 6545: train loss: 0.47609591484069824\n",
      "Epoch 6546: train loss: 0.47609570622444153\n",
      "Epoch 6547: train loss: 0.4760955274105072\n",
      "Epoch 6548: train loss: 0.47609537839889526\n",
      "Epoch 6549: train loss: 0.47609516978263855\n",
      "Epoch 6550: train loss: 0.4760950207710266\n",
      "Epoch 6551: train loss: 0.4760948717594147\n",
      "Epoch 6552: train loss: 0.47609466314315796\n",
      "Epoch 6553: train loss: 0.47609445452690125\n",
      "Epoch 6554: train loss: 0.4760942757129669\n",
      "Epoch 6555: train loss: 0.4760940670967102\n",
      "Epoch 6556: train loss: 0.47609391808509827\n",
      "Epoch 6557: train loss: 0.47609376907348633\n",
      "Epoch 6558: train loss: 0.4760935604572296\n",
      "Epoch 6559: train loss: 0.4760933518409729\n",
      "Epoch 6560: train loss: 0.4760931730270386\n",
      "Epoch 6561: train loss: 0.47609302401542664\n",
      "Epoch 6562: train loss: 0.4760928153991699\n",
      "Epoch 6563: train loss: 0.4760926067829132\n",
      "Epoch 6564: train loss: 0.4760923981666565\n",
      "Epoch 6565: train loss: 0.47609224915504456\n",
      "Epoch 6566: train loss: 0.4760921001434326\n",
      "Epoch 6567: train loss: 0.4760918915271759\n",
      "Epoch 6568: train loss: 0.47609177231788635\n",
      "Epoch 6569: train loss: 0.47609156370162964\n",
      "Epoch 6570: train loss: 0.4760913550853729\n",
      "Epoch 6571: train loss: 0.4760911464691162\n",
      "Epoch 6572: train loss: 0.4760909974575043\n",
      "Epoch 6573: train loss: 0.4760908782482147\n",
      "Epoch 6574: train loss: 0.4760906398296356\n",
      "Epoch 6575: train loss: 0.47609052062034607\n",
      "Epoch 6576: train loss: 0.47609031200408936\n",
      "Epoch 6577: train loss: 0.4760901629924774\n",
      "Epoch 6578: train loss: 0.4760898947715759\n",
      "Epoch 6579: train loss: 0.476089745759964\n",
      "Epoch 6580: train loss: 0.47608959674835205\n",
      "Epoch 6581: train loss: 0.47608935832977295\n",
      "Epoch 6582: train loss: 0.476089209318161\n",
      "Epoch 6583: train loss: 0.4760890007019043\n",
      "Epoch 6584: train loss: 0.47608885169029236\n",
      "Epoch 6585: train loss: 0.47608864307403564\n",
      "Epoch 6586: train loss: 0.4760884940624237\n",
      "Epoch 6587: train loss: 0.4760883152484894\n",
      "Epoch 6588: train loss: 0.47608810663223267\n",
      "Epoch 6589: train loss: 0.4760879576206207\n",
      "Epoch 6590: train loss: 0.4760878086090088\n",
      "Epoch 6591: train loss: 0.4760875999927521\n",
      "Epoch 6592: train loss: 0.47608739137649536\n",
      "Epoch 6593: train loss: 0.4760872423648834\n",
      "Epoch 6594: train loss: 0.4760870337486267\n",
      "Epoch 6595: train loss: 0.4760868549346924\n",
      "Epoch 6596: train loss: 0.47608664631843567\n",
      "Epoch 6597: train loss: 0.4760865569114685\n",
      "Epoch 6598: train loss: 0.476086288690567\n",
      "Epoch 6599: train loss: 0.4760861396789551\n",
      "Epoch 6600: train loss: 0.47608599066734314\n",
      "Epoch 6601: train loss: 0.4760857820510864\n",
      "Epoch 6602: train loss: 0.4760855436325073\n",
      "Epoch 6603: train loss: 0.4760853946208954\n",
      "Epoch 6604: train loss: 0.47608524560928345\n",
      "Epoch 6605: train loss: 0.4760850965976715\n",
      "Epoch 6606: train loss: 0.4760848879814148\n",
      "Epoch 6607: train loss: 0.47608470916748047\n",
      "Epoch 6608: train loss: 0.47608450055122375\n",
      "Epoch 6609: train loss: 0.4760843515396118\n",
      "Epoch 6610: train loss: 0.4760842025279999\n",
      "Epoch 6611: train loss: 0.47608399391174316\n",
      "Epoch 6612: train loss: 0.4760838449001312\n",
      "Epoch 6613: train loss: 0.4760836660861969\n",
      "Epoch 6614: train loss: 0.4760834574699402\n",
      "Epoch 6615: train loss: 0.47608324885368347\n",
      "Epoch 6616: train loss: 0.47608304023742676\n",
      "Epoch 6617: train loss: 0.4760828912258148\n",
      "Epoch 6618: train loss: 0.4760827422142029\n",
      "Epoch 6619: train loss: 0.47608259320259094\n",
      "Epoch 6620: train loss: 0.47608238458633423\n",
      "Epoch 6621: train loss: 0.4760822057723999\n",
      "Epoch 6622: train loss: 0.4760819971561432\n",
      "Epoch 6623: train loss: 0.47608184814453125\n",
      "Epoch 6624: train loss: 0.47608163952827454\n",
      "Epoch 6625: train loss: 0.4760814905166626\n",
      "Epoch 6626: train loss: 0.4760812819004059\n",
      "Epoch 6627: train loss: 0.47608110308647156\n",
      "Epoch 6628: train loss: 0.4760809540748596\n",
      "Epoch 6629: train loss: 0.4760808050632477\n",
      "Epoch 6630: train loss: 0.47608059644699097\n",
      "Epoch 6631: train loss: 0.47608044743537903\n",
      "Epoch 6632: train loss: 0.47608017921447754\n",
      "Epoch 6633: train loss: 0.476080060005188\n",
      "Epoch 6634: train loss: 0.4760798513889313\n",
      "Epoch 6635: train loss: 0.47607964277267456\n",
      "Epoch 6636: train loss: 0.4760795533657074\n",
      "Epoch 6637: train loss: 0.4760793447494507\n",
      "Epoch 6638: train loss: 0.47607919573783875\n",
      "Epoch 6639: train loss: 0.4760790169239044\n",
      "Epoch 6640: train loss: 0.4760788083076477\n",
      "Epoch 6641: train loss: 0.476078599691391\n",
      "Epoch 6642: train loss: 0.47607845067977905\n",
      "Epoch 6643: train loss: 0.47607824206352234\n",
      "Epoch 6644: train loss: 0.4760780930519104\n",
      "Epoch 6645: train loss: 0.4760779142379761\n",
      "Epoch 6646: train loss: 0.47607776522636414\n",
      "Epoch 6647: train loss: 0.4760775566101074\n",
      "Epoch 6648: train loss: 0.4760773479938507\n",
      "Epoch 6649: train loss: 0.47607719898223877\n",
      "Epoch 6650: train loss: 0.47607704997062683\n",
      "Epoch 6651: train loss: 0.4760768413543701\n",
      "Epoch 6652: train loss: 0.4760766625404358\n",
      "Epoch 6653: train loss: 0.47607651352882385\n",
      "Epoch 6654: train loss: 0.47607630491256714\n",
      "Epoch 6655: train loss: 0.4760761559009552\n",
      "Epoch 6656: train loss: 0.47607600688934326\n",
      "Epoch 6657: train loss: 0.47607579827308655\n",
      "Epoch 6658: train loss: 0.47607558965682983\n",
      "Epoch 6659: train loss: 0.4760754108428955\n",
      "Epoch 6660: train loss: 0.47607526183128357\n",
      "Epoch 6661: train loss: 0.47607505321502686\n",
      "Epoch 6662: train loss: 0.4760749042034149\n",
      "Epoch 6663: train loss: 0.476074755191803\n",
      "Epoch 6664: train loss: 0.47607454657554626\n",
      "Epoch 6665: train loss: 0.47607436776161194\n",
      "Epoch 6666: train loss: 0.4760741591453552\n",
      "Epoch 6667: train loss: 0.4760740101337433\n",
      "Epoch 6668: train loss: 0.47607386112213135\n",
      "Epoch 6669: train loss: 0.476073682308197\n",
      "Epoch 6670: train loss: 0.4760734736919403\n",
      "Epoch 6671: train loss: 0.4760732650756836\n",
      "Epoch 6672: train loss: 0.47607317566871643\n",
      "Epoch 6673: train loss: 0.4760729670524597\n",
      "Epoch 6674: train loss: 0.476072758436203\n",
      "Epoch 6675: train loss: 0.4760725498199463\n",
      "Epoch 6676: train loss: 0.47607240080833435\n",
      "Epoch 6677: train loss: 0.4760722219944\n",
      "Epoch 6678: train loss: 0.4760720133781433\n",
      "Epoch 6679: train loss: 0.47607186436653137\n",
      "Epoch 6680: train loss: 0.47607171535491943\n",
      "Epoch 6681: train loss: 0.4760715663433075\n",
      "Epoch 6682: train loss: 0.47607138752937317\n",
      "Epoch 6683: train loss: 0.47607117891311646\n",
      "Epoch 6684: train loss: 0.4760710299015045\n",
      "Epoch 6685: train loss: 0.4760708212852478\n",
      "Epoch 6686: train loss: 0.47607067227363586\n",
      "Epoch 6687: train loss: 0.4760705232620239\n",
      "Epoch 6688: train loss: 0.47607025504112244\n",
      "Epoch 6689: train loss: 0.4760700762271881\n",
      "Epoch 6690: train loss: 0.47606992721557617\n",
      "Epoch 6691: train loss: 0.47606977820396423\n",
      "Epoch 6692: train loss: 0.4760695695877075\n",
      "Epoch 6693: train loss: 0.4760694205760956\n",
      "Epoch 6694: train loss: 0.47606921195983887\n",
      "Epoch 6695: train loss: 0.4760690927505493\n",
      "Epoch 6696: train loss: 0.4760688841342926\n",
      "Epoch 6697: train loss: 0.4760686755180359\n",
      "Epoch 6698: train loss: 0.47606852650642395\n",
      "Epoch 6699: train loss: 0.476068377494812\n",
      "Epoch 6700: train loss: 0.4760681986808777\n",
      "Epoch 6701: train loss: 0.47606799006462097\n",
      "Epoch 6702: train loss: 0.47606784105300903\n",
      "Epoch 6703: train loss: 0.4760676324367523\n",
      "Epoch 6704: train loss: 0.4760674834251404\n",
      "Epoch 6705: train loss: 0.47606733441352844\n",
      "Epoch 6706: train loss: 0.4760671555995941\n",
      "Epoch 6707: train loss: 0.4760669469833374\n",
      "Epoch 6708: train loss: 0.47606679797172546\n",
      "Epoch 6709: train loss: 0.47606658935546875\n",
      "Epoch 6710: train loss: 0.4760664403438568\n",
      "Epoch 6711: train loss: 0.4760662615299225\n",
      "Epoch 6712: train loss: 0.47606605291366577\n",
      "Epoch 6713: train loss: 0.47606590390205383\n",
      "Epoch 6714: train loss: 0.4760656952857971\n",
      "Epoch 6715: train loss: 0.4760655462741852\n",
      "Epoch 6716: train loss: 0.47606539726257324\n",
      "Epoch 6717: train loss: 0.4760652184486389\n",
      "Epoch 6718: train loss: 0.4760650098323822\n",
      "Epoch 6719: train loss: 0.47606486082077026\n",
      "Epoch 6720: train loss: 0.47606465220451355\n",
      "Epoch 6721: train loss: 0.4760645031929016\n",
      "Epoch 6722: train loss: 0.4760642945766449\n",
      "Epoch 6723: train loss: 0.47606414556503296\n",
      "Epoch 6724: train loss: 0.47606396675109863\n",
      "Epoch 6725: train loss: 0.4760637581348419\n",
      "Epoch 6726: train loss: 0.47606360912323\n",
      "Epoch 6727: train loss: 0.47606346011161804\n",
      "Epoch 6728: train loss: 0.47606325149536133\n",
      "Epoch 6729: train loss: 0.4760631024837494\n",
      "Epoch 6730: train loss: 0.47606292366981506\n",
      "Epoch 6731: train loss: 0.4760627746582031\n",
      "Epoch 6732: train loss: 0.4760625660419464\n",
      "Epoch 6733: train loss: 0.4760624170303345\n",
      "Epoch 6734: train loss: 0.47606223821640015\n",
      "Epoch 6735: train loss: 0.47606202960014343\n",
      "Epoch 6736: train loss: 0.4760618805885315\n",
      "Epoch 6737: train loss: 0.47606173157691956\n",
      "Epoch 6738: train loss: 0.47606152296066284\n",
      "Epoch 6739: train loss: 0.4760613739490509\n",
      "Epoch 6740: train loss: 0.4760611653327942\n",
      "Epoch 6741: train loss: 0.47606098651885986\n",
      "Epoch 6742: train loss: 0.4760608971118927\n",
      "Epoch 6743: train loss: 0.476060688495636\n",
      "Epoch 6744: train loss: 0.47606053948402405\n",
      "Epoch 6745: train loss: 0.4760603606700897\n",
      "Epoch 6746: train loss: 0.4760601222515106\n",
      "Epoch 6747: train loss: 0.4760599434375763\n",
      "Epoch 6748: train loss: 0.47605979442596436\n",
      "Epoch 6749: train loss: 0.4760596454143524\n",
      "Epoch 6750: train loss: 0.4760594367980957\n",
      "Epoch 6751: train loss: 0.47605928778648376\n",
      "Epoch 6752: train loss: 0.47605910897254944\n",
      "Epoch 6753: train loss: 0.4760589599609375\n",
      "Epoch 6754: train loss: 0.4760587513446808\n",
      "Epoch 6755: train loss: 0.47605860233306885\n",
      "Epoch 6756: train loss: 0.4760584235191345\n",
      "Epoch 6757: train loss: 0.4760582149028778\n",
      "Epoch 6758: train loss: 0.4760580062866211\n",
      "Epoch 6759: train loss: 0.47605785727500916\n",
      "Epoch 6760: train loss: 0.4760577082633972\n",
      "Epoch 6761: train loss: 0.4760575592517853\n",
      "Epoch 6762: train loss: 0.47605735063552856\n",
      "Epoch 6763: train loss: 0.47605717182159424\n",
      "Epoch 6764: train loss: 0.4760569632053375\n",
      "Epoch 6765: train loss: 0.4760568141937256\n",
      "Epoch 6766: train loss: 0.47605666518211365\n",
      "Epoch 6767: train loss: 0.47605645656585693\n",
      "Epoch 6768: train loss: 0.4760563373565674\n",
      "Epoch 6769: train loss: 0.47605612874031067\n",
      "Epoch 6770: train loss: 0.47605597972869873\n",
      "Epoch 6771: train loss: 0.4760558307170868\n",
      "Epoch 6772: train loss: 0.4760556221008301\n",
      "Epoch 6773: train loss: 0.47605547308921814\n",
      "Epoch 6774: train loss: 0.4760552942752838\n",
      "Epoch 6775: train loss: 0.4760550856590271\n",
      "Epoch 6776: train loss: 0.47605493664741516\n",
      "Epoch 6777: train loss: 0.4760547876358032\n",
      "Epoch 6778: train loss: 0.4760546088218689\n",
      "Epoch 6779: train loss: 0.4760544002056122\n",
      "Epoch 6780: train loss: 0.47605425119400024\n",
      "Epoch 6781: train loss: 0.4760541021823883\n",
      "Epoch 6782: train loss: 0.4760538339614868\n",
      "Epoch 6783: train loss: 0.47605374455451965\n",
      "Epoch 6784: train loss: 0.4760535657405853\n",
      "Epoch 6785: train loss: 0.4760534167289734\n",
      "Epoch 6786: train loss: 0.4760532081127167\n",
      "Epoch 6787: train loss: 0.47605299949645996\n",
      "Epoch 6788: train loss: 0.476052850484848\n",
      "Epoch 6789: train loss: 0.4760527014732361\n",
      "Epoch 6790: train loss: 0.47605252265930176\n",
      "Epoch 6791: train loss: 0.47605231404304504\n",
      "Epoch 6792: train loss: 0.4760521650314331\n",
      "Epoch 6793: train loss: 0.47605201601982117\n",
      "Epoch 6794: train loss: 0.47605180740356445\n",
      "Epoch 6795: train loss: 0.4760516881942749\n",
      "Epoch 6796: train loss: 0.4760514795780182\n",
      "Epoch 6797: train loss: 0.47605133056640625\n",
      "Epoch 6798: train loss: 0.4760511815547943\n",
      "Epoch 6799: train loss: 0.4760509133338928\n",
      "Epoch 6800: train loss: 0.47605079412460327\n",
      "Epoch 6801: train loss: 0.47605058550834656\n",
      "Epoch 6802: train loss: 0.4760504364967346\n",
      "Epoch 6803: train loss: 0.4760502874851227\n",
      "Epoch 6804: train loss: 0.47605013847351074\n",
      "Epoch 6805: train loss: 0.47604992985725403\n",
      "Epoch 6806: train loss: 0.4760497510433197\n",
      "Epoch 6807: train loss: 0.47604960203170776\n",
      "Epoch 6808: train loss: 0.4760494530200958\n",
      "Epoch 6809: train loss: 0.4760492444038391\n",
      "Epoch 6810: train loss: 0.4760490953922272\n",
      "Epoch 6811: train loss: 0.47604888677597046\n",
      "Epoch 6812: train loss: 0.4760487675666809\n",
      "Epoch 6813: train loss: 0.4760485589504242\n",
      "Epoch 6814: train loss: 0.4760483503341675\n",
      "Epoch 6815: train loss: 0.47604820132255554\n",
      "Epoch 6816: train loss: 0.4760480523109436\n",
      "Epoch 6817: train loss: 0.4760478734970093\n",
      "Epoch 6818: train loss: 0.47604772448539734\n",
      "Epoch 6819: train loss: 0.4760475754737854\n",
      "Epoch 6820: train loss: 0.4760473668575287\n",
      "Epoch 6821: train loss: 0.47604718804359436\n",
      "Epoch 6822: train loss: 0.47604697942733765\n",
      "Epoch 6823: train loss: 0.4760468304157257\n",
      "Epoch 6824: train loss: 0.47604668140411377\n",
      "Epoch 6825: train loss: 0.47604653239250183\n",
      "Epoch 6826: train loss: 0.4760463535785675\n",
      "Epoch 6827: train loss: 0.4760461449623108\n",
      "Epoch 6828: train loss: 0.47604599595069885\n",
      "Epoch 6829: train loss: 0.4760458469390869\n",
      "Epoch 6830: train loss: 0.4760456383228302\n",
      "Epoch 6831: train loss: 0.47604548931121826\n",
      "Epoch 6832: train loss: 0.47604531049728394\n",
      "Epoch 6833: train loss: 0.476045161485672\n",
      "Epoch 6834: train loss: 0.4760449528694153\n",
      "Epoch 6835: train loss: 0.47604480385780334\n",
      "Epoch 6836: train loss: 0.4760446548461914\n",
      "Epoch 6837: train loss: 0.4760444760322571\n",
      "Epoch 6838: train loss: 0.47604426741600037\n",
      "Epoch 6839: train loss: 0.4760441184043884\n",
      "Epoch 6840: train loss: 0.4760439693927765\n",
      "Epoch 6841: train loss: 0.47604382038116455\n",
      "Epoch 6842: train loss: 0.47604358196258545\n",
      "Epoch 6843: train loss: 0.4760434925556183\n",
      "Epoch 6844: train loss: 0.4760432839393616\n",
      "Epoch 6845: train loss: 0.47604307532310486\n",
      "Epoch 6846: train loss: 0.4760429263114929\n",
      "Epoch 6847: train loss: 0.4760427176952362\n",
      "Epoch 6848: train loss: 0.47604259848594666\n",
      "Epoch 6849: train loss: 0.4760424494743347\n",
      "Epoch 6850: train loss: 0.476042240858078\n",
      "Epoch 6851: train loss: 0.47604209184646606\n",
      "Epoch 6852: train loss: 0.4760418236255646\n",
      "Epoch 6853: train loss: 0.4760418236255646\n",
      "Epoch 6854: train loss: 0.4760415554046631\n",
      "Epoch 6855: train loss: 0.47604140639305115\n",
      "Epoch 6856: train loss: 0.47604119777679443\n",
      "Epoch 6857: train loss: 0.4760410785675049\n",
      "Epoch 6858: train loss: 0.47604086995124817\n",
      "Epoch 6859: train loss: 0.47604066133499146\n",
      "Epoch 6860: train loss: 0.4760405719280243\n",
      "Epoch 6861: train loss: 0.4760403633117676\n",
      "Epoch 6862: train loss: 0.47604015469551086\n",
      "Epoch 6863: train loss: 0.4760400950908661\n",
      "Epoch 6864: train loss: 0.4760398268699646\n",
      "Epoch 6865: train loss: 0.47603967785835266\n",
      "Epoch 6866: train loss: 0.4760395288467407\n",
      "Epoch 6867: train loss: 0.4760393500328064\n",
      "Epoch 6868: train loss: 0.47603920102119446\n",
      "Epoch 6869: train loss: 0.4760390520095825\n",
      "Epoch 6870: train loss: 0.4760389029979706\n",
      "Epoch 6871: train loss: 0.47603869438171387\n",
      "Epoch 6872: train loss: 0.47603848576545715\n",
      "Epoch 6873: train loss: 0.4760383665561676\n",
      "Epoch 6874: train loss: 0.4760381579399109\n",
      "Epoch 6875: train loss: 0.47603800892829895\n",
      "Epoch 6876: train loss: 0.476037859916687\n",
      "Epoch 6877: train loss: 0.4760376811027527\n",
      "Epoch 6878: train loss: 0.47603753209114075\n",
      "Epoch 6879: train loss: 0.47603732347488403\n",
      "Epoch 6880: train loss: 0.4760371744632721\n",
      "Epoch 6881: train loss: 0.47603702545166016\n",
      "Epoch 6882: train loss: 0.47603681683540344\n",
      "Epoch 6883: train loss: 0.4760366380214691\n",
      "Epoch 6884: train loss: 0.4760364890098572\n",
      "Epoch 6885: train loss: 0.47603633999824524\n",
      "Epoch 6886: train loss: 0.4760361313819885\n",
      "Epoch 6887: train loss: 0.4760359525680542\n",
      "Epoch 6888: train loss: 0.47603580355644226\n",
      "Epoch 6889: train loss: 0.4760356545448303\n",
      "Epoch 6890: train loss: 0.4760354459285736\n",
      "Epoch 6891: train loss: 0.47603529691696167\n",
      "Epoch 6892: train loss: 0.47603511810302734\n",
      "Epoch 6893: train loss: 0.4760350286960602\n",
      "Epoch 6894: train loss: 0.47603482007980347\n",
      "Epoch 6895: train loss: 0.47603461146354675\n",
      "Epoch 6896: train loss: 0.4760344624519348\n",
      "Epoch 6897: train loss: 0.4760342836380005\n",
      "Epoch 6898: train loss: 0.47603413462638855\n",
      "Epoch 6899: train loss: 0.47603392601013184\n",
      "Epoch 6900: train loss: 0.4760337769985199\n",
      "Epoch 6901: train loss: 0.47603362798690796\n",
      "Epoch 6902: train loss: 0.47603344917297363\n",
      "Epoch 6903: train loss: 0.4760332405567169\n",
      "Epoch 6904: train loss: 0.47603315114974976\n",
      "Epoch 6905: train loss: 0.47603294253349304\n",
      "Epoch 6906: train loss: 0.4760327935218811\n",
      "Epoch 6907: train loss: 0.4760326147079468\n",
      "Epoch 6908: train loss: 0.47603240609169006\n",
      "Epoch 6909: train loss: 0.4760322570800781\n",
      "Epoch 6910: train loss: 0.4760321080684662\n",
      "Epoch 6911: train loss: 0.47603192925453186\n",
      "Epoch 6912: train loss: 0.4760317802429199\n",
      "Epoch 6913: train loss: 0.476031631231308\n",
      "Epoch 6914: train loss: 0.47603142261505127\n",
      "Epoch 6915: train loss: 0.47603121399879456\n",
      "Epoch 6916: train loss: 0.476031094789505\n",
      "Epoch 6917: train loss: 0.47603094577789307\n",
      "Epoch 6918: train loss: 0.47603073716163635\n",
      "Epoch 6919: train loss: 0.4760305881500244\n",
      "Epoch 6920: train loss: 0.4760304391384125\n",
      "Epoch 6921: train loss: 0.47603023052215576\n",
      "Epoch 6922: train loss: 0.4760301113128662\n",
      "Epoch 6923: train loss: 0.4760299623012543\n",
      "Epoch 6924: train loss: 0.47602975368499756\n",
      "Epoch 6925: train loss: 0.4760296046733856\n",
      "Epoch 6926: train loss: 0.4760294258594513\n",
      "Epoch 6927: train loss: 0.47602927684783936\n",
      "Epoch 6928: train loss: 0.47602906823158264\n",
      "Epoch 6929: train loss: 0.4760289192199707\n",
      "Epoch 6930: train loss: 0.47602877020835876\n",
      "Epoch 6931: train loss: 0.47602856159210205\n",
      "Epoch 6932: train loss: 0.4760283827781677\n",
      "Epoch 6933: train loss: 0.476028174161911\n",
      "Epoch 6934: train loss: 0.47602808475494385\n",
      "Epoch 6935: train loss: 0.47602787613868713\n",
      "Epoch 6936: train loss: 0.4760276973247528\n",
      "Epoch 6937: train loss: 0.47602760791778564\n",
      "Epoch 6938: train loss: 0.47602739930152893\n",
      "Epoch 6939: train loss: 0.476027250289917\n",
      "Epoch 6940: train loss: 0.47602707147598267\n",
      "Epoch 6941: train loss: 0.47602686285972595\n",
      "Epoch 6942: train loss: 0.476026713848114\n",
      "Epoch 6943: train loss: 0.4760265648365021\n",
      "Epoch 6944: train loss: 0.47602641582489014\n",
      "Epoch 6945: train loss: 0.4760262370109558\n",
      "Epoch 6946: train loss: 0.4760260283946991\n",
      "Epoch 6947: train loss: 0.47602593898773193\n",
      "Epoch 6948: train loss: 0.4760257303714752\n",
      "Epoch 6949: train loss: 0.4760255813598633\n",
      "Epoch 6950: train loss: 0.47602540254592896\n",
      "Epoch 6951: train loss: 0.476025253534317\n",
      "Epoch 6952: train loss: 0.4760251045227051\n",
      "Epoch 6953: train loss: 0.47602489590644836\n",
      "Epoch 6954: train loss: 0.4760247468948364\n",
      "Epoch 6955: train loss: 0.4760245680809021\n",
      "Epoch 6956: train loss: 0.4760243594646454\n",
      "Epoch 6957: train loss: 0.4760242700576782\n",
      "Epoch 6958: train loss: 0.4760240912437439\n",
      "Epoch 6959: train loss: 0.4760238826274872\n",
      "Epoch 6960: train loss: 0.47602373361587524\n",
      "Epoch 6961: train loss: 0.4760235846042633\n",
      "Epoch 6962: train loss: 0.47602343559265137\n",
      "Epoch 6963: train loss: 0.47602325677871704\n",
      "Epoch 6964: train loss: 0.4760231077671051\n",
      "Epoch 6965: train loss: 0.4760228991508484\n",
      "Epoch 6966: train loss: 0.47602275013923645\n",
      "Epoch 6967: train loss: 0.4760226011276245\n",
      "Epoch 6968: train loss: 0.4760224223136902\n",
      "Epoch 6969: train loss: 0.47602227330207825\n",
      "Epoch 6970: train loss: 0.47602206468582153\n",
      "Epoch 6971: train loss: 0.4760219156742096\n",
      "Epoch 6972: train loss: 0.47602176666259766\n",
      "Epoch 6973: train loss: 0.47602158784866333\n",
      "Epoch 6974: train loss: 0.4760214388370514\n",
      "Epoch 6975: train loss: 0.47602128982543945\n",
      "Epoch 6976: train loss: 0.4760211408138275\n",
      "Epoch 6977: train loss: 0.4760209321975708\n",
      "Epoch 6978: train loss: 0.4760207533836365\n",
      "Epoch 6979: train loss: 0.47602060437202454\n",
      "Epoch 6980: train loss: 0.4760204553604126\n",
      "Epoch 6981: train loss: 0.47602027654647827\n",
      "Epoch 6982: train loss: 0.47602012753486633\n",
      "Epoch 6983: train loss: 0.4760199189186096\n",
      "Epoch 6984: train loss: 0.4760197699069977\n",
      "Epoch 6985: train loss: 0.47601962089538574\n",
      "Epoch 6986: train loss: 0.47601941227912903\n",
      "Epoch 6987: train loss: 0.4760192334651947\n",
      "Epoch 6988: train loss: 0.47601908445358276\n",
      "Epoch 6989: train loss: 0.4760189354419708\n",
      "Epoch 6990: train loss: 0.4760187864303589\n",
      "Epoch 6991: train loss: 0.47601860761642456\n",
      "Epoch 6992: train loss: 0.47601839900016785\n",
      "Epoch 6993: train loss: 0.4760182499885559\n",
      "Epoch 6994: train loss: 0.47601810097694397\n",
      "Epoch 6995: train loss: 0.4760179817676544\n",
      "Epoch 6996: train loss: 0.4760177731513977\n",
      "Epoch 6997: train loss: 0.476017564535141\n",
      "Epoch 6998: train loss: 0.47601747512817383\n",
      "Epoch 6999: train loss: 0.4760172665119171\n",
      "Epoch 7000: train loss: 0.4760171175003052\n",
      "Epoch 7001: train loss: 0.47601693868637085\n",
      "Epoch 7002: train loss: 0.4760167896747589\n",
      "Epoch 7003: train loss: 0.476016640663147\n",
      "Epoch 7004: train loss: 0.47601646184921265\n",
      "Epoch 7005: train loss: 0.4760163128376007\n",
      "Epoch 7006: train loss: 0.476016104221344\n",
      "Epoch 7007: train loss: 0.47601595520973206\n",
      "Epoch 7008: train loss: 0.4760158061981201\n",
      "Epoch 7009: train loss: 0.4760155975818634\n",
      "Epoch 7010: train loss: 0.47601547837257385\n",
      "Epoch 7011: train loss: 0.4760153293609619\n",
      "Epoch 7012: train loss: 0.47601518034935\n",
      "Epoch 7013: train loss: 0.47601497173309326\n",
      "Epoch 7014: train loss: 0.4760148525238037\n",
      "Epoch 7015: train loss: 0.4760147035121918\n",
      "Epoch 7016: train loss: 0.47601449489593506\n",
      "Epoch 7017: train loss: 0.4760143458843231\n",
      "Epoch 7018: train loss: 0.4760141670703888\n",
      "Epoch 7019: train loss: 0.47601401805877686\n",
      "Epoch 7020: train loss: 0.4760138690471649\n",
      "Epoch 7021: train loss: 0.4760136604309082\n",
      "Epoch 7022: train loss: 0.4760134518146515\n",
      "Epoch 7023: train loss: 0.47601330280303955\n",
      "Epoch 7024: train loss: 0.47601318359375\n",
      "Epoch 7025: train loss: 0.47601303458213806\n",
      "Epoch 7026: train loss: 0.47601285576820374\n",
      "Epoch 7027: train loss: 0.476012647151947\n",
      "Epoch 7028: train loss: 0.47601255774497986\n",
      "Epoch 7029: train loss: 0.47601234912872314\n",
      "Epoch 7030: train loss: 0.4760122001171112\n",
      "Epoch 7031: train loss: 0.4760120213031769\n",
      "Epoch 7032: train loss: 0.47601187229156494\n",
      "Epoch 7033: train loss: 0.476011723279953\n",
      "Epoch 7034: train loss: 0.47601157426834106\n",
      "Epoch 7035: train loss: 0.47601139545440674\n",
      "Epoch 7036: train loss: 0.4760112464427948\n",
      "Epoch 7037: train loss: 0.4760110378265381\n",
      "Epoch 7038: train loss: 0.47601088881492615\n",
      "Epoch 7039: train loss: 0.47601068019866943\n",
      "Epoch 7040: train loss: 0.4760105609893799\n",
      "Epoch 7041: train loss: 0.47601041197776794\n",
      "Epoch 7042: train loss: 0.476010262966156\n",
      "Epoch 7043: train loss: 0.47601011395454407\n",
      "Epoch 7044: train loss: 0.47600990533828735\n",
      "Epoch 7045: train loss: 0.476009726524353\n",
      "Epoch 7046: train loss: 0.4760095775127411\n",
      "Epoch 7047: train loss: 0.47600942850112915\n",
      "Epoch 7048: train loss: 0.4760092496871948\n",
      "Epoch 7049: train loss: 0.4760091006755829\n",
      "Epoch 7050: train loss: 0.47600895166397095\n",
      "Epoch 7051: train loss: 0.47600874304771423\n",
      "Epoch 7052: train loss: 0.4760085940361023\n",
      "Epoch 7053: train loss: 0.47600847482681274\n",
      "Epoch 7054: train loss: 0.47600826621055603\n",
      "Epoch 7055: train loss: 0.4760081171989441\n",
      "Epoch 7056: train loss: 0.47600796818733215\n",
      "Epoch 7057: train loss: 0.47600775957107544\n",
      "Epoch 7058: train loss: 0.4760075807571411\n",
      "Epoch 7059: train loss: 0.4760074317455292\n",
      "Epoch 7060: train loss: 0.476007342338562\n",
      "Epoch 7061: train loss: 0.4760071337223053\n",
      "Epoch 7062: train loss: 0.47600701451301575\n",
      "Epoch 7063: train loss: 0.47600680589675903\n",
      "Epoch 7064: train loss: 0.4760066568851471\n",
      "Epoch 7065: train loss: 0.47600650787353516\n",
      "Epoch 7066: train loss: 0.47600632905960083\n",
      "Epoch 7067: train loss: 0.4760061800479889\n",
      "Epoch 7068: train loss: 0.47600603103637695\n",
      "Epoch 7069: train loss: 0.47600582242012024\n",
      "Epoch 7070: train loss: 0.4760057032108307\n",
      "Epoch 7071: train loss: 0.476005494594574\n",
      "Epoch 7072: train loss: 0.47600528597831726\n",
      "Epoch 7073: train loss: 0.4760051965713501\n",
      "Epoch 7074: train loss: 0.47600501775741577\n",
      "Epoch 7075: train loss: 0.47600486874580383\n",
      "Epoch 7076: train loss: 0.4760047197341919\n",
      "Epoch 7077: train loss: 0.4760045111179352\n",
      "Epoch 7078: train loss: 0.47600439190864563\n",
      "Epoch 7079: train loss: 0.4760042428970337\n",
      "Epoch 7080: train loss: 0.476004034280777\n",
      "Epoch 7081: train loss: 0.47600388526916504\n",
      "Epoch 7082: train loss: 0.4760036766529083\n",
      "Epoch 7083: train loss: 0.4760035574436188\n",
      "Epoch 7084: train loss: 0.4760034680366516\n",
      "Epoch 7085: train loss: 0.4760031998157501\n",
      "Epoch 7086: train loss: 0.47600311040878296\n",
      "Epoch 7087: train loss: 0.47600293159484863\n",
      "Epoch 7088: train loss: 0.4760027825832367\n",
      "Epoch 7089: train loss: 0.47600257396698\n",
      "Epoch 7090: train loss: 0.47600236535072327\n",
      "Epoch 7091: train loss: 0.4760022759437561\n",
      "Epoch 7092: train loss: 0.4760020971298218\n",
      "Epoch 7093: train loss: 0.47600194811820984\n",
      "Epoch 7094: train loss: 0.4760017395019531\n",
      "Epoch 7095: train loss: 0.4760016202926636\n",
      "Epoch 7096: train loss: 0.47600141167640686\n",
      "Epoch 7097: train loss: 0.4760012626647949\n",
      "Epoch 7098: train loss: 0.47600117325782776\n",
      "Epoch 7099: train loss: 0.47600096464157104\n",
      "Epoch 7100: train loss: 0.4760007858276367\n",
      "Epoch 7101: train loss: 0.4760006368160248\n",
      "Epoch 7102: train loss: 0.47600042819976807\n",
      "Epoch 7103: train loss: 0.4760003387928009\n",
      "Epoch 7104: train loss: 0.4760001599788666\n",
      "Epoch 7105: train loss: 0.47600001096725464\n",
      "Epoch 7106: train loss: 0.4759998619556427\n",
      "Epoch 7107: train loss: 0.47599971294403076\n",
      "Epoch 7108: train loss: 0.47599953413009644\n",
      "Epoch 7109: train loss: 0.4759993255138397\n",
      "Epoch 7110: train loss: 0.47599923610687256\n",
      "Epoch 7111: train loss: 0.4759990870952606\n",
      "Epoch 7112: train loss: 0.4759988784790039\n",
      "Epoch 7113: train loss: 0.4759986996650696\n",
      "Epoch 7114: train loss: 0.47599855065345764\n",
      "Epoch 7115: train loss: 0.4759984016418457\n",
      "Epoch 7116: train loss: 0.47599825263023376\n",
      "Epoch 7117: train loss: 0.4759981334209442\n",
      "Epoch 7118: train loss: 0.4759979248046875\n",
      "Epoch 7119: train loss: 0.47599777579307556\n",
      "Epoch 7120: train loss: 0.47599759697914124\n",
      "Epoch 7121: train loss: 0.4759973883628845\n",
      "Epoch 7122: train loss: 0.47599729895591736\n",
      "Epoch 7123: train loss: 0.4759971499443054\n",
      "Epoch 7124: train loss: 0.4759969711303711\n",
      "Epoch 7125: train loss: 0.4759967625141144\n",
      "Epoch 7126: train loss: 0.47599661350250244\n",
      "Epoch 7127: train loss: 0.4759964644908905\n",
      "Epoch 7128: train loss: 0.47599631547927856\n",
      "Epoch 7129: train loss: 0.47599613666534424\n",
      "Epoch 7130: train loss: 0.4759960472583771\n",
      "Epoch 7131: train loss: 0.47599583864212036\n",
      "Epoch 7132: train loss: 0.4759956896305084\n",
      "Epoch 7133: train loss: 0.4759955108165741\n",
      "Epoch 7134: train loss: 0.47599536180496216\n",
      "Epoch 7135: train loss: 0.47599515318870544\n",
      "Epoch 7136: train loss: 0.4759950041770935\n",
      "Epoch 7137: train loss: 0.47599488496780396\n",
      "Epoch 7138: train loss: 0.475994735956192\n",
      "Epoch 7139: train loss: 0.4759945869445801\n",
      "Epoch 7140: train loss: 0.47599437832832336\n",
      "Epoch 7141: train loss: 0.47599416971206665\n",
      "Epoch 7142: train loss: 0.4759940505027771\n",
      "Epoch 7143: train loss: 0.4759938418865204\n",
      "Epoch 7144: train loss: 0.4759937524795532\n",
      "Epoch 7145: train loss: 0.4759935736656189\n",
      "Epoch 7146: train loss: 0.47599342465400696\n",
      "Epoch 7147: train loss: 0.475993275642395\n",
      "Epoch 7148: train loss: 0.4759931266307831\n",
      "Epoch 7149: train loss: 0.47599300742149353\n",
      "Epoch 7150: train loss: 0.4759927988052368\n",
      "Epoch 7151: train loss: 0.4759925901889801\n",
      "Epoch 7152: train loss: 0.47599244117736816\n",
      "Epoch 7153: train loss: 0.4759923219680786\n",
      "Epoch 7154: train loss: 0.4759921133518219\n",
      "Epoch 7155: train loss: 0.47599196434020996\n",
      "Epoch 7156: train loss: 0.475991815328598\n",
      "Epoch 7157: train loss: 0.4759916663169861\n",
      "Epoch 7158: train loss: 0.47599148750305176\n",
      "Epoch 7159: train loss: 0.4759913384914398\n",
      "Epoch 7160: train loss: 0.47599124908447266\n",
      "Epoch 7161: train loss: 0.47599104046821594\n",
      "Epoch 7162: train loss: 0.4759908616542816\n",
      "Epoch 7163: train loss: 0.4759907126426697\n",
      "Epoch 7164: train loss: 0.47599056363105774\n",
      "Epoch 7165: train loss: 0.4759904146194458\n",
      "Epoch 7166: train loss: 0.4759901762008667\n",
      "Epoch 7167: train loss: 0.47599008679389954\n",
      "Epoch 7168: train loss: 0.4759899377822876\n",
      "Epoch 7169: train loss: 0.47598975896835327\n",
      "Epoch 7170: train loss: 0.47598960995674133\n",
      "Epoch 7171: train loss: 0.4759894609451294\n",
      "Epoch 7172: train loss: 0.47598931193351746\n",
      "Epoch 7173: train loss: 0.47598913311958313\n",
      "Epoch 7174: train loss: 0.4759889245033264\n",
      "Epoch 7175: train loss: 0.4759887754917145\n",
      "Epoch 7176: train loss: 0.4759886860847473\n",
      "Epoch 7177: train loss: 0.475988507270813\n",
      "Epoch 7178: train loss: 0.4759882986545563\n",
      "Epoch 7179: train loss: 0.47598814964294434\n",
      "Epoch 7180: train loss: 0.4759880602359772\n",
      "Epoch 7181: train loss: 0.47598788142204285\n",
      "Epoch 7182: train loss: 0.47598767280578613\n",
      "Epoch 7183: train loss: 0.4759875237941742\n",
      "Epoch 7184: train loss: 0.47598737478256226\n",
      "Epoch 7185: train loss: 0.4759872257709503\n",
      "Epoch 7186: train loss: 0.475987046957016\n",
      "Epoch 7187: train loss: 0.4759868383407593\n",
      "Epoch 7188: train loss: 0.4759867489337921\n",
      "Epoch 7189: train loss: 0.4759865999221802\n",
      "Epoch 7190: train loss: 0.4759863615036011\n",
      "Epoch 7191: train loss: 0.47598621249198914\n",
      "Epoch 7192: train loss: 0.475986123085022\n",
      "Epoch 7193: train loss: 0.47598594427108765\n",
      "Epoch 7194: train loss: 0.4759857952594757\n",
      "Epoch 7195: train loss: 0.47598564624786377\n",
      "Epoch 7196: train loss: 0.47598549723625183\n",
      "Epoch 7197: train loss: 0.4759852886199951\n",
      "Epoch 7198: train loss: 0.47598516941070557\n",
      "Epoch 7199: train loss: 0.47598502039909363\n",
      "Epoch 7200: train loss: 0.4759848713874817\n",
      "Epoch 7201: train loss: 0.475984662771225\n",
      "Epoch 7202: train loss: 0.4759845435619354\n",
      "Epoch 7203: train loss: 0.4759843349456787\n",
      "Epoch 7204: train loss: 0.47598424553871155\n",
      "Epoch 7205: train loss: 0.4759840667247772\n",
      "Epoch 7206: train loss: 0.4759838581085205\n",
      "Epoch 7207: train loss: 0.47598376870155334\n",
      "Epoch 7208: train loss: 0.47598356008529663\n",
      "Epoch 7209: train loss: 0.4759834408760071\n",
      "Epoch 7210: train loss: 0.47598329186439514\n",
      "Epoch 7211: train loss: 0.4759830832481384\n",
      "Epoch 7212: train loss: 0.47598299384117126\n",
      "Epoch 7213: train loss: 0.47598278522491455\n",
      "Epoch 7214: train loss: 0.475982666015625\n",
      "Epoch 7215: train loss: 0.4759824573993683\n",
      "Epoch 7216: train loss: 0.47598230838775635\n",
      "Epoch 7217: train loss: 0.475982129573822\n",
      "Epoch 7218: train loss: 0.4759819805622101\n",
      "Epoch 7219: train loss: 0.47598183155059814\n",
      "Epoch 7220: train loss: 0.4759816825389862\n",
      "Epoch 7221: train loss: 0.4759815037250519\n",
      "Epoch 7222: train loss: 0.47598135471343994\n",
      "Epoch 7223: train loss: 0.475981205701828\n",
      "Epoch 7224: train loss: 0.47598105669021606\n",
      "Epoch 7225: train loss: 0.47598087787628174\n",
      "Epoch 7226: train loss: 0.475980669260025\n",
      "Epoch 7227: train loss: 0.47598057985305786\n",
      "Epoch 7228: train loss: 0.4759804308414459\n",
      "Epoch 7229: train loss: 0.4759802520275116\n",
      "Epoch 7230: train loss: 0.47598016262054443\n",
      "Epoch 7231: train loss: 0.4759799540042877\n",
      "Epoch 7232: train loss: 0.4759798049926758\n",
      "Epoch 7233: train loss: 0.47597962617874146\n",
      "Epoch 7234: train loss: 0.4759794771671295\n",
      "Epoch 7235: train loss: 0.4759793281555176\n",
      "Epoch 7236: train loss: 0.475979208946228\n",
      "Epoch 7237: train loss: 0.4759790003299713\n",
      "Epoch 7238: train loss: 0.4759788513183594\n",
      "Epoch 7239: train loss: 0.47597870230674744\n",
      "Epoch 7240: train loss: 0.4759785830974579\n",
      "Epoch 7241: train loss: 0.47597837448120117\n",
      "Epoch 7242: train loss: 0.47597816586494446\n",
      "Epoch 7243: train loss: 0.4759780764579773\n",
      "Epoch 7244: train loss: 0.47597789764404297\n",
      "Epoch 7245: train loss: 0.47597774863243103\n",
      "Epoch 7246: train loss: 0.4759775996208191\n",
      "Epoch 7247: train loss: 0.47597745060920715\n",
      "Epoch 7248: train loss: 0.4759772717952728\n",
      "Epoch 7249: train loss: 0.4759770631790161\n",
      "Epoch 7250: train loss: 0.47597697377204895\n",
      "Epoch 7251: train loss: 0.475976824760437\n",
      "Epoch 7252: train loss: 0.4759766459465027\n",
      "Epoch 7253: train loss: 0.47597649693489075\n",
      "Epoch 7254: train loss: 0.4759764075279236\n",
      "Epoch 7255: train loss: 0.47597619891166687\n",
      "Epoch 7256: train loss: 0.47597602009773254\n",
      "Epoch 7257: train loss: 0.4759758710861206\n",
      "Epoch 7258: train loss: 0.47597572207450867\n",
      "Epoch 7259: train loss: 0.47597557306289673\n",
      "Epoch 7260: train loss: 0.4759754538536072\n",
      "Epoch 7261: train loss: 0.47597524523735046\n",
      "Epoch 7262: train loss: 0.4759750962257385\n",
      "Epoch 7263: train loss: 0.475974977016449\n",
      "Epoch 7264: train loss: 0.47597482800483704\n",
      "Epoch 7265: train loss: 0.4759746193885803\n",
      "Epoch 7266: train loss: 0.47597450017929077\n",
      "Epoch 7267: train loss: 0.47597429156303406\n",
      "Epoch 7268: train loss: 0.4759742021560669\n",
      "Epoch 7269: train loss: 0.4759739935398102\n",
      "Epoch 7270: train loss: 0.47597384452819824\n",
      "Epoch 7271: train loss: 0.4759736657142639\n",
      "Epoch 7272: train loss: 0.47597357630729675\n",
      "Epoch 7273: train loss: 0.47597336769104004\n",
      "Epoch 7274: train loss: 0.4759732186794281\n",
      "Epoch 7275: train loss: 0.4759730398654938\n",
      "Epoch 7276: train loss: 0.4759729504585266\n",
      "Epoch 7277: train loss: 0.4759727418422699\n",
      "Epoch 7278: train loss: 0.47597262263298035\n",
      "Epoch 7279: train loss: 0.4759724736213684\n",
      "Epoch 7280: train loss: 0.47597232460975647\n",
      "Epoch 7281: train loss: 0.47597217559814453\n",
      "Epoch 7282: train loss: 0.4759719669818878\n",
      "Epoch 7283: train loss: 0.47597184777259827\n",
      "Epoch 7284: train loss: 0.47597163915634155\n",
      "Epoch 7285: train loss: 0.4759715497493744\n",
      "Epoch 7286: train loss: 0.4759713411331177\n",
      "Epoch 7287: train loss: 0.47597116231918335\n",
      "Epoch 7288: train loss: 0.4759710729122162\n",
      "Epoch 7289: train loss: 0.47597089409828186\n",
      "Epoch 7290: train loss: 0.4759707450866699\n",
      "Epoch 7291: train loss: 0.4759705364704132\n",
      "Epoch 7292: train loss: 0.47597044706344604\n",
      "Epoch 7293: train loss: 0.4759703278541565\n",
      "Epoch 7294: train loss: 0.4759701192378998\n",
      "Epoch 7295: train loss: 0.47596991062164307\n",
      "Epoch 7296: train loss: 0.4759698212146759\n",
      "Epoch 7297: train loss: 0.4759696424007416\n",
      "Epoch 7298: train loss: 0.47596949338912964\n",
      "Epoch 7299: train loss: 0.4759693443775177\n",
      "Epoch 7300: train loss: 0.47596922516822815\n",
      "Epoch 7301: train loss: 0.47596901655197144\n",
      "Epoch 7302: train loss: 0.4759688675403595\n",
      "Epoch 7303: train loss: 0.47596871852874756\n",
      "Epoch 7304: train loss: 0.4759685695171356\n",
      "Epoch 7305: train loss: 0.4759683907032013\n",
      "Epoch 7306: train loss: 0.47596824169158936\n",
      "Epoch 7307: train loss: 0.4759681522846222\n",
      "Epoch 7308: train loss: 0.47596797347068787\n",
      "Epoch 7309: train loss: 0.4759678244590759\n",
      "Epoch 7310: train loss: 0.4759676158428192\n",
      "Epoch 7311: train loss: 0.47596752643585205\n",
      "Epoch 7312: train loss: 0.4759673476219177\n",
      "Epoch 7313: train loss: 0.475967139005661\n",
      "Epoch 7314: train loss: 0.47596704959869385\n",
      "Epoch 7315: train loss: 0.4759668707847595\n",
      "Epoch 7316: train loss: 0.4759667217731476\n",
      "Epoch 7317: train loss: 0.47596657276153564\n",
      "Epoch 7318: train loss: 0.4759664535522461\n",
      "Epoch 7319: train loss: 0.4759662449359894\n",
      "Epoch 7320: train loss: 0.4759661555290222\n",
      "Epoch 7321: train loss: 0.4759660065174103\n",
      "Epoch 7322: train loss: 0.47596579790115356\n",
      "Epoch 7323: train loss: 0.47596561908721924\n",
      "Epoch 7324: train loss: 0.4759654700756073\n",
      "Epoch 7325: train loss: 0.47596538066864014\n",
      "Epoch 7326: train loss: 0.4759651720523834\n",
      "Epoch 7327: train loss: 0.47596505284309387\n",
      "Epoch 7328: train loss: 0.47596484422683716\n",
      "Epoch 7329: train loss: 0.4759646952152252\n",
      "Epoch 7330: train loss: 0.4759645462036133\n",
      "Epoch 7331: train loss: 0.47596436738967896\n",
      "Epoch 7332: train loss: 0.475964218378067\n",
      "Epoch 7333: train loss: 0.4759640693664551\n",
      "Epoch 7334: train loss: 0.4759639501571655\n",
      "Epoch 7335: train loss: 0.4759638011455536\n",
      "Epoch 7336: train loss: 0.47596365213394165\n",
      "Epoch 7337: train loss: 0.4759634733200073\n",
      "Epoch 7338: train loss: 0.4759633243083954\n",
      "Epoch 7339: train loss: 0.47596317529678345\n",
      "Epoch 7340: train loss: 0.4759630560874939\n",
      "Epoch 7341: train loss: 0.4759628474712372\n",
      "Epoch 7342: train loss: 0.47596275806427\n",
      "Epoch 7343: train loss: 0.4759625494480133\n",
      "Epoch 7344: train loss: 0.47596240043640137\n",
      "Epoch 7345: train loss: 0.47596222162246704\n",
      "Epoch 7346: train loss: 0.4759621322154999\n",
      "Epoch 7347: train loss: 0.47596192359924316\n",
      "Epoch 7348: train loss: 0.4759618639945984\n",
      "Epoch 7349: train loss: 0.4759616553783417\n",
      "Epoch 7350: train loss: 0.47596144676208496\n",
      "Epoch 7351: train loss: 0.4759613573551178\n",
      "Epoch 7352: train loss: 0.4759611487388611\n",
      "Epoch 7353: train loss: 0.47596102952957153\n",
      "Epoch 7354: train loss: 0.47596094012260437\n",
      "Epoch 7355: train loss: 0.47596076130867004\n",
      "Epoch 7356: train loss: 0.4759606122970581\n",
      "Epoch 7357: train loss: 0.47596046328544617\n",
      "Epoch 7358: train loss: 0.47596025466918945\n",
      "Epoch 7359: train loss: 0.4759601056575775\n",
      "Epoch 7360: train loss: 0.47595998644828796\n",
      "Epoch 7361: train loss: 0.47595977783203125\n",
      "Epoch 7362: train loss: 0.4759596586227417\n",
      "Epoch 7363: train loss: 0.47595950961112976\n",
      "Epoch 7364: train loss: 0.4759593605995178\n",
      "Epoch 7365: train loss: 0.4759592115879059\n",
      "Epoch 7366: train loss: 0.47595903277397156\n",
      "Epoch 7367: train loss: 0.4759589433670044\n",
      "Epoch 7368: train loss: 0.4759587347507477\n",
      "Epoch 7369: train loss: 0.47595858573913574\n",
      "Epoch 7370: train loss: 0.4759584069252014\n",
      "Epoch 7371: train loss: 0.4759582579135895\n",
      "Epoch 7372: train loss: 0.4759581685066223\n",
      "Epoch 7373: train loss: 0.475957989692688\n",
      "Epoch 7374: train loss: 0.47595784068107605\n",
      "Epoch 7375: train loss: 0.47595763206481934\n",
      "Epoch 7376: train loss: 0.4759575426578522\n",
      "Epoch 7377: train loss: 0.47595736384391785\n",
      "Epoch 7378: train loss: 0.4759572148323059\n",
      "Epoch 7379: train loss: 0.47595712542533875\n",
      "Epoch 7380: train loss: 0.47595691680908203\n",
      "Epoch 7381: train loss: 0.4759567975997925\n",
      "Epoch 7382: train loss: 0.47595658898353577\n",
      "Epoch 7383: train loss: 0.47595643997192383\n",
      "Epoch 7384: train loss: 0.4759562909603119\n",
      "Epoch 7385: train loss: 0.47595611214637756\n",
      "Epoch 7386: train loss: 0.4759560227394104\n",
      "Epoch 7387: train loss: 0.4759558439254761\n",
      "Epoch 7388: train loss: 0.47595569491386414\n",
      "Epoch 7389: train loss: 0.4759555459022522\n",
      "Epoch 7390: train loss: 0.47595539689064026\n",
      "Epoch 7391: train loss: 0.47595521807670593\n",
      "Epoch 7392: train loss: 0.47595512866973877\n",
      "Epoch 7393: train loss: 0.47595492005348206\n",
      "Epoch 7394: train loss: 0.4759547710418701\n",
      "Epoch 7395: train loss: 0.47595465183258057\n",
      "Epoch 7396: train loss: 0.47595444321632385\n",
      "Epoch 7397: train loss: 0.4759543538093567\n",
      "Epoch 7398: train loss: 0.47595417499542236\n",
      "Epoch 7399: train loss: 0.47595396637916565\n",
      "Epoch 7400: train loss: 0.4759538769721985\n",
      "Epoch 7401: train loss: 0.47595372796058655\n",
      "Epoch 7402: train loss: 0.4759535491466522\n",
      "Epoch 7403: train loss: 0.4759534001350403\n",
      "Epoch 7404: train loss: 0.4759533107280731\n",
      "Epoch 7405: train loss: 0.4759531021118164\n",
      "Epoch 7406: train loss: 0.47595298290252686\n",
      "Epoch 7407: train loss: 0.4759528338909149\n",
      "Epoch 7408: train loss: 0.475952684879303\n",
      "Epoch 7409: train loss: 0.47595250606536865\n",
      "Epoch 7410: train loss: 0.4759523570537567\n",
      "Epoch 7411: train loss: 0.4759522080421448\n",
      "Epoch 7412: train loss: 0.47595202922821045\n",
      "Epoch 7413: train loss: 0.4759518802165985\n",
      "Epoch 7414: train loss: 0.4759517312049866\n",
      "Epoch 7415: train loss: 0.475951611995697\n",
      "Epoch 7416: train loss: 0.4759514033794403\n",
      "Epoch 7417: train loss: 0.47595125436782837\n",
      "Epoch 7418: train loss: 0.4759511947631836\n",
      "Epoch 7419: train loss: 0.4759509861469269\n",
      "Epoch 7420: train loss: 0.47595083713531494\n",
      "Epoch 7421: train loss: 0.475950688123703\n",
      "Epoch 7422: train loss: 0.47595053911209106\n",
      "Epoch 7423: train loss: 0.4759504199028015\n",
      "Epoch 7424: train loss: 0.4759502708911896\n",
      "Epoch 7425: train loss: 0.47595006227493286\n",
      "Epoch 7426: train loss: 0.4759499430656433\n",
      "Epoch 7427: train loss: 0.47594979405403137\n",
      "Epoch 7428: train loss: 0.47594964504241943\n",
      "Epoch 7429: train loss: 0.4759494364261627\n",
      "Epoch 7430: train loss: 0.47594931721687317\n",
      "Epoch 7431: train loss: 0.475949227809906\n",
      "Epoch 7432: train loss: 0.4759490191936493\n",
      "Epoch 7433: train loss: 0.47594887018203735\n",
      "Epoch 7434: train loss: 0.475948691368103\n",
      "Epoch 7435: train loss: 0.4759485423564911\n",
      "Epoch 7436: train loss: 0.4759484529495239\n",
      "Epoch 7437: train loss: 0.4759482741355896\n",
      "Epoch 7438: train loss: 0.47594812512397766\n",
      "Epoch 7439: train loss: 0.4759479761123657\n",
      "Epoch 7440: train loss: 0.4759477972984314\n",
      "Epoch 7441: train loss: 0.47594770789146423\n",
      "Epoch 7442: train loss: 0.4759474992752075\n",
      "Epoch 7443: train loss: 0.4759473502635956\n",
      "Epoch 7444: train loss: 0.47594723105430603\n",
      "Epoch 7445: train loss: 0.4759470820426941\n",
      "Epoch 7446: train loss: 0.47594693303108215\n",
      "Epoch 7447: train loss: 0.47594672441482544\n",
      "Epoch 7448: train loss: 0.4759466052055359\n",
      "Epoch 7449: train loss: 0.4759465157985687\n",
      "Epoch 7450: train loss: 0.4759463369846344\n",
      "Epoch 7451: train loss: 0.47594618797302246\n",
      "Epoch 7452: train loss: 0.47594597935676575\n",
      "Epoch 7453: train loss: 0.4759458899497986\n",
      "Epoch 7454: train loss: 0.47594568133354187\n",
      "Epoch 7455: train loss: 0.4759455621242523\n",
      "Epoch 7456: train loss: 0.47594547271728516\n",
      "Epoch 7457: train loss: 0.47594529390335083\n",
      "Epoch 7458: train loss: 0.4759450852870941\n",
      "Epoch 7459: train loss: 0.47594499588012695\n",
      "Epoch 7460: train loss: 0.475944846868515\n",
      "Epoch 7461: train loss: 0.4759446382522583\n",
      "Epoch 7462: train loss: 0.47594451904296875\n",
      "Epoch 7463: train loss: 0.4759443700313568\n",
      "Epoch 7464: train loss: 0.47594425082206726\n",
      "Epoch 7465: train loss: 0.4759441018104553\n",
      "Epoch 7466: train loss: 0.4759439527988434\n",
      "Epoch 7467: train loss: 0.47594377398490906\n",
      "Epoch 7468: train loss: 0.4759436249732971\n",
      "Epoch 7469: train loss: 0.4759434759616852\n",
      "Epoch 7470: train loss: 0.47594332695007324\n",
      "Epoch 7471: train loss: 0.4759431481361389\n",
      "Epoch 7472: train loss: 0.47594305872917175\n",
      "Epoch 7473: train loss: 0.47594285011291504\n",
      "Epoch 7474: train loss: 0.4759427011013031\n",
      "Epoch 7475: train loss: 0.47594258189201355\n",
      "Epoch 7476: train loss: 0.4759424328804016\n",
      "Epoch 7477: train loss: 0.47594231367111206\n",
      "Epoch 7478: train loss: 0.47594207525253296\n",
      "Epoch 7479: train loss: 0.4759419560432434\n",
      "Epoch 7480: train loss: 0.47594180703163147\n",
      "Epoch 7481: train loss: 0.4759416878223419\n",
      "Epoch 7482: train loss: 0.47594153881073\n",
      "Epoch 7483: train loss: 0.47594138979911804\n",
      "Epoch 7484: train loss: 0.4759412407875061\n",
      "Epoch 7485: train loss: 0.4759410619735718\n",
      "Epoch 7486: train loss: 0.47594091296195984\n",
      "Epoch 7487: train loss: 0.4759408235549927\n",
      "Epoch 7488: train loss: 0.4759405851364136\n",
      "Epoch 7489: train loss: 0.4759404957294464\n",
      "Epoch 7490: train loss: 0.4759403467178345\n",
      "Epoch 7491: train loss: 0.4759402275085449\n",
      "Epoch 7492: train loss: 0.4759400188922882\n",
      "Epoch 7493: train loss: 0.47593992948532104\n",
      "Epoch 7494: train loss: 0.47593972086906433\n",
      "Epoch 7495: train loss: 0.47593954205513\n",
      "Epoch 7496: train loss: 0.47593945264816284\n",
      "Epoch 7497: train loss: 0.4759393036365509\n",
      "Epoch 7498: train loss: 0.4759391248226166\n",
      "Epoch 7499: train loss: 0.47593897581100464\n",
      "Epoch 7500: train loss: 0.4759388267993927\n",
      "Epoch 7501: train loss: 0.47593870759010315\n",
      "Epoch 7502: train loss: 0.4759385585784912\n",
      "Epoch 7503: train loss: 0.4759384095668793\n",
      "Epoch 7504: train loss: 0.47593826055526733\n",
      "Epoch 7505: train loss: 0.475938081741333\n",
      "Epoch 7506: train loss: 0.47593793272972107\n",
      "Epoch 7507: train loss: 0.4759378433227539\n",
      "Epoch 7508: train loss: 0.4759376645088196\n",
      "Epoch 7509: train loss: 0.47593751549720764\n",
      "Epoch 7510: train loss: 0.4759373664855957\n",
      "Epoch 7511: train loss: 0.47593721747398376\n",
      "Epoch 7512: train loss: 0.47593703866004944\n",
      "Epoch 7513: train loss: 0.4759369492530823\n",
      "Epoch 7514: train loss: 0.47593674063682556\n",
      "Epoch 7515: train loss: 0.475936621427536\n",
      "Epoch 7516: train loss: 0.4759364724159241\n",
      "Epoch 7517: train loss: 0.47593632340431213\n",
      "Epoch 7518: train loss: 0.4759362041950226\n",
      "Epoch 7519: train loss: 0.47593599557876587\n",
      "Epoch 7520: train loss: 0.4759359061717987\n",
      "Epoch 7521: train loss: 0.4759357273578644\n",
      "Epoch 7522: train loss: 0.47593557834625244\n",
      "Epoch 7523: train loss: 0.4759354293346405\n",
      "Epoch 7524: train loss: 0.47593531012535095\n",
      "Epoch 7525: train loss: 0.475935161113739\n",
      "Epoch 7526: train loss: 0.4759350121021271\n",
      "Epoch 7527: train loss: 0.47593486309051514\n",
      "Epoch 7528: train loss: 0.4759346842765808\n",
      "Epoch 7529: train loss: 0.47593453526496887\n",
      "Epoch 7530: train loss: 0.47593438625335693\n",
      "Epoch 7531: train loss: 0.4759342670440674\n",
      "Epoch 7532: train loss: 0.47593411803245544\n",
      "Epoch 7533: train loss: 0.4759339690208435\n",
      "Epoch 7534: train loss: 0.47593382000923157\n",
      "Epoch 7535: train loss: 0.47593364119529724\n",
      "Epoch 7536: train loss: 0.4759334921836853\n",
      "Epoch 7537: train loss: 0.47593334317207336\n",
      "Epoch 7538: train loss: 0.4759332239627838\n",
      "Epoch 7539: train loss: 0.47593313455581665\n",
      "Epoch 7540: train loss: 0.47593292593955994\n",
      "Epoch 7541: train loss: 0.4759328067302704\n",
      "Epoch 7542: train loss: 0.47593265771865845\n",
      "Epoch 7543: train loss: 0.4759325087070465\n",
      "Epoch 7544: train loss: 0.47593238949775696\n",
      "Epoch 7545: train loss: 0.47593218088150024\n",
      "Epoch 7546: train loss: 0.4759320318698883\n",
      "Epoch 7547: train loss: 0.47593188285827637\n",
      "Epoch 7548: train loss: 0.4759318232536316\n",
      "Epoch 7549: train loss: 0.4759315550327301\n",
      "Epoch 7550: train loss: 0.47593146562576294\n",
      "Epoch 7551: train loss: 0.4759313464164734\n",
      "Epoch 7552: train loss: 0.47593119740486145\n",
      "Epoch 7553: train loss: 0.4759310483932495\n",
      "Epoch 7554: train loss: 0.4759308397769928\n",
      "Epoch 7555: train loss: 0.47593072056770325\n",
      "Epoch 7556: train loss: 0.4759306311607361\n",
      "Epoch 7557: train loss: 0.47593042254447937\n",
      "Epoch 7558: train loss: 0.4759303033351898\n",
      "Epoch 7559: train loss: 0.4759301543235779\n",
      "Epoch 7560: train loss: 0.47593000531196594\n",
      "Epoch 7561: train loss: 0.4759298264980316\n",
      "Epoch 7562: train loss: 0.47592973709106445\n",
      "Epoch 7563: train loss: 0.4759295880794525\n",
      "Epoch 7564: train loss: 0.4759293794631958\n",
      "Epoch 7565: train loss: 0.475929319858551\n",
      "Epoch 7566: train loss: 0.47592905163764954\n",
      "Epoch 7567: train loss: 0.47592899203300476\n",
      "Epoch 7568: train loss: 0.4759288430213928\n",
      "Epoch 7569: train loss: 0.4759286344051361\n",
      "Epoch 7570: train loss: 0.47592851519584656\n",
      "Epoch 7571: train loss: 0.4759283661842346\n",
      "Epoch 7572: train loss: 0.4759282171726227\n",
      "Epoch 7573: train loss: 0.47592806816101074\n",
      "Epoch 7574: train loss: 0.4759279489517212\n",
      "Epoch 7575: train loss: 0.4759277403354645\n",
      "Epoch 7576: train loss: 0.4759276509284973\n",
      "Epoch 7577: train loss: 0.475927472114563\n",
      "Epoch 7578: train loss: 0.47592732310295105\n",
      "Epoch 7579: train loss: 0.4759272336959839\n",
      "Epoch 7580: train loss: 0.47592705488204956\n",
      "Epoch 7581: train loss: 0.4759269058704376\n",
      "Epoch 7582: train loss: 0.4759267568588257\n",
      "Epoch 7583: train loss: 0.47592663764953613\n",
      "Epoch 7584: train loss: 0.4759264886379242\n",
      "Epoch 7585: train loss: 0.47592633962631226\n",
      "Epoch 7586: train loss: 0.4759261906147003\n",
      "Epoch 7587: train loss: 0.475926011800766\n",
      "Epoch 7588: train loss: 0.47592592239379883\n",
      "Epoch 7589: train loss: 0.4759257137775421\n",
      "Epoch 7590: train loss: 0.47592559456825256\n",
      "Epoch 7591: train loss: 0.4759254455566406\n",
      "Epoch 7592: train loss: 0.4759253263473511\n",
      "Epoch 7593: train loss: 0.47592517733573914\n",
      "Epoch 7594: train loss: 0.475925087928772\n",
      "Epoch 7595: train loss: 0.47592487931251526\n",
      "Epoch 7596: train loss: 0.47592470049858093\n",
      "Epoch 7597: train loss: 0.475924551486969\n",
      "Epoch 7598: train loss: 0.47592446208000183\n",
      "Epoch 7599: train loss: 0.4759242832660675\n",
      "Epoch 7600: train loss: 0.47592413425445557\n",
      "Epoch 7601: train loss: 0.47592398524284363\n",
      "Epoch 7602: train loss: 0.47592392563819885\n",
      "Epoch 7603: train loss: 0.47592371702194214\n",
      "Epoch 7604: train loss: 0.4759235680103302\n",
      "Epoch 7605: train loss: 0.47592341899871826\n",
      "Epoch 7606: train loss: 0.47592324018478394\n",
      "Epoch 7607: train loss: 0.475923091173172\n",
      "Epoch 7608: train loss: 0.47592300176620483\n",
      "Epoch 7609: train loss: 0.4759228229522705\n",
      "Epoch 7610: train loss: 0.47592267394065857\n",
      "Epoch 7611: train loss: 0.4759225845336914\n",
      "Epoch 7612: train loss: 0.4759224057197571\n",
      "Epoch 7613: train loss: 0.47592219710350037\n",
      "Epoch 7614: train loss: 0.475922167301178\n",
      "Epoch 7615: train loss: 0.47592195868492126\n",
      "Epoch 7616: train loss: 0.47592177987098694\n",
      "Epoch 7617: train loss: 0.475921630859375\n",
      "Epoch 7618: train loss: 0.47592151165008545\n",
      "Epoch 7619: train loss: 0.4759213626384735\n",
      "Epoch 7620: train loss: 0.4759212136268616\n",
      "Epoch 7621: train loss: 0.475921094417572\n",
      "Epoch 7622: train loss: 0.4759208858013153\n",
      "Epoch 7623: train loss: 0.47592079639434814\n",
      "Epoch 7624: train loss: 0.4759206771850586\n",
      "Epoch 7625: train loss: 0.4759204685688019\n",
      "Epoch 7626: train loss: 0.4759203791618347\n",
      "Epoch 7627: train loss: 0.4759202301502228\n",
      "Epoch 7628: train loss: 0.4759201109409332\n",
      "Epoch 7629: train loss: 0.4759199023246765\n",
      "Epoch 7630: train loss: 0.47591981291770935\n",
      "Epoch 7631: train loss: 0.475919634103775\n",
      "Epoch 7632: train loss: 0.4759194850921631\n",
      "Epoch 7633: train loss: 0.47591933608055115\n",
      "Epoch 7634: train loss: 0.4759192168712616\n",
      "Epoch 7635: train loss: 0.47591906785964966\n",
      "Epoch 7636: train loss: 0.4759189188480377\n",
      "Epoch 7637: train loss: 0.4759187698364258\n",
      "Epoch 7638: train loss: 0.47591865062713623\n",
      "Epoch 7639: train loss: 0.4759185016155243\n",
      "Epoch 7640: train loss: 0.47591835260391235\n",
      "Epoch 7641: train loss: 0.475918173789978\n",
      "Epoch 7642: train loss: 0.47591808438301086\n",
      "Epoch 7643: train loss: 0.47591787576675415\n",
      "Epoch 7644: train loss: 0.4759177565574646\n",
      "Epoch 7645: train loss: 0.47591766715049744\n",
      "Epoch 7646: train loss: 0.4759174585342407\n",
      "Epoch 7647: train loss: 0.47591733932495117\n",
      "Epoch 7648: train loss: 0.47591719031333923\n",
      "Epoch 7649: train loss: 0.4759170413017273\n",
      "Epoch 7650: train loss: 0.47591686248779297\n",
      "Epoch 7651: train loss: 0.4759167730808258\n",
      "Epoch 7652: train loss: 0.47591662406921387\n",
      "Epoch 7653: train loss: 0.47591644525527954\n",
      "Epoch 7654: train loss: 0.4759162962436676\n",
      "Epoch 7655: train loss: 0.47591620683670044\n",
      "Epoch 7656: train loss: 0.4759160280227661\n",
      "Epoch 7657: train loss: 0.4759158790111542\n",
      "Epoch 7658: train loss: 0.47591572999954224\n",
      "Epoch 7659: train loss: 0.4759156107902527\n",
      "Epoch 7660: train loss: 0.47591546177864075\n",
      "Epoch 7661: train loss: 0.4759153723716736\n",
      "Epoch 7662: train loss: 0.47591516375541687\n",
      "Epoch 7663: train loss: 0.4759150445461273\n",
      "Epoch 7664: train loss: 0.47591495513916016\n",
      "Epoch 7665: train loss: 0.47591474652290344\n",
      "Epoch 7666: train loss: 0.4759146273136139\n",
      "Epoch 7667: train loss: 0.47591447830200195\n",
      "Epoch 7668: train loss: 0.47591432929039\n",
      "Epoch 7669: train loss: 0.4759141504764557\n",
      "Epoch 7670: train loss: 0.4759140610694885\n",
      "Epoch 7671: train loss: 0.4759138822555542\n",
      "Epoch 7672: train loss: 0.47591373324394226\n",
      "Epoch 7673: train loss: 0.4759135842323303\n",
      "Epoch 7674: train loss: 0.4759134352207184\n",
      "Epoch 7675: train loss: 0.47591331601142883\n",
      "Epoch 7676: train loss: 0.4759131669998169\n",
      "Epoch 7677: train loss: 0.47591301798820496\n",
      "Epoch 7678: train loss: 0.47591283917427063\n",
      "Epoch 7679: train loss: 0.4759126901626587\n",
      "Epoch 7680: train loss: 0.47591254115104675\n",
      "Epoch 7681: train loss: 0.4759124219417572\n",
      "Epoch 7682: train loss: 0.47591233253479004\n",
      "Epoch 7683: train loss: 0.4759121239185333\n",
      "Epoch 7684: train loss: 0.4759120047092438\n",
      "Epoch 7685: train loss: 0.47591185569763184\n",
      "Epoch 7686: train loss: 0.4759117662906647\n",
      "Epoch 7687: train loss: 0.47591158747673035\n",
      "Epoch 7688: train loss: 0.47591137886047363\n",
      "Epoch 7689: train loss: 0.47591128945350647\n",
      "Epoch 7690: train loss: 0.4759111702442169\n",
      "Epoch 7691: train loss: 0.475911021232605\n",
      "Epoch 7692: train loss: 0.47591087222099304\n",
      "Epoch 7693: train loss: 0.4759107232093811\n",
      "Epoch 7694: train loss: 0.47591060400009155\n",
      "Epoch 7695: train loss: 0.47591039538383484\n",
      "Epoch 7696: train loss: 0.4759103059768677\n",
      "Epoch 7697: train loss: 0.47591012716293335\n",
      "Epoch 7698: train loss: 0.4759099781513214\n",
      "Epoch 7699: train loss: 0.47590991854667664\n",
      "Epoch 7700: train loss: 0.4759097099304199\n",
      "Epoch 7701: train loss: 0.47590962052345276\n",
      "Epoch 7702: train loss: 0.47590944170951843\n",
      "Epoch 7703: train loss: 0.4759092926979065\n",
      "Epoch 7704: train loss: 0.47590914368629456\n",
      "Epoch 7705: train loss: 0.4759089946746826\n",
      "Epoch 7706: train loss: 0.47590887546539307\n",
      "Epoch 7707: train loss: 0.47590872645378113\n",
      "Epoch 7708: train loss: 0.4759085774421692\n",
      "Epoch 7709: train loss: 0.47590845823287964\n",
      "Epoch 7710: train loss: 0.4759083092212677\n",
      "Epoch 7711: train loss: 0.47590816020965576\n",
      "Epoch 7712: train loss: 0.4759080410003662\n",
      "Epoch 7713: train loss: 0.4759078323841095\n",
      "Epoch 7714: train loss: 0.47590774297714233\n",
      "Epoch 7715: train loss: 0.475907564163208\n",
      "Epoch 7716: train loss: 0.47590747475624084\n",
      "Epoch 7717: train loss: 0.4759073257446289\n",
      "Epoch 7718: train loss: 0.4759071469306946\n",
      "Epoch 7719: train loss: 0.47590699791908264\n",
      "Epoch 7720: train loss: 0.4759068489074707\n",
      "Epoch 7721: train loss: 0.47590672969818115\n",
      "Epoch 7722: train loss: 0.4759065806865692\n",
      "Epoch 7723: train loss: 0.47590649127960205\n",
      "Epoch 7724: train loss: 0.4759063124656677\n",
      "Epoch 7725: train loss: 0.4759061634540558\n",
      "Epoch 7726: train loss: 0.47590601444244385\n",
      "Epoch 7727: train loss: 0.4759058952331543\n",
      "Epoch 7728: train loss: 0.47590574622154236\n",
      "Epoch 7729: train loss: 0.4759055972099304\n",
      "Epoch 7730: train loss: 0.4759054183959961\n",
      "Epoch 7731: train loss: 0.47590532898902893\n",
      "Epoch 7732: train loss: 0.475905179977417\n",
      "Epoch 7733: train loss: 0.47590500116348267\n",
      "Epoch 7734: train loss: 0.4759048521518707\n",
      "Epoch 7735: train loss: 0.47590476274490356\n",
      "Epoch 7736: train loss: 0.47590458393096924\n",
      "Epoch 7737: train loss: 0.4759044945240021\n",
      "Epoch 7738: train loss: 0.47590428590774536\n",
      "Epoch 7739: train loss: 0.4759041666984558\n",
      "Epoch 7740: train loss: 0.47590407729148865\n",
      "Epoch 7741: train loss: 0.47590386867523193\n",
      "Epoch 7742: train loss: 0.4759037494659424\n",
      "Epoch 7743: train loss: 0.47590354084968567\n",
      "Epoch 7744: train loss: 0.4759035110473633\n",
      "Epoch 7745: train loss: 0.47590333223342896\n",
      "Epoch 7746: train loss: 0.4759032428264618\n",
      "Epoch 7747: train loss: 0.47590309381484985\n",
      "Epoch 7748: train loss: 0.4759029150009155\n",
      "Epoch 7749: train loss: 0.4759027659893036\n",
      "Epoch 7750: train loss: 0.47590261697769165\n",
      "Epoch 7751: train loss: 0.4759024381637573\n",
      "Epoch 7752: train loss: 0.47590234875679016\n",
      "Epoch 7753: train loss: 0.4759022295475006\n",
      "Epoch 7754: train loss: 0.4759020209312439\n",
      "Epoch 7755: train loss: 0.47590187191963196\n",
      "Epoch 7756: train loss: 0.4759018123149872\n",
      "Epoch 7757: train loss: 0.47590166330337524\n",
      "Epoch 7758: train loss: 0.4759015142917633\n",
      "Epoch 7759: train loss: 0.47590136528015137\n",
      "Epoch 7760: train loss: 0.47590118646621704\n",
      "Epoch 7761: train loss: 0.4759010970592499\n",
      "Epoch 7762: train loss: 0.47590094804763794\n",
      "Epoch 7763: train loss: 0.4759007692337036\n",
      "Epoch 7764: train loss: 0.4759006202220917\n",
      "Epoch 7765: train loss: 0.4759005308151245\n",
      "Epoch 7766: train loss: 0.47590041160583496\n",
      "Epoch 7767: train loss: 0.475900262594223\n",
      "Epoch 7768: train loss: 0.4759001135826111\n",
      "Epoch 7769: train loss: 0.47589990496635437\n",
      "Epoch 7770: train loss: 0.4758997857570648\n",
      "Epoch 7771: train loss: 0.47589969635009766\n",
      "Epoch 7772: train loss: 0.47589951753616333\n",
      "Epoch 7773: train loss: 0.4758993685245514\n",
      "Epoch 7774: train loss: 0.47589927911758423\n",
      "Epoch 7775: train loss: 0.4758991003036499\n",
      "Epoch 7776: train loss: 0.47589895129203796\n",
      "Epoch 7777: train loss: 0.475898802280426\n",
      "Epoch 7778: train loss: 0.4758986234664917\n",
      "Epoch 7779: train loss: 0.47589853405952454\n",
      "Epoch 7780: train loss: 0.475898414850235\n",
      "Epoch 7781: train loss: 0.47589826583862305\n",
      "Epoch 7782: train loss: 0.4758981168270111\n",
      "Epoch 7783: train loss: 0.47589796781539917\n",
      "Epoch 7784: train loss: 0.47589778900146484\n",
      "Epoch 7785: train loss: 0.4758976995944977\n",
      "Epoch 7786: train loss: 0.47589755058288574\n",
      "Epoch 7787: train loss: 0.4758974313735962\n",
      "Epoch 7788: train loss: 0.47589728236198425\n",
      "Epoch 7789: train loss: 0.4758971333503723\n",
      "Epoch 7790: train loss: 0.475896954536438\n",
      "Epoch 7791: train loss: 0.4758968651294708\n",
      "Epoch 7792: train loss: 0.4758966565132141\n",
      "Epoch 7793: train loss: 0.47589659690856934\n",
      "Epoch 7794: train loss: 0.4758964478969574\n",
      "Epoch 7795: train loss: 0.47589629888534546\n",
      "Epoch 7796: train loss: 0.47589612007141113\n",
      "Epoch 7797: train loss: 0.4758959710597992\n",
      "Epoch 7798: train loss: 0.47589588165283203\n",
      "Epoch 7799: train loss: 0.4758957028388977\n",
      "Epoch 7800: train loss: 0.47589555382728577\n",
      "Epoch 7801: train loss: 0.4758954644203186\n",
      "Epoch 7802: train loss: 0.4758952856063843\n",
      "Epoch 7803: train loss: 0.4758951961994171\n",
      "Epoch 7804: train loss: 0.4758949875831604\n",
      "Epoch 7805: train loss: 0.47589486837387085\n",
      "Epoch 7806: train loss: 0.4758947193622589\n",
      "Epoch 7807: train loss: 0.47589460015296936\n",
      "Epoch 7808: train loss: 0.4758944511413574\n",
      "Epoch 7809: train loss: 0.4758943021297455\n",
      "Epoch 7810: train loss: 0.47589415311813354\n",
      "Epoch 7811: train loss: 0.475894033908844\n",
      "Epoch 7812: train loss: 0.47589388489723206\n",
      "Epoch 7813: train loss: 0.4758937656879425\n",
      "Epoch 7814: train loss: 0.47589367628097534\n",
      "Epoch 7815: train loss: 0.47589346766471863\n",
      "Epoch 7816: train loss: 0.4758933484554291\n",
      "Epoch 7817: train loss: 0.47589319944381714\n",
      "Epoch 7818: train loss: 0.47589311003685\n",
      "Epoch 7819: train loss: 0.47589293122291565\n",
      "Epoch 7820: train loss: 0.4758927822113037\n",
      "Epoch 7821: train loss: 0.47589269280433655\n",
      "Epoch 7822: train loss: 0.4758925139904022\n",
      "Epoch 7823: train loss: 0.4758923649787903\n",
      "Epoch 7824: train loss: 0.47589221596717834\n",
      "Epoch 7825: train loss: 0.4758920669555664\n",
      "Epoch 7826: train loss: 0.47589200735092163\n",
      "Epoch 7827: train loss: 0.4758917987346649\n",
      "Epoch 7828: train loss: 0.475891649723053\n",
      "Epoch 7829: train loss: 0.4758915305137634\n",
      "Epoch 7830: train loss: 0.4758913815021515\n",
      "Epoch 7831: train loss: 0.47589126229286194\n",
      "Epoch 7832: train loss: 0.4758911728858948\n",
      "Epoch 7833: train loss: 0.47589096426963806\n",
      "Epoch 7834: train loss: 0.4758908450603485\n",
      "Epoch 7835: train loss: 0.4758906960487366\n",
      "Epoch 7836: train loss: 0.475890576839447\n",
      "Epoch 7837: train loss: 0.4758904278278351\n",
      "Epoch 7838: train loss: 0.47589027881622314\n",
      "Epoch 7839: train loss: 0.4758901298046112\n",
      "Epoch 7840: train loss: 0.4758899509906769\n",
      "Epoch 7841: train loss: 0.4758898615837097\n",
      "Epoch 7842: train loss: 0.4758897125720978\n",
      "Epoch 7843: train loss: 0.47588953375816345\n",
      "Epoch 7844: train loss: 0.4758894443511963\n",
      "Epoch 7845: train loss: 0.47588929533958435\n",
      "Epoch 7846: train loss: 0.4758891761302948\n",
      "Epoch 7847: train loss: 0.47588902711868286\n",
      "Epoch 7848: train loss: 0.4758888781070709\n",
      "Epoch 7849: train loss: 0.4758886992931366\n",
      "Epoch 7850: train loss: 0.47588860988616943\n",
      "Epoch 7851: train loss: 0.4758884608745575\n",
      "Epoch 7852: train loss: 0.47588834166526794\n",
      "Epoch 7853: train loss: 0.475888192653656\n",
      "Epoch 7854: train loss: 0.47588804364204407\n",
      "Epoch 7855: train loss: 0.47588786482810974\n",
      "Epoch 7856: train loss: 0.47588783502578735\n",
      "Epoch 7857: train loss: 0.475887656211853\n",
      "Epoch 7858: train loss: 0.4758875072002411\n",
      "Epoch 7859: train loss: 0.47588735818862915\n",
      "Epoch 7860: train loss: 0.4758872389793396\n",
      "Epoch 7861: train loss: 0.47588708996772766\n",
      "Epoch 7862: train loss: 0.4758869707584381\n",
      "Epoch 7863: train loss: 0.47588682174682617\n",
      "Epoch 7864: train loss: 0.47588667273521423\n",
      "Epoch 7865: train loss: 0.4758865237236023\n",
      "Epoch 7866: train loss: 0.47588640451431274\n",
      "Epoch 7867: train loss: 0.4758862555027008\n",
      "Epoch 7868: train loss: 0.47588610649108887\n",
      "Epoch 7869: train loss: 0.4758859872817993\n",
      "Epoch 7870: train loss: 0.47588589787483215\n",
      "Epoch 7871: train loss: 0.47588568925857544\n",
      "Epoch 7872: train loss: 0.4758855700492859\n",
      "Epoch 7873: train loss: 0.47588542103767395\n",
      "Epoch 7874: train loss: 0.4758853018283844\n",
      "Epoch 7875: train loss: 0.4758850932121277\n",
      "Epoch 7876: train loss: 0.4758850038051605\n",
      "Epoch 7877: train loss: 0.47588488459587097\n",
      "Epoch 7878: train loss: 0.47588473558425903\n",
      "Epoch 7879: train loss: 0.47588464617729187\n",
      "Epoch 7880: train loss: 0.47588446736335754\n",
      "Epoch 7881: train loss: 0.4758843183517456\n",
      "Epoch 7882: train loss: 0.47588422894477844\n",
      "Epoch 7883: train loss: 0.4758840501308441\n",
      "Epoch 7884: train loss: 0.4758839011192322\n",
      "Epoch 7885: train loss: 0.475883811712265\n",
      "Epoch 7886: train loss: 0.4758836328983307\n",
      "Epoch 7887: train loss: 0.47588348388671875\n",
      "Epoch 7888: train loss: 0.4758833646774292\n",
      "Epoch 7889: train loss: 0.47588321566581726\n",
      "Epoch 7890: train loss: 0.4758830666542053\n",
      "Epoch 7891: train loss: 0.4758829176425934\n",
      "Epoch 7892: train loss: 0.47588279843330383\n",
      "Epoch 7893: train loss: 0.4758826494216919\n",
      "Epoch 7894: train loss: 0.47588253021240234\n",
      "Epoch 7895: train loss: 0.4758824408054352\n",
      "Epoch 7896: train loss: 0.47588223218917847\n",
      "Epoch 7897: train loss: 0.4758821725845337\n",
      "Epoch 7898: train loss: 0.475881963968277\n",
      "Epoch 7899: train loss: 0.4758818745613098\n",
      "Epoch 7900: train loss: 0.4758816957473755\n",
      "Epoch 7901: train loss: 0.4758816063404083\n",
      "Epoch 7902: train loss: 0.4758813977241516\n",
      "Epoch 7903: train loss: 0.47588127851486206\n",
      "Epoch 7904: train loss: 0.4758811295032501\n",
      "Epoch 7905: train loss: 0.47588104009628296\n",
      "Epoch 7906: train loss: 0.47588086128234863\n",
      "Epoch 7907: train loss: 0.47588077187538147\n",
      "Epoch 7908: train loss: 0.47588056325912476\n",
      "Epoch 7909: train loss: 0.4758804440498352\n",
      "Epoch 7910: train loss: 0.47588035464286804\n",
      "Epoch 7911: train loss: 0.4758802056312561\n",
      "Epoch 7912: train loss: 0.47588008642196655\n",
      "Epoch 7913: train loss: 0.4758799374103546\n",
      "Epoch 7914: train loss: 0.47587981820106506\n",
      "Epoch 7915: train loss: 0.4758796691894531\n",
      "Epoch 7916: train loss: 0.4758795201778412\n",
      "Epoch 7917: train loss: 0.47587934136390686\n",
      "Epoch 7918: train loss: 0.4758791923522949\n",
      "Epoch 7919: train loss: 0.47587910294532776\n",
      "Epoch 7920: train loss: 0.47587892413139343\n",
      "Epoch 7921: train loss: 0.4758787751197815\n",
      "Epoch 7922: train loss: 0.47587868571281433\n",
      "Epoch 7923: train loss: 0.4758785665035248\n",
      "Epoch 7924: train loss: 0.47587835788726807\n",
      "Epoch 7925: train loss: 0.4758782982826233\n",
      "Epoch 7926: train loss: 0.47587814927101135\n",
      "Epoch 7927: train loss: 0.4758780002593994\n",
      "Epoch 7928: train loss: 0.4758778512477875\n",
      "Epoch 7929: train loss: 0.47587767243385315\n",
      "Epoch 7930: train loss: 0.47587764263153076\n",
      "Epoch 7931: train loss: 0.47587743401527405\n",
      "Epoch 7932: train loss: 0.4758773148059845\n",
      "Epoch 7933: train loss: 0.47587722539901733\n",
      "Epoch 7934: train loss: 0.475877046585083\n",
      "Epoch 7935: train loss: 0.47587689757347107\n",
      "Epoch 7936: train loss: 0.4758768081665039\n",
      "Epoch 7937: train loss: 0.4758766293525696\n",
      "Epoch 7938: train loss: 0.4758765399456024\n",
      "Epoch 7939: train loss: 0.4758763313293457\n",
      "Epoch 7940: train loss: 0.47587621212005615\n",
      "Epoch 7941: train loss: 0.4758760631084442\n",
      "Epoch 7942: train loss: 0.47587597370147705\n",
      "Epoch 7943: train loss: 0.4758757948875427\n",
      "Epoch 7944: train loss: 0.4758756458759308\n",
      "Epoch 7945: train loss: 0.47587552666664124\n",
      "Epoch 7946: train loss: 0.4758754372596741\n",
      "Epoch 7947: train loss: 0.47587528824806213\n",
      "Epoch 7948: train loss: 0.4758751690387726\n",
      "Epoch 7949: train loss: 0.47587496042251587\n",
      "Epoch 7950: train loss: 0.4758748710155487\n",
      "Epoch 7951: train loss: 0.4758746922016144\n",
      "Epoch 7952: train loss: 0.4758746027946472\n",
      "Epoch 7953: train loss: 0.47587448358535767\n",
      "Epoch 7954: train loss: 0.4758743345737457\n",
      "Epoch 7955: train loss: 0.4758741855621338\n",
      "Epoch 7956: train loss: 0.47587406635284424\n",
      "Epoch 7957: train loss: 0.4758738577365875\n",
      "Epoch 7958: train loss: 0.47587376832962036\n",
      "Epoch 7959: train loss: 0.4758736193180084\n",
      "Epoch 7960: train loss: 0.47587350010871887\n",
      "Epoch 7961: train loss: 0.47587335109710693\n",
      "Epoch 7962: train loss: 0.4758732318878174\n",
      "Epoch 7963: train loss: 0.47587308287620544\n",
      "Epoch 7964: train loss: 0.4758729934692383\n",
      "Epoch 7965: train loss: 0.47587281465530396\n",
      "Epoch 7966: train loss: 0.475872665643692\n",
      "Epoch 7967: train loss: 0.47587257623672485\n",
      "Epoch 7968: train loss: 0.4758723974227905\n",
      "Epoch 7969: train loss: 0.47587230801582336\n",
      "Epoch 7970: train loss: 0.47587209939956665\n",
      "Epoch 7971: train loss: 0.4758719801902771\n",
      "Epoch 7972: train loss: 0.47587189078330994\n",
      "Epoch 7973: train loss: 0.4758717715740204\n",
      "Epoch 7974: train loss: 0.47587162256240845\n",
      "Epoch 7975: train loss: 0.4758714735507965\n",
      "Epoch 7976: train loss: 0.47587135434150696\n",
      "Epoch 7977: train loss: 0.475871205329895\n",
      "Epoch 7978: train loss: 0.4758710563182831\n",
      "Epoch 7979: train loss: 0.47587087750434875\n",
      "Epoch 7980: train loss: 0.4758707880973816\n",
      "Epoch 7981: train loss: 0.47587063908576965\n",
      "Epoch 7982: train loss: 0.4758705198764801\n",
      "Epoch 7983: train loss: 0.47587037086486816\n",
      "Epoch 7984: train loss: 0.4758702516555786\n",
      "Epoch 7985: train loss: 0.4758701026439667\n",
      "Epoch 7986: train loss: 0.47586995363235474\n",
      "Epoch 7987: train loss: 0.47586989402770996\n",
      "Epoch 7988: train loss: 0.47586968541145325\n",
      "Epoch 7989: train loss: 0.4758695363998413\n",
      "Epoch 7990: train loss: 0.47586941719055176\n",
      "Epoch 7991: train loss: 0.4758692681789398\n",
      "Epoch 7992: train loss: 0.47586917877197266\n",
      "Epoch 7993: train loss: 0.4758690595626831\n",
      "Epoch 7994: train loss: 0.47586891055107117\n",
      "Epoch 7995: train loss: 0.4758687913417816\n",
      "Epoch 7996: train loss: 0.4758686423301697\n",
      "Epoch 7997: train loss: 0.47586849331855774\n",
      "Epoch 7998: train loss: 0.4758683741092682\n",
      "Epoch 7999: train loss: 0.47586822509765625\n",
      "Epoch 8000: train loss: 0.4758681058883667\n",
      "Epoch 8001: train loss: 0.47586795687675476\n",
      "Epoch 8002: train loss: 0.4758678078651428\n",
      "Epoch 8003: train loss: 0.47586768865585327\n",
      "Epoch 8004: train loss: 0.47586753964424133\n",
      "Epoch 8005: train loss: 0.4758673906326294\n",
      "Epoch 8006: train loss: 0.47586727142333984\n",
      "Epoch 8007: train loss: 0.4758671224117279\n",
      "Epoch 8008: train loss: 0.47586703300476074\n",
      "Epoch 8009: train loss: 0.4758668541908264\n",
      "Epoch 8010: train loss: 0.4758667051792145\n",
      "Epoch 8011: train loss: 0.4758666157722473\n",
      "Epoch 8012: train loss: 0.475866436958313\n",
      "Epoch 8013: train loss: 0.47586628794670105\n",
      "Epoch 8014: train loss: 0.4758662283420563\n",
      "Epoch 8015: train loss: 0.47586607933044434\n",
      "Epoch 8016: train loss: 0.4758659303188324\n",
      "Epoch 8017: train loss: 0.4758657217025757\n",
      "Epoch 8018: train loss: 0.47586560249328613\n",
      "Epoch 8019: train loss: 0.47586557269096375\n",
      "Epoch 8020: train loss: 0.4758653938770294\n",
      "Epoch 8021: train loss: 0.4758652448654175\n",
      "Epoch 8022: train loss: 0.4758651554584503\n",
      "Epoch 8023: train loss: 0.475864976644516\n",
      "Epoch 8024: train loss: 0.47586488723754883\n",
      "Epoch 8025: train loss: 0.4758646786212921\n",
      "Epoch 8026: train loss: 0.47586455941200256\n",
      "Epoch 8027: train loss: 0.4758644700050354\n",
      "Epoch 8028: train loss: 0.4758642911911011\n",
      "Epoch 8029: train loss: 0.4758642017841339\n",
      "Epoch 8030: train loss: 0.475864052772522\n",
      "Epoch 8031: train loss: 0.4758639335632324\n",
      "Epoch 8032: train loss: 0.4758637845516205\n",
      "Epoch 8033: train loss: 0.47586363554000854\n",
      "Epoch 8034: train loss: 0.475863516330719\n",
      "Epoch 8035: train loss: 0.47586342692375183\n",
      "Epoch 8036: train loss: 0.4758632481098175\n",
      "Epoch 8037: train loss: 0.47586309909820557\n",
      "Epoch 8038: train loss: 0.47586295008659363\n",
      "Epoch 8039: train loss: 0.4758628010749817\n",
      "Epoch 8040: train loss: 0.47586268186569214\n",
      "Epoch 8041: train loss: 0.475862592458725\n",
      "Epoch 8042: train loss: 0.47586238384246826\n",
      "Epoch 8043: train loss: 0.4758623242378235\n",
      "Epoch 8044: train loss: 0.47586217522621155\n",
      "Epoch 8045: train loss: 0.475862056016922\n",
      "Epoch 8046: train loss: 0.47586190700531006\n",
      "Epoch 8047: train loss: 0.4758617579936981\n",
      "Epoch 8048: train loss: 0.47586163878440857\n",
      "Epoch 8049: train loss: 0.47586148977279663\n",
      "Epoch 8050: train loss: 0.4758613407611847\n",
      "Epoch 8051: train loss: 0.4758612811565399\n",
      "Epoch 8052: train loss: 0.475861132144928\n",
      "Epoch 8053: train loss: 0.47586095333099365\n",
      "Epoch 8054: train loss: 0.4758608043193817\n",
      "Epoch 8055: train loss: 0.47586071491241455\n",
      "Epoch 8056: train loss: 0.475860595703125\n",
      "Epoch 8057: train loss: 0.47586044669151306\n",
      "Epoch 8058: train loss: 0.47586026787757874\n",
      "Epoch 8059: train loss: 0.4758601784706116\n",
      "Epoch 8060: train loss: 0.47586002945899963\n",
      "Epoch 8061: train loss: 0.4758599102497101\n",
      "Epoch 8062: train loss: 0.47585976123809814\n",
      "Epoch 8063: train loss: 0.4758596420288086\n",
      "Epoch 8064: train loss: 0.47585949301719666\n",
      "Epoch 8065: train loss: 0.4758593440055847\n",
      "Epoch 8066: train loss: 0.47585922479629517\n",
      "Epoch 8067: train loss: 0.4758590757846832\n",
      "Epoch 8068: train loss: 0.4758589267730713\n",
      "Epoch 8069: train loss: 0.4758588671684265\n",
      "Epoch 8070: train loss: 0.4758587181568146\n",
      "Epoch 8071: train loss: 0.47585856914520264\n",
      "Epoch 8072: train loss: 0.4758583903312683\n",
      "Epoch 8073: train loss: 0.47585830092430115\n",
      "Epoch 8074: train loss: 0.4758581519126892\n",
      "Epoch 8075: train loss: 0.47585809230804443\n",
      "Epoch 8076: train loss: 0.4758578836917877\n",
      "Epoch 8077: train loss: 0.47585776448249817\n",
      "Epoch 8078: train loss: 0.47585761547088623\n",
      "Epoch 8079: train loss: 0.4758574664592743\n",
      "Epoch 8080: train loss: 0.47585734724998474\n",
      "Epoch 8081: train loss: 0.4758572578430176\n",
      "Epoch 8082: train loss: 0.47585710883140564\n",
      "Epoch 8083: train loss: 0.4758569896221161\n",
      "Epoch 8084: train loss: 0.47585684061050415\n",
      "Epoch 8085: train loss: 0.4758566617965698\n",
      "Epoch 8086: train loss: 0.47585657238960266\n",
      "Epoch 8087: train loss: 0.4758564233779907\n",
      "Epoch 8088: train loss: 0.47585630416870117\n",
      "Epoch 8089: train loss: 0.475856214761734\n",
      "Epoch 8090: train loss: 0.4758560061454773\n",
      "Epoch 8091: train loss: 0.4758559465408325\n",
      "Epoch 8092: train loss: 0.4758557975292206\n",
      "Epoch 8093: train loss: 0.47585567831993103\n",
      "Epoch 8094: train loss: 0.4758555293083191\n",
      "Epoch 8095: train loss: 0.47585538029670715\n",
      "Epoch 8096: train loss: 0.4758552610874176\n",
      "Epoch 8097: train loss: 0.47585511207580566\n",
      "Epoch 8098: train loss: 0.4758549630641937\n",
      "Epoch 8099: train loss: 0.4758548438549042\n",
      "Epoch 8100: train loss: 0.475854754447937\n",
      "Epoch 8101: train loss: 0.4758545756340027\n",
      "Epoch 8102: train loss: 0.47585442662239075\n",
      "Epoch 8103: train loss: 0.47585436701774597\n",
      "Epoch 8104: train loss: 0.47585421800613403\n",
      "Epoch 8105: train loss: 0.4758540689945221\n",
      "Epoch 8106: train loss: 0.47585394978523254\n",
      "Epoch 8107: train loss: 0.4758538007736206\n",
      "Epoch 8108: train loss: 0.47585365176200867\n",
      "Epoch 8109: train loss: 0.4758535325527191\n",
      "Epoch 8110: train loss: 0.4758533835411072\n",
      "Epoch 8111: train loss: 0.47585329413414\n",
      "Epoch 8112: train loss: 0.4758531153202057\n",
      "Epoch 8113: train loss: 0.47585296630859375\n",
      "Epoch 8114: train loss: 0.4758528172969818\n",
      "Epoch 8115: train loss: 0.47585275769233704\n",
      "Epoch 8116: train loss: 0.4758526086807251\n",
      "Epoch 8117: train loss: 0.47585242986679077\n",
      "Epoch 8118: train loss: 0.4758523404598236\n",
      "Epoch 8119: train loss: 0.47585219144821167\n",
      "Epoch 8120: train loss: 0.47585201263427734\n",
      "Epoch 8121: train loss: 0.4758519232273102\n",
      "Epoch 8122: train loss: 0.47585180401802063\n",
      "Epoch 8123: train loss: 0.47585171461105347\n",
      "Epoch 8124: train loss: 0.47585156559944153\n",
      "Epoch 8125: train loss: 0.4758513867855072\n",
      "Epoch 8126: train loss: 0.47585123777389526\n",
      "Epoch 8127: train loss: 0.4758511781692505\n",
      "Epoch 8128: train loss: 0.4758509695529938\n",
      "Epoch 8129: train loss: 0.4758508801460266\n",
      "Epoch 8130: train loss: 0.4758507311344147\n",
      "Epoch 8131: train loss: 0.47585055232048035\n",
      "Epoch 8132: train loss: 0.4758504629135132\n",
      "Epoch 8133: train loss: 0.47585034370422363\n",
      "Epoch 8134: train loss: 0.47585025429725647\n",
      "Epoch 8135: train loss: 0.47585010528564453\n",
      "Epoch 8136: train loss: 0.4758499264717102\n",
      "Epoch 8137: train loss: 0.47584983706474304\n",
      "Epoch 8138: train loss: 0.4758497178554535\n",
      "Epoch 8139: train loss: 0.47584962844848633\n",
      "Epoch 8140: train loss: 0.4758494198322296\n",
      "Epoch 8141: train loss: 0.47584930062294006\n",
      "Epoch 8142: train loss: 0.4758491516113281\n",
      "Epoch 8143: train loss: 0.4758490324020386\n",
      "Epoch 8144: train loss: 0.47584888339042664\n",
      "Epoch 8145: train loss: 0.4758487343788147\n",
      "Epoch 8146: train loss: 0.47584861516952515\n",
      "Epoch 8147: train loss: 0.4758484661579132\n",
      "Epoch 8148: train loss: 0.47584837675094604\n",
      "Epoch 8149: train loss: 0.4758481979370117\n",
      "Epoch 8150: train loss: 0.47584810853004456\n",
      "Epoch 8151: train loss: 0.475847989320755\n",
      "Epoch 8152: train loss: 0.47584784030914307\n",
      "Epoch 8153: train loss: 0.4758477509021759\n",
      "Epoch 8154: train loss: 0.4758475720882416\n",
      "Epoch 8155: train loss: 0.4758474826812744\n",
      "Epoch 8156: train loss: 0.4758473336696625\n",
      "Epoch 8157: train loss: 0.4758472144603729\n",
      "Epoch 8158: train loss: 0.475847065448761\n",
      "Epoch 8159: train loss: 0.47584694623947144\n",
      "Epoch 8160: train loss: 0.4758467972278595\n",
      "Epoch 8161: train loss: 0.47584670782089233\n",
      "Epoch 8162: train loss: 0.4758464992046356\n",
      "Epoch 8163: train loss: 0.47584643959999084\n",
      "Epoch 8164: train loss: 0.4758462905883789\n",
      "Epoch 8165: train loss: 0.47584617137908936\n",
      "Epoch 8166: train loss: 0.4758460819721222\n",
      "Epoch 8167: train loss: 0.4758458733558655\n",
      "Epoch 8168: train loss: 0.4758457541465759\n",
      "Epoch 8169: train loss: 0.47584566473960876\n",
      "Epoch 8170: train loss: 0.4758455455303192\n",
      "Epoch 8171: train loss: 0.4758453965187073\n",
      "Epoch 8172: train loss: 0.47584521770477295\n",
      "Epoch 8173: train loss: 0.475845068693161\n",
      "Epoch 8174: train loss: 0.47584497928619385\n",
      "Epoch 8175: train loss: 0.4758448004722595\n",
      "Epoch 8176: train loss: 0.4758446514606476\n",
      "Epoch 8177: train loss: 0.4758445918560028\n",
      "Epoch 8178: train loss: 0.47584444284439087\n",
      "Epoch 8179: train loss: 0.47584429383277893\n",
      "Epoch 8180: train loss: 0.4758441746234894\n",
      "Epoch 8181: train loss: 0.4758440852165222\n",
      "Epoch 8182: train loss: 0.4758439362049103\n",
      "Epoch 8183: train loss: 0.47584375739097595\n",
      "Epoch 8184: train loss: 0.4758436679840088\n",
      "Epoch 8185: train loss: 0.47584354877471924\n",
      "Epoch 8186: train loss: 0.4758433401584625\n",
      "Epoch 8187: train loss: 0.47584325075149536\n",
      "Epoch 8188: train loss: 0.4758431315422058\n",
      "Epoch 8189: train loss: 0.47584298253059387\n",
      "Epoch 8190: train loss: 0.4758428931236267\n",
      "Epoch 8191: train loss: 0.4758427143096924\n",
      "Epoch 8192: train loss: 0.4758426249027252\n",
      "Epoch 8193: train loss: 0.47584250569343567\n",
      "Epoch 8194: train loss: 0.4758424162864685\n",
      "Epoch 8195: train loss: 0.47584226727485657\n",
      "Epoch 8196: train loss: 0.475842148065567\n",
      "Epoch 8197: train loss: 0.4758419394493103\n",
      "Epoch 8198: train loss: 0.4758418798446655\n",
      "Epoch 8199: train loss: 0.4758417308330536\n",
      "Epoch 8200: train loss: 0.4758416414260864\n",
      "Epoch 8201: train loss: 0.4758414626121521\n",
      "Epoch 8202: train loss: 0.47584131360054016\n",
      "Epoch 8203: train loss: 0.4758411645889282\n",
      "Epoch 8204: train loss: 0.47584110498428345\n",
      "Epoch 8205: train loss: 0.4758409559726715\n",
      "Epoch 8206: train loss: 0.47584083676338196\n",
      "Epoch 8207: train loss: 0.47584068775177\n",
      "Epoch 8208: train loss: 0.4758405387401581\n",
      "Epoch 8209: train loss: 0.47584041953086853\n",
      "Epoch 8210: train loss: 0.4758402705192566\n",
      "Epoch 8211: train loss: 0.47584015130996704\n",
      "Epoch 8212: train loss: 0.4758400022983551\n",
      "Epoch 8213: train loss: 0.47583985328674316\n",
      "Epoch 8214: train loss: 0.4758397936820984\n",
      "Epoch 8215: train loss: 0.4758395850658417\n",
      "Epoch 8216: train loss: 0.4758394956588745\n",
      "Epoch 8217: train loss: 0.4758393168449402\n",
      "Epoch 8218: train loss: 0.475839227437973\n",
      "Epoch 8219: train loss: 0.47583916783332825\n",
      "Epoch 8220: train loss: 0.47583895921707153\n",
      "Epoch 8221: train loss: 0.47583886981010437\n",
      "Epoch 8222: train loss: 0.47583869099617004\n",
      "Epoch 8223: train loss: 0.4758386015892029\n",
      "Epoch 8224: train loss: 0.47583845257759094\n",
      "Epoch 8225: train loss: 0.4758383333683014\n",
      "Epoch 8226: train loss: 0.47583818435668945\n",
      "Epoch 8227: train loss: 0.4758380651473999\n",
      "Epoch 8228: train loss: 0.47583791613578796\n",
      "Epoch 8229: train loss: 0.4758378267288208\n",
      "Epoch 8230: train loss: 0.47583770751953125\n",
      "Epoch 8231: train loss: 0.4758375585079193\n",
      "Epoch 8232: train loss: 0.47583743929862976\n",
      "Epoch 8233: train loss: 0.4758372902870178\n",
      "Epoch 8234: train loss: 0.47583717107772827\n",
      "Epoch 8235: train loss: 0.47583696246147156\n",
      "Epoch 8236: train loss: 0.47583693265914917\n",
      "Epoch 8237: train loss: 0.47583675384521484\n",
      "Epoch 8238: train loss: 0.4758366644382477\n",
      "Epoch 8239: train loss: 0.47583645582199097\n",
      "Epoch 8240: train loss: 0.4758363366127014\n",
      "Epoch 8241: train loss: 0.47583624720573425\n",
      "Epoch 8242: train loss: 0.4758360981941223\n",
      "Epoch 8243: train loss: 0.47583597898483276\n",
      "Epoch 8244: train loss: 0.4758358895778656\n",
      "Epoch 8245: train loss: 0.4758357107639313\n",
      "Epoch 8246: train loss: 0.4758356213569641\n",
      "Epoch 8247: train loss: 0.47583550214767456\n",
      "Epoch 8248: train loss: 0.4758353531360626\n",
      "Epoch 8249: train loss: 0.4758352041244507\n",
      "Epoch 8250: train loss: 0.4758351445198059\n",
      "Epoch 8251: train loss: 0.47583499550819397\n",
      "Epoch 8252: train loss: 0.47583484649658203\n",
      "Epoch 8253: train loss: 0.4758346676826477\n",
      "Epoch 8254: train loss: 0.4758346378803253\n",
      "Epoch 8255: train loss: 0.475834459066391\n",
      "Epoch 8256: train loss: 0.4758342504501343\n",
      "Epoch 8257: train loss: 0.4758342206478119\n",
      "Epoch 8258: train loss: 0.47583404183387756\n",
      "Epoch 8259: train loss: 0.4758338928222656\n",
      "Epoch 8260: train loss: 0.47583383321762085\n",
      "Epoch 8261: train loss: 0.4758337438106537\n",
      "Epoch 8262: train loss: 0.47583356499671936\n",
      "Epoch 8263: train loss: 0.4758334159851074\n",
      "Epoch 8264: train loss: 0.47583332657814026\n",
      "Epoch 8265: train loss: 0.47583314776420593\n",
      "Epoch 8266: train loss: 0.475832998752594\n",
      "Epoch 8267: train loss: 0.47583290934562683\n",
      "Epoch 8268: train loss: 0.4758327901363373\n",
      "Epoch 8269: train loss: 0.47583264112472534\n",
      "Epoch 8270: train loss: 0.4758325219154358\n",
      "Epoch 8271: train loss: 0.47583237290382385\n",
      "Epoch 8272: train loss: 0.4758322834968567\n",
      "Epoch 8273: train loss: 0.47583216428756714\n",
      "Epoch 8274: train loss: 0.4758320152759552\n",
      "Epoch 8275: train loss: 0.47583189606666565\n",
      "Epoch 8276: train loss: 0.4758317470550537\n",
      "Epoch 8277: train loss: 0.4758315980434418\n",
      "Epoch 8278: train loss: 0.4758314788341522\n",
      "Epoch 8279: train loss: 0.4758313298225403\n",
      "Epoch 8280: train loss: 0.4758312702178955\n",
      "Epoch 8281: train loss: 0.47583112120628357\n",
      "Epoch 8282: train loss: 0.47583097219467163\n",
      "Epoch 8283: train loss: 0.4758308529853821\n",
      "Epoch 8284: train loss: 0.47583070397377014\n",
      "Epoch 8285: train loss: 0.475830614566803\n",
      "Epoch 8286: train loss: 0.4758304953575134\n",
      "Epoch 8287: train loss: 0.4758303463459015\n",
      "Epoch 8288: train loss: 0.47583019733428955\n",
      "Epoch 8289: train loss: 0.475830078125\n",
      "Epoch 8290: train loss: 0.47582995891571045\n",
      "Epoch 8291: train loss: 0.4758298099040985\n",
      "Epoch 8292: train loss: 0.47582972049713135\n",
      "Epoch 8293: train loss: 0.475829541683197\n",
      "Epoch 8294: train loss: 0.47582945227622986\n",
      "Epoch 8295: train loss: 0.4758293032646179\n",
      "Epoch 8296: train loss: 0.47582918405532837\n",
      "Epoch 8297: train loss: 0.47582903504371643\n",
      "Epoch 8298: train loss: 0.4758289158344269\n",
      "Epoch 8299: train loss: 0.47582876682281494\n",
      "Epoch 8300: train loss: 0.4758286774158478\n",
      "Epoch 8301: train loss: 0.47582849860191345\n",
      "Epoch 8302: train loss: 0.4758284091949463\n",
      "Epoch 8303: train loss: 0.47582826018333435\n",
      "Epoch 8304: train loss: 0.4758282005786896\n",
      "Epoch 8305: train loss: 0.47582805156707764\n",
      "Epoch 8306: train loss: 0.4758278727531433\n",
      "Epoch 8307: train loss: 0.47582778334617615\n",
      "Epoch 8308: train loss: 0.4758276343345642\n",
      "Epoch 8309: train loss: 0.47582757472991943\n",
      "Epoch 8310: train loss: 0.4758274257183075\n",
      "Epoch 8311: train loss: 0.47582724690437317\n",
      "Epoch 8312: train loss: 0.475827157497406\n",
      "Epoch 8313: train loss: 0.47582703828811646\n",
      "Epoch 8314: train loss: 0.47582682967185974\n",
      "Epoch 8315: train loss: 0.4758267402648926\n",
      "Epoch 8316: train loss: 0.475826621055603\n",
      "Epoch 8317: train loss: 0.47582653164863586\n",
      "Epoch 8318: train loss: 0.4758263826370239\n",
      "Epoch 8319: train loss: 0.4758262634277344\n",
      "Epoch 8320: train loss: 0.4758261442184448\n",
      "Epoch 8321: train loss: 0.4758259952068329\n",
      "Epoch 8322: train loss: 0.47582584619522095\n",
      "Epoch 8323: train loss: 0.4758257269859314\n",
      "Epoch 8324: train loss: 0.475825697183609\n",
      "Epoch 8325: train loss: 0.4758254885673523\n",
      "Epoch 8326: train loss: 0.47582536935806274\n",
      "Epoch 8327: train loss: 0.4758252799510956\n",
      "Epoch 8328: train loss: 0.47582510113716125\n",
      "Epoch 8329: train loss: 0.4758249521255493\n",
      "Epoch 8330: train loss: 0.47582486271858215\n",
      "Epoch 8331: train loss: 0.4758247435092926\n",
      "Epoch 8332: train loss: 0.47582459449768066\n",
      "Epoch 8333: train loss: 0.4758244454860687\n",
      "Epoch 8334: train loss: 0.4758243262767792\n",
      "Epoch 8335: train loss: 0.475824236869812\n",
      "Epoch 8336: train loss: 0.47582411766052246\n",
      "Epoch 8337: train loss: 0.4758239686489105\n",
      "Epoch 8338: train loss: 0.47582384943962097\n",
      "Epoch 8339: train loss: 0.47582370042800903\n",
      "Epoch 8340: train loss: 0.4758235514163971\n",
      "Epoch 8341: train loss: 0.47582343220710754\n",
      "Epoch 8342: train loss: 0.4758233428001404\n",
      "Epoch 8343: train loss: 0.47582322359085083\n",
      "Epoch 8344: train loss: 0.4758230745792389\n",
      "Epoch 8345: train loss: 0.47582292556762695\n",
      "Epoch 8346: train loss: 0.4758228063583374\n",
      "Epoch 8347: train loss: 0.47582265734672546\n",
      "Epoch 8348: train loss: 0.4758225977420807\n",
      "Epoch 8349: train loss: 0.47582244873046875\n",
      "Epoch 8350: train loss: 0.4758223295211792\n",
      "Epoch 8351: train loss: 0.47582218050956726\n",
      "Epoch 8352: train loss: 0.4758220314979553\n",
      "Epoch 8353: train loss: 0.47582191228866577\n",
      "Epoch 8354: train loss: 0.47582176327705383\n",
      "Epoch 8355: train loss: 0.47582167387008667\n",
      "Epoch 8356: train loss: 0.47582149505615234\n",
      "Epoch 8357: train loss: 0.47582146525382996\n",
      "Epoch 8358: train loss: 0.47582128643989563\n",
      "Epoch 8359: train loss: 0.47582119703292847\n",
      "Epoch 8360: train loss: 0.4758210778236389\n",
      "Epoch 8361: train loss: 0.475820928812027\n",
      "Epoch 8362: train loss: 0.47582077980041504\n",
      "Epoch 8363: train loss: 0.4758206307888031\n",
      "Epoch 8364: train loss: 0.4758205711841583\n",
      "Epoch 8365: train loss: 0.4758204221725464\n",
      "Epoch 8366: train loss: 0.4758203625679016\n",
      "Epoch 8367: train loss: 0.4758202135562897\n",
      "Epoch 8368: train loss: 0.47582000494003296\n",
      "Epoch 8369: train loss: 0.4758199453353882\n",
      "Epoch 8370: train loss: 0.47581979632377625\n",
      "Epoch 8371: train loss: 0.4758196771144867\n",
      "Epoch 8372: train loss: 0.47581952810287476\n",
      "Epoch 8373: train loss: 0.4758194088935852\n",
      "Epoch 8374: train loss: 0.47581925988197327\n",
      "Epoch 8375: train loss: 0.47581911087036133\n",
      "Epoch 8376: train loss: 0.4758189916610718\n",
      "Epoch 8377: train loss: 0.4758189022541046\n",
      "Epoch 8378: train loss: 0.47581878304481506\n",
      "Epoch 8379: train loss: 0.4758186340332031\n",
      "Epoch 8380: train loss: 0.4758185148239136\n",
      "Epoch 8381: train loss: 0.47581836581230164\n",
      "Epoch 8382: train loss: 0.4758182764053345\n",
      "Epoch 8383: train loss: 0.47581809759140015\n",
      "Epoch 8384: train loss: 0.475818008184433\n",
      "Epoch 8385: train loss: 0.47581788897514343\n",
      "Epoch 8386: train loss: 0.47581779956817627\n",
      "Epoch 8387: train loss: 0.47581765055656433\n",
      "Epoch 8388: train loss: 0.4758175313472748\n",
      "Epoch 8389: train loss: 0.47581738233566284\n",
      "Epoch 8390: train loss: 0.4758172631263733\n",
      "Epoch 8391: train loss: 0.47581711411476135\n",
      "Epoch 8392: train loss: 0.4758170247077942\n",
      "Epoch 8393: train loss: 0.47581690549850464\n",
      "Epoch 8394: train loss: 0.4758168160915375\n",
      "Epoch 8395: train loss: 0.47581660747528076\n",
      "Epoch 8396: train loss: 0.475816547870636\n",
      "Epoch 8397: train loss: 0.47581639885902405\n",
      "Epoch 8398: train loss: 0.4758162200450897\n",
      "Epoch 8399: train loss: 0.47581613063812256\n",
      "Epoch 8400: train loss: 0.475816011428833\n",
      "Epoch 8401: train loss: 0.47581586241722107\n",
      "Epoch 8402: train loss: 0.4758157730102539\n",
      "Epoch 8403: train loss: 0.4758155941963196\n",
      "Epoch 8404: train loss: 0.4758155643939972\n",
      "Epoch 8405: train loss: 0.47581538558006287\n",
      "Epoch 8406: train loss: 0.4758152961730957\n",
      "Epoch 8407: train loss: 0.47581514716148376\n",
      "Epoch 8408: train loss: 0.475815087556839\n",
      "Epoch 8409: train loss: 0.47581493854522705\n",
      "Epoch 8410: train loss: 0.4758147597312927\n",
      "Epoch 8411: train loss: 0.4758146107196808\n",
      "Epoch 8412: train loss: 0.47581449151039124\n",
      "Epoch 8413: train loss: 0.4758144021034241\n",
      "Epoch 8414: train loss: 0.47581425309181213\n",
      "Epoch 8415: train loss: 0.4758140742778778\n",
      "Epoch 8416: train loss: 0.47581398487091064\n",
      "Epoch 8417: train loss: 0.4758138656616211\n",
      "Epoch 8418: train loss: 0.47581371665000916\n",
      "Epoch 8419: train loss: 0.4758135676383972\n",
      "Epoch 8420: train loss: 0.47581350803375244\n",
      "Epoch 8421: train loss: 0.4758133590221405\n",
      "Epoch 8422: train loss: 0.47581323981285095\n",
      "Epoch 8423: train loss: 0.4758131504058838\n",
      "Epoch 8424: train loss: 0.47581300139427185\n",
      "Epoch 8425: train loss: 0.4758128821849823\n",
      "Epoch 8426: train loss: 0.47581273317337036\n",
      "Epoch 8427: train loss: 0.4758126139640808\n",
      "Epoch 8428: train loss: 0.47581252455711365\n",
      "Epoch 8429: train loss: 0.4758124053478241\n",
      "Epoch 8430: train loss: 0.47581231594085693\n",
      "Epoch 8431: train loss: 0.4758121967315674\n",
      "Epoch 8432: train loss: 0.47581198811531067\n",
      "Epoch 8433: train loss: 0.4758118987083435\n",
      "Epoch 8434: train loss: 0.47581174969673157\n",
      "Epoch 8435: train loss: 0.47581157088279724\n",
      "Epoch 8436: train loss: 0.4758114814758301\n",
      "Epoch 8437: train loss: 0.4758113622665405\n",
      "Epoch 8438: train loss: 0.47581127285957336\n",
      "Epoch 8439: train loss: 0.4758111238479614\n",
      "Epoch 8440: train loss: 0.4758110046386719\n",
      "Epoch 8441: train loss: 0.4758109450340271\n",
      "Epoch 8442: train loss: 0.4758107364177704\n",
      "Epoch 8443: train loss: 0.4758106470108032\n",
      "Epoch 8444: train loss: 0.47581052780151367\n",
      "Epoch 8445: train loss: 0.47581037878990173\n",
      "Epoch 8446: train loss: 0.4758102297782898\n",
      "Epoch 8447: train loss: 0.47581011056900024\n",
      "Epoch 8448: train loss: 0.4758100211620331\n",
      "Epoch 8449: train loss: 0.47580990195274353\n",
      "Epoch 8450: train loss: 0.4758097529411316\n",
      "Epoch 8451: train loss: 0.47580963373184204\n",
      "Epoch 8452: train loss: 0.4758094847202301\n",
      "Epoch 8453: train loss: 0.47580939531326294\n",
      "Epoch 8454: train loss: 0.4758092164993286\n",
      "Epoch 8455: train loss: 0.4758091866970062\n",
      "Epoch 8456: train loss: 0.4758090078830719\n",
      "Epoch 8457: train loss: 0.47580885887145996\n",
      "Epoch 8458: train loss: 0.4758087694644928\n",
      "Epoch 8459: train loss: 0.47580865025520325\n",
      "Epoch 8460: train loss: 0.4758085012435913\n",
      "Epoch 8461: train loss: 0.47580838203430176\n",
      "Epoch 8462: train loss: 0.4758082330226898\n",
      "Epoch 8463: train loss: 0.47580814361572266\n",
      "Epoch 8464: train loss: 0.4758080244064331\n",
      "Epoch 8465: train loss: 0.47580787539482117\n",
      "Epoch 8466: train loss: 0.4758077561855316\n",
      "Epoch 8467: train loss: 0.4758076071739197\n",
      "Epoch 8468: train loss: 0.4758075177669525\n",
      "Epoch 8469: train loss: 0.47580739855766296\n",
      "Epoch 8470: train loss: 0.4758073091506958\n",
      "Epoch 8471: train loss: 0.4758071303367615\n",
      "Epoch 8472: train loss: 0.4758070409297943\n",
      "Epoch 8473: train loss: 0.47580686211586\n",
      "Epoch 8474: train loss: 0.4758068323135376\n",
      "Epoch 8475: train loss: 0.47580665349960327\n",
      "Epoch 8476: train loss: 0.47580650448799133\n",
      "Epoch 8477: train loss: 0.47580641508102417\n",
      "Epoch 8478: train loss: 0.47580623626708984\n",
      "Epoch 8479: train loss: 0.4758061468601227\n",
      "Epoch 8480: train loss: 0.47580602765083313\n",
      "Epoch 8481: train loss: 0.4758058786392212\n",
      "Epoch 8482: train loss: 0.47580578923225403\n",
      "Epoch 8483: train loss: 0.4758056700229645\n",
      "Epoch 8484: train loss: 0.4758055806159973\n",
      "Epoch 8485: train loss: 0.475805401802063\n",
      "Epoch 8486: train loss: 0.4758053123950958\n",
      "Epoch 8487: train loss: 0.4758051931858063\n",
      "Epoch 8488: train loss: 0.47580504417419434\n",
      "Epoch 8489: train loss: 0.4758048951625824\n",
      "Epoch 8490: train loss: 0.47580477595329285\n",
      "Epoch 8491: train loss: 0.4758046865463257\n",
      "Epoch 8492: train loss: 0.47580456733703613\n",
      "Epoch 8493: train loss: 0.47580447793006897\n",
      "Epoch 8494: train loss: 0.47580432891845703\n",
      "Epoch 8495: train loss: 0.4758041501045227\n",
      "Epoch 8496: train loss: 0.47580406069755554\n",
      "Epoch 8497: train loss: 0.475803941488266\n",
      "Epoch 8498: train loss: 0.47580379247665405\n",
      "Epoch 8499: train loss: 0.4758036434650421\n",
      "Epoch 8500: train loss: 0.47580352425575256\n",
      "Epoch 8501: train loss: 0.4758034348487854\n",
      "Epoch 8502: train loss: 0.4758032560348511\n",
      "Epoch 8503: train loss: 0.4758032262325287\n",
      "Epoch 8504: train loss: 0.47580304741859436\n",
      "Epoch 8505: train loss: 0.4758029580116272\n",
      "Epoch 8506: train loss: 0.47580280900001526\n",
      "Epoch 8507: train loss: 0.4758026897907257\n",
      "Epoch 8508: train loss: 0.47580260038375854\n",
      "Epoch 8509: train loss: 0.4758024215698242\n",
      "Epoch 8510: train loss: 0.47580233216285706\n",
      "Epoch 8511: train loss: 0.4758021831512451\n",
      "Epoch 8512: train loss: 0.47580206394195557\n",
      "Epoch 8513: train loss: 0.4758019745349884\n",
      "Epoch 8514: train loss: 0.47580185532569885\n",
      "Epoch 8515: train loss: 0.4758017659187317\n",
      "Epoch 8516: train loss: 0.47580158710479736\n",
      "Epoch 8517: train loss: 0.4758014380931854\n",
      "Epoch 8518: train loss: 0.47580134868621826\n",
      "Epoch 8519: train loss: 0.4758012294769287\n",
      "Epoch 8520: train loss: 0.4758010804653168\n",
      "Epoch 8521: train loss: 0.4758009612560272\n",
      "Epoch 8522: train loss: 0.4758008122444153\n",
      "Epoch 8523: train loss: 0.4758007228374481\n",
      "Epoch 8524: train loss: 0.47580060362815857\n",
      "Epoch 8525: train loss: 0.4758005142211914\n",
      "Epoch 8526: train loss: 0.4758003354072571\n",
      "Epoch 8527: train loss: 0.4758002460002899\n",
      "Epoch 8528: train loss: 0.47580012679100037\n",
      "Epoch 8529: train loss: 0.4757999777793884\n",
      "Epoch 8530: train loss: 0.47579988837242126\n",
      "Epoch 8531: train loss: 0.47579970955848694\n",
      "Epoch 8532: train loss: 0.4757996201515198\n",
      "Epoch 8533: train loss: 0.475799560546875\n",
      "Epoch 8534: train loss: 0.47579941153526306\n",
      "Epoch 8535: train loss: 0.47579923272132874\n",
      "Epoch 8536: train loss: 0.4757990837097168\n",
      "Epoch 8537: train loss: 0.475799024105072\n",
      "Epoch 8538: train loss: 0.4757988154888153\n",
      "Epoch 8539: train loss: 0.47579872608184814\n",
      "Epoch 8540: train loss: 0.4757986068725586\n",
      "Epoch 8541: train loss: 0.47579851746559143\n",
      "Epoch 8542: train loss: 0.4757983982563019\n",
      "Epoch 8543: train loss: 0.47579824924468994\n",
      "Epoch 8544: train loss: 0.4757981598377228\n",
      "Epoch 8545: train loss: 0.47579798102378845\n",
      "Epoch 8546: train loss: 0.47579795122146606\n",
      "Epoch 8547: train loss: 0.47579777240753174\n",
      "Epoch 8548: train loss: 0.4757976233959198\n",
      "Epoch 8549: train loss: 0.475797563791275\n",
      "Epoch 8550: train loss: 0.4757974147796631\n",
      "Epoch 8551: train loss: 0.47579726576805115\n",
      "Epoch 8552: train loss: 0.47579720616340637\n",
      "Epoch 8553: train loss: 0.47579705715179443\n",
      "Epoch 8554: train loss: 0.4757969379425049\n",
      "Epoch 8555: train loss: 0.47579678893089294\n",
      "Epoch 8556: train loss: 0.475796639919281\n",
      "Epoch 8557: train loss: 0.47579658031463623\n",
      "Epoch 8558: train loss: 0.4757964313030243\n",
      "Epoch 8559: train loss: 0.47579631209373474\n",
      "Epoch 8560: train loss: 0.4757962226867676\n",
      "Epoch 8561: train loss: 0.475796103477478\n",
      "Epoch 8562: train loss: 0.4757958948612213\n",
      "Epoch 8563: train loss: 0.47579580545425415\n",
      "Epoch 8564: train loss: 0.4757957458496094\n",
      "Epoch 8565: train loss: 0.47579553723335266\n",
      "Epoch 8566: train loss: 0.4757954776287079\n",
      "Epoch 8567: train loss: 0.47579532861709595\n",
      "Epoch 8568: train loss: 0.4757952094078064\n",
      "Epoch 8569: train loss: 0.47579512000083923\n",
      "Epoch 8570: train loss: 0.4757949709892273\n",
      "Epoch 8571: train loss: 0.47579485177993774\n",
      "Epoch 8572: train loss: 0.4757947027683258\n",
      "Epoch 8573: train loss: 0.47579464316368103\n",
      "Epoch 8574: train loss: 0.4757944941520691\n",
      "Epoch 8575: train loss: 0.47579434514045715\n",
      "Epoch 8576: train loss: 0.4757942259311676\n",
      "Epoch 8577: train loss: 0.47579413652420044\n",
      "Epoch 8578: train loss: 0.4757940173149109\n",
      "Epoch 8579: train loss: 0.47579386830329895\n",
      "Epoch 8580: train loss: 0.4757937490940094\n",
      "Epoch 8581: train loss: 0.47579365968704224\n",
      "Epoch 8582: train loss: 0.4757935106754303\n",
      "Epoch 8583: train loss: 0.47579339146614075\n",
      "Epoch 8584: train loss: 0.4757933020591736\n",
      "Epoch 8585: train loss: 0.47579318284988403\n",
      "Epoch 8586: train loss: 0.4757930338382721\n",
      "Epoch 8587: train loss: 0.47579291462898254\n",
      "Epoch 8588: train loss: 0.4757927656173706\n",
      "Epoch 8589: train loss: 0.47579261660575867\n",
      "Epoch 8590: train loss: 0.4757925570011139\n",
      "Epoch 8591: train loss: 0.47579246759414673\n",
      "Epoch 8592: train loss: 0.4757922887802124\n",
      "Epoch 8593: train loss: 0.47579213976860046\n",
      "Epoch 8594: train loss: 0.4757920503616333\n",
      "Epoch 8595: train loss: 0.47579193115234375\n",
      "Epoch 8596: train loss: 0.4757918119430542\n",
      "Epoch 8597: train loss: 0.47579166293144226\n",
      "Epoch 8598: train loss: 0.4757915735244751\n",
      "Epoch 8599: train loss: 0.47579145431518555\n",
      "Epoch 8600: train loss: 0.4757913053035736\n",
      "Epoch 8601: train loss: 0.47579118609428406\n",
      "Epoch 8602: train loss: 0.47579115629196167\n",
      "Epoch 8603: train loss: 0.47579094767570496\n",
      "Epoch 8604: train loss: 0.47579076886177063\n",
      "Epoch 8605: train loss: 0.47579073905944824\n",
      "Epoch 8606: train loss: 0.4757905602455139\n",
      "Epoch 8607: train loss: 0.47579047083854675\n",
      "Epoch 8608: train loss: 0.4757903516292572\n",
      "Epoch 8609: train loss: 0.47579026222229004\n",
      "Epoch 8610: train loss: 0.4757901430130005\n",
      "Epoch 8611: train loss: 0.4757899343967438\n",
      "Epoch 8612: train loss: 0.4757898449897766\n",
      "Epoch 8613: train loss: 0.47578972578048706\n",
      "Epoch 8614: train loss: 0.4757896363735199\n",
      "Epoch 8615: train loss: 0.47578951716423035\n",
      "Epoch 8616: train loss: 0.4757893681526184\n",
      "Epoch 8617: train loss: 0.47578927874565125\n",
      "Epoch 8618: train loss: 0.4757891595363617\n",
      "Epoch 8619: train loss: 0.47578901052474976\n",
      "Epoch 8620: train loss: 0.4757888913154602\n",
      "Epoch 8621: train loss: 0.47578880190849304\n",
      "Epoch 8622: train loss: 0.4757886826992035\n",
      "Epoch 8623: train loss: 0.47578853368759155\n",
      "Epoch 8624: train loss: 0.4757883846759796\n",
      "Epoch 8625: train loss: 0.47578826546669006\n",
      "Epoch 8626: train loss: 0.4757881760597229\n",
      "Epoch 8627: train loss: 0.47578805685043335\n",
      "Epoch 8628: train loss: 0.4757879078388214\n",
      "Epoch 8629: train loss: 0.47578784823417664\n",
      "Epoch 8630: train loss: 0.4757876992225647\n",
      "Epoch 8631: train loss: 0.47578755021095276\n",
      "Epoch 8632: train loss: 0.4757874310016632\n",
      "Epoch 8633: train loss: 0.47578734159469604\n",
      "Epoch 8634: train loss: 0.4757872223854065\n",
      "Epoch 8635: train loss: 0.47578707337379456\n",
      "Epoch 8636: train loss: 0.4757870137691498\n",
      "Epoch 8637: train loss: 0.47578686475753784\n",
      "Epoch 8638: train loss: 0.4757867157459259\n",
      "Epoch 8639: train loss: 0.4757865369319916\n",
      "Epoch 8640: train loss: 0.4757864475250244\n",
      "Epoch 8641: train loss: 0.47578632831573486\n",
      "Epoch 8642: train loss: 0.4757862389087677\n",
      "Epoch 8643: train loss: 0.4757861793041229\n",
      "Epoch 8644: train loss: 0.4757859706878662\n",
      "Epoch 8645: train loss: 0.47578591108322144\n",
      "Epoch 8646: train loss: 0.4757857620716095\n",
      "Epoch 8647: train loss: 0.47578567266464233\n",
      "Epoch 8648: train loss: 0.4757855534553528\n",
      "Epoch 8649: train loss: 0.47578540444374084\n",
      "Epoch 8650: train loss: 0.4757852852344513\n",
      "Epoch 8651: train loss: 0.47578519582748413\n",
      "Epoch 8652: train loss: 0.4757850468158722\n",
      "Epoch 8653: train loss: 0.47578492760658264\n",
      "Epoch 8654: train loss: 0.4757848381996155\n",
      "Epoch 8655: train loss: 0.47578465938568115\n",
      "Epoch 8656: train loss: 0.475784569978714\n",
      "Epoch 8657: train loss: 0.47578445076942444\n",
      "Epoch 8658: train loss: 0.4757843017578125\n",
      "Epoch 8659: train loss: 0.47578418254852295\n",
      "Epoch 8660: train loss: 0.4757840931415558\n",
      "Epoch 8661: train loss: 0.47578394412994385\n",
      "Epoch 8662: train loss: 0.4757837653160095\n",
      "Epoch 8663: train loss: 0.47578367590904236\n",
      "Epoch 8664: train loss: 0.4757836163043976\n",
      "Epoch 8665: train loss: 0.47578346729278564\n",
      "Epoch 8666: train loss: 0.4757833182811737\n",
      "Epoch 8667: train loss: 0.47578319907188416\n",
      "Epoch 8668: train loss: 0.4757831394672394\n",
      "Epoch 8669: train loss: 0.47578293085098267\n",
      "Epoch 8670: train loss: 0.4757828414440155\n",
      "Epoch 8671: train loss: 0.47578272223472595\n",
      "Epoch 8672: train loss: 0.4757826328277588\n",
      "Epoch 8673: train loss: 0.47578251361846924\n",
      "Epoch 8674: train loss: 0.4757824242115021\n",
      "Epoch 8675: train loss: 0.47578227519989014\n",
      "Epoch 8676: train loss: 0.4757821559906006\n",
      "Epoch 8677: train loss: 0.4757820665836334\n",
      "Epoch 8678: train loss: 0.4757818877696991\n",
      "Epoch 8679: train loss: 0.47578179836273193\n",
      "Epoch 8680: train loss: 0.47578164935112\n",
      "Epoch 8681: train loss: 0.47578153014183044\n",
      "Epoch 8682: train loss: 0.4757814407348633\n",
      "Epoch 8683: train loss: 0.47578132152557373\n",
      "Epoch 8684: train loss: 0.47578123211860657\n",
      "Epoch 8685: train loss: 0.47578105330467224\n",
      "Epoch 8686: train loss: 0.4757809638977051\n",
      "Epoch 8687: train loss: 0.4757808446884155\n",
      "Epoch 8688: train loss: 0.47578075528144836\n",
      "Epoch 8689: train loss: 0.4757806062698364\n",
      "Epoch 8690: train loss: 0.4757804274559021\n",
      "Epoch 8691: train loss: 0.47578033804893494\n",
      "Epoch 8692: train loss: 0.4757802188396454\n",
      "Epoch 8693: train loss: 0.4757801294326782\n",
      "Epoch 8694: train loss: 0.47578001022338867\n",
      "Epoch 8695: train loss: 0.47577986121177673\n",
      "Epoch 8696: train loss: 0.47577980160713196\n",
      "Epoch 8697: train loss: 0.47577965259552\n",
      "Epoch 8698: train loss: 0.47577953338623047\n",
      "Epoch 8699: train loss: 0.47577938437461853\n",
      "Epoch 8700: train loss: 0.47577932476997375\n",
      "Epoch 8701: train loss: 0.4757791757583618\n",
      "Epoch 8702: train loss: 0.4757790267467499\n",
      "Epoch 8703: train loss: 0.4757789075374603\n",
      "Epoch 8704: train loss: 0.47577881813049316\n",
      "Epoch 8705: train loss: 0.4757786989212036\n",
      "Epoch 8706: train loss: 0.47577860951423645\n",
      "Epoch 8707: train loss: 0.4757784605026245\n",
      "Epoch 8708: train loss: 0.47577834129333496\n",
      "Epoch 8709: train loss: 0.475778192281723\n",
      "Epoch 8710: train loss: 0.47577807307243347\n",
      "Epoch 8711: train loss: 0.4757779836654663\n",
      "Epoch 8712: train loss: 0.47577786445617676\n",
      "Epoch 8713: train loss: 0.4757777750492096\n",
      "Epoch 8714: train loss: 0.47577762603759766\n",
      "Epoch 8715: train loss: 0.4757775664329529\n",
      "Epoch 8716: train loss: 0.47577741742134094\n",
      "Epoch 8717: train loss: 0.4757772386074066\n",
      "Epoch 8718: train loss: 0.47577714920043945\n",
      "Epoch 8719: train loss: 0.4757770299911499\n",
      "Epoch 8720: train loss: 0.47577688097953796\n",
      "Epoch 8721: train loss: 0.4757768213748932\n",
      "Epoch 8722: train loss: 0.47577667236328125\n",
      "Epoch 8723: train loss: 0.4757765531539917\n",
      "Epoch 8724: train loss: 0.475776344537735\n",
      "Epoch 8725: train loss: 0.4757763147354126\n",
      "Epoch 8726: train loss: 0.47577619552612305\n",
      "Epoch 8727: train loss: 0.4757761061191559\n",
      "Epoch 8728: train loss: 0.47577592730522156\n",
      "Epoch 8729: train loss: 0.4757758378982544\n",
      "Epoch 8730: train loss: 0.47577571868896484\n",
      "Epoch 8731: train loss: 0.4757756292819977\n",
      "Epoch 8732: train loss: 0.47577548027038574\n",
      "Epoch 8733: train loss: 0.4757753610610962\n",
      "Epoch 8734: train loss: 0.47577521204948425\n",
      "Epoch 8735: train loss: 0.4757750928401947\n",
      "Epoch 8736: train loss: 0.47577500343322754\n",
      "Epoch 8737: train loss: 0.475774884223938\n",
      "Epoch 8738: train loss: 0.4757747948169708\n",
      "Epoch 8739: train loss: 0.4757746458053589\n",
      "Epoch 8740: train loss: 0.47577446699142456\n",
      "Epoch 8741: train loss: 0.4757744371891022\n",
      "Epoch 8742: train loss: 0.47577425837516785\n",
      "Epoch 8743: train loss: 0.4757741689682007\n",
      "Epoch 8744: train loss: 0.47577404975891113\n",
      "Epoch 8745: train loss: 0.47577396035194397\n",
      "Epoch 8746: train loss: 0.4757738411426544\n",
      "Epoch 8747: train loss: 0.4757736921310425\n",
      "Epoch 8748: train loss: 0.4757736027240753\n",
      "Epoch 8749: train loss: 0.475773423910141\n",
      "Epoch 8750: train loss: 0.47577333450317383\n",
      "Epoch 8751: train loss: 0.4757732152938843\n",
      "Epoch 8752: train loss: 0.4757731258869171\n",
      "Epoch 8753: train loss: 0.47577300667762756\n",
      "Epoch 8754: train loss: 0.4757728576660156\n",
      "Epoch 8755: train loss: 0.4757727384567261\n",
      "Epoch 8756: train loss: 0.4757726490497589\n",
      "Epoch 8757: train loss: 0.475772500038147\n",
      "Epoch 8758: train loss: 0.4757723808288574\n",
      "Epoch 8759: train loss: 0.47577232122421265\n",
      "Epoch 8760: train loss: 0.4757721722126007\n",
      "Epoch 8761: train loss: 0.47577208280563354\n",
      "Epoch 8762: train loss: 0.475771963596344\n",
      "Epoch 8763: train loss: 0.47577187418937683\n",
      "Epoch 8764: train loss: 0.4757717549800873\n",
      "Epoch 8765: train loss: 0.47577160596847534\n",
      "Epoch 8766: train loss: 0.4757714569568634\n",
      "Epoch 8767: train loss: 0.47577133774757385\n",
      "Epoch 8768: train loss: 0.4757711887359619\n",
      "Epoch 8769: train loss: 0.47577106952667236\n",
      "Epoch 8770: train loss: 0.4757709801197052\n",
      "Epoch 8771: train loss: 0.47577086091041565\n",
      "Epoch 8772: train loss: 0.4757707715034485\n",
      "Epoch 8773: train loss: 0.47577065229415894\n",
      "Epoch 8774: train loss: 0.475770503282547\n",
      "Epoch 8775: train loss: 0.47577041387557983\n",
      "Epoch 8776: train loss: 0.4757702946662903\n",
      "Epoch 8777: train loss: 0.4757702052593231\n",
      "Epoch 8778: train loss: 0.47577008605003357\n",
      "Epoch 8779: train loss: 0.47576993703842163\n",
      "Epoch 8780: train loss: 0.4757698178291321\n",
      "Epoch 8781: train loss: 0.47576966881752014\n",
      "Epoch 8782: train loss: 0.475769579410553\n",
      "Epoch 8783: train loss: 0.4757694602012634\n",
      "Epoch 8784: train loss: 0.47576937079429626\n",
      "Epoch 8785: train loss: 0.4757692515850067\n",
      "Epoch 8786: train loss: 0.4757691025733948\n",
      "Epoch 8787: train loss: 0.4757689833641052\n",
      "Epoch 8788: train loss: 0.4757688343524933\n",
      "Epoch 8789: train loss: 0.47576871514320374\n",
      "Epoch 8790: train loss: 0.4757686257362366\n",
      "Epoch 8791: train loss: 0.475768506526947\n",
      "Epoch 8792: train loss: 0.47576841711997986\n",
      "Epoch 8793: train loss: 0.4757682681083679\n",
      "Epoch 8794: train loss: 0.47576814889907837\n",
      "Epoch 8795: train loss: 0.4757680594921112\n",
      "Epoch 8796: train loss: 0.47576794028282166\n",
      "Epoch 8797: train loss: 0.4757678806781769\n",
      "Epoch 8798: train loss: 0.47576767206192017\n",
      "Epoch 8799: train loss: 0.475767582654953\n",
      "Epoch 8800: train loss: 0.47576746344566345\n",
      "Epoch 8801: train loss: 0.4757673740386963\n",
      "Epoch 8802: train loss: 0.4757671654224396\n",
      "Epoch 8803: train loss: 0.4757671058177948\n",
      "Epoch 8804: train loss: 0.47576701641082764\n",
      "Epoch 8805: train loss: 0.4757668375968933\n",
      "Epoch 8806: train loss: 0.47576674818992615\n",
      "Epoch 8807: train loss: 0.4757666289806366\n",
      "Epoch 8808: train loss: 0.47576653957366943\n",
      "Epoch 8809: train loss: 0.4757664203643799\n",
      "Epoch 8810: train loss: 0.47576627135276794\n",
      "Epoch 8811: train loss: 0.4757661819458008\n",
      "Epoch 8812: train loss: 0.47576606273651123\n",
      "Epoch 8813: train loss: 0.4757659137248993\n",
      "Epoch 8814: train loss: 0.4757658541202545\n",
      "Epoch 8815: train loss: 0.4757657051086426\n",
      "Epoch 8816: train loss: 0.475765585899353\n",
      "Epoch 8817: train loss: 0.47576549649238586\n",
      "Epoch 8818: train loss: 0.4757653772830963\n",
      "Epoch 8819: train loss: 0.47576528787612915\n",
      "Epoch 8820: train loss: 0.4757651090621948\n",
      "Epoch 8821: train loss: 0.47576501965522766\n",
      "Epoch 8822: train loss: 0.4757649004459381\n",
      "Epoch 8823: train loss: 0.47576475143432617\n",
      "Epoch 8824: train loss: 0.475764662027359\n",
      "Epoch 8825: train loss: 0.47576454281806946\n",
      "Epoch 8826: train loss: 0.4757644534111023\n",
      "Epoch 8827: train loss: 0.47576427459716797\n",
      "Epoch 8828: train loss: 0.47576412558555603\n",
      "Epoch 8829: train loss: 0.47576403617858887\n",
      "Epoch 8830: train loss: 0.4757639765739441\n",
      "Epoch 8831: train loss: 0.47576385736465454\n",
      "Epoch 8832: train loss: 0.4757637083530426\n",
      "Epoch 8833: train loss: 0.47576361894607544\n",
      "Epoch 8834: train loss: 0.4757634997367859\n",
      "Epoch 8835: train loss: 0.4757634103298187\n",
      "Epoch 8836: train loss: 0.4757632315158844\n",
      "Epoch 8837: train loss: 0.47576314210891724\n",
      "Epoch 8838: train loss: 0.4757629930973053\n",
      "Epoch 8839: train loss: 0.47576287388801575\n",
      "Epoch 8840: train loss: 0.47576281428337097\n",
      "Epoch 8841: train loss: 0.47576266527175903\n",
      "Epoch 8842: train loss: 0.47576257586479187\n",
      "Epoch 8843: train loss: 0.4757624566555023\n",
      "Epoch 8844: train loss: 0.4757623076438904\n",
      "Epoch 8845: train loss: 0.47576218843460083\n",
      "Epoch 8846: train loss: 0.47576209902763367\n",
      "Epoch 8847: train loss: 0.4757619798183441\n",
      "Epoch 8848: train loss: 0.4757618308067322\n",
      "Epoch 8849: train loss: 0.475761741399765\n",
      "Epoch 8850: train loss: 0.47576162219047546\n",
      "Epoch 8851: train loss: 0.4757615327835083\n",
      "Epoch 8852: train loss: 0.47576141357421875\n",
      "Epoch 8853: train loss: 0.4757612943649292\n",
      "Epoch 8854: train loss: 0.47576114535331726\n",
      "Epoch 8855: train loss: 0.4757609963417053\n",
      "Epoch 8856: train loss: 0.47576093673706055\n",
      "Epoch 8857: train loss: 0.4757607877254486\n",
      "Epoch 8858: train loss: 0.47576066851615906\n",
      "Epoch 8859: train loss: 0.4757605791091919\n",
      "Epoch 8860: train loss: 0.47576045989990234\n",
      "Epoch 8861: train loss: 0.4757603108882904\n",
      "Epoch 8862: train loss: 0.47576025128364563\n",
      "Epoch 8863: train loss: 0.47576016187667847\n",
      "Epoch 8864: train loss: 0.47576001286506653\n",
      "Epoch 8865: train loss: 0.4757598340511322\n",
      "Epoch 8866: train loss: 0.4757598042488098\n",
      "Epoch 8867: train loss: 0.4757596254348755\n",
      "Epoch 8868: train loss: 0.4757595360279083\n",
      "Epoch 8869: train loss: 0.4757594168186188\n",
      "Epoch 8870: train loss: 0.47575926780700684\n",
      "Epoch 8871: train loss: 0.47575920820236206\n",
      "Epoch 8872: train loss: 0.4757590591907501\n",
      "Epoch 8873: train loss: 0.4757589101791382\n",
      "Epoch 8874: train loss: 0.4757588505744934\n",
      "Epoch 8875: train loss: 0.47575870156288147\n",
      "Epoch 8876: train loss: 0.4757585823535919\n",
      "Epoch 8877: train loss: 0.47575849294662476\n",
      "Epoch 8878: train loss: 0.4757583439350128\n",
      "Epoch 8879: train loss: 0.47575822472572327\n",
      "Epoch 8880: train loss: 0.4757581353187561\n",
      "Epoch 8881: train loss: 0.47575801610946655\n",
      "Epoch 8882: train loss: 0.4757579267024994\n",
      "Epoch 8883: train loss: 0.47575780749320984\n",
      "Epoch 8884: train loss: 0.4757576584815979\n",
      "Epoch 8885: train loss: 0.47575753927230835\n",
      "Epoch 8886: train loss: 0.4757574498653412\n",
      "Epoch 8887: train loss: 0.47575733065605164\n",
      "Epoch 8888: train loss: 0.4757572412490845\n",
      "Epoch 8889: train loss: 0.47575706243515015\n",
      "Epoch 8890: train loss: 0.47575703263282776\n",
      "Epoch 8891: train loss: 0.47575685381889343\n",
      "Epoch 8892: train loss: 0.47575676441192627\n",
      "Epoch 8893: train loss: 0.47575661540031433\n",
      "Epoch 8894: train loss: 0.4757564961910248\n",
      "Epoch 8895: train loss: 0.4757564067840576\n",
      "Epoch 8896: train loss: 0.47575634717941284\n",
      "Epoch 8897: train loss: 0.4757561981678009\n",
      "Epoch 8898: train loss: 0.47575607895851135\n",
      "Epoch 8899: train loss: 0.4757559895515442\n",
      "Epoch 8900: train loss: 0.47575581073760986\n",
      "Epoch 8901: train loss: 0.4757556617259979\n",
      "Epoch 8902: train loss: 0.47575560212135315\n",
      "Epoch 8903: train loss: 0.475755512714386\n",
      "Epoch 8904: train loss: 0.47575536370277405\n",
      "Epoch 8905: train loss: 0.4757553040981293\n",
      "Epoch 8906: train loss: 0.47575515508651733\n",
      "Epoch 8907: train loss: 0.4757550358772278\n",
      "Epoch 8908: train loss: 0.4757549464702606\n",
      "Epoch 8909: train loss: 0.4757547676563263\n",
      "Epoch 8910: train loss: 0.47575467824935913\n",
      "Epoch 8911: train loss: 0.4757545590400696\n",
      "Epoch 8912: train loss: 0.4757544696331024\n",
      "Epoch 8913: train loss: 0.4757543206214905\n",
      "Epoch 8914: train loss: 0.4757542610168457\n",
      "Epoch 8915: train loss: 0.47575411200523376\n",
      "Epoch 8916: train loss: 0.4757539927959442\n",
      "Epoch 8917: train loss: 0.47575390338897705\n",
      "Epoch 8918: train loss: 0.4757537245750427\n",
      "Epoch 8919: train loss: 0.47575363516807556\n",
      "Epoch 8920: train loss: 0.475753515958786\n",
      "Epoch 8921: train loss: 0.4757533669471741\n",
      "Epoch 8922: train loss: 0.4757532477378845\n",
      "Epoch 8923: train loss: 0.47575315833091736\n",
      "Epoch 8924: train loss: 0.4757530391216278\n",
      "Epoch 8925: train loss: 0.47575294971466064\n",
      "Epoch 8926: train loss: 0.4757528305053711\n",
      "Epoch 8927: train loss: 0.47575268149375916\n",
      "Epoch 8928: train loss: 0.4757526218891144\n",
      "Epoch 8929: train loss: 0.47575247287750244\n",
      "Epoch 8930: train loss: 0.4757523834705353\n",
      "Epoch 8931: train loss: 0.4757522642612457\n",
      "Epoch 8932: train loss: 0.47575217485427856\n",
      "Epoch 8933: train loss: 0.47575199604034424\n",
      "Epoch 8934: train loss: 0.4757519066333771\n",
      "Epoch 8935: train loss: 0.4757517874240875\n",
      "Epoch 8936: train loss: 0.47575169801712036\n",
      "Epoch 8937: train loss: 0.4757515788078308\n",
      "Epoch 8938: train loss: 0.47575148940086365\n",
      "Epoch 8939: train loss: 0.4757513403892517\n",
      "Epoch 8940: train loss: 0.47575122117996216\n",
      "Epoch 8941: train loss: 0.475751131772995\n",
      "Epoch 8942: train loss: 0.47575101256370544\n",
      "Epoch 8943: train loss: 0.4757509231567383\n",
      "Epoch 8944: train loss: 0.47575074434280396\n",
      "Epoch 8945: train loss: 0.4757506549358368\n",
      "Epoch 8946: train loss: 0.47575053572654724\n",
      "Epoch 8947: train loss: 0.4757504463195801\n",
      "Epoch 8948: train loss: 0.47575029730796814\n",
      "Epoch 8949: train loss: 0.47575023770332336\n",
      "Epoch 8950: train loss: 0.4757500886917114\n",
      "Epoch 8951: train loss: 0.4757499694824219\n",
      "Epoch 8952: train loss: 0.4757498502731323\n",
      "Epoch 8953: train loss: 0.4757497012615204\n",
      "Epoch 8954: train loss: 0.4757496118545532\n",
      "Epoch 8955: train loss: 0.47574955224990845\n",
      "Epoch 8956: train loss: 0.4757494032382965\n",
      "Epoch 8957: train loss: 0.47574928402900696\n",
      "Epoch 8958: train loss: 0.475749135017395\n",
      "Epoch 8959: train loss: 0.47574907541275024\n",
      "Epoch 8960: train loss: 0.4757489264011383\n",
      "Epoch 8961: train loss: 0.47574880719184875\n",
      "Epoch 8962: train loss: 0.4757486581802368\n",
      "Epoch 8963: train loss: 0.47574856877326965\n",
      "Epoch 8964: train loss: 0.4757485091686249\n",
      "Epoch 8965: train loss: 0.4757483899593353\n",
      "Epoch 8966: train loss: 0.4757482409477234\n",
      "Epoch 8967: train loss: 0.4757481515407562\n",
      "Epoch 8968: train loss: 0.4757479727268219\n",
      "Epoch 8969: train loss: 0.4757479429244995\n",
      "Epoch 8970: train loss: 0.4757477641105652\n",
      "Epoch 8971: train loss: 0.475747674703598\n",
      "Epoch 8972: train loss: 0.47574755549430847\n",
      "Epoch 8973: train loss: 0.4757474660873413\n",
      "Epoch 8974: train loss: 0.47574734687805176\n",
      "Epoch 8975: train loss: 0.4757472574710846\n",
      "Epoch 8976: train loss: 0.47574710845947266\n",
      "Epoch 8977: train loss: 0.4757469892501831\n",
      "Epoch 8978: train loss: 0.47574689984321594\n",
      "Epoch 8979: train loss: 0.4757467806339264\n",
      "Epoch 8980: train loss: 0.47574669122695923\n",
      "Epoch 8981: train loss: 0.4757465720176697\n",
      "Epoch 8982: train loss: 0.47574642300605774\n",
      "Epoch 8983: train loss: 0.4757462739944458\n",
      "Epoch 8984: train loss: 0.47574615478515625\n",
      "Epoch 8985: train loss: 0.4757460951805115\n",
      "Epoch 8986: train loss: 0.4757460057735443\n",
      "Epoch 8987: train loss: 0.47574582695961\n",
      "Epoch 8988: train loss: 0.4757457375526428\n",
      "Epoch 8989: train loss: 0.47574561834335327\n",
      "Epoch 8990: train loss: 0.4757455289363861\n",
      "Epoch 8991: train loss: 0.47574540972709656\n",
      "Epoch 8992: train loss: 0.4757453203201294\n",
      "Epoch 8993: train loss: 0.47574520111083984\n",
      "Epoch 8994: train loss: 0.4757450520992279\n",
      "Epoch 8995: train loss: 0.47574496269226074\n",
      "Epoch 8996: train loss: 0.4757448434829712\n",
      "Epoch 8997: train loss: 0.47574469447135925\n",
      "Epoch 8998: train loss: 0.4757446348667145\n",
      "Epoch 8999: train loss: 0.4757445454597473\n",
      "Epoch 9000: train loss: 0.475744366645813\n",
      "Epoch 9001: train loss: 0.4757442772388458\n",
      "Epoch 9002: train loss: 0.4757441580295563\n",
      "Epoch 9003: train loss: 0.4757440686225891\n",
      "Epoch 9004: train loss: 0.4757439196109772\n",
      "Epoch 9005: train loss: 0.4757438004016876\n",
      "Epoch 9006: train loss: 0.47574371099472046\n",
      "Epoch 9007: train loss: 0.4757435917854309\n",
      "Epoch 9008: train loss: 0.47574350237846375\n",
      "Epoch 9009: train loss: 0.4757433235645294\n",
      "Epoch 9010: train loss: 0.47574323415756226\n",
      "Epoch 9011: train loss: 0.4757431149482727\n",
      "Epoch 9012: train loss: 0.47574302554130554\n",
      "Epoch 9013: train loss: 0.475742906332016\n",
      "Epoch 9014: train loss: 0.47574281692504883\n",
      "Epoch 9015: train loss: 0.4757426977157593\n",
      "Epoch 9016: train loss: 0.47574254870414734\n",
      "Epoch 9017: train loss: 0.4757424592971802\n",
      "Epoch 9018: train loss: 0.4757423996925354\n",
      "Epoch 9019: train loss: 0.47574228048324585\n",
      "Epoch 9020: train loss: 0.4757421314716339\n",
      "Epoch 9021: train loss: 0.475741982460022\n",
      "Epoch 9022: train loss: 0.4757418632507324\n",
      "Epoch 9023: train loss: 0.47574177384376526\n",
      "Epoch 9024: train loss: 0.4757416546344757\n",
      "Epoch 9025: train loss: 0.47574156522750854\n",
      "Epoch 9026: train loss: 0.475741446018219\n",
      "Epoch 9027: train loss: 0.47574129700660706\n",
      "Epoch 9028: train loss: 0.4757412374019623\n",
      "Epoch 9029: train loss: 0.4757411479949951\n",
      "Epoch 9030: train loss: 0.4757409691810608\n",
      "Epoch 9031: train loss: 0.47574087977409363\n",
      "Epoch 9032: train loss: 0.4757407605648041\n",
      "Epoch 9033: train loss: 0.47574061155319214\n",
      "Epoch 9034: train loss: 0.475740522146225\n",
      "Epoch 9035: train loss: 0.4757404625415802\n",
      "Epoch 9036: train loss: 0.47574031352996826\n",
      "Epoch 9037: train loss: 0.4757402539253235\n",
      "Epoch 9038: train loss: 0.47574010491371155\n",
      "Epoch 9039: train loss: 0.4757399260997772\n",
      "Epoch 9040: train loss: 0.47573989629745483\n",
      "Epoch 9041: train loss: 0.4757397174835205\n",
      "Epoch 9042: train loss: 0.4757396876811981\n",
      "Epoch 9043: train loss: 0.47573956847190857\n",
      "Epoch 9044: train loss: 0.47573941946029663\n",
      "Epoch 9045: train loss: 0.4757393002510071\n",
      "Epoch 9046: train loss: 0.4757392108440399\n",
      "Epoch 9047: train loss: 0.475739061832428\n",
      "Epoch 9048: train loss: 0.4757389426231384\n",
      "Epoch 9049: train loss: 0.47573885321617126\n",
      "Epoch 9050: train loss: 0.4757387340068817\n",
      "Epoch 9051: train loss: 0.47573864459991455\n",
      "Epoch 9052: train loss: 0.475738525390625\n",
      "Epoch 9053: train loss: 0.47573840618133545\n",
      "Epoch 9054: train loss: 0.4757383167743683\n",
      "Epoch 9055: train loss: 0.47573819756507874\n",
      "Epoch 9056: train loss: 0.4757380485534668\n",
      "Epoch 9057: train loss: 0.47573795914649963\n",
      "Epoch 9058: train loss: 0.4757377803325653\n",
      "Epoch 9059: train loss: 0.47573769092559814\n",
      "Epoch 9060: train loss: 0.4757375717163086\n",
      "Epoch 9061: train loss: 0.47573748230934143\n",
      "Epoch 9062: train loss: 0.47573742270469666\n",
      "Epoch 9063: train loss: 0.4757372736930847\n",
      "Epoch 9064: train loss: 0.47573715448379517\n",
      "Epoch 9065: train loss: 0.475737065076828\n",
      "Epoch 9066: train loss: 0.47573691606521606\n",
      "Epoch 9067: train loss: 0.4757367968559265\n",
      "Epoch 9068: train loss: 0.47573673725128174\n",
      "Epoch 9069: train loss: 0.475736528635025\n",
      "Epoch 9070: train loss: 0.47573649883270264\n",
      "Epoch 9071: train loss: 0.4757363200187683\n",
      "Epoch 9072: train loss: 0.47573623061180115\n",
      "Epoch 9073: train loss: 0.47573617100715637\n",
      "Epoch 9074: train loss: 0.47573602199554443\n",
      "Epoch 9075: train loss: 0.4757359027862549\n",
      "Epoch 9076: train loss: 0.4757358133792877\n",
      "Epoch 9077: train loss: 0.47573569416999817\n",
      "Epoch 9078: train loss: 0.47573554515838623\n",
      "Epoch 9079: train loss: 0.47573548555374146\n",
      "Epoch 9080: train loss: 0.4757353365421295\n",
      "Epoch 9081: train loss: 0.47573524713516235\n",
      "Epoch 9082: train loss: 0.4757351279258728\n",
      "Epoch 9083: train loss: 0.47573503851890564\n",
      "Epoch 9084: train loss: 0.4757349193096161\n",
      "Epoch 9085: train loss: 0.4757348299026489\n",
      "Epoch 9086: train loss: 0.4757346510887146\n",
      "Epoch 9087: train loss: 0.47573456168174744\n",
      "Epoch 9088: train loss: 0.4757344424724579\n",
      "Epoch 9089: train loss: 0.4757343530654907\n",
      "Epoch 9090: train loss: 0.47573423385620117\n",
      "Epoch 9091: train loss: 0.47573408484458923\n",
      "Epoch 9092: train loss: 0.47573402523994446\n",
      "Epoch 9093: train loss: 0.4757338762283325\n",
      "Epoch 9094: train loss: 0.47573375701904297\n",
      "Epoch 9095: train loss: 0.4757337272167206\n",
      "Epoch 9096: train loss: 0.47573354840278625\n",
      "Epoch 9097: train loss: 0.4757334589958191\n",
      "Epoch 9098: train loss: 0.47573333978652954\n",
      "Epoch 9099: train loss: 0.4757332503795624\n",
      "Epoch 9100: train loss: 0.4757331311702728\n",
      "Epoch 9101: train loss: 0.4757329821586609\n",
      "Epoch 9102: train loss: 0.4757328927516937\n",
      "Epoch 9103: train loss: 0.47573283314704895\n",
      "Epoch 9104: train loss: 0.475732684135437\n",
      "Epoch 9105: train loss: 0.47573256492614746\n",
      "Epoch 9106: train loss: 0.4757324755191803\n",
      "Epoch 9107: train loss: 0.47573229670524597\n",
      "Epoch 9108: train loss: 0.4757322072982788\n",
      "Epoch 9109: train loss: 0.47573208808898926\n",
      "Epoch 9110: train loss: 0.4757319986820221\n",
      "Epoch 9111: train loss: 0.47573187947273254\n",
      "Epoch 9112: train loss: 0.47573184967041016\n",
      "Epoch 9113: train loss: 0.47573164105415344\n",
      "Epoch 9114: train loss: 0.4757315218448639\n",
      "Epoch 9115: train loss: 0.47573143243789673\n",
      "Epoch 9116: train loss: 0.4757313132286072\n",
      "Epoch 9117: train loss: 0.47573122382164\n",
      "Epoch 9118: train loss: 0.47573116421699524\n",
      "Epoch 9119: train loss: 0.4757309556007385\n",
      "Epoch 9120: train loss: 0.47573089599609375\n",
      "Epoch 9121: train loss: 0.4757307767868042\n",
      "Epoch 9122: train loss: 0.47573068737983704\n",
      "Epoch 9123: train loss: 0.4757305681705475\n",
      "Epoch 9124: train loss: 0.4757304787635803\n",
      "Epoch 9125: train loss: 0.4757303297519684\n",
      "Epoch 9126: train loss: 0.4757302701473236\n",
      "Epoch 9127: train loss: 0.47573012113571167\n",
      "Epoch 9128: train loss: 0.47572994232177734\n",
      "Epoch 9129: train loss: 0.47572991251945496\n",
      "Epoch 9130: train loss: 0.47572973370552063\n",
      "Epoch 9131: train loss: 0.47572964429855347\n",
      "Epoch 9132: train loss: 0.4757295250892639\n",
      "Epoch 9133: train loss: 0.47572943568229675\n",
      "Epoch 9134: train loss: 0.4757293164730072\n",
      "Epoch 9135: train loss: 0.47572922706604004\n",
      "Epoch 9136: train loss: 0.4757291078567505\n",
      "Epoch 9137: train loss: 0.4757290184497833\n",
      "Epoch 9138: train loss: 0.47572895884513855\n",
      "Epoch 9139: train loss: 0.47572875022888184\n",
      "Epoch 9140: train loss: 0.47572869062423706\n",
      "Epoch 9141: train loss: 0.4757286012172699\n",
      "Epoch 9142: train loss: 0.47572845220565796\n",
      "Epoch 9143: train loss: 0.4757283926010132\n",
      "Epoch 9144: train loss: 0.47572824358940125\n",
      "Epoch 9145: train loss: 0.4757281243801117\n",
      "Epoch 9146: train loss: 0.47572797536849976\n",
      "Epoch 9147: train loss: 0.4757278561592102\n",
      "Epoch 9148: train loss: 0.4757278263568878\n",
      "Epoch 9149: train loss: 0.4757276475429535\n",
      "Epoch 9150: train loss: 0.47572755813598633\n",
      "Epoch 9151: train loss: 0.4757274389266968\n",
      "Epoch 9152: train loss: 0.4757273495197296\n",
      "Epoch 9153: train loss: 0.4757272005081177\n",
      "Epoch 9154: train loss: 0.4757271409034729\n",
      "Epoch 9155: train loss: 0.47572702169418335\n",
      "Epoch 9156: train loss: 0.4757269322872162\n",
      "Epoch 9157: train loss: 0.47572681307792664\n",
      "Epoch 9158: train loss: 0.4757266640663147\n",
      "Epoch 9159: train loss: 0.47572651505470276\n",
      "Epoch 9160: train loss: 0.47572651505470276\n",
      "Epoch 9161: train loss: 0.47572633624076843\n",
      "Epoch 9162: train loss: 0.47572624683380127\n",
      "Epoch 9163: train loss: 0.47572609782218933\n",
      "Epoch 9164: train loss: 0.4757259786128998\n",
      "Epoch 9165: train loss: 0.4757258892059326\n",
      "Epoch 9166: train loss: 0.47572576999664307\n",
      "Epoch 9167: train loss: 0.4757257103919983\n",
      "Epoch 9168: train loss: 0.47572562098503113\n",
      "Epoch 9169: train loss: 0.4757254719734192\n",
      "Epoch 9170: train loss: 0.47572535276412964\n",
      "Epoch 9171: train loss: 0.4757252633571625\n",
      "Epoch 9172: train loss: 0.47572508454322815\n",
      "Epoch 9173: train loss: 0.47572505474090576\n",
      "Epoch 9174: train loss: 0.47572484612464905\n",
      "Epoch 9175: train loss: 0.47572484612464905\n",
      "Epoch 9176: train loss: 0.4757246673107147\n",
      "Epoch 9177: train loss: 0.47572457790374756\n",
      "Epoch 9178: train loss: 0.475724458694458\n",
      "Epoch 9179: train loss: 0.47572430968284607\n",
      "Epoch 9180: train loss: 0.4757242202758789\n",
      "Epoch 9181: train loss: 0.47572410106658936\n",
      "Epoch 9182: train loss: 0.4757240414619446\n",
      "Epoch 9183: train loss: 0.47572389245033264\n",
      "Epoch 9184: train loss: 0.4757238030433655\n",
      "Epoch 9185: train loss: 0.4757236838340759\n",
      "Epoch 9186: train loss: 0.47572359442710876\n",
      "Epoch 9187: train loss: 0.4757234752178192\n",
      "Epoch 9188: train loss: 0.47572338581085205\n",
      "Epoch 9189: train loss: 0.4757232666015625\n",
      "Epoch 9190: train loss: 0.47572314739227295\n",
      "Epoch 9191: train loss: 0.4757230579853058\n",
      "Epoch 9192: train loss: 0.47572290897369385\n",
      "Epoch 9193: train loss: 0.4757227897644043\n",
      "Epoch 9194: train loss: 0.47572270035743713\n",
      "Epoch 9195: train loss: 0.4757225811481476\n",
      "Epoch 9196: train loss: 0.4757225215435028\n",
      "Epoch 9197: train loss: 0.4757223129272461\n",
      "Epoch 9198: train loss: 0.4757222831249237\n",
      "Epoch 9199: train loss: 0.4757221043109894\n",
      "Epoch 9200: train loss: 0.4757220149040222\n",
      "Epoch 9201: train loss: 0.47572189569473267\n",
      "Epoch 9202: train loss: 0.4757218062877655\n",
      "Epoch 9203: train loss: 0.47572168707847595\n",
      "Epoch 9204: train loss: 0.4757215976715088\n",
      "Epoch 9205: train loss: 0.47572147846221924\n",
      "Epoch 9206: train loss: 0.4757213890552521\n",
      "Epoch 9207: train loss: 0.4757212698459625\n",
      "Epoch 9208: train loss: 0.47572118043899536\n",
      "Epoch 9209: train loss: 0.4757210612297058\n",
      "Epoch 9210: train loss: 0.47572097182273865\n",
      "Epoch 9211: train loss: 0.4757208228111267\n",
      "Epoch 9212: train loss: 0.47572076320648193\n",
      "Epoch 9213: train loss: 0.47572061419487\n",
      "Epoch 9214: train loss: 0.47572049498558044\n",
      "Epoch 9215: train loss: 0.4757204055786133\n",
      "Epoch 9216: train loss: 0.47572028636932373\n",
      "Epoch 9217: train loss: 0.4757201373577118\n",
      "Epoch 9218: train loss: 0.47572001814842224\n",
      "Epoch 9219: train loss: 0.47571998834609985\n",
      "Epoch 9220: train loss: 0.4757198691368103\n",
      "Epoch 9221: train loss: 0.47571972012519836\n",
      "Epoch 9222: train loss: 0.4757196009159088\n",
      "Epoch 9223: train loss: 0.47571951150894165\n",
      "Epoch 9224: train loss: 0.4757193922996521\n",
      "Epoch 9225: train loss: 0.47571930289268494\n",
      "Epoch 9226: train loss: 0.4757191836833954\n",
      "Epoch 9227: train loss: 0.47571903467178345\n",
      "Epoch 9228: train loss: 0.4757189154624939\n",
      "Epoch 9229: train loss: 0.4757188856601715\n",
      "Epoch 9230: train loss: 0.47571876645088196\n",
      "Epoch 9231: train loss: 0.4757186770439148\n",
      "Epoch 9232: train loss: 0.47571849822998047\n",
      "Epoch 9233: train loss: 0.4757184088230133\n",
      "Epoch 9234: train loss: 0.47571828961372375\n",
      "Epoch 9235: train loss: 0.4757182002067566\n",
      "Epoch 9236: train loss: 0.47571808099746704\n",
      "Epoch 9237: train loss: 0.4757179915904999\n",
      "Epoch 9238: train loss: 0.47571784257888794\n",
      "Epoch 9239: train loss: 0.4757177233695984\n",
      "Epoch 9240: train loss: 0.4757176637649536\n",
      "Epoch 9241: train loss: 0.4757175147533417\n",
      "Epoch 9242: train loss: 0.4757174551486969\n",
      "Epoch 9243: train loss: 0.47571730613708496\n",
      "Epoch 9244: train loss: 0.4757172167301178\n",
      "Epoch 9245: train loss: 0.47571709752082825\n",
      "Epoch 9246: train loss: 0.4757170081138611\n",
      "Epoch 9247: train loss: 0.47571688890457153\n",
      "Epoch 9248: train loss: 0.47571679949760437\n",
      "Epoch 9249: train loss: 0.4757166802883148\n",
      "Epoch 9250: train loss: 0.47571659088134766\n",
      "Epoch 9251: train loss: 0.47571641206741333\n",
      "Epoch 9252: train loss: 0.47571632266044617\n",
      "Epoch 9253: train loss: 0.4757162034511566\n",
      "Epoch 9254: train loss: 0.47571617364883423\n",
      "Epoch 9255: train loss: 0.4757160544395447\n",
      "Epoch 9256: train loss: 0.47571590542793274\n",
      "Epoch 9257: train loss: 0.4757157862186432\n",
      "Epoch 9258: train loss: 0.47571563720703125\n",
      "Epoch 9259: train loss: 0.4757155776023865\n",
      "Epoch 9260: train loss: 0.47571542859077454\n",
      "Epoch 9261: train loss: 0.47571536898612976\n",
      "Epoch 9262: train loss: 0.4757152795791626\n",
      "Epoch 9263: train loss: 0.47571516036987305\n",
      "Epoch 9264: train loss: 0.4757150113582611\n",
      "Epoch 9265: train loss: 0.47571489214897156\n",
      "Epoch 9266: train loss: 0.4757148027420044\n",
      "Epoch 9267: train loss: 0.47571468353271484\n",
      "Epoch 9268: train loss: 0.47571465373039246\n",
      "Epoch 9269: train loss: 0.4757145345211029\n",
      "Epoch 9270: train loss: 0.47571438550949097\n",
      "Epoch 9271: train loss: 0.4757142663002014\n",
      "Epoch 9272: train loss: 0.47571417689323425\n",
      "Epoch 9273: train loss: 0.4757140576839447\n",
      "Epoch 9274: train loss: 0.47571396827697754\n",
      "Epoch 9275: train loss: 0.475713849067688\n",
      "Epoch 9276: train loss: 0.4757137596607208\n",
      "Epoch 9277: train loss: 0.4757136404514313\n",
      "Epoch 9278: train loss: 0.47571349143981934\n",
      "Epoch 9279: train loss: 0.47571343183517456\n",
      "Epoch 9280: train loss: 0.4757132828235626\n",
      "Epoch 9281: train loss: 0.47571319341659546\n",
      "Epoch 9282: train loss: 0.4757130742073059\n",
      "Epoch 9283: train loss: 0.47571298480033875\n",
      "Epoch 9284: train loss: 0.4757128655910492\n",
      "Epoch 9285: train loss: 0.47571277618408203\n",
      "Epoch 9286: train loss: 0.4757126569747925\n",
      "Epoch 9287: train loss: 0.47571250796318054\n",
      "Epoch 9288: train loss: 0.475712388753891\n",
      "Epoch 9289: train loss: 0.47571229934692383\n",
      "Epoch 9290: train loss: 0.47571223974227905\n",
      "Epoch 9291: train loss: 0.4757120907306671\n",
      "Epoch 9292: train loss: 0.47571197152137756\n",
      "Epoch 9293: train loss: 0.4757118821144104\n",
      "Epoch 9294: train loss: 0.47571176290512085\n",
      "Epoch 9295: train loss: 0.4757116734981537\n",
      "Epoch 9296: train loss: 0.47571155428886414\n",
      "Epoch 9297: train loss: 0.475711464881897\n",
      "Epoch 9298: train loss: 0.4757113456726074\n",
      "Epoch 9299: train loss: 0.4757111966609955\n",
      "Epoch 9300: train loss: 0.4757111370563507\n",
      "Epoch 9301: train loss: 0.47571104764938354\n",
      "Epoch 9302: train loss: 0.475710928440094\n",
      "Epoch 9303: train loss: 0.47571077942848206\n",
      "Epoch 9304: train loss: 0.4757106602191925\n",
      "Epoch 9305: train loss: 0.47571051120758057\n",
      "Epoch 9306: train loss: 0.47571051120758057\n",
      "Epoch 9307: train loss: 0.47571030259132385\n",
      "Epoch 9308: train loss: 0.47571030259132385\n",
      "Epoch 9309: train loss: 0.47571009397506714\n",
      "Epoch 9310: train loss: 0.47571003437042236\n",
      "Epoch 9311: train loss: 0.4757099449634552\n",
      "Epoch 9312: train loss: 0.47570979595184326\n",
      "Epoch 9313: train loss: 0.4757097363471985\n",
      "Epoch 9314: train loss: 0.47570958733558655\n",
      "Epoch 9315: train loss: 0.475709468126297\n",
      "Epoch 9316: train loss: 0.47570937871932983\n",
      "Epoch 9317: train loss: 0.4757092595100403\n",
      "Epoch 9318: train loss: 0.4757091701030731\n",
      "Epoch 9319: train loss: 0.47570905089378357\n",
      "Epoch 9320: train loss: 0.4757089614868164\n",
      "Epoch 9321: train loss: 0.47570884227752686\n",
      "Epoch 9322: train loss: 0.4757087528705597\n",
      "Epoch 9323: train loss: 0.47570863366127014\n",
      "Epoch 9324: train loss: 0.475708544254303\n",
      "Epoch 9325: train loss: 0.4757084250450134\n",
      "Epoch 9326: train loss: 0.47570833563804626\n",
      "Epoch 9327: train loss: 0.4757082164287567\n",
      "Epoch 9328: train loss: 0.47570812702178955\n",
      "Epoch 9329: train loss: 0.4757080078125\n",
      "Epoch 9330: train loss: 0.47570788860321045\n",
      "Epoch 9331: train loss: 0.4757077395915985\n",
      "Epoch 9332: train loss: 0.47570767998695374\n",
      "Epoch 9333: train loss: 0.4757075309753418\n",
      "Epoch 9334: train loss: 0.47570744156837463\n",
      "Epoch 9335: train loss: 0.4757073223590851\n",
      "Epoch 9336: train loss: 0.4757072627544403\n",
      "Epoch 9337: train loss: 0.47570711374282837\n",
      "Epoch 9338: train loss: 0.4757070243358612\n",
      "Epoch 9339: train loss: 0.47570690512657166\n",
      "Epoch 9340: train loss: 0.4757067561149597\n",
      "Epoch 9341: train loss: 0.47570669651031494\n",
      "Epoch 9342: train loss: 0.4757066071033478\n",
      "Epoch 9343: train loss: 0.47570642828941345\n",
      "Epoch 9344: train loss: 0.47570639848709106\n",
      "Epoch 9345: train loss: 0.47570621967315674\n",
      "Epoch 9346: train loss: 0.4757061302661896\n",
      "Epoch 9347: train loss: 0.4757060110569\n",
      "Epoch 9348: train loss: 0.47570592164993286\n",
      "Epoch 9349: train loss: 0.4757058620452881\n",
      "Epoch 9350: train loss: 0.47570571303367615\n",
      "Epoch 9351: train loss: 0.4757055640220642\n",
      "Epoch 9352: train loss: 0.47570550441741943\n",
      "Epoch 9353: train loss: 0.4757053554058075\n",
      "Epoch 9354: train loss: 0.4757052958011627\n",
      "Epoch 9355: train loss: 0.47570517659187317\n",
      "Epoch 9356: train loss: 0.475705087184906\n",
      "Epoch 9357: train loss: 0.47570496797561646\n",
      "Epoch 9358: train loss: 0.4757048785686493\n",
      "Epoch 9359: train loss: 0.47570472955703735\n",
      "Epoch 9360: train loss: 0.4757046699523926\n",
      "Epoch 9361: train loss: 0.475704550743103\n",
      "Epoch 9362: train loss: 0.47570446133613586\n",
      "Epoch 9363: train loss: 0.4757043421268463\n",
      "Epoch 9364: train loss: 0.4757041931152344\n",
      "Epoch 9365: train loss: 0.4757040739059448\n",
      "Epoch 9366: train loss: 0.47570404410362244\n",
      "Epoch 9367: train loss: 0.4757039248943329\n",
      "Epoch 9368: train loss: 0.4757038354873657\n",
      "Epoch 9369: train loss: 0.4757036566734314\n",
      "Epoch 9370: train loss: 0.47570356726646423\n",
      "Epoch 9371: train loss: 0.4757034480571747\n",
      "Epoch 9372: train loss: 0.47570329904556274\n",
      "Epoch 9373: train loss: 0.47570329904556274\n",
      "Epoch 9374: train loss: 0.4757031500339508\n",
      "Epoch 9375: train loss: 0.47570303082466125\n",
      "Epoch 9376: train loss: 0.4757028818130493\n",
      "Epoch 9377: train loss: 0.47570282220840454\n",
      "Epoch 9378: train loss: 0.4757027328014374\n",
      "Epoch 9379: train loss: 0.4757026135921478\n",
      "Epoch 9380: train loss: 0.4757024645805359\n",
      "Epoch 9381: train loss: 0.4757024049758911\n",
      "Epoch 9382: train loss: 0.47570231556892395\n",
      "Epoch 9383: train loss: 0.4757021963596344\n",
      "Epoch 9384: train loss: 0.47570204734802246\n",
      "Epoch 9385: train loss: 0.4757019579410553\n",
      "Epoch 9386: train loss: 0.47570183873176575\n",
      "Epoch 9387: train loss: 0.47570177912712097\n",
      "Epoch 9388: train loss: 0.47570163011550903\n",
      "Epoch 9389: train loss: 0.47570157051086426\n",
      "Epoch 9390: train loss: 0.4757014214992523\n",
      "Epoch 9391: train loss: 0.47570133209228516\n",
      "Epoch 9392: train loss: 0.4757012724876404\n",
      "Epoch 9393: train loss: 0.47570115327835083\n",
      "Epoch 9394: train loss: 0.4757010042667389\n",
      "Epoch 9395: train loss: 0.47570091485977173\n",
      "Epoch 9396: train loss: 0.4757007956504822\n",
      "Epoch 9397: train loss: 0.475700706243515\n",
      "Epoch 9398: train loss: 0.47570058703422546\n",
      "Epoch 9399: train loss: 0.4757004976272583\n",
      "Epoch 9400: train loss: 0.47570037841796875\n",
      "Epoch 9401: train loss: 0.4757002592086792\n",
      "Epoch 9402: train loss: 0.47570016980171204\n",
      "Epoch 9403: train loss: 0.4757000505924225\n",
      "Epoch 9404: train loss: 0.47569990158081055\n",
      "Epoch 9405: train loss: 0.4756998121738434\n",
      "Epoch 9406: train loss: 0.47569969296455383\n",
      "Epoch 9407: train loss: 0.47569960355758667\n",
      "Epoch 9408: train loss: 0.4756995439529419\n",
      "Epoch 9409: train loss: 0.47569942474365234\n",
      "Epoch 9410: train loss: 0.4756992757320404\n",
      "Epoch 9411: train loss: 0.47569918632507324\n",
      "Epoch 9412: train loss: 0.47569912672042847\n",
      "Epoch 9413: train loss: 0.4756990075111389\n",
      "Epoch 9414: train loss: 0.475698858499527\n",
      "Epoch 9415: train loss: 0.4756987690925598\n",
      "Epoch 9416: train loss: 0.4756985902786255\n",
      "Epoch 9417: train loss: 0.4756985902786255\n",
      "Epoch 9418: train loss: 0.47569844126701355\n",
      "Epoch 9419: train loss: 0.4756983518600464\n",
      "Epoch 9420: train loss: 0.47569823265075684\n",
      "Epoch 9421: train loss: 0.4756981432437897\n",
      "Epoch 9422: train loss: 0.4756980240345001\n",
      "Epoch 9423: train loss: 0.47569796442985535\n",
      "Epoch 9424: train loss: 0.47569775581359863\n",
      "Epoch 9425: train loss: 0.47569766640663147\n",
      "Epoch 9426: train loss: 0.4756975471973419\n",
      "Epoch 9427: train loss: 0.47569751739501953\n",
      "Epoch 9428: train loss: 0.47569739818573\n",
      "Epoch 9429: train loss: 0.4756973087787628\n",
      "Epoch 9430: train loss: 0.47569724917411804\n",
      "Epoch 9431: train loss: 0.4756971001625061\n",
      "Epoch 9432: train loss: 0.47569698095321655\n",
      "Epoch 9433: train loss: 0.4756968319416046\n",
      "Epoch 9434: train loss: 0.47569671273231506\n",
      "Epoch 9435: train loss: 0.4756966233253479\n",
      "Epoch 9436: train loss: 0.4756965637207031\n",
      "Epoch 9437: train loss: 0.4756964445114136\n",
      "Epoch 9438: train loss: 0.47569629549980164\n",
      "Epoch 9439: train loss: 0.4756962060928345\n",
      "Epoch 9440: train loss: 0.4756961464881897\n",
      "Epoch 9441: train loss: 0.47569602727890015\n",
      "Epoch 9442: train loss: 0.4756958782672882\n",
      "Epoch 9443: train loss: 0.47569578886032104\n",
      "Epoch 9444: train loss: 0.4756956696510315\n",
      "Epoch 9445: train loss: 0.47569558024406433\n",
      "Epoch 9446: train loss: 0.4756954610347748\n",
      "Epoch 9447: train loss: 0.47569540143013\n",
      "Epoch 9448: train loss: 0.47569531202316284\n",
      "Epoch 9449: train loss: 0.4756951630115509\n",
      "Epoch 9450: train loss: 0.47569504380226135\n",
      "Epoch 9451: train loss: 0.4756948947906494\n",
      "Epoch 9452: train loss: 0.47569483518600464\n",
      "Epoch 9453: train loss: 0.4756947457790375\n",
      "Epoch 9454: train loss: 0.4756946265697479\n",
      "Epoch 9455: train loss: 0.47569453716278076\n",
      "Epoch 9456: train loss: 0.4756944179534912\n",
      "Epoch 9457: train loss: 0.47569432854652405\n",
      "Epoch 9458: train loss: 0.4756942093372345\n",
      "Epoch 9459: train loss: 0.47569411993026733\n",
      "Epoch 9460: train loss: 0.4756940007209778\n",
      "Epoch 9461: train loss: 0.4756939113140106\n",
      "Epoch 9462: train loss: 0.47569379210472107\n",
      "Epoch 9463: train loss: 0.4756937026977539\n",
      "Epoch 9464: train loss: 0.47569364309310913\n",
      "Epoch 9465: train loss: 0.4756934940814972\n",
      "Epoch 9466: train loss: 0.47569337487220764\n",
      "Epoch 9467: train loss: 0.4756932854652405\n",
      "Epoch 9468: train loss: 0.4756931662559509\n",
      "Epoch 9469: train loss: 0.475693017244339\n",
      "Epoch 9470: train loss: 0.4756929576396942\n",
      "Epoch 9471: train loss: 0.47569286823272705\n",
      "Epoch 9472: train loss: 0.4756927490234375\n",
      "Epoch 9473: train loss: 0.47569260001182556\n",
      "Epoch 9474: train loss: 0.475692480802536\n",
      "Epoch 9475: train loss: 0.47569242119789124\n",
      "Epoch 9476: train loss: 0.4756923317909241\n",
      "Epoch 9477: train loss: 0.47569218277931213\n",
      "Epoch 9478: train loss: 0.47569212317466736\n",
      "Epoch 9479: train loss: 0.4756920039653778\n",
      "Epoch 9480: train loss: 0.47569191455841064\n",
      "Epoch 9481: train loss: 0.4756917953491211\n",
      "Epoch 9482: train loss: 0.47569170594215393\n",
      "Epoch 9483: train loss: 0.4756915867328644\n",
      "Epoch 9484: train loss: 0.47569143772125244\n",
      "Epoch 9485: train loss: 0.4756913483142853\n",
      "Epoch 9486: train loss: 0.4756912887096405\n",
      "Epoch 9487: train loss: 0.47569116950035095\n",
      "Epoch 9488: train loss: 0.4756910800933838\n",
      "Epoch 9489: train loss: 0.47569093108177185\n",
      "Epoch 9490: train loss: 0.4756908118724823\n",
      "Epoch 9491: train loss: 0.47569072246551514\n",
      "Epoch 9492: train loss: 0.4756906032562256\n",
      "Epoch 9493: train loss: 0.4756905138492584\n",
      "Epoch 9494: train loss: 0.47569039463996887\n",
      "Epoch 9495: train loss: 0.4756903052330017\n",
      "Epoch 9496: train loss: 0.47569024562835693\n",
      "Epoch 9497: train loss: 0.475690096616745\n",
      "Epoch 9498: train loss: 0.47568997740745544\n",
      "Epoch 9499: train loss: 0.4756898880004883\n",
      "Epoch 9500: train loss: 0.47568976879119873\n",
      "Epoch 9501: train loss: 0.47568967938423157\n",
      "Epoch 9502: train loss: 0.475689560174942\n",
      "Epoch 9503: train loss: 0.47568947076797485\n",
      "Epoch 9504: train loss: 0.4756893515586853\n",
      "Epoch 9505: train loss: 0.47568926215171814\n",
      "Epoch 9506: train loss: 0.47568920254707336\n",
      "Epoch 9507: train loss: 0.4756890535354614\n",
      "Epoch 9508: train loss: 0.4756889343261719\n",
      "Epoch 9509: train loss: 0.4756888151168823\n",
      "Epoch 9510: train loss: 0.47568872570991516\n",
      "Epoch 9511: train loss: 0.4756885766983032\n",
      "Epoch 9512: train loss: 0.47568851709365845\n",
      "Epoch 9513: train loss: 0.4756883978843689\n",
      "Epoch 9514: train loss: 0.47568824887275696\n",
      "Epoch 9515: train loss: 0.4756881892681122\n",
      "Epoch 9516: train loss: 0.475688099861145\n",
      "Epoch 9517: train loss: 0.4756879508495331\n",
      "Epoch 9518: train loss: 0.47568783164024353\n",
      "Epoch 9519: train loss: 0.47568783164024353\n",
      "Epoch 9520: train loss: 0.4756876826286316\n",
      "Epoch 9521: train loss: 0.4756876230239868\n",
      "Epoch 9522: train loss: 0.4756874740123749\n",
      "Epoch 9523: train loss: 0.47568732500076294\n",
      "Epoch 9524: train loss: 0.4756872057914734\n",
      "Epoch 9525: train loss: 0.4756871461868286\n",
      "Epoch 9526: train loss: 0.4756871163845062\n",
      "Epoch 9527: train loss: 0.4756869375705719\n",
      "Epoch 9528: train loss: 0.47568684816360474\n",
      "Epoch 9529: train loss: 0.4756867289543152\n",
      "Epoch 9530: train loss: 0.475686639547348\n",
      "Epoch 9531: train loss: 0.47568652033805847\n",
      "Epoch 9532: train loss: 0.47568637132644653\n",
      "Epoch 9533: train loss: 0.47568631172180176\n",
      "Epoch 9534: train loss: 0.4756861627101898\n",
      "Epoch 9535: train loss: 0.47568610310554504\n",
      "Epoch 9536: train loss: 0.4756860136985779\n",
      "Epoch 9537: train loss: 0.47568589448928833\n",
      "Epoch 9538: train loss: 0.47568580508232117\n",
      "Epoch 9539: train loss: 0.4756856858730316\n",
      "Epoch 9540: train loss: 0.47568559646606445\n",
      "Epoch 9541: train loss: 0.4756854474544525\n",
      "Epoch 9542: train loss: 0.47568538784980774\n",
      "Epoch 9543: train loss: 0.4756852686405182\n",
      "Epoch 9544: train loss: 0.475685179233551\n",
      "Epoch 9545: train loss: 0.4756850600242615\n",
      "Epoch 9546: train loss: 0.4756849706172943\n",
      "Epoch 9547: train loss: 0.47568479180336\n",
      "Epoch 9548: train loss: 0.4756847023963928\n",
      "Epoch 9549: train loss: 0.47568464279174805\n",
      "Epoch 9550: train loss: 0.4756845533847809\n",
      "Epoch 9551: train loss: 0.47568443417549133\n",
      "Epoch 9552: train loss: 0.4756842851638794\n",
      "Epoch 9553: train loss: 0.4756842255592346\n",
      "Epoch 9554: train loss: 0.47568413615226746\n",
      "Epoch 9555: train loss: 0.4756840169429779\n",
      "Epoch 9556: train loss: 0.47568392753601074\n",
      "Epoch 9557: train loss: 0.4756838083267212\n",
      "Epoch 9558: train loss: 0.47568365931510925\n",
      "Epoch 9559: train loss: 0.4756835997104645\n",
      "Epoch 9560: train loss: 0.4756835103034973\n",
      "Epoch 9561: train loss: 0.475683331489563\n",
      "Epoch 9562: train loss: 0.4756833016872406\n",
      "Epoch 9563: train loss: 0.4756831228733063\n",
      "Epoch 9564: train loss: 0.4756830930709839\n",
      "Epoch 9565: train loss: 0.47568297386169434\n",
      "Epoch 9566: train loss: 0.4756828844547272\n",
      "Epoch 9567: train loss: 0.4756827652454376\n",
      "Epoch 9568: train loss: 0.47568267583847046\n",
      "Epoch 9569: train loss: 0.4756825566291809\n",
      "Epoch 9570: train loss: 0.47568240761756897\n",
      "Epoch 9571: train loss: 0.4756823480129242\n",
      "Epoch 9572: train loss: 0.47568225860595703\n",
      "Epoch 9573: train loss: 0.4756821393966675\n",
      "Epoch 9574: train loss: 0.4756820499897003\n",
      "Epoch 9575: train loss: 0.47568193078041077\n",
      "Epoch 9576: train loss: 0.4756818413734436\n",
      "Epoch 9577: train loss: 0.47568178176879883\n",
      "Epoch 9578: train loss: 0.4756816327571869\n",
      "Epoch 9579: train loss: 0.47568151354789734\n",
      "Epoch 9580: train loss: 0.4756814241409302\n",
      "Epoch 9581: train loss: 0.4756813049316406\n",
      "Epoch 9582: train loss: 0.4756811857223511\n",
      "Epoch 9583: train loss: 0.4756810963153839\n",
      "Epoch 9584: train loss: 0.47568097710609436\n",
      "Epoch 9585: train loss: 0.4756808876991272\n",
      "Epoch 9586: train loss: 0.47568076848983765\n",
      "Epoch 9587: train loss: 0.4756806790828705\n",
      "Epoch 9588: train loss: 0.47568055987358093\n",
      "Epoch 9589: train loss: 0.47568053007125854\n",
      "Epoch 9590: train loss: 0.4756803512573242\n",
      "Epoch 9591: train loss: 0.47568026185035706\n",
      "Epoch 9592: train loss: 0.4756801426410675\n",
      "Epoch 9593: train loss: 0.47568005323410034\n",
      "Epoch 9594: train loss: 0.4756799340248108\n",
      "Epoch 9595: train loss: 0.47567984461784363\n",
      "Epoch 9596: train loss: 0.4756797254085541\n",
      "Epoch 9597: train loss: 0.4756796360015869\n",
      "Epoch 9598: train loss: 0.47567951679229736\n",
      "Epoch 9599: train loss: 0.4756794273853302\n",
      "Epoch 9600: train loss: 0.4756793677806854\n",
      "Epoch 9601: train loss: 0.47567927837371826\n",
      "Epoch 9602: train loss: 0.47567909955978394\n",
      "Epoch 9603: train loss: 0.47567906975746155\n",
      "Epoch 9604: train loss: 0.475678950548172\n",
      "Epoch 9605: train loss: 0.47567880153656006\n",
      "Epoch 9606: train loss: 0.4756787419319153\n",
      "Epoch 9607: train loss: 0.4756786525249481\n",
      "Epoch 9608: train loss: 0.47567853331565857\n",
      "Epoch 9609: train loss: 0.4756784439086914\n",
      "Epoch 9610: train loss: 0.4756782650947571\n",
      "Epoch 9611: train loss: 0.4756782352924347\n",
      "Epoch 9612: train loss: 0.47567811608314514\n",
      "Epoch 9613: train loss: 0.4756779670715332\n",
      "Epoch 9614: train loss: 0.4756779074668884\n",
      "Epoch 9615: train loss: 0.47567781805992126\n",
      "Epoch 9616: train loss: 0.4756776988506317\n",
      "Epoch 9617: train loss: 0.47567760944366455\n",
      "Epoch 9618: train loss: 0.475677490234375\n",
      "Epoch 9619: train loss: 0.47567737102508545\n",
      "Epoch 9620: train loss: 0.4756772816181183\n",
      "Epoch 9621: train loss: 0.47567716240882874\n",
      "Epoch 9622: train loss: 0.4756770730018616\n",
      "Epoch 9623: train loss: 0.475676953792572\n",
      "Epoch 9624: train loss: 0.47567686438560486\n",
      "Epoch 9625: train loss: 0.4756767451763153\n",
      "Epoch 9626: train loss: 0.47567665576934814\n",
      "Epoch 9627: train loss: 0.4756765365600586\n",
      "Epoch 9628: train loss: 0.47567644715309143\n",
      "Epoch 9629: train loss: 0.4756763279438019\n",
      "Epoch 9630: train loss: 0.4756762385368347\n",
      "Epoch 9631: train loss: 0.47567611932754517\n",
      "Epoch 9632: train loss: 0.475676029920578\n",
      "Epoch 9633: train loss: 0.4756759703159332\n",
      "Epoch 9634: train loss: 0.4756758213043213\n",
      "Epoch 9635: train loss: 0.47567570209503174\n",
      "Epoch 9636: train loss: 0.47567567229270935\n",
      "Epoch 9637: train loss: 0.475675493478775\n",
      "Epoch 9638: train loss: 0.475675493478775\n",
      "Epoch 9639: train loss: 0.4756752848625183\n",
      "Epoch 9640: train loss: 0.47567519545555115\n",
      "Epoch 9641: train loss: 0.47567519545555115\n",
      "Epoch 9642: train loss: 0.4756750464439392\n",
      "Epoch 9643: train loss: 0.47567492723464966\n",
      "Epoch 9644: train loss: 0.4756747782230377\n",
      "Epoch 9645: train loss: 0.47567471861839294\n",
      "Epoch 9646: train loss: 0.475674569606781\n",
      "Epoch 9647: train loss: 0.47567451000213623\n",
      "Epoch 9648: train loss: 0.4756743609905243\n",
      "Epoch 9649: train loss: 0.4756743013858795\n",
      "Epoch 9650: train loss: 0.47567421197891235\n",
      "Epoch 9651: train loss: 0.475674033164978\n",
      "Epoch 9652: train loss: 0.47567400336265564\n",
      "Epoch 9653: train loss: 0.4756738245487213\n",
      "Epoch 9654: train loss: 0.4756737947463989\n",
      "Epoch 9655: train loss: 0.47567373514175415\n",
      "Epoch 9656: train loss: 0.4756735563278198\n",
      "Epoch 9657: train loss: 0.47567346692085266\n",
      "Epoch 9658: train loss: 0.4756733477115631\n",
      "Epoch 9659: train loss: 0.47567325830459595\n",
      "Epoch 9660: train loss: 0.4756731390953064\n",
      "Epoch 9661: train loss: 0.475673109292984\n",
      "Epoch 9662: train loss: 0.47567299008369446\n",
      "Epoch 9663: train loss: 0.4756728410720825\n",
      "Epoch 9664: train loss: 0.47567278146743774\n",
      "Epoch 9665: train loss: 0.4756726324558258\n",
      "Epoch 9666: train loss: 0.47567257285118103\n",
      "Epoch 9667: train loss: 0.47567248344421387\n",
      "Epoch 9668: train loss: 0.47567230463027954\n",
      "Epoch 9669: train loss: 0.4756721556186676\n",
      "Epoch 9670: train loss: 0.4756720960140228\n",
      "Epoch 9671: train loss: 0.47567200660705566\n",
      "Epoch 9672: train loss: 0.4756718873977661\n",
      "Epoch 9673: train loss: 0.4756718575954437\n",
      "Epoch 9674: train loss: 0.4756717383861542\n",
      "Epoch 9675: train loss: 0.475671648979187\n",
      "Epoch 9676: train loss: 0.47567152976989746\n",
      "Epoch 9677: train loss: 0.4756714403629303\n",
      "Epoch 9678: train loss: 0.47567132115364075\n",
      "Epoch 9679: train loss: 0.4756712317466736\n",
      "Epoch 9680: train loss: 0.47567111253738403\n",
      "Epoch 9681: train loss: 0.47567102313041687\n",
      "Epoch 9682: train loss: 0.47567084431648254\n",
      "Epoch 9683: train loss: 0.47567081451416016\n",
      "Epoch 9684: train loss: 0.4756706953048706\n",
      "Epoch 9685: train loss: 0.47567060589790344\n",
      "Epoch 9686: train loss: 0.4756704866886139\n",
      "Epoch 9687: train loss: 0.47567039728164673\n",
      "Epoch 9688: train loss: 0.4756702780723572\n",
      "Epoch 9689: train loss: 0.47567018866539\n",
      "Epoch 9690: train loss: 0.4756700098514557\n",
      "Epoch 9691: train loss: 0.4756699800491333\n",
      "Epoch 9692: train loss: 0.47566986083984375\n",
      "Epoch 9693: train loss: 0.475669801235199\n",
      "Epoch 9694: train loss: 0.47566965222358704\n",
      "Epoch 9695: train loss: 0.4756695330142975\n",
      "Epoch 9696: train loss: 0.4756694436073303\n",
      "Epoch 9697: train loss: 0.47566932439804077\n",
      "Epoch 9698: train loss: 0.4756692945957184\n",
      "Epoch 9699: train loss: 0.47566911578178406\n",
      "Epoch 9700: train loss: 0.47566908597946167\n",
      "Epoch 9701: train loss: 0.4756689667701721\n",
      "Epoch 9702: train loss: 0.47566887736320496\n",
      "Epoch 9703: train loss: 0.47566869854927063\n",
      "Epoch 9704: train loss: 0.47566866874694824\n",
      "Epoch 9705: train loss: 0.4756685495376587\n",
      "Epoch 9706: train loss: 0.47566840052604675\n",
      "Epoch 9707: train loss: 0.475668340921402\n",
      "Epoch 9708: train loss: 0.47566819190979004\n",
      "Epoch 9709: train loss: 0.47566813230514526\n",
      "Epoch 9710: train loss: 0.4756680428981781\n",
      "Epoch 9711: train loss: 0.47566792368888855\n",
      "Epoch 9712: train loss: 0.4756678640842438\n",
      "Epoch 9713: train loss: 0.47566771507263184\n",
      "Epoch 9714: train loss: 0.4756676256656647\n",
      "Epoch 9715: train loss: 0.4756675064563751\n",
      "Epoch 9716: train loss: 0.47566741704940796\n",
      "Epoch 9717: train loss: 0.4756672978401184\n",
      "Epoch 9718: train loss: 0.47566723823547363\n",
      "Epoch 9719: train loss: 0.4756670892238617\n",
      "Epoch 9720: train loss: 0.4756670296192169\n",
      "Epoch 9721: train loss: 0.47566694021224976\n",
      "Epoch 9722: train loss: 0.4756668210029602\n",
      "Epoch 9723: train loss: 0.47566673159599304\n",
      "Epoch 9724: train loss: 0.4756665825843811\n",
      "Epoch 9725: train loss: 0.47566652297973633\n",
      "Epoch 9726: train loss: 0.4756664037704468\n",
      "Epoch 9727: train loss: 0.4756663143634796\n",
      "Epoch 9728: train loss: 0.47566625475883484\n",
      "Epoch 9729: train loss: 0.4756661057472229\n",
      "Epoch 9730: train loss: 0.47566598653793335\n",
      "Epoch 9731: train loss: 0.4756658971309662\n",
      "Epoch 9732: train loss: 0.47566577792167664\n",
      "Epoch 9733: train loss: 0.4756656885147095\n",
      "Epoch 9734: train loss: 0.4756656289100647\n",
      "Epoch 9735: train loss: 0.47566547989845276\n",
      "Epoch 9736: train loss: 0.4756653606891632\n",
      "Epoch 9737: train loss: 0.47566530108451843\n",
      "Epoch 9738: train loss: 0.4756651520729065\n",
      "Epoch 9739: train loss: 0.47566506266593933\n",
      "Epoch 9740: train loss: 0.47566500306129456\n",
      "Epoch 9741: train loss: 0.475664883852005\n",
      "Epoch 9742: train loss: 0.47566473484039307\n",
      "Epoch 9743: train loss: 0.4756646752357483\n",
      "Epoch 9744: train loss: 0.47566452622413635\n",
      "Epoch 9745: train loss: 0.4756644666194916\n",
      "Epoch 9746: train loss: 0.47566431760787964\n",
      "Epoch 9747: train loss: 0.47566425800323486\n",
      "Epoch 9748: train loss: 0.4756641685962677\n",
      "Epoch 9749: train loss: 0.47566404938697815\n",
      "Epoch 9750: train loss: 0.475663959980011\n",
      "Epoch 9751: train loss: 0.4756639003753662\n",
      "Epoch 9752: train loss: 0.4756637513637543\n",
      "Epoch 9753: train loss: 0.4756636917591095\n",
      "Epoch 9754: train loss: 0.47566354274749756\n",
      "Epoch 9755: train loss: 0.4756634831428528\n",
      "Epoch 9756: train loss: 0.4756633937358856\n",
      "Epoch 9757: train loss: 0.47566327452659607\n",
      "Epoch 9758: train loss: 0.47566312551498413\n",
      "Epoch 9759: train loss: 0.47566306591033936\n",
      "Epoch 9760: train loss: 0.4756629765033722\n",
      "Epoch 9761: train loss: 0.47566285729408264\n",
      "Epoch 9762: train loss: 0.47566279768943787\n",
      "Epoch 9763: train loss: 0.4756626486778259\n",
      "Epoch 9764: train loss: 0.47566255927085876\n",
      "Epoch 9765: train loss: 0.4756624400615692\n",
      "Epoch 9766: train loss: 0.47566238045692444\n",
      "Epoch 9767: train loss: 0.4756622314453125\n",
      "Epoch 9768: train loss: 0.47566211223602295\n",
      "Epoch 9769: train loss: 0.47566208243370056\n",
      "Epoch 9770: train loss: 0.475661963224411\n",
      "Epoch 9771: train loss: 0.47566187381744385\n",
      "Epoch 9772: train loss: 0.4756617546081543\n",
      "Epoch 9773: train loss: 0.47566166520118713\n",
      "Epoch 9774: train loss: 0.47566160559654236\n",
      "Epoch 9775: train loss: 0.4756614565849304\n",
      "Epoch 9776: train loss: 0.47566133737564087\n",
      "Epoch 9777: train loss: 0.4756612479686737\n",
      "Epoch 9778: train loss: 0.47566112875938416\n",
      "Epoch 9779: train loss: 0.475661039352417\n",
      "Epoch 9780: train loss: 0.47566092014312744\n",
      "Epoch 9781: train loss: 0.4756608307361603\n",
      "Epoch 9782: train loss: 0.4756607115268707\n",
      "Epoch 9783: train loss: 0.47566065192222595\n",
      "Epoch 9784: train loss: 0.4756605625152588\n",
      "Epoch 9785: train loss: 0.47566044330596924\n",
      "Epoch 9786: train loss: 0.4756603538990021\n",
      "Epoch 9787: train loss: 0.4756602346897125\n",
      "Epoch 9788: train loss: 0.47566014528274536\n",
      "Epoch 9789: train loss: 0.4756600260734558\n",
      "Epoch 9790: train loss: 0.47565987706184387\n",
      "Epoch 9791: train loss: 0.4756598174571991\n",
      "Epoch 9792: train loss: 0.4756597876548767\n",
      "Epoch 9793: train loss: 0.4756596088409424\n",
      "Epoch 9794: train loss: 0.47565957903862\n",
      "Epoch 9795: train loss: 0.47565945982933044\n",
      "Epoch 9796: train loss: 0.4756593704223633\n",
      "Epoch 9797: train loss: 0.47565925121307373\n",
      "Epoch 9798: train loss: 0.47565916180610657\n",
      "Epoch 9799: train loss: 0.475659042596817\n",
      "Epoch 9800: train loss: 0.47565895318984985\n",
      "Epoch 9801: train loss: 0.4756588339805603\n",
      "Epoch 9802: train loss: 0.47565874457359314\n",
      "Epoch 9803: train loss: 0.4756586253643036\n",
      "Epoch 9804: train loss: 0.47565847635269165\n",
      "Epoch 9805: train loss: 0.4756584167480469\n",
      "Epoch 9806: train loss: 0.4756582975387573\n",
      "Epoch 9807: train loss: 0.47565820813179016\n",
      "Epoch 9808: train loss: 0.4756580889225006\n",
      "Epoch 9809: train loss: 0.47565799951553345\n",
      "Epoch 9810: train loss: 0.4756578803062439\n",
      "Epoch 9811: train loss: 0.4756578505039215\n",
      "Epoch 9812: train loss: 0.47565773129463196\n",
      "Epoch 9813: train loss: 0.4756576418876648\n",
      "Epoch 9814: train loss: 0.47565752267837524\n",
      "Epoch 9815: train loss: 0.4756574332714081\n",
      "Epoch 9816: train loss: 0.47565731406211853\n",
      "Epoch 9817: train loss: 0.47565722465515137\n",
      "Epoch 9818: train loss: 0.4756571054458618\n",
      "Epoch 9819: train loss: 0.47565701603889465\n",
      "Epoch 9820: train loss: 0.4756568968296051\n",
      "Epoch 9821: train loss: 0.47565680742263794\n",
      "Epoch 9822: train loss: 0.4756566882133484\n",
      "Epoch 9823: train loss: 0.4756566286087036\n",
      "Epoch 9824: train loss: 0.47565653920173645\n",
      "Epoch 9825: train loss: 0.4756563901901245\n",
      "Epoch 9826: train loss: 0.47565633058547974\n",
      "Epoch 9827: train loss: 0.4756562113761902\n",
      "Epoch 9828: train loss: 0.47565606236457825\n",
      "Epoch 9829: train loss: 0.47565600275993347\n",
      "Epoch 9830: train loss: 0.4756559133529663\n",
      "Epoch 9831: train loss: 0.47565579414367676\n",
      "Epoch 9832: train loss: 0.4756557047367096\n",
      "Epoch 9833: train loss: 0.47565558552742004\n",
      "Epoch 9834: train loss: 0.4756554961204529\n",
      "Epoch 9835: train loss: 0.4756554365158081\n",
      "Epoch 9836: train loss: 0.47565528750419617\n",
      "Epoch 9837: train loss: 0.4756552278995514\n",
      "Epoch 9838: train loss: 0.47565513849258423\n",
      "Epoch 9839: train loss: 0.4756550192832947\n",
      "Epoch 9840: train loss: 0.4756549298763275\n",
      "Epoch 9841: train loss: 0.47565481066703796\n",
      "Epoch 9842: train loss: 0.4756547212600708\n",
      "Epoch 9843: train loss: 0.47565460205078125\n",
      "Epoch 9844: train loss: 0.4756545424461365\n",
      "Epoch 9845: train loss: 0.47565439343452454\n",
      "Epoch 9846: train loss: 0.475654274225235\n",
      "Epoch 9847: train loss: 0.4756541848182678\n",
      "Epoch 9848: train loss: 0.47565406560897827\n",
      "Epoch 9849: train loss: 0.4756540358066559\n",
      "Epoch 9850: train loss: 0.47565385699272156\n",
      "Epoch 9851: train loss: 0.47565382719039917\n",
      "Epoch 9852: train loss: 0.4756537079811096\n",
      "Epoch 9853: train loss: 0.47565361857414246\n",
      "Epoch 9854: train loss: 0.4756534993648529\n",
      "Epoch 9855: train loss: 0.47565343976020813\n",
      "Epoch 9856: train loss: 0.47565335035324097\n",
      "Epoch 9857: train loss: 0.47565320134162903\n",
      "Epoch 9858: train loss: 0.4756530821323395\n",
      "Epoch 9859: train loss: 0.4756529927253723\n",
      "Epoch 9860: train loss: 0.47565293312072754\n",
      "Epoch 9861: train loss: 0.4756527841091156\n",
      "Epoch 9862: train loss: 0.47565266489982605\n",
      "Epoch 9863: train loss: 0.4756526052951813\n",
      "Epoch 9864: train loss: 0.4756525158882141\n",
      "Epoch 9865: train loss: 0.47565239667892456\n",
      "Epoch 9866: train loss: 0.4756523072719574\n",
      "Epoch 9867: train loss: 0.47565218806266785\n",
      "Epoch 9868: train loss: 0.4756520986557007\n",
      "Epoch 9869: train loss: 0.47565197944641113\n",
      "Epoch 9870: train loss: 0.47565194964408875\n",
      "Epoch 9871: train loss: 0.4756517708301544\n",
      "Epoch 9872: train loss: 0.47565168142318726\n",
      "Epoch 9873: train loss: 0.4756516218185425\n",
      "Epoch 9874: train loss: 0.47565147280693054\n",
      "Epoch 9875: train loss: 0.47565141320228577\n",
      "Epoch 9876: train loss: 0.4756513237953186\n",
      "Epoch 9877: train loss: 0.47565120458602905\n",
      "Epoch 9878: train loss: 0.4756511151790619\n",
      "Epoch 9879: train loss: 0.47565099596977234\n",
      "Epoch 9880: train loss: 0.47565093636512756\n",
      "Epoch 9881: train loss: 0.4756507873535156\n",
      "Epoch 9882: train loss: 0.47565072774887085\n",
      "Epoch 9883: train loss: 0.4756505787372589\n",
      "Epoch 9884: train loss: 0.47565051913261414\n",
      "Epoch 9885: train loss: 0.475650429725647\n",
      "Epoch 9886: train loss: 0.4756503105163574\n",
      "Epoch 9887: train loss: 0.47565022110939026\n",
      "Epoch 9888: train loss: 0.4756501019001007\n",
      "Epoch 9889: train loss: 0.47565001249313354\n",
      "Epoch 9890: train loss: 0.475649893283844\n",
      "Epoch 9891: train loss: 0.4756498336791992\n",
      "Epoch 9892: train loss: 0.4756496846675873\n",
      "Epoch 9893: train loss: 0.4756496250629425\n",
      "Epoch 9894: train loss: 0.47564947605133057\n",
      "Epoch 9895: train loss: 0.4756494164466858\n",
      "Epoch 9896: train loss: 0.47564932703971863\n",
      "Epoch 9897: train loss: 0.4756492078304291\n",
      "Epoch 9898: train loss: 0.47564905881881714\n",
      "Epoch 9899: train loss: 0.47564899921417236\n",
      "Epoch 9900: train loss: 0.4756489098072052\n",
      "Epoch 9901: train loss: 0.4756488502025604\n",
      "Epoch 9902: train loss: 0.47564876079559326\n",
      "Epoch 9903: train loss: 0.47564858198165894\n",
      "Epoch 9904: train loss: 0.47564855217933655\n",
      "Epoch 9905: train loss: 0.475648432970047\n",
      "Epoch 9906: train loss: 0.4756483733654022\n",
      "Epoch 9907: train loss: 0.4756482243537903\n",
      "Epoch 9908: train loss: 0.4756481647491455\n",
      "Epoch 9909: train loss: 0.47564801573753357\n",
      "Epoch 9910: train loss: 0.4756479263305664\n",
      "Epoch 9911: train loss: 0.47564780712127686\n",
      "Epoch 9912: train loss: 0.4756477475166321\n",
      "Epoch 9913: train loss: 0.4756476581096649\n",
      "Epoch 9914: train loss: 0.47564753890037537\n",
      "Epoch 9915: train loss: 0.4756474494934082\n",
      "Epoch 9916: train loss: 0.47564733028411865\n",
      "Epoch 9917: train loss: 0.47564730048179626\n",
      "Epoch 9918: train loss: 0.47564712166786194\n",
      "Epoch 9919: train loss: 0.47564709186553955\n",
      "Epoch 9920: train loss: 0.4756469130516052\n",
      "Epoch 9921: train loss: 0.47564685344696045\n",
      "Epoch 9922: train loss: 0.4756467640399933\n",
      "Epoch 9923: train loss: 0.47564661502838135\n",
      "Epoch 9924: train loss: 0.4756464958190918\n",
      "Epoch 9925: train loss: 0.475646436214447\n",
      "Epoch 9926: train loss: 0.4756462872028351\n",
      "Epoch 9927: train loss: 0.4756462275981903\n",
      "Epoch 9928: train loss: 0.47564613819122314\n",
      "Epoch 9929: train loss: 0.47564607858657837\n",
      "Epoch 9930: train loss: 0.47564592957496643\n",
      "Epoch 9931: train loss: 0.47564586997032166\n",
      "Epoch 9932: train loss: 0.4756457805633545\n",
      "Epoch 9933: train loss: 0.47564566135406494\n",
      "Epoch 9934: train loss: 0.475645512342453\n",
      "Epoch 9935: train loss: 0.4756454527378082\n",
      "Epoch 9936: train loss: 0.47564536333084106\n",
      "Epoch 9937: train loss: 0.4756452441215515\n",
      "Epoch 9938: train loss: 0.47564515471458435\n",
      "Epoch 9939: train loss: 0.4756450951099396\n",
      "Epoch 9940: train loss: 0.47564494609832764\n",
      "Epoch 9941: train loss: 0.47564488649368286\n",
      "Epoch 9942: train loss: 0.4756447672843933\n",
      "Epoch 9943: train loss: 0.47564467787742615\n",
      "Epoch 9944: train loss: 0.47564461827278137\n",
      "Epoch 9945: train loss: 0.47564446926116943\n",
      "Epoch 9946: train loss: 0.4756443500518799\n",
      "Epoch 9947: train loss: 0.4756443202495575\n",
      "Epoch 9948: train loss: 0.47564420104026794\n",
      "Epoch 9949: train loss: 0.475644052028656\n",
      "Epoch 9950: train loss: 0.47564393281936646\n",
      "Epoch 9951: train loss: 0.47564390301704407\n",
      "Epoch 9952: train loss: 0.4756437838077545\n",
      "Epoch 9953: train loss: 0.47564369440078735\n",
      "Epoch 9954: train loss: 0.4756435751914978\n",
      "Epoch 9955: train loss: 0.47564348578453064\n",
      "Epoch 9956: train loss: 0.4756433665752411\n",
      "Epoch 9957: train loss: 0.4756432771682739\n",
      "Epoch 9958: train loss: 0.47564321756362915\n",
      "Epoch 9959: train loss: 0.4756430983543396\n",
      "Epoch 9960: train loss: 0.47564300894737244\n",
      "Epoch 9961: train loss: 0.4756428897380829\n",
      "Epoch 9962: train loss: 0.4756428003311157\n",
      "Epoch 9963: train loss: 0.47564274072647095\n",
      "Epoch 9964: train loss: 0.4756426215171814\n",
      "Epoch 9965: train loss: 0.47564253211021423\n",
      "Epoch 9966: train loss: 0.4756423830986023\n",
      "Epoch 9967: train loss: 0.4756423830986023\n",
      "Epoch 9968: train loss: 0.47564220428466797\n",
      "Epoch 9969: train loss: 0.4756421744823456\n",
      "Epoch 9970: train loss: 0.47564199566841125\n",
      "Epoch 9971: train loss: 0.47564196586608887\n",
      "Epoch 9972: train loss: 0.4756418466567993\n",
      "Epoch 9973: train loss: 0.4756416976451874\n",
      "Epoch 9974: train loss: 0.4756415784358978\n",
      "Epoch 9975: train loss: 0.47564148902893066\n",
      "Epoch 9976: train loss: 0.4756414294242859\n",
      "Epoch 9977: train loss: 0.4756413400173187\n",
      "Epoch 9978: train loss: 0.4756412208080292\n",
      "Epoch 9979: train loss: 0.475641131401062\n",
      "Epoch 9980: train loss: 0.47564101219177246\n",
      "Epoch 9981: train loss: 0.4756409227848053\n",
      "Epoch 9982: train loss: 0.47564080357551575\n",
      "Epoch 9983: train loss: 0.47564074397087097\n",
      "Epoch 9984: train loss: 0.4756406545639038\n",
      "Epoch 9985: train loss: 0.47564053535461426\n",
      "Epoch 9986: train loss: 0.4756404459476471\n",
      "Epoch 9987: train loss: 0.47564032673835754\n",
      "Epoch 9988: train loss: 0.4756402373313904\n",
      "Epoch 9989: train loss: 0.47564011812210083\n",
      "Epoch 9990: train loss: 0.47564008831977844\n",
      "Epoch 9991: train loss: 0.4756399691104889\n",
      "Epoch 9992: train loss: 0.47563987970352173\n",
      "Epoch 9993: train loss: 0.4756397604942322\n",
      "Epoch 9994: train loss: 0.475639671087265\n",
      "Epoch 9995: train loss: 0.47563955187797546\n",
      "Epoch 9996: train loss: 0.4756394624710083\n",
      "Epoch 9997: train loss: 0.47563934326171875\n",
      "Epoch 9998: train loss: 0.475639283657074\n",
      "Epoch 9999: train loss: 0.47563913464546204\n",
      "Epoch 10000: train loss: 0.47563907504081726\n",
      "Epoch 10001: train loss: 0.4756389260292053\n",
      "Epoch 10002: train loss: 0.47563886642456055\n",
      "Epoch 10003: train loss: 0.4756387770175934\n",
      "Epoch 10004: train loss: 0.4756387174129486\n",
      "Epoch 10005: train loss: 0.47563856840133667\n",
      "Epoch 10006: train loss: 0.4756384491920471\n",
      "Epoch 10007: train loss: 0.47563838958740234\n",
      "Epoch 10008: train loss: 0.4756383001804352\n",
      "Epoch 10009: train loss: 0.47563818097114563\n",
      "Epoch 10010: train loss: 0.47563809156417847\n",
      "Epoch 10011: train loss: 0.4756379723548889\n",
      "Epoch 10012: train loss: 0.47563788294792175\n",
      "Epoch 10013: train loss: 0.4756377637386322\n",
      "Epoch 10014: train loss: 0.47563767433166504\n",
      "Epoch 10015: train loss: 0.47563761472702026\n",
      "Epoch 10016: train loss: 0.4756375253200531\n",
      "Epoch 10017: train loss: 0.4756374657154083\n",
      "Epoch 10018: train loss: 0.4756373167037964\n",
      "Epoch 10019: train loss: 0.4756372570991516\n",
      "Epoch 10020: train loss: 0.4756371080875397\n",
      "Epoch 10021: train loss: 0.4756370484828949\n",
      "Epoch 10022: train loss: 0.47563692927360535\n",
      "Epoch 10023: train loss: 0.4756368398666382\n",
      "Epoch 10024: train loss: 0.47563672065734863\n",
      "Epoch 10025: train loss: 0.47563663125038147\n",
      "Epoch 10026: train loss: 0.4756365716457367\n",
      "Epoch 10027: train loss: 0.47563642263412476\n",
      "Epoch 10028: train loss: 0.4756363034248352\n",
      "Epoch 10029: train loss: 0.4756362736225128\n",
      "Epoch 10030: train loss: 0.4756360948085785\n",
      "Epoch 10031: train loss: 0.4756360650062561\n",
      "Epoch 10032: train loss: 0.47563594579696655\n",
      "Epoch 10033: train loss: 0.4756358563899994\n",
      "Epoch 10034: train loss: 0.47563573718070984\n",
      "Epoch 10035: train loss: 0.4756356477737427\n",
      "Epoch 10036: train loss: 0.4756355881690979\n",
      "Epoch 10037: train loss: 0.47563546895980835\n",
      "Epoch 10038: train loss: 0.4756353795528412\n",
      "Epoch 10039: train loss: 0.47563526034355164\n",
      "Epoch 10040: train loss: 0.4756351709365845\n",
      "Epoch 10041: train loss: 0.4756350517272949\n",
      "Epoch 10042: train loss: 0.47563496232032776\n",
      "Epoch 10043: train loss: 0.475634902715683\n",
      "Epoch 10044: train loss: 0.47563475370407104\n",
      "Epoch 10045: train loss: 0.4756346344947815\n",
      "Epoch 10046: train loss: 0.4756345748901367\n",
      "Epoch 10047: train loss: 0.47563454508781433\n",
      "Epoch 10048: train loss: 0.4756344258785248\n",
      "Epoch 10049: train loss: 0.47563427686691284\n",
      "Epoch 10050: train loss: 0.47563421726226807\n",
      "Epoch 10051: train loss: 0.4756341278553009\n",
      "Epoch 10052: train loss: 0.47563400864601135\n",
      "Epoch 10053: train loss: 0.4756339192390442\n",
      "Epoch 10054: train loss: 0.47563380002975464\n",
      "Epoch 10055: train loss: 0.4756337106227875\n",
      "Epoch 10056: train loss: 0.4756335914134979\n",
      "Epoch 10057: train loss: 0.47563350200653076\n",
      "Epoch 10058: train loss: 0.4756333827972412\n",
      "Epoch 10059: train loss: 0.47563332319259644\n",
      "Epoch 10060: train loss: 0.4756332337856293\n",
      "Epoch 10061: train loss: 0.47563308477401733\n",
      "Epoch 10062: train loss: 0.4756329655647278\n",
      "Epoch 10063: train loss: 0.475632905960083\n",
      "Epoch 10064: train loss: 0.4756328761577606\n",
      "Epoch 10065: train loss: 0.4756326973438263\n",
      "Epoch 10066: train loss: 0.4756326675415039\n",
      "Epoch 10067: train loss: 0.47563254833221436\n",
      "Epoch 10068: train loss: 0.4756324589252472\n",
      "Epoch 10069: train loss: 0.47563233971595764\n",
      "Epoch 10070: train loss: 0.47563228011131287\n",
      "Epoch 10071: train loss: 0.4756321310997009\n",
      "Epoch 10072: train loss: 0.47563204169273376\n",
      "Epoch 10073: train loss: 0.4756319224834442\n",
      "Epoch 10074: train loss: 0.47563186287879944\n",
      "Epoch 10075: train loss: 0.4756317734718323\n",
      "Epoch 10076: train loss: 0.4756316542625427\n",
      "Epoch 10077: train loss: 0.47563156485557556\n",
      "Epoch 10078: train loss: 0.4756315052509308\n",
      "Epoch 10079: train loss: 0.47563138604164124\n",
      "Epoch 10080: train loss: 0.4756312966346741\n",
      "Epoch 10081: train loss: 0.4756311774253845\n",
      "Epoch 10082: train loss: 0.47563108801841736\n",
      "Epoch 10083: train loss: 0.4756309688091278\n",
      "Epoch 10084: train loss: 0.4756309390068054\n",
      "Epoch 10085: train loss: 0.47563081979751587\n",
      "Epoch 10086: train loss: 0.47563067078590393\n",
      "Epoch 10087: train loss: 0.47563061118125916\n",
      "Epoch 10088: train loss: 0.475630521774292\n",
      "Epoch 10089: train loss: 0.4756304621696472\n",
      "Epoch 10090: train loss: 0.4756303131580353\n",
      "Epoch 10091: train loss: 0.4756301939487457\n",
      "Epoch 10092: train loss: 0.47563013434410095\n",
      "Epoch 10093: train loss: 0.4756300449371338\n",
      "Epoch 10094: train loss: 0.47562992572784424\n",
      "Epoch 10095: train loss: 0.4756298363208771\n",
      "Epoch 10096: train loss: 0.4756297171115875\n",
      "Epoch 10097: train loss: 0.47562968730926514\n",
      "Epoch 10098: train loss: 0.4756295680999756\n",
      "Epoch 10099: train loss: 0.47562941908836365\n",
      "Epoch 10100: train loss: 0.4756292998790741\n",
      "Epoch 10101: train loss: 0.4756292700767517\n",
      "Epoch 10102: train loss: 0.47562915086746216\n",
      "Epoch 10103: train loss: 0.475629061460495\n",
      "Epoch 10104: train loss: 0.47562888264656067\n",
      "Epoch 10105: train loss: 0.4756288528442383\n",
      "Epoch 10106: train loss: 0.4756287932395935\n",
      "Epoch 10107: train loss: 0.47562867403030396\n",
      "Epoch 10108: train loss: 0.4756285846233368\n",
      "Epoch 10109: train loss: 0.47562846541404724\n",
      "Epoch 10110: train loss: 0.4756283760070801\n",
      "Epoch 10111: train loss: 0.4756282567977905\n",
      "Epoch 10112: train loss: 0.47562816739082336\n",
      "Epoch 10113: train loss: 0.4756280481815338\n",
      "Epoch 10114: train loss: 0.4756280183792114\n",
      "Epoch 10115: train loss: 0.4756278395652771\n",
      "Epoch 10116: train loss: 0.4756277799606323\n",
      "Epoch 10117: train loss: 0.47562769055366516\n",
      "Epoch 10118: train loss: 0.4756276309490204\n",
      "Epoch 10119: train loss: 0.4756275415420532\n",
      "Epoch 10120: train loss: 0.47562742233276367\n",
      "Epoch 10121: train loss: 0.4756273329257965\n",
      "Epoch 10122: train loss: 0.47562721371650696\n",
      "Epoch 10123: train loss: 0.4756271243095398\n",
      "Epoch 10124: train loss: 0.47562700510025024\n",
      "Epoch 10125: train loss: 0.47562694549560547\n",
      "Epoch 10126: train loss: 0.47562679648399353\n",
      "Epoch 10127: train loss: 0.47562670707702637\n",
      "Epoch 10128: train loss: 0.4756266474723816\n",
      "Epoch 10129: train loss: 0.4756265878677368\n",
      "Epoch 10130: train loss: 0.47562649846076965\n",
      "Epoch 10131: train loss: 0.4756263792514801\n",
      "Epoch 10132: train loss: 0.47562628984451294\n",
      "Epoch 10133: train loss: 0.4756261706352234\n",
      "Epoch 10134: train loss: 0.4756260812282562\n",
      "Epoch 10135: train loss: 0.4756259620189667\n",
      "Epoch 10136: train loss: 0.4756258726119995\n",
      "Epoch 10137: train loss: 0.47562575340270996\n",
      "Epoch 10138: train loss: 0.4756256937980652\n",
      "Epoch 10139: train loss: 0.475625604391098\n",
      "Epoch 10140: train loss: 0.47562548518180847\n",
      "Epoch 10141: train loss: 0.47562533617019653\n",
      "Epoch 10142: train loss: 0.47562533617019653\n",
      "Epoch 10143: train loss: 0.4756251871585846\n",
      "Epoch 10144: train loss: 0.47562506794929504\n",
      "Epoch 10145: train loss: 0.47562503814697266\n",
      "Epoch 10146: train loss: 0.4756249189376831\n",
      "Epoch 10147: train loss: 0.47562482953071594\n",
      "Epoch 10148: train loss: 0.4756247103214264\n",
      "Epoch 10149: train loss: 0.47562462091445923\n",
      "Epoch 10150: train loss: 0.47562456130981445\n",
      "Epoch 10151: train loss: 0.4756244421005249\n",
      "Epoch 10152: train loss: 0.47562429308891296\n",
      "Epoch 10153: train loss: 0.4756242334842682\n",
      "Epoch 10154: train loss: 0.4756242036819458\n",
      "Epoch 10155: train loss: 0.4756239652633667\n",
      "Epoch 10156: train loss: 0.4756239354610443\n",
      "Epoch 10157: train loss: 0.47562387585639954\n",
      "Epoch 10158: train loss: 0.47562375664711\n",
      "Epoch 10159: train loss: 0.4756236672401428\n",
      "Epoch 10160: train loss: 0.47562354803085327\n",
      "Epoch 10161: train loss: 0.4756234586238861\n",
      "Epoch 10162: train loss: 0.47562333941459656\n",
      "Epoch 10163: train loss: 0.4756232500076294\n",
      "Epoch 10164: train loss: 0.4756231904029846\n",
      "Epoch 10165: train loss: 0.47562310099601746\n",
      "Epoch 10166: train loss: 0.4756229817867279\n",
      "Epoch 10167: train loss: 0.47562292218208313\n",
      "Epoch 10168: train loss: 0.47562283277511597\n",
      "Epoch 10169: train loss: 0.4756227135658264\n",
      "Epoch 10170: train loss: 0.47562262415885925\n",
      "Epoch 10171: train loss: 0.4756225049495697\n",
      "Epoch 10172: train loss: 0.47562241554260254\n",
      "Epoch 10173: train loss: 0.475622296333313\n",
      "Epoch 10174: train loss: 0.4756222665309906\n",
      "Epoch 10175: train loss: 0.47562214732170105\n",
      "Epoch 10176: train loss: 0.4756219983100891\n",
      "Epoch 10177: train loss: 0.47562193870544434\n",
      "Epoch 10178: train loss: 0.4756218492984772\n",
      "Epoch 10179: train loss: 0.4756217300891876\n",
      "Epoch 10180: train loss: 0.47562164068222046\n",
      "Epoch 10181: train loss: 0.4756215810775757\n",
      "Epoch 10182: train loss: 0.47562146186828613\n",
      "Epoch 10183: train loss: 0.47562137246131897\n",
      "Epoch 10184: train loss: 0.4756213128566742\n",
      "Epoch 10185: train loss: 0.47562116384506226\n",
      "Epoch 10186: train loss: 0.4756211042404175\n",
      "Epoch 10187: train loss: 0.4756210148334503\n",
      "Epoch 10188: train loss: 0.47562095522880554\n",
      "Epoch 10189: train loss: 0.4756208062171936\n",
      "Epoch 10190: train loss: 0.47562068700790405\n",
      "Epoch 10191: train loss: 0.4756206274032593\n",
      "Epoch 10192: train loss: 0.4756205379962921\n",
      "Epoch 10193: train loss: 0.47562041878700256\n",
      "Epoch 10194: train loss: 0.4756203293800354\n",
      "Epoch 10195: train loss: 0.47562021017074585\n",
      "Epoch 10196: train loss: 0.4756200611591339\n",
      "Epoch 10197: train loss: 0.47562000155448914\n",
      "Epoch 10198: train loss: 0.475619912147522\n",
      "Epoch 10199: train loss: 0.4756198525428772\n",
      "Epoch 10200: train loss: 0.47561970353126526\n",
      "Epoch 10201: train loss: 0.4756196439266205\n",
      "Epoch 10202: train loss: 0.47561952471733093\n",
      "Epoch 10203: train loss: 0.47561943531036377\n",
      "Epoch 10204: train loss: 0.475619375705719\n",
      "Epoch 10205: train loss: 0.47561928629875183\n",
      "Epoch 10206: train loss: 0.4756191670894623\n",
      "Epoch 10207: train loss: 0.4756190776824951\n",
      "Epoch 10208: train loss: 0.47561895847320557\n",
      "Epoch 10209: train loss: 0.47561895847320557\n",
      "Epoch 10210: train loss: 0.47561880946159363\n",
      "Epoch 10211: train loss: 0.4756186604499817\n",
      "Epoch 10212: train loss: 0.4756186008453369\n",
      "Epoch 10213: train loss: 0.47561848163604736\n",
      "Epoch 10214: train loss: 0.475618451833725\n",
      "Epoch 10215: train loss: 0.4756183326244354\n",
      "Epoch 10216: train loss: 0.47561824321746826\n",
      "Epoch 10217: train loss: 0.4756181836128235\n",
      "Epoch 10218: train loss: 0.47561803460121155\n",
      "Epoch 10219: train loss: 0.475617915391922\n",
      "Epoch 10220: train loss: 0.475617915391922\n",
      "Epoch 10221: train loss: 0.47561776638031006\n",
      "Epoch 10222: train loss: 0.4756176471710205\n",
      "Epoch 10223: train loss: 0.47561755776405334\n",
      "Epoch 10224: train loss: 0.4756174385547638\n",
      "Epoch 10225: train loss: 0.4756174087524414\n",
      "Epoch 10226: train loss: 0.4756172299385071\n",
      "Epoch 10227: train loss: 0.4756172001361847\n",
      "Epoch 10228: train loss: 0.47561708092689514\n",
      "Epoch 10229: train loss: 0.475616991519928\n",
      "Epoch 10230: train loss: 0.4756168723106384\n",
      "Epoch 10231: train loss: 0.47561678290367126\n",
      "Epoch 10232: train loss: 0.4756167232990265\n",
      "Epoch 10233: train loss: 0.47561660408973694\n",
      "Epoch 10234: train loss: 0.47561657428741455\n",
      "Epoch 10235: train loss: 0.4756163954734802\n",
      "Epoch 10236: train loss: 0.47561630606651306\n",
      "Epoch 10237: train loss: 0.4756162464618683\n",
      "Epoch 10238: train loss: 0.47561612725257874\n",
      "Epoch 10239: train loss: 0.4756160378456116\n",
      "Epoch 10240: train loss: 0.475615918636322\n",
      "Epoch 10241: train loss: 0.47561582922935486\n",
      "Epoch 10242: train loss: 0.4756157100200653\n",
      "Epoch 10243: train loss: 0.4756156802177429\n",
      "Epoch 10244: train loss: 0.47561556100845337\n",
      "Epoch 10245: train loss: 0.4756154716014862\n",
      "Epoch 10246: train loss: 0.47561541199684143\n",
      "Epoch 10247: train loss: 0.4756152629852295\n",
      "Epoch 10248: train loss: 0.4756152033805847\n",
      "Epoch 10249: train loss: 0.47561514377593994\n",
      "Epoch 10250: train loss: 0.4756150543689728\n",
      "Epoch 10251: train loss: 0.47561487555503845\n",
      "Epoch 10252: train loss: 0.47561484575271606\n",
      "Epoch 10253: train loss: 0.4756147265434265\n",
      "Epoch 10254: train loss: 0.47561463713645935\n",
      "Epoch 10255: train loss: 0.4756145179271698\n",
      "Epoch 10256: train loss: 0.475614458322525\n",
      "Epoch 10257: train loss: 0.4756143093109131\n",
      "Epoch 10258: train loss: 0.4756142497062683\n",
      "Epoch 10259: train loss: 0.47561416029930115\n",
      "Epoch 10260: train loss: 0.4756140410900116\n",
      "Epoch 10261: train loss: 0.47561395168304443\n",
      "Epoch 10262: train loss: 0.4756138324737549\n",
      "Epoch 10263: train loss: 0.4756137430667877\n",
      "Epoch 10264: train loss: 0.47561368346214294\n",
      "Epoch 10265: train loss: 0.4756135940551758\n",
      "Epoch 10266: train loss: 0.475613534450531\n",
      "Epoch 10267: train loss: 0.47561347484588623\n",
      "Epoch 10268: train loss: 0.4756132662296295\n",
      "Epoch 10269: train loss: 0.47561320662498474\n",
      "Epoch 10270: train loss: 0.4756131172180176\n",
      "Epoch 10271: train loss: 0.475612998008728\n",
      "Epoch 10272: train loss: 0.47561290860176086\n",
      "Epoch 10273: train loss: 0.4756128489971161\n",
      "Epoch 10274: train loss: 0.4756127595901489\n",
      "Epoch 10275: train loss: 0.4756125807762146\n",
      "Epoch 10276: train loss: 0.4756125211715698\n",
      "Epoch 10277: train loss: 0.47561243176460266\n",
      "Epoch 10278: train loss: 0.4756123721599579\n",
      "Epoch 10279: train loss: 0.4756122827529907\n",
      "Epoch 10280: train loss: 0.47561216354370117\n",
      "Epoch 10281: train loss: 0.4756121039390564\n",
      "Epoch 10282: train loss: 0.47561195492744446\n",
      "Epoch 10283: train loss: 0.4756118655204773\n",
      "Epoch 10284: train loss: 0.4756118059158325\n",
      "Epoch 10285: train loss: 0.47561168670654297\n",
      "Epoch 10286: train loss: 0.4756115972995758\n",
      "Epoch 10287: train loss: 0.47561147809028625\n",
      "Epoch 10288: train loss: 0.4756113886833191\n",
      "Epoch 10289: train loss: 0.4756113290786743\n",
      "Epoch 10290: train loss: 0.47561123967170715\n",
      "Epoch 10291: train loss: 0.4756111204624176\n",
      "Epoch 10292: train loss: 0.47561103105545044\n",
      "Epoch 10293: train loss: 0.4756109118461609\n",
      "Epoch 10294: train loss: 0.4756108224391937\n",
      "Epoch 10295: train loss: 0.4756107032299042\n",
      "Epoch 10296: train loss: 0.4756106436252594\n",
      "Epoch 10297: train loss: 0.47561055421829224\n",
      "Epoch 10298: train loss: 0.4756104350090027\n",
      "Epoch 10299: train loss: 0.4756103456020355\n",
      "Epoch 10300: train loss: 0.47561022639274597\n",
      "Epoch 10301: train loss: 0.4756101965904236\n",
      "Epoch 10302: train loss: 0.47561007738113403\n",
      "Epoch 10303: train loss: 0.47561001777648926\n",
      "Epoch 10304: train loss: 0.4756099283695221\n",
      "Epoch 10305: train loss: 0.47560977935791016\n",
      "Epoch 10306: train loss: 0.4756097197532654\n",
      "Epoch 10307: train loss: 0.47560960054397583\n",
      "Epoch 10308: train loss: 0.47560951113700867\n",
      "Epoch 10309: train loss: 0.4756094515323639\n",
      "Epoch 10310: train loss: 0.47560936212539673\n",
      "Epoch 10311: train loss: 0.4756092429161072\n",
      "Epoch 10312: train loss: 0.47560915350914\n",
      "Epoch 10313: train loss: 0.47560903429985046\n",
      "Epoch 10314: train loss: 0.4756089746952057\n",
      "Epoch 10315: train loss: 0.4756088852882385\n",
      "Epoch 10316: train loss: 0.475608766078949\n",
      "Epoch 10317: train loss: 0.4756086766719818\n",
      "Epoch 10318: train loss: 0.47560855746269226\n",
      "Epoch 10319: train loss: 0.4756084978580475\n",
      "Epoch 10320: train loss: 0.4756084084510803\n",
      "Epoch 10321: train loss: 0.47560834884643555\n",
      "Epoch 10322: train loss: 0.4756081998348236\n",
      "Epoch 10323: train loss: 0.47560814023017883\n",
      "Epoch 10324: train loss: 0.4756079912185669\n",
      "Epoch 10325: train loss: 0.4756079912185669\n",
      "Epoch 10326: train loss: 0.47560784220695496\n",
      "Epoch 10327: train loss: 0.4756077229976654\n",
      "Epoch 10328: train loss: 0.47560763359069824\n",
      "Epoch 10329: train loss: 0.47560757398605347\n",
      "Epoch 10330: train loss: 0.4756074547767639\n",
      "Epoch 10331: train loss: 0.47560736536979675\n",
      "Epoch 10332: train loss: 0.4756072461605072\n",
      "Epoch 10333: train loss: 0.4756072163581848\n",
      "Epoch 10334: train loss: 0.47560709714889526\n",
      "Epoch 10335: train loss: 0.4756070077419281\n",
      "Epoch 10336: train loss: 0.4756068289279938\n",
      "Epoch 10337: train loss: 0.4756067991256714\n",
      "Epoch 10338: train loss: 0.47560667991638184\n",
      "Epoch 10339: train loss: 0.47560662031173706\n",
      "Epoch 10340: train loss: 0.4756065309047699\n",
      "Epoch 10341: train loss: 0.47560641169548035\n",
      "Epoch 10342: train loss: 0.47560638189315796\n",
      "Epoch 10343: train loss: 0.4756062626838684\n",
      "Epoch 10344: train loss: 0.47560617327690125\n",
      "Epoch 10345: train loss: 0.4756059944629669\n",
      "Epoch 10346: train loss: 0.47560596466064453\n",
      "Epoch 10347: train loss: 0.47560590505599976\n",
      "Epoch 10348: train loss: 0.4756057858467102\n",
      "Epoch 10349: train loss: 0.47560569643974304\n",
      "Epoch 10350: train loss: 0.4756055772304535\n",
      "Epoch 10351: train loss: 0.47560548782348633\n",
      "Epoch 10352: train loss: 0.47560542821884155\n",
      "Epoch 10353: train loss: 0.4756053388118744\n",
      "Epoch 10354: train loss: 0.47560515999794006\n",
      "Epoch 10355: train loss: 0.4756051301956177\n",
      "Epoch 10356: train loss: 0.4756050705909729\n",
      "Epoch 10357: train loss: 0.4756048917770386\n",
      "Epoch 10358: train loss: 0.4756048619747162\n",
      "Epoch 10359: train loss: 0.47560468316078186\n",
      "Epoch 10360: train loss: 0.4756046533584595\n",
      "Epoch 10361: train loss: 0.4756045937538147\n",
      "Epoch 10362: train loss: 0.47560447454452515\n",
      "Epoch 10363: train loss: 0.475604385137558\n",
      "Epoch 10364: train loss: 0.47560426592826843\n",
      "Epoch 10365: train loss: 0.47560417652130127\n",
      "Epoch 10366: train loss: 0.4756040573120117\n",
      "Epoch 10367: train loss: 0.47560402750968933\n",
      "Epoch 10368: train loss: 0.4756039083003998\n",
      "Epoch 10369: train loss: 0.4756038188934326\n",
      "Epoch 10370: train loss: 0.47560369968414307\n",
      "Epoch 10371: train loss: 0.4756036102771759\n",
      "Epoch 10372: train loss: 0.47560349106788635\n",
      "Epoch 10373: train loss: 0.47560349106788635\n",
      "Epoch 10374: train loss: 0.4756033420562744\n",
      "Epoch 10375: train loss: 0.47560322284698486\n",
      "Epoch 10376: train loss: 0.4756031930446625\n",
      "Epoch 10377: train loss: 0.4756030738353729\n",
      "Epoch 10378: train loss: 0.47560298442840576\n",
      "Epoch 10379: train loss: 0.4756028652191162\n",
      "Epoch 10380: train loss: 0.47560277581214905\n",
      "Epoch 10381: train loss: 0.4756027162075043\n",
      "Epoch 10382: train loss: 0.4756025969982147\n",
      "Epoch 10383: train loss: 0.47560250759124756\n",
      "Epoch 10384: train loss: 0.475602388381958\n",
      "Epoch 10385: train loss: 0.47560229897499084\n",
      "Epoch 10386: train loss: 0.47560223937034607\n",
      "Epoch 10387: train loss: 0.4756021499633789\n",
      "Epoch 10388: train loss: 0.47560203075408936\n",
      "Epoch 10389: train loss: 0.4756019413471222\n",
      "Epoch 10390: train loss: 0.47560182213783264\n",
      "Epoch 10391: train loss: 0.47560176253318787\n",
      "Epoch 10392: train loss: 0.4756016731262207\n",
      "Epoch 10393: train loss: 0.47560155391693115\n",
      "Epoch 10394: train loss: 0.475601464509964\n",
      "Epoch 10395: train loss: 0.4756014049053192\n",
      "Epoch 10396: train loss: 0.47560131549835205\n",
      "Epoch 10397: train loss: 0.4756011962890625\n",
      "Epoch 10398: train loss: 0.47560107707977295\n",
      "Epoch 10399: train loss: 0.47560104727745056\n",
      "Epoch 10400: train loss: 0.475600928068161\n",
      "Epoch 10401: train loss: 0.47560086846351624\n",
      "Epoch 10402: train loss: 0.4756007194519043\n",
      "Epoch 10403: train loss: 0.4756006598472595\n",
      "Epoch 10404: train loss: 0.47560057044029236\n",
      "Epoch 10405: train loss: 0.4756004512310028\n",
      "Epoch 10406: train loss: 0.47560036182403564\n",
      "Epoch 10407: train loss: 0.4756002426147461\n",
      "Epoch 10408: train loss: 0.47560015320777893\n",
      "Epoch 10409: train loss: 0.47560015320777893\n",
      "Epoch 10410: train loss: 0.475600004196167\n",
      "Epoch 10411: train loss: 0.47559988498687744\n",
      "Epoch 10412: train loss: 0.4755997955799103\n",
      "Epoch 10413: train loss: 0.4755997359752655\n",
      "Epoch 10414: train loss: 0.47559961676597595\n",
      "Epoch 10415: train loss: 0.4755995273590088\n",
      "Epoch 10416: train loss: 0.47559940814971924\n",
      "Epoch 10417: train loss: 0.4755993187427521\n",
      "Epoch 10418: train loss: 0.4755991995334625\n",
      "Epoch 10419: train loss: 0.47559911012649536\n",
      "Epoch 10420: train loss: 0.47559911012649536\n",
      "Epoch 10421: train loss: 0.4755989909172058\n",
      "Epoch 10422: train loss: 0.47559884190559387\n",
      "Epoch 10423: train loss: 0.4755987823009491\n",
      "Epoch 10424: train loss: 0.47559869289398193\n",
      "Epoch 10425: train loss: 0.4755985736846924\n",
      "Epoch 10426: train loss: 0.47559854388237\n",
      "Epoch 10427: train loss: 0.47559842467308044\n",
      "Epoch 10428: train loss: 0.47559836506843567\n",
      "Epoch 10429: train loss: 0.47559821605682373\n",
      "Epoch 10430: train loss: 0.47559815645217896\n",
      "Epoch 10431: train loss: 0.4755980670452118\n",
      "Epoch 10432: train loss: 0.47559794783592224\n",
      "Epoch 10433: train loss: 0.4755978584289551\n",
      "Epoch 10434: train loss: 0.4755977988243103\n",
      "Epoch 10435: train loss: 0.47559764981269836\n",
      "Epoch 10436: train loss: 0.4755975902080536\n",
      "Epoch 10437: train loss: 0.4755975008010864\n",
      "Epoch 10438: train loss: 0.47559744119644165\n",
      "Epoch 10439: train loss: 0.4755973219871521\n",
      "Epoch 10440: train loss: 0.47559723258018494\n",
      "Epoch 10441: train loss: 0.4755971133708954\n",
      "Epoch 10442: train loss: 0.4755970537662506\n",
      "Epoch 10443: train loss: 0.47559696435928345\n",
      "Epoch 10444: train loss: 0.4755968451499939\n",
      "Epoch 10445: train loss: 0.4755968153476715\n",
      "Epoch 10446: train loss: 0.47559669613838196\n",
      "Epoch 10447: train loss: 0.47559654712677\n",
      "Epoch 10448: train loss: 0.47559648752212524\n",
      "Epoch 10449: train loss: 0.4755963981151581\n",
      "Epoch 10450: train loss: 0.47559627890586853\n",
      "Epoch 10451: train loss: 0.47559621930122375\n",
      "Epoch 10452: train loss: 0.4755960702896118\n",
      "Epoch 10453: train loss: 0.47559601068496704\n",
      "Epoch 10454: train loss: 0.4755959212779999\n",
      "Epoch 10455: train loss: 0.4755958616733551\n",
      "Epoch 10456: train loss: 0.47559577226638794\n",
      "Epoch 10457: train loss: 0.4755956530570984\n",
      "Epoch 10458: train loss: 0.4755955636501312\n",
      "Epoch 10459: train loss: 0.4755954444408417\n",
      "Epoch 10460: train loss: 0.4755953848361969\n",
      "Epoch 10461: train loss: 0.47559529542922974\n",
      "Epoch 10462: train loss: 0.4755951762199402\n",
      "Epoch 10463: train loss: 0.475595086812973\n",
      "Epoch 10464: train loss: 0.47559496760368347\n",
      "Epoch 10465: train loss: 0.4755949378013611\n",
      "Epoch 10466: train loss: 0.47559475898742676\n",
      "Epoch 10467: train loss: 0.47559472918510437\n",
      "Epoch 10468: train loss: 0.4755946099758148\n",
      "Epoch 10469: train loss: 0.47559455037117004\n",
      "Epoch 10470: train loss: 0.4755944609642029\n",
      "Epoch 10471: train loss: 0.47559434175491333\n",
      "Epoch 10472: train loss: 0.47559431195259094\n",
      "Epoch 10473: train loss: 0.4755941927433014\n",
      "Epoch 10474: train loss: 0.47559410333633423\n",
      "Epoch 10475: train loss: 0.47559404373168945\n",
      "Epoch 10476: train loss: 0.4755938947200775\n",
      "Epoch 10477: train loss: 0.47559383511543274\n",
      "Epoch 10478: train loss: 0.4755937159061432\n",
      "Epoch 10479: train loss: 0.475593626499176\n",
      "Epoch 10480: train loss: 0.4755935072898865\n",
      "Epoch 10481: train loss: 0.4755934476852417\n",
      "Epoch 10482: train loss: 0.47559335827827454\n",
      "Epoch 10483: train loss: 0.475593239068985\n",
      "Epoch 10484: train loss: 0.4755931496620178\n",
      "Epoch 10485: train loss: 0.47559309005737305\n",
      "Epoch 10486: train loss: 0.4755930006504059\n",
      "Epoch 10487: train loss: 0.47559288144111633\n",
      "Epoch 10488: train loss: 0.47559279203414917\n",
      "Epoch 10489: train loss: 0.4755927324295044\n",
      "Epoch 10490: train loss: 0.47559258341789246\n",
      "Epoch 10491: train loss: 0.4755925238132477\n",
      "Epoch 10492: train loss: 0.4755924642086029\n",
      "Epoch 10493: train loss: 0.47559237480163574\n",
      "Epoch 10494: train loss: 0.4755922555923462\n",
      "Epoch 10495: train loss: 0.47559216618537903\n",
      "Epoch 10496: train loss: 0.4755920469760895\n",
      "Epoch 10497: train loss: 0.4755919575691223\n",
      "Epoch 10498: train loss: 0.47559183835983276\n",
      "Epoch 10499: train loss: 0.475591778755188\n",
      "Epoch 10500: train loss: 0.4755916893482208\n",
      "Epoch 10501: train loss: 0.47559162974357605\n",
      "Epoch 10502: train loss: 0.4755915403366089\n",
      "Epoch 10503: train loss: 0.47559142112731934\n",
      "Epoch 10504: train loss: 0.4755913317203522\n",
      "Epoch 10505: train loss: 0.4755912125110626\n",
      "Epoch 10506: train loss: 0.47559115290641785\n",
      "Epoch 10507: train loss: 0.4755910634994507\n",
      "Epoch 10508: train loss: 0.47559094429016113\n",
      "Epoch 10509: train loss: 0.47559085488319397\n",
      "Epoch 10510: train loss: 0.4755907952785492\n",
      "Epoch 10511: train loss: 0.47559070587158203\n",
      "Epoch 10512: train loss: 0.4755905866622925\n",
      "Epoch 10513: train loss: 0.4755904972553253\n",
      "Epoch 10514: train loss: 0.47559043765068054\n",
      "Epoch 10515: train loss: 0.475590318441391\n",
      "Epoch 10516: train loss: 0.47559022903442383\n",
      "Epoch 10517: train loss: 0.47559016942977905\n",
      "Epoch 10518: train loss: 0.4755900800228119\n",
      "Epoch 10519: train loss: 0.47558996081352234\n",
      "Epoch 10520: train loss: 0.4755898714065552\n",
      "Epoch 10521: train loss: 0.4755897521972656\n",
      "Epoch 10522: train loss: 0.47558969259262085\n",
      "Epoch 10523: train loss: 0.4755896031856537\n",
      "Epoch 10524: train loss: 0.47558948397636414\n",
      "Epoch 10525: train loss: 0.475589394569397\n",
      "Epoch 10526: train loss: 0.4755893349647522\n",
      "Epoch 10527: train loss: 0.4755892753601074\n",
      "Epoch 10528: train loss: 0.4755891263484955\n",
      "Epoch 10529: train loss: 0.47558900713920593\n",
      "Epoch 10530: train loss: 0.47558891773223877\n",
      "Epoch 10531: train loss: 0.475588858127594\n",
      "Epoch 10532: train loss: 0.4755887985229492\n",
      "Epoch 10533: train loss: 0.4755886495113373\n",
      "Epoch 10534: train loss: 0.4755885899066925\n",
      "Epoch 10535: train loss: 0.47558850049972534\n",
      "Epoch 10536: train loss: 0.47558844089508057\n",
      "Epoch 10537: train loss: 0.4755883514881134\n",
      "Epoch 10538: train loss: 0.47558823227882385\n",
      "Epoch 10539: train loss: 0.4755881428718567\n",
      "Epoch 10540: train loss: 0.47558802366256714\n",
      "Epoch 10541: train loss: 0.47558796405792236\n",
      "Epoch 10542: train loss: 0.4755878746509552\n",
      "Epoch 10543: train loss: 0.47558775544166565\n",
      "Epoch 10544: train loss: 0.47558772563934326\n",
      "Epoch 10545: train loss: 0.47558754682540894\n",
      "Epoch 10546: train loss: 0.4755874574184418\n",
      "Epoch 10547: train loss: 0.475587397813797\n",
      "Epoch 10548: train loss: 0.47558730840682983\n",
      "Epoch 10549: train loss: 0.4755871891975403\n",
      "Epoch 10550: train loss: 0.4755871295928955\n",
      "Epoch 10551: train loss: 0.47558704018592834\n",
      "Epoch 10552: train loss: 0.4755868911743164\n",
      "Epoch 10553: train loss: 0.47558683156967163\n",
      "Epoch 10554: train loss: 0.47558677196502686\n",
      "Epoch 10555: train loss: 0.4755866825580597\n",
      "Epoch 10556: train loss: 0.47558656334877014\n",
      "Epoch 10557: train loss: 0.47558650374412537\n",
      "Epoch 10558: train loss: 0.4755864143371582\n",
      "Epoch 10559: train loss: 0.47558629512786865\n",
      "Epoch 10560: train loss: 0.47558626532554626\n",
      "Epoch 10561: train loss: 0.4755861461162567\n",
      "Epoch 10562: train loss: 0.47558605670928955\n",
      "Epoch 10563: train loss: 0.4755859375\n",
      "Epoch 10564: train loss: 0.47558581829071045\n",
      "Epoch 10565: train loss: 0.4755857288837433\n",
      "Epoch 10566: train loss: 0.47558560967445374\n",
      "Epoch 10567: train loss: 0.47558557987213135\n",
      "Epoch 10568: train loss: 0.4755855202674866\n",
      "Epoch 10569: train loss: 0.47558537125587463\n",
      "Epoch 10570: train loss: 0.47558531165122986\n",
      "Epoch 10571: train loss: 0.4755851924419403\n",
      "Epoch 10572: train loss: 0.47558510303497314\n",
      "Epoch 10573: train loss: 0.47558504343032837\n",
      "Epoch 10574: train loss: 0.47558489441871643\n",
      "Epoch 10575: train loss: 0.47558483481407166\n",
      "Epoch 10576: train loss: 0.4755847454071045\n",
      "Epoch 10577: train loss: 0.47558462619781494\n",
      "Epoch 10578: train loss: 0.47558456659317017\n",
      "Epoch 10579: train loss: 0.475584477186203\n",
      "Epoch 10580: train loss: 0.4755844175815582\n",
      "Epoch 10581: train loss: 0.4755842685699463\n",
      "Epoch 10582: train loss: 0.4755842089653015\n",
      "Epoch 10583: train loss: 0.47558411955833435\n",
      "Epoch 10584: train loss: 0.4755840003490448\n",
      "Epoch 10585: train loss: 0.4755839407444\n",
      "Epoch 10586: train loss: 0.47558385133743286\n",
      "Epoch 10587: train loss: 0.4755837321281433\n",
      "Epoch 10588: train loss: 0.47558364272117615\n",
      "Epoch 10589: train loss: 0.47558358311653137\n",
      "Epoch 10590: train loss: 0.4755835235118866\n",
      "Epoch 10591: train loss: 0.47558337450027466\n",
      "Epoch 10592: train loss: 0.4755832850933075\n",
      "Epoch 10593: train loss: 0.4755832254886627\n",
      "Epoch 10594: train loss: 0.47558310627937317\n",
      "Epoch 10595: train loss: 0.4755830764770508\n",
      "Epoch 10596: train loss: 0.47558295726776123\n",
      "Epoch 10597: train loss: 0.47558286786079407\n",
      "Epoch 10598: train loss: 0.47558268904685974\n",
      "Epoch 10599: train loss: 0.47558268904685974\n",
      "Epoch 10600: train loss: 0.4755825400352478\n",
      "Epoch 10601: train loss: 0.475582480430603\n",
      "Epoch 10602: train loss: 0.47558245062828064\n",
      "Epoch 10603: train loss: 0.4755823314189911\n",
      "Epoch 10604: train loss: 0.4755822420120239\n",
      "Epoch 10605: train loss: 0.47558218240737915\n",
      "Epoch 10606: train loss: 0.4755820631980896\n",
      "Epoch 10607: train loss: 0.47558191418647766\n",
      "Epoch 10608: train loss: 0.4755818545818329\n",
      "Epoch 10609: train loss: 0.4755817651748657\n",
      "Epoch 10610: train loss: 0.47558164596557617\n",
      "Epoch 10611: train loss: 0.4755815863609314\n",
      "Epoch 10612: train loss: 0.47558149695396423\n",
      "Epoch 10613: train loss: 0.4755813777446747\n",
      "Epoch 10614: train loss: 0.4755813479423523\n",
      "Epoch 10615: train loss: 0.47558122873306274\n",
      "Epoch 10616: train loss: 0.4755811393260956\n",
      "Epoch 10617: train loss: 0.47558102011680603\n",
      "Epoch 10618: train loss: 0.47558093070983887\n",
      "Epoch 10619: train loss: 0.4755808711051941\n",
      "Epoch 10620: train loss: 0.47558075189590454\n",
      "Epoch 10621: train loss: 0.47558072209358215\n",
      "Epoch 10622: train loss: 0.4755805432796478\n",
      "Epoch 10623: train loss: 0.47558051347732544\n",
      "Epoch 10624: train loss: 0.4755803942680359\n",
      "Epoch 10625: train loss: 0.4755803048610687\n",
      "Epoch 10626: train loss: 0.47558024525642395\n",
      "Epoch 10627: train loss: 0.4755801856517792\n",
      "Epoch 10628: train loss: 0.475580096244812\n",
      "Epoch 10629: train loss: 0.47557997703552246\n",
      "Epoch 10630: train loss: 0.4755798876285553\n",
      "Epoch 10631: train loss: 0.47557976841926575\n",
      "Epoch 10632: train loss: 0.4755796790122986\n",
      "Epoch 10633: train loss: 0.4755796194076538\n",
      "Epoch 10634: train loss: 0.47557950019836426\n",
      "Epoch 10635: train loss: 0.4755794107913971\n",
      "Epoch 10636: train loss: 0.4755793511867523\n",
      "Epoch 10637: train loss: 0.47557926177978516\n",
      "Epoch 10638: train loss: 0.4755791425704956\n",
      "Epoch 10639: train loss: 0.47557908296585083\n",
      "Epoch 10640: train loss: 0.47557899355888367\n",
      "Epoch 10641: train loss: 0.47557884454727173\n",
      "Epoch 10642: train loss: 0.47557878494262695\n",
      "Epoch 10643: train loss: 0.4755786657333374\n",
      "Epoch 10644: train loss: 0.47557857632637024\n",
      "Epoch 10645: train loss: 0.47557851672172546\n",
      "Epoch 10646: train loss: 0.4755784273147583\n",
      "Epoch 10647: train loss: 0.47557830810546875\n",
      "Epoch 10648: train loss: 0.4755781888961792\n",
      "Epoch 10649: train loss: 0.4755781590938568\n",
      "Epoch 10650: train loss: 0.4755779802799225\n",
      "Epoch 10651: train loss: 0.4755779802799225\n",
      "Epoch 10652: train loss: 0.47557783126831055\n",
      "Epoch 10653: train loss: 0.47557777166366577\n",
      "Epoch 10654: train loss: 0.4755776822566986\n",
      "Epoch 10655: train loss: 0.47557756304740906\n",
      "Epoch 10656: train loss: 0.47557756304740906\n",
      "Epoch 10657: train loss: 0.4755774736404419\n",
      "Epoch 10658: train loss: 0.47557732462882996\n",
      "Epoch 10659: train loss: 0.4755772650241852\n",
      "Epoch 10660: train loss: 0.47557714581489563\n",
      "Epoch 10661: train loss: 0.47557711601257324\n",
      "Epoch 10662: train loss: 0.4755769968032837\n",
      "Epoch 10663: train loss: 0.47557690739631653\n",
      "Epoch 10664: train loss: 0.475576788187027\n",
      "Epoch 10665: train loss: 0.4755766987800598\n",
      "Epoch 10666: train loss: 0.47557663917541504\n",
      "Epoch 10667: train loss: 0.4755765199661255\n",
      "Epoch 10668: train loss: 0.4755764901638031\n",
      "Epoch 10669: train loss: 0.47557637095451355\n",
      "Epoch 10670: train loss: 0.4755762219429016\n",
      "Epoch 10671: train loss: 0.4755762219429016\n",
      "Epoch 10672: train loss: 0.47557610273361206\n",
      "Epoch 10673: train loss: 0.4755760133266449\n",
      "Epoch 10674: train loss: 0.47557589411735535\n",
      "Epoch 10675: train loss: 0.4755758047103882\n",
      "Epoch 10676: train loss: 0.47557568550109863\n",
      "Epoch 10677: train loss: 0.47557565569877625\n",
      "Epoch 10678: train loss: 0.4755755364894867\n",
      "Epoch 10679: train loss: 0.4755754768848419\n",
      "Epoch 10680: train loss: 0.47557544708251953\n",
      "Epoch 10681: train loss: 0.4755752682685852\n",
      "Epoch 10682: train loss: 0.47557517886161804\n",
      "Epoch 10683: train loss: 0.4755750596523285\n",
      "Epoch 10684: train loss: 0.47557497024536133\n",
      "Epoch 10685: train loss: 0.4755748510360718\n",
      "Epoch 10686: train loss: 0.4755748510360718\n",
      "Epoch 10687: train loss: 0.4755747616291046\n",
      "Epoch 10688: train loss: 0.47557464241981506\n",
      "Epoch 10689: train loss: 0.4755745530128479\n",
      "Epoch 10690: train loss: 0.47557443380355835\n",
      "Epoch 10691: train loss: 0.4755743443965912\n",
      "Epoch 10692: train loss: 0.47557422518730164\n",
      "Epoch 10693: train loss: 0.47557416558265686\n",
      "Epoch 10694: train loss: 0.4755741357803345\n",
      "Epoch 10695: train loss: 0.4755740165710449\n",
      "Epoch 10696: train loss: 0.47557392716407776\n",
      "Epoch 10697: train loss: 0.4755738079547882\n",
      "Epoch 10698: train loss: 0.47557374835014343\n",
      "Epoch 10699: train loss: 0.47557365894317627\n",
      "Epoch 10700: train loss: 0.4755735397338867\n",
      "Epoch 10701: train loss: 0.47557345032691956\n",
      "Epoch 10702: train loss: 0.4755733907222748\n",
      "Epoch 10703: train loss: 0.4755733013153076\n",
      "Epoch 10704: train loss: 0.47557324171066284\n",
      "Epoch 10705: train loss: 0.4755730926990509\n",
      "Epoch 10706: train loss: 0.4755730926990509\n",
      "Epoch 10707: train loss: 0.47557297348976135\n",
      "Epoch 10708: train loss: 0.4755728244781494\n",
      "Epoch 10709: train loss: 0.47557276487350464\n",
      "Epoch 10710: train loss: 0.47557270526885986\n",
      "Epoch 10711: train loss: 0.4755726158618927\n",
      "Epoch 10712: train loss: 0.47557249665260315\n",
      "Epoch 10713: train loss: 0.475572407245636\n",
      "Epoch 10714: train loss: 0.4755723476409912\n",
      "Epoch 10715: train loss: 0.4755721986293793\n",
      "Epoch 10716: train loss: 0.4755721390247345\n",
      "Epoch 10717: train loss: 0.47557204961776733\n",
      "Epoch 10718: train loss: 0.4755719304084778\n",
      "Epoch 10719: train loss: 0.475571870803833\n",
      "Epoch 10720: train loss: 0.47557178139686584\n",
      "Epoch 10721: train loss: 0.47557172179222107\n",
      "Epoch 10722: train loss: 0.47557157278060913\n",
      "Epoch 10723: train loss: 0.47557151317596436\n",
      "Epoch 10724: train loss: 0.4755714237689972\n",
      "Epoch 10725: train loss: 0.47557130455970764\n",
      "Epoch 10726: train loss: 0.47557130455970764\n",
      "Epoch 10727: train loss: 0.4755712151527405\n",
      "Epoch 10728: train loss: 0.4755710959434509\n",
      "Epoch 10729: train loss: 0.47557100653648376\n",
      "Epoch 10730: train loss: 0.475570946931839\n",
      "Epoch 10731: train loss: 0.47557082772254944\n",
      "Epoch 10732: train loss: 0.4755707383155823\n",
      "Epoch 10733: train loss: 0.4755706191062927\n",
      "Epoch 10734: train loss: 0.47557055950164795\n",
      "Epoch 10735: train loss: 0.4755704700946808\n",
      "Epoch 10736: train loss: 0.47557035088539124\n",
      "Epoch 10737: train loss: 0.4755702614784241\n",
      "Epoch 10738: train loss: 0.4755702018737793\n",
      "Epoch 10739: train loss: 0.47557011246681213\n",
      "Epoch 10740: train loss: 0.47557005286216736\n",
      "Epoch 10741: train loss: 0.4755699336528778\n",
      "Epoch 10742: train loss: 0.47556978464126587\n",
      "Epoch 10743: train loss: 0.4755697250366211\n",
      "Epoch 10744: train loss: 0.47556963562965393\n",
      "Epoch 10745: train loss: 0.47556963562965393\n",
      "Epoch 10746: train loss: 0.475569486618042\n",
      "Epoch 10747: train loss: 0.47556930780410767\n",
      "Epoch 10748: train loss: 0.47556930780410767\n",
      "Epoch 10749: train loss: 0.4755691587924957\n",
      "Epoch 10750: train loss: 0.47556909918785095\n",
      "Epoch 10751: train loss: 0.4755690097808838\n",
      "Epoch 10752: train loss: 0.475568950176239\n",
      "Epoch 10753: train loss: 0.47556886076927185\n",
      "Epoch 10754: train loss: 0.4755688011646271\n",
      "Epoch 10755: train loss: 0.47556865215301514\n",
      "Epoch 10756: train loss: 0.47556859254837036\n",
      "Epoch 10757: train loss: 0.4755684733390808\n",
      "Epoch 10758: train loss: 0.4755684435367584\n",
      "Epoch 10759: train loss: 0.4755682647228241\n",
      "Epoch 10760: train loss: 0.4755682349205017\n",
      "Epoch 10761: train loss: 0.47556817531585693\n",
      "Epoch 10762: train loss: 0.4755680561065674\n",
      "Epoch 10763: train loss: 0.4755679666996002\n",
      "Epoch 10764: train loss: 0.47556784749031067\n",
      "Epoch 10765: train loss: 0.4755677580833435\n",
      "Epoch 10766: train loss: 0.47556763887405396\n",
      "Epoch 10767: train loss: 0.47556760907173157\n",
      "Epoch 10768: train loss: 0.475567489862442\n",
      "Epoch 10769: train loss: 0.47556743025779724\n",
      "Epoch 10770: train loss: 0.4755673408508301\n",
      "Epoch 10771: train loss: 0.4755672216415405\n",
      "Epoch 10772: train loss: 0.47556719183921814\n",
      "Epoch 10773: train loss: 0.47556713223457336\n",
      "Epoch 10774: train loss: 0.4755670130252838\n",
      "Epoch 10775: train loss: 0.47556692361831665\n",
      "Epoch 10776: train loss: 0.4755668044090271\n",
      "Epoch 10777: train loss: 0.4755667448043823\n",
      "Epoch 10778: train loss: 0.4755665957927704\n",
      "Epoch 10779: train loss: 0.4755665957927704\n",
      "Epoch 10780: train loss: 0.47556638717651367\n",
      "Epoch 10781: train loss: 0.4755663275718689\n",
      "Epoch 10782: train loss: 0.4755662977695465\n",
      "Epoch 10783: train loss: 0.47556617856025696\n",
      "Epoch 10784: train loss: 0.4755660891532898\n",
      "Epoch 10785: train loss: 0.475566029548645\n",
      "Epoch 10786: train loss: 0.47556596994400024\n",
      "Epoch 10787: train loss: 0.4755658209323883\n",
      "Epoch 10788: train loss: 0.47556576132774353\n",
      "Epoch 10789: train loss: 0.47556567192077637\n",
      "Epoch 10790: train loss: 0.4755656123161316\n",
      "Epoch 10791: train loss: 0.47556546330451965\n",
      "Epoch 10792: train loss: 0.4755654036998749\n",
      "Epoch 10793: train loss: 0.4755652844905853\n",
      "Epoch 10794: train loss: 0.47556525468826294\n",
      "Epoch 10795: train loss: 0.4755651354789734\n",
      "Epoch 10796: train loss: 0.4755650460720062\n",
      "Epoch 10797: train loss: 0.4755649268627167\n",
      "Epoch 10798: train loss: 0.4755648374557495\n",
      "Epoch 10799: train loss: 0.47556471824645996\n",
      "Epoch 10800: train loss: 0.47556471824645996\n",
      "Epoch 10801: train loss: 0.475564569234848\n",
      "Epoch 10802: train loss: 0.47556445002555847\n",
      "Epoch 10803: train loss: 0.4755644202232361\n",
      "Epoch 10804: train loss: 0.4755643606185913\n",
      "Epoch 10805: train loss: 0.47556424140930176\n",
      "Epoch 10806: train loss: 0.4755641520023346\n",
      "Epoch 10807: train loss: 0.47556403279304504\n",
      "Epoch 10808: train loss: 0.47556400299072266\n",
      "Epoch 10809: train loss: 0.4755638837814331\n",
      "Epoch 10810: train loss: 0.47556379437446594\n",
      "Epoch 10811: train loss: 0.4755636751651764\n",
      "Epoch 10812: train loss: 0.4755636155605316\n",
      "Epoch 10813: train loss: 0.47556352615356445\n",
      "Epoch 10814: train loss: 0.4755634069442749\n",
      "Epoch 10815: train loss: 0.4755633771419525\n",
      "Epoch 10816: train loss: 0.47556325793266296\n",
      "Epoch 10817: train loss: 0.4755631685256958\n",
      "Epoch 10818: train loss: 0.475563108921051\n",
      "Epoch 10819: train loss: 0.4755629897117615\n",
      "Epoch 10820: train loss: 0.4755629003047943\n",
      "Epoch 10821: train loss: 0.47556284070014954\n",
      "Epoch 10822: train loss: 0.47556272149086\n",
      "Epoch 10823: train loss: 0.4755626320838928\n",
      "Epoch 10824: train loss: 0.47556257247924805\n",
      "Epoch 10825: train loss: 0.4755624234676361\n",
      "Epoch 10826: train loss: 0.4755624234676361\n",
      "Epoch 10827: train loss: 0.47556227445602417\n",
      "Epoch 10828: train loss: 0.4755621552467346\n",
      "Epoch 10829: train loss: 0.4755621552467346\n",
      "Epoch 10830: train loss: 0.47556206583976746\n",
      "Epoch 10831: train loss: 0.47556188702583313\n",
      "Epoch 10832: train loss: 0.47556185722351074\n",
      "Epoch 10833: train loss: 0.4755617380142212\n",
      "Epoch 10834: train loss: 0.4755616784095764\n",
      "Epoch 10835: train loss: 0.47556158900260925\n",
      "Epoch 10836: train loss: 0.4755615293979645\n",
      "Epoch 10837: train loss: 0.47556138038635254\n",
      "Epoch 10838: train loss: 0.47556132078170776\n",
      "Epoch 10839: train loss: 0.4755612313747406\n",
      "Epoch 10840: train loss: 0.4755611717700958\n",
      "Epoch 10841: train loss: 0.47556111216545105\n",
      "Epoch 10842: train loss: 0.4755609631538391\n",
      "Epoch 10843: train loss: 0.47556090354919434\n",
      "Epoch 10844: train loss: 0.4755607545375824\n",
      "Epoch 10845: train loss: 0.4755606949329376\n",
      "Epoch 10846: train loss: 0.47556060552597046\n",
      "Epoch 10847: train loss: 0.4755605459213257\n",
      "Epoch 10848: train loss: 0.47556042671203613\n",
      "Epoch 10849: train loss: 0.47556039690971375\n",
      "Epoch 10850: train loss: 0.4755602180957794\n",
      "Epoch 10851: train loss: 0.47556018829345703\n",
      "Epoch 10852: train loss: 0.47556012868881226\n",
      "Epoch 10853: train loss: 0.4755600094795227\n",
      "Epoch 10854: train loss: 0.47555992007255554\n",
      "Epoch 10855: train loss: 0.47555992007255554\n",
      "Epoch 10856: train loss: 0.47555971145629883\n",
      "Epoch 10857: train loss: 0.4755595922470093\n",
      "Epoch 10858: train loss: 0.4755595922470093\n",
      "Epoch 10859: train loss: 0.4755595028400421\n",
      "Epoch 10860: train loss: 0.47555938363075256\n",
      "Epoch 10861: train loss: 0.4755592942237854\n",
      "Epoch 10862: train loss: 0.47555917501449585\n",
      "Epoch 10863: train loss: 0.4755591154098511\n",
      "Epoch 10864: train loss: 0.4755590260028839\n",
      "Epoch 10865: train loss: 0.47555890679359436\n",
      "Epoch 10866: train loss: 0.475558876991272\n",
      "Epoch 10867: train loss: 0.4755587577819824\n",
      "Epoch 10868: train loss: 0.47555866837501526\n",
      "Epoch 10869: train loss: 0.4755585491657257\n",
      "Epoch 10870: train loss: 0.47555848956108093\n",
      "Epoch 10871: train loss: 0.47555840015411377\n",
      "Epoch 10872: train loss: 0.475558340549469\n",
      "Epoch 10873: train loss: 0.47555825114250183\n",
      "Epoch 10874: train loss: 0.4755581319332123\n",
      "Epoch 10875: train loss: 0.4755580723285675\n",
      "Epoch 10876: train loss: 0.4755580425262451\n",
      "Epoch 10877: train loss: 0.4755578637123108\n",
      "Epoch 10878: train loss: 0.4755578339099884\n",
      "Epoch 10879: train loss: 0.47555777430534363\n",
      "Epoch 10880: train loss: 0.4755576252937317\n",
      "Epoch 10881: train loss: 0.4755575656890869\n",
      "Epoch 10882: train loss: 0.47555744647979736\n",
      "Epoch 10883: train loss: 0.4755573570728302\n",
      "Epoch 10884: train loss: 0.4755572974681854\n",
      "Epoch 10885: train loss: 0.4755571484565735\n",
      "Epoch 10886: train loss: 0.4755570888519287\n",
      "Epoch 10887: train loss: 0.47555699944496155\n",
      "Epoch 10888: train loss: 0.4755569398403168\n",
      "Epoch 10889: train loss: 0.475556880235672\n",
      "Epoch 10890: train loss: 0.47555679082870483\n",
      "Epoch 10891: train loss: 0.4755566716194153\n",
      "Epoch 10892: train loss: 0.4755565822124481\n",
      "Epoch 10893: train loss: 0.47555646300315857\n",
      "Epoch 10894: train loss: 0.4755564033985138\n",
      "Epoch 10895: train loss: 0.47555631399154663\n",
      "Epoch 10896: train loss: 0.47555625438690186\n",
      "Epoch 10897: train loss: 0.4755561649799347\n",
      "Epoch 10898: train loss: 0.47555604577064514\n",
      "Epoch 10899: train loss: 0.475555956363678\n",
      "Epoch 10900: train loss: 0.4755558371543884\n",
      "Epoch 10901: train loss: 0.47555577754974365\n",
      "Epoch 10902: train loss: 0.47555574774742126\n",
      "Epoch 10903: train loss: 0.4755556285381317\n",
      "Epoch 10904: train loss: 0.47555553913116455\n",
      "Epoch 10905: train loss: 0.475555419921875\n",
      "Epoch 10906: train loss: 0.4755553603172302\n",
      "Epoch 10907: train loss: 0.47555527091026306\n",
      "Epoch 10908: train loss: 0.4755551517009735\n",
      "Epoch 10909: train loss: 0.47555509209632874\n",
      "Epoch 10910: train loss: 0.4755550026893616\n",
      "Epoch 10911: train loss: 0.475554883480072\n",
      "Epoch 10912: train loss: 0.47555485367774963\n",
      "Epoch 10913: train loss: 0.47555479407310486\n",
      "Epoch 10914: train loss: 0.4755546748638153\n",
      "Epoch 10915: train loss: 0.47555458545684814\n",
      "Epoch 10916: train loss: 0.4755544662475586\n",
      "Epoch 10917: train loss: 0.4755544364452362\n",
      "Epoch 10918: train loss: 0.4755542576313019\n",
      "Epoch 10919: train loss: 0.4755542278289795\n",
      "Epoch 10920: train loss: 0.4755541682243347\n",
      "Epoch 10921: train loss: 0.4755540192127228\n",
      "Epoch 10922: train loss: 0.475553959608078\n",
      "Epoch 10923: train loss: 0.4755539000034332\n",
      "Epoch 10924: train loss: 0.47555381059646606\n",
      "Epoch 10925: train loss: 0.4755536913871765\n",
      "Epoch 10926: train loss: 0.47555363178253174\n",
      "Epoch 10927: train loss: 0.4755535423755646\n",
      "Epoch 10928: train loss: 0.475553423166275\n",
      "Epoch 10929: train loss: 0.47555333375930786\n",
      "Epoch 10930: train loss: 0.4755532145500183\n",
      "Epoch 10931: train loss: 0.4755532145500183\n",
      "Epoch 10932: train loss: 0.47555306553840637\n",
      "Epoch 10933: train loss: 0.4755530059337616\n",
      "Epoch 10934: train loss: 0.47555291652679443\n",
      "Epoch 10935: train loss: 0.4755527973175049\n",
      "Epoch 10936: train loss: 0.4755527675151825\n",
      "Epoch 10937: train loss: 0.4755527079105377\n",
      "Epoch 10938: train loss: 0.4755525588989258\n",
      "Epoch 10939: train loss: 0.475552499294281\n",
      "Epoch 10940: train loss: 0.47555238008499146\n",
      "Epoch 10941: train loss: 0.47555235028266907\n",
      "Epoch 10942: train loss: 0.4755522310733795\n",
      "Epoch 10943: train loss: 0.47555217146873474\n",
      "Epoch 10944: train loss: 0.4755520820617676\n",
      "Epoch 10945: train loss: 0.475551962852478\n",
      "Epoch 10946: train loss: 0.47555187344551086\n",
      "Epoch 10947: train loss: 0.4755518138408661\n",
      "Epoch 10948: train loss: 0.4755517542362213\n",
      "Epoch 10949: train loss: 0.47555166482925415\n",
      "Epoch 10950: train loss: 0.4755514860153198\n",
      "Epoch 10951: train loss: 0.47555145621299744\n",
      "Epoch 10952: train loss: 0.47555139660835266\n",
      "Epoch 10953: train loss: 0.4755512475967407\n",
      "Epoch 10954: train loss: 0.47555118799209595\n",
      "Epoch 10955: train loss: 0.4755510687828064\n",
      "Epoch 10956: train loss: 0.47555097937583923\n",
      "Epoch 10957: train loss: 0.47555091977119446\n",
      "Epoch 10958: train loss: 0.4755508303642273\n",
      "Epoch 10959: train loss: 0.4755507707595825\n",
      "Epoch 10960: train loss: 0.47555065155029297\n",
      "Epoch 10961: train loss: 0.4755505621433258\n",
      "Epoch 10962: train loss: 0.47555050253868103\n",
      "Epoch 10963: train loss: 0.47555041313171387\n",
      "Epoch 10964: train loss: 0.4755502939224243\n",
      "Epoch 10965: train loss: 0.47555023431777954\n",
      "Epoch 10966: train loss: 0.4755501449108124\n",
      "Epoch 10967: train loss: 0.4755500257015228\n",
      "Epoch 10968: train loss: 0.47554999589920044\n",
      "Epoch 10969: train loss: 0.4755498766899109\n",
      "Epoch 10970: train loss: 0.4755497872829437\n",
      "Epoch 10971: train loss: 0.4755496680736542\n",
      "Epoch 10972: train loss: 0.4755496084690094\n",
      "Epoch 10973: train loss: 0.475549578666687\n",
      "Epoch 10974: train loss: 0.47554945945739746\n",
      "Epoch 10975: train loss: 0.4755493700504303\n",
      "Epoch 10976: train loss: 0.47554925084114075\n",
      "Epoch 10977: train loss: 0.4755491614341736\n",
      "Epoch 10978: train loss: 0.4755491018295288\n",
      "Epoch 10979: train loss: 0.47554904222488403\n",
      "Epoch 10980: train loss: 0.47554895281791687\n",
      "Epoch 10981: train loss: 0.4755488336086273\n",
      "Epoch 10982: train loss: 0.47554874420166016\n",
      "Epoch 10983: train loss: 0.4755486249923706\n",
      "Epoch 10984: train loss: 0.47554856538772583\n",
      "Epoch 10985: train loss: 0.47554853558540344\n",
      "Epoch 10986: train loss: 0.4755484163761139\n",
      "Epoch 10987: train loss: 0.47554832696914673\n",
      "Epoch 10988: train loss: 0.4755482077598572\n",
      "Epoch 10989: train loss: 0.4755481481552124\n",
      "Epoch 10990: train loss: 0.47554805874824524\n",
      "Epoch 10991: train loss: 0.4755479395389557\n",
      "Epoch 10992: train loss: 0.4755479097366333\n",
      "Epoch 10993: train loss: 0.47554779052734375\n",
      "Epoch 10994: train loss: 0.4755476713180542\n",
      "Epoch 10995: train loss: 0.4755476415157318\n",
      "Epoch 10996: train loss: 0.47554758191108704\n",
      "Epoch 10997: train loss: 0.4755474328994751\n",
      "Epoch 10998: train loss: 0.4755473732948303\n",
      "Epoch 10999: train loss: 0.47554731369018555\n",
      "Epoch 11000: train loss: 0.4755472242832184\n",
      "Epoch 11001: train loss: 0.47554710507392883\n",
      "Epoch 11002: train loss: 0.47554701566696167\n",
      "Epoch 11003: train loss: 0.4755468964576721\n",
      "Epoch 11004: train loss: 0.47554683685302734\n",
      "Epoch 11005: train loss: 0.4755467474460602\n",
      "Epoch 11006: train loss: 0.4755466878414154\n",
      "Epoch 11007: train loss: 0.47554662823677063\n",
      "Epoch 11008: train loss: 0.4755464792251587\n",
      "Epoch 11009: train loss: 0.47554638981819153\n",
      "Epoch 11010: train loss: 0.47554638981819153\n",
      "Epoch 11011: train loss: 0.4755462110042572\n",
      "Epoch 11012: train loss: 0.4755461812019348\n",
      "Epoch 11013: train loss: 0.47554612159729004\n",
      "Epoch 11014: train loss: 0.4755460023880005\n",
      "Epoch 11015: train loss: 0.47554585337638855\n",
      "Epoch 11016: train loss: 0.4755457937717438\n",
      "Epoch 11017: train loss: 0.4755457043647766\n",
      "Epoch 11018: train loss: 0.47554564476013184\n",
      "Epoch 11019: train loss: 0.47554558515548706\n",
      "Epoch 11020: train loss: 0.4755454957485199\n",
      "Epoch 11021: train loss: 0.47554537653923035\n",
      "Epoch 11022: train loss: 0.4755452871322632\n",
      "Epoch 11023: train loss: 0.4755452275276184\n",
      "Epoch 11024: train loss: 0.47554513812065125\n",
      "Epoch 11025: train loss: 0.4755450189113617\n",
      "Epoch 11026: train loss: 0.4755449593067169\n",
      "Epoch 11027: train loss: 0.47554486989974976\n",
      "Epoch 11028: train loss: 0.4755447506904602\n",
      "Epoch 11029: train loss: 0.47554466128349304\n",
      "Epoch 11030: train loss: 0.47554466128349304\n",
      "Epoch 11031: train loss: 0.4755445420742035\n",
      "Epoch 11032: train loss: 0.47554445266723633\n",
      "Epoch 11033: train loss: 0.4755443334579468\n",
      "Epoch 11034: train loss: 0.4755443036556244\n",
      "Epoch 11035: train loss: 0.47554418444633484\n",
      "Epoch 11036: train loss: 0.47554412484169006\n",
      "Epoch 11037: train loss: 0.4755439758300781\n",
      "Epoch 11038: train loss: 0.47554391622543335\n",
      "Epoch 11039: train loss: 0.4755438268184662\n",
      "Epoch 11040: train loss: 0.47554370760917664\n",
      "Epoch 11041: train loss: 0.47554370760917664\n",
      "Epoch 11042: train loss: 0.4755436182022095\n",
      "Epoch 11043: train loss: 0.47554343938827515\n",
      "Epoch 11044: train loss: 0.475543349981308\n",
      "Epoch 11045: train loss: 0.475543349981308\n",
      "Epoch 11046: train loss: 0.47554323077201843\n",
      "Epoch 11047: train loss: 0.47554314136505127\n",
      "Epoch 11048: train loss: 0.4755430817604065\n",
      "Epoch 11049: train loss: 0.47554299235343933\n",
      "Epoch 11050: train loss: 0.4755428731441498\n",
      "Epoch 11051: train loss: 0.4755427837371826\n",
      "Epoch 11052: train loss: 0.4755427837371826\n",
      "Epoch 11053: train loss: 0.4755426049232483\n",
      "Epoch 11054: train loss: 0.4755425751209259\n",
      "Epoch 11055: train loss: 0.4755423963069916\n",
      "Epoch 11056: train loss: 0.4755423665046692\n",
      "Epoch 11057: train loss: 0.4755423069000244\n",
      "Epoch 11058: train loss: 0.47554218769073486\n",
      "Epoch 11059: train loss: 0.4755420982837677\n",
      "Epoch 11060: train loss: 0.4755420386791229\n",
      "Epoch 11061: train loss: 0.47554194927215576\n",
      "Epoch 11062: train loss: 0.4755418300628662\n",
      "Epoch 11063: train loss: 0.47554174065589905\n",
      "Epoch 11064: train loss: 0.4755416810512543\n",
      "Epoch 11065: train loss: 0.4755416214466095\n",
      "Epoch 11066: train loss: 0.4755415618419647\n",
      "Epoch 11067: train loss: 0.4755414128303528\n",
      "Epoch 11068: train loss: 0.475541353225708\n",
      "Epoch 11069: train loss: 0.47554120421409607\n",
      "Epoch 11070: train loss: 0.4755411446094513\n",
      "Epoch 11071: train loss: 0.4755411148071289\n",
      "Epoch 11072: train loss: 0.47554099559783936\n",
      "Epoch 11073: train loss: 0.4755409061908722\n",
      "Epoch 11074: train loss: 0.47554078698158264\n",
      "Epoch 11075: train loss: 0.47554072737693787\n",
      "Epoch 11076: train loss: 0.4755406379699707\n",
      "Epoch 11077: train loss: 0.4755405783653259\n",
      "Epoch 11078: train loss: 0.47554051876068115\n",
      "Epoch 11079: train loss: 0.4755403697490692\n",
      "Epoch 11080: train loss: 0.47554031014442444\n",
      "Epoch 11081: train loss: 0.4755402207374573\n",
      "Epoch 11082: train loss: 0.4755401015281677\n",
      "Epoch 11083: train loss: 0.47554004192352295\n",
      "Epoch 11084: train loss: 0.47554001212120056\n",
      "Epoch 11085: train loss: 0.475539892911911\n",
      "Epoch 11086: train loss: 0.47553980350494385\n",
      "Epoch 11087: train loss: 0.4755397439002991\n",
      "Epoch 11088: train loss: 0.4755396246910095\n",
      "Epoch 11089: train loss: 0.47553953528404236\n",
      "Epoch 11090: train loss: 0.4755394160747528\n",
      "Epoch 11091: train loss: 0.4755393862724304\n",
      "Epoch 11092: train loss: 0.47553926706314087\n",
      "Epoch 11093: train loss: 0.4755392074584961\n",
      "Epoch 11094: train loss: 0.47553911805152893\n",
      "Epoch 11095: train loss: 0.4755389988422394\n",
      "Epoch 11096: train loss: 0.475538969039917\n",
      "Epoch 11097: train loss: 0.47553884983062744\n",
      "Epoch 11098: train loss: 0.47553879022598267\n",
      "Epoch 11099: train loss: 0.4755387008190155\n",
      "Epoch 11100: train loss: 0.47553858160972595\n",
      "Epoch 11101: train loss: 0.4755384922027588\n",
      "Epoch 11102: train loss: 0.475538432598114\n",
      "Epoch 11103: train loss: 0.47553837299346924\n",
      "Epoch 11104: train loss: 0.4755382239818573\n",
      "Epoch 11105: train loss: 0.4755381643772125\n",
      "Epoch 11106: train loss: 0.47553807497024536\n",
      "Epoch 11107: train loss: 0.4755379557609558\n",
      "Epoch 11108: train loss: 0.4755379259586334\n",
      "Epoch 11109: train loss: 0.4755377471446991\n",
      "Epoch 11110: train loss: 0.4755377471446991\n",
      "Epoch 11111: train loss: 0.47553765773773193\n",
      "Epoch 11112: train loss: 0.4755375385284424\n",
      "Epoch 11113: train loss: 0.4755374491214752\n",
      "Epoch 11114: train loss: 0.47553738951683044\n",
      "Epoch 11115: train loss: 0.47553732991218567\n",
      "Epoch 11116: train loss: 0.4755372405052185\n",
      "Epoch 11117: train loss: 0.47553712129592896\n",
      "Epoch 11118: train loss: 0.4755370318889618\n",
      "Epoch 11119: train loss: 0.475536972284317\n",
      "Epoch 11120: train loss: 0.47553691267967224\n",
      "Epoch 11121: train loss: 0.4755368232727051\n",
      "Epoch 11122: train loss: 0.4755367636680603\n",
      "Epoch 11123: train loss: 0.47553667426109314\n",
      "Epoch 11124: train loss: 0.4755364954471588\n",
      "Epoch 11125: train loss: 0.4755364656448364\n",
      "Epoch 11126: train loss: 0.47553640604019165\n",
      "Epoch 11127: train loss: 0.4755362868309021\n",
      "Epoch 11128: train loss: 0.47553619742393494\n",
      "Epoch 11129: train loss: 0.47553613781929016\n",
      "Epoch 11130: train loss: 0.4755359888076782\n",
      "Epoch 11131: train loss: 0.4755359888076782\n",
      "Epoch 11132: train loss: 0.4755358099937439\n",
      "Epoch 11133: train loss: 0.4755357801914215\n",
      "Epoch 11134: train loss: 0.47553566098213196\n",
      "Epoch 11135: train loss: 0.4755356013774872\n",
      "Epoch 11136: train loss: 0.4755355715751648\n",
      "Epoch 11137: train loss: 0.47553545236587524\n",
      "Epoch 11138: train loss: 0.4755353629589081\n",
      "Epoch 11139: train loss: 0.47553524374961853\n",
      "Epoch 11140: train loss: 0.47553518414497375\n",
      "Epoch 11141: train loss: 0.4755350351333618\n",
      "Epoch 11142: train loss: 0.4755350351333618\n",
      "Epoch 11143: train loss: 0.47553494572639465\n",
      "Epoch 11144: train loss: 0.4755348861217499\n",
      "Epoch 11145: train loss: 0.4755347669124603\n",
      "Epoch 11146: train loss: 0.47553467750549316\n",
      "Epoch 11147: train loss: 0.4755346179008484\n",
      "Epoch 11148: train loss: 0.4755345284938812\n",
      "Epoch 11149: train loss: 0.4755344092845917\n",
      "Epoch 11150: train loss: 0.4755343198776245\n",
      "Epoch 11151: train loss: 0.47553426027297974\n",
      "Epoch 11152: train loss: 0.47553420066833496\n",
      "Epoch 11153: train loss: 0.475534051656723\n",
      "Epoch 11154: train loss: 0.47553399205207825\n",
      "Epoch 11155: train loss: 0.4755339026451111\n",
      "Epoch 11156: train loss: 0.4755338430404663\n",
      "Epoch 11157: train loss: 0.47553372383117676\n",
      "Epoch 11158: train loss: 0.4755336344242096\n",
      "Epoch 11159: train loss: 0.4755335748195648\n",
      "Epoch 11160: train loss: 0.47553348541259766\n",
      "Epoch 11161: train loss: 0.4755334258079529\n",
      "Epoch 11162: train loss: 0.47553330659866333\n",
      "Epoch 11163: train loss: 0.47553321719169617\n",
      "Epoch 11164: train loss: 0.4755331575870514\n",
      "Epoch 11165: train loss: 0.47553306818008423\n",
      "Epoch 11166: train loss: 0.4755329489707947\n",
      "Epoch 11167: train loss: 0.4755328893661499\n",
      "Epoch 11168: train loss: 0.47553279995918274\n",
      "Epoch 11169: train loss: 0.47553274035453796\n",
      "Epoch 11170: train loss: 0.4755326509475708\n",
      "Epoch 11171: train loss: 0.47553253173828125\n",
      "Epoch 11172: train loss: 0.4755324721336365\n",
      "Epoch 11173: train loss: 0.4755323827266693\n",
      "Epoch 11174: train loss: 0.47553226351737976\n",
      "Epoch 11175: train loss: 0.475532203912735\n",
      "Epoch 11176: train loss: 0.4755321145057678\n",
      "Epoch 11177: train loss: 0.47553199529647827\n",
      "Epoch 11178: train loss: 0.47553199529647827\n",
      "Epoch 11179: train loss: 0.47553184628486633\n",
      "Epoch 11180: train loss: 0.47553178668022156\n",
      "Epoch 11181: train loss: 0.4755316972732544\n",
      "Epoch 11182: train loss: 0.4755316376686096\n",
      "Epoch 11183: train loss: 0.47553157806396484\n",
      "Epoch 11184: train loss: 0.4755314886569977\n",
      "Epoch 11185: train loss: 0.47553136944770813\n",
      "Epoch 11186: train loss: 0.47553128004074097\n",
      "Epoch 11187: train loss: 0.4755311608314514\n",
      "Epoch 11188: train loss: 0.47553107142448425\n",
      "Epoch 11189: train loss: 0.4755310118198395\n",
      "Epoch 11190: train loss: 0.4755309522151947\n",
      "Epoch 11191: train loss: 0.47553086280822754\n",
      "Epoch 11192: train loss: 0.475530743598938\n",
      "Epoch 11193: train loss: 0.4755307137966156\n",
      "Epoch 11194: train loss: 0.47553059458732605\n",
      "Epoch 11195: train loss: 0.4755305051803589\n",
      "Epoch 11196: train loss: 0.4755305051803589\n",
      "Epoch 11197: train loss: 0.47553032636642456\n",
      "Epoch 11198: train loss: 0.4755302369594574\n",
      "Epoch 11199: train loss: 0.4755301773548126\n",
      "Epoch 11200: train loss: 0.47553011775016785\n",
      "Epoch 11201: train loss: 0.4755299687385559\n",
      "Epoch 11202: train loss: 0.47552990913391113\n",
      "Epoch 11203: train loss: 0.47552981972694397\n",
      "Epoch 11204: train loss: 0.4755297601222992\n",
      "Epoch 11205: train loss: 0.4755297005176544\n",
      "Epoch 11206: train loss: 0.4755295515060425\n",
      "Epoch 11207: train loss: 0.4755294919013977\n",
      "Epoch 11208: train loss: 0.4755294620990753\n",
      "Epoch 11209: train loss: 0.47552934288978577\n",
      "Epoch 11210: train loss: 0.4755292534828186\n",
      "Epoch 11211: train loss: 0.47552919387817383\n",
      "Epoch 11212: train loss: 0.4755290448665619\n",
      "Epoch 11213: train loss: 0.4755289852619171\n",
      "Epoch 11214: train loss: 0.47552886605262756\n",
      "Epoch 11215: train loss: 0.4755288362503052\n",
      "Epoch 11216: train loss: 0.4755287766456604\n",
      "Epoch 11217: train loss: 0.47552865743637085\n",
      "Epoch 11218: train loss: 0.4755285680294037\n",
      "Epoch 11219: train loss: 0.4755285084247589\n",
      "Epoch 11220: train loss: 0.47552838921546936\n",
      "Epoch 11221: train loss: 0.475528359413147\n",
      "Epoch 11222: train loss: 0.4755282402038574\n",
      "Epoch 11223: train loss: 0.47552815079689026\n",
      "Epoch 11224: train loss: 0.4755280911922455\n",
      "Epoch 11225: train loss: 0.47552797198295593\n",
      "Epoch 11226: train loss: 0.47552794218063354\n",
      "Epoch 11227: train loss: 0.475527822971344\n",
      "Epoch 11228: train loss: 0.4755277633666992\n",
      "Epoch 11229: train loss: 0.4755276143550873\n",
      "Epoch 11230: train loss: 0.4755275547504425\n",
      "Epoch 11231: train loss: 0.47552746534347534\n",
      "Epoch 11232: train loss: 0.47552740573883057\n",
      "Epoch 11233: train loss: 0.4755273163318634\n",
      "Epoch 11234: train loss: 0.47552725672721863\n",
      "Epoch 11235: train loss: 0.4755271375179291\n",
      "Epoch 11236: train loss: 0.4755271077156067\n",
      "Epoch 11237: train loss: 0.47552698850631714\n",
      "Epoch 11238: train loss: 0.47552689909935\n",
      "Epoch 11239: train loss: 0.4755267798900604\n",
      "Epoch 11240: train loss: 0.47552672028541565\n",
      "Epoch 11241: train loss: 0.4755266308784485\n",
      "Epoch 11242: train loss: 0.47552651166915894\n",
      "Epoch 11243: train loss: 0.47552648186683655\n",
      "Epoch 11244: train loss: 0.475526362657547\n",
      "Epoch 11245: train loss: 0.4755263030529022\n",
      "Epoch 11246: train loss: 0.47552621364593506\n",
      "Epoch 11247: train loss: 0.4755260944366455\n",
      "Epoch 11248: train loss: 0.4755260646343231\n",
      "Epoch 11249: train loss: 0.47552594542503357\n",
      "Epoch 11250: train loss: 0.4755258858203888\n",
      "Epoch 11251: train loss: 0.47552573680877686\n",
      "Epoch 11252: train loss: 0.4755256772041321\n",
      "Epoch 11253: train loss: 0.4755256474018097\n",
      "Epoch 11254: train loss: 0.47552552819252014\n",
      "Epoch 11255: train loss: 0.47552552819252014\n",
      "Epoch 11256: train loss: 0.4755253791809082\n",
      "Epoch 11257: train loss: 0.4755253195762634\n",
      "Epoch 11258: train loss: 0.47552523016929626\n",
      "Epoch 11259: train loss: 0.4755251705646515\n",
      "Epoch 11260: train loss: 0.4755251109600067\n",
      "Epoch 11261: train loss: 0.4755249619483948\n",
      "Epoch 11262: train loss: 0.47552490234375\n",
      "Epoch 11263: train loss: 0.47552478313446045\n",
      "Epoch 11264: train loss: 0.4755246937274933\n",
      "Epoch 11265: train loss: 0.47552457451820374\n",
      "Epoch 11266: train loss: 0.47552454471588135\n",
      "Epoch 11267: train loss: 0.4755244255065918\n",
      "Epoch 11268: train loss: 0.475524365901947\n",
      "Epoch 11269: train loss: 0.47552427649497986\n",
      "Epoch 11270: train loss: 0.4755242168903351\n",
      "Epoch 11271: train loss: 0.4755241274833679\n",
      "Epoch 11272: train loss: 0.47552400827407837\n",
      "Epoch 11273: train loss: 0.4755239486694336\n",
      "Epoch 11274: train loss: 0.47552385926246643\n",
      "Epoch 11275: train loss: 0.47552379965782166\n",
      "Epoch 11276: train loss: 0.4755236506462097\n",
      "Epoch 11277: train loss: 0.47552359104156494\n",
      "Epoch 11278: train loss: 0.47552353143692017\n",
      "Epoch 11279: train loss: 0.475523442029953\n",
      "Epoch 11280: train loss: 0.47552332282066345\n",
      "Epoch 11281: train loss: 0.47552332282066345\n",
      "Epoch 11282: train loss: 0.4755232334136963\n",
      "Epoch 11283: train loss: 0.47552311420440674\n",
      "Epoch 11284: train loss: 0.4755230247974396\n",
      "Epoch 11285: train loss: 0.47552290558815\n",
      "Epoch 11286: train loss: 0.47552287578582764\n",
      "Epoch 11287: train loss: 0.4755227565765381\n",
      "Epoch 11288: train loss: 0.4755226969718933\n",
      "Epoch 11289: train loss: 0.47552260756492615\n",
      "Epoch 11290: train loss: 0.47552254796028137\n",
      "Epoch 11291: train loss: 0.4755224883556366\n",
      "Epoch 11292: train loss: 0.47552239894866943\n",
      "Epoch 11293: train loss: 0.4755222797393799\n",
      "Epoch 11294: train loss: 0.4755221903324127\n",
      "Epoch 11295: train loss: 0.47552207112312317\n",
      "Epoch 11296: train loss: 0.4755220413208008\n",
      "Epoch 11297: train loss: 0.47552192211151123\n",
      "Epoch 11298: train loss: 0.47552183270454407\n",
      "Epoch 11299: train loss: 0.4755217730998993\n",
      "Epoch 11300: train loss: 0.4755217134952545\n",
      "Epoch 11301: train loss: 0.47552162408828735\n",
      "Epoch 11302: train loss: 0.4755215048789978\n",
      "Epoch 11303: train loss: 0.475521445274353\n",
      "Epoch 11304: train loss: 0.47552135586738586\n",
      "Epoch 11305: train loss: 0.4755212962627411\n",
      "Epoch 11306: train loss: 0.4755212068557739\n",
      "Epoch 11307: train loss: 0.47552114725112915\n",
      "Epoch 11308: train loss: 0.4755209684371948\n",
      "Epoch 11309: train loss: 0.47552093863487244\n",
      "Epoch 11310: train loss: 0.47552087903022766\n",
      "Epoch 11311: train loss: 0.4755207598209381\n",
      "Epoch 11312: train loss: 0.4755207300186157\n",
      "Epoch 11313: train loss: 0.4755205512046814\n",
      "Epoch 11314: train loss: 0.475520521402359\n",
      "Epoch 11315: train loss: 0.47552046179771423\n",
      "Epoch 11316: train loss: 0.4755203425884247\n",
      "Epoch 11317: train loss: 0.4755203127861023\n",
      "Epoch 11318: train loss: 0.47552019357681274\n",
      "Epoch 11319: train loss: 0.4755201041698456\n",
      "Epoch 11320: train loss: 0.4755200445652008\n",
      "Epoch 11321: train loss: 0.47551992535591125\n",
      "Epoch 11322: train loss: 0.4755198359489441\n",
      "Epoch 11323: train loss: 0.4755197763442993\n",
      "Epoch 11324: train loss: 0.47551968693733215\n",
      "Epoch 11325: train loss: 0.4755195677280426\n",
      "Epoch 11326: train loss: 0.4755195081233978\n",
      "Epoch 11327: train loss: 0.47551947832107544\n",
      "Epoch 11328: train loss: 0.4755192995071411\n",
      "Epoch 11329: train loss: 0.4755192697048187\n",
      "Epoch 11330: train loss: 0.47551921010017395\n",
      "Epoch 11331: train loss: 0.4755190908908844\n",
      "Epoch 11332: train loss: 0.47551900148391724\n",
      "Epoch 11333: train loss: 0.47551894187927246\n",
      "Epoch 11334: train loss: 0.4755188524723053\n",
      "Epoch 11335: train loss: 0.47551873326301575\n",
      "Epoch 11336: train loss: 0.47551867365837097\n",
      "Epoch 11337: train loss: 0.4755185842514038\n",
      "Epoch 11338: train loss: 0.47551852464675903\n",
      "Epoch 11339: train loss: 0.47551843523979187\n",
      "Epoch 11340: train loss: 0.4755183756351471\n",
      "Epoch 11341: train loss: 0.47551825642585754\n",
      "Epoch 11342: train loss: 0.47551822662353516\n",
      "Epoch 11343: train loss: 0.47551804780960083\n",
      "Epoch 11344: train loss: 0.47551801800727844\n",
      "Epoch 11345: train loss: 0.4755178987979889\n",
      "Epoch 11346: train loss: 0.4755178391933441\n",
      "Epoch 11347: train loss: 0.47551780939102173\n",
      "Epoch 11348: train loss: 0.4755176901817322\n",
      "Epoch 11349: train loss: 0.475517600774765\n",
      "Epoch 11350: train loss: 0.47551754117012024\n",
      "Epoch 11351: train loss: 0.47551748156547546\n",
      "Epoch 11352: train loss: 0.4755173921585083\n",
      "Epoch 11353: train loss: 0.47551727294921875\n",
      "Epoch 11354: train loss: 0.4755171537399292\n",
      "Epoch 11355: train loss: 0.47551706433296204\n",
      "Epoch 11356: train loss: 0.47551700472831726\n",
      "Epoch 11357: train loss: 0.4755169451236725\n",
      "Epoch 11358: train loss: 0.4755168557167053\n",
      "Epoch 11359: train loss: 0.47551673650741577\n",
      "Epoch 11360: train loss: 0.47551673650741577\n",
      "Epoch 11361: train loss: 0.47551658749580383\n",
      "Epoch 11362: train loss: 0.47551652789115906\n",
      "Epoch 11363: train loss: 0.4755164384841919\n",
      "Epoch 11364: train loss: 0.4755163788795471\n",
      "Epoch 11365: train loss: 0.47551628947257996\n",
      "Epoch 11366: train loss: 0.47551611065864563\n",
      "Epoch 11367: train loss: 0.47551611065864563\n",
      "Epoch 11368: train loss: 0.47551602125167847\n",
      "Epoch 11369: train loss: 0.4755159616470337\n",
      "Epoch 11370: train loss: 0.47551587224006653\n",
      "Epoch 11371: train loss: 0.47551581263542175\n",
      "Epoch 11372: train loss: 0.4755156934261322\n",
      "Epoch 11373: train loss: 0.47551560401916504\n",
      "Epoch 11374: train loss: 0.47551554441452026\n",
      "Epoch 11375: train loss: 0.4755154550075531\n",
      "Epoch 11376: train loss: 0.47551533579826355\n",
      "Epoch 11377: train loss: 0.4755152761936188\n",
      "Epoch 11378: train loss: 0.4755151867866516\n",
      "Epoch 11379: train loss: 0.47551512718200684\n",
      "Epoch 11380: train loss: 0.47551506757736206\n",
      "Epoch 11381: train loss: 0.4755149781703949\n",
      "Epoch 11382: train loss: 0.47551485896110535\n",
      "Epoch 11383: train loss: 0.4755147695541382\n",
      "Epoch 11384: train loss: 0.4755147099494934\n",
      "Epoch 11385: train loss: 0.47551462054252625\n",
      "Epoch 11386: train loss: 0.4755145013332367\n",
      "Epoch 11387: train loss: 0.4755144417285919\n",
      "Epoch 11388: train loss: 0.47551441192626953\n",
      "Epoch 11389: train loss: 0.47551429271698\n",
      "Epoch 11390: train loss: 0.4755142033100128\n",
      "Epoch 11391: train loss: 0.47551414370536804\n",
      "Epoch 11392: train loss: 0.47551408410072327\n",
      "Epoch 11393: train loss: 0.4755139946937561\n",
      "Epoch 11394: train loss: 0.4755138158798218\n",
      "Epoch 11395: train loss: 0.4755137860774994\n",
      "Epoch 11396: train loss: 0.4755137264728546\n",
      "Epoch 11397: train loss: 0.47551360726356506\n",
      "Epoch 11398: train loss: 0.4755135178565979\n",
      "Epoch 11399: train loss: 0.4755134582519531\n",
      "Epoch 11400: train loss: 0.47551339864730835\n",
      "Epoch 11401: train loss: 0.4755133092403412\n",
      "Epoch 11402: train loss: 0.47551319003105164\n",
      "Epoch 11403: train loss: 0.47551313042640686\n",
      "Epoch 11404: train loss: 0.4755130410194397\n",
      "Epoch 11405: train loss: 0.47551292181015015\n",
      "Epoch 11406: train loss: 0.47551289200782776\n",
      "Epoch 11407: train loss: 0.475512832403183\n",
      "Epoch 11408: train loss: 0.47551271319389343\n",
      "Epoch 11409: train loss: 0.47551262378692627\n",
      "Epoch 11410: train loss: 0.4755125641822815\n",
      "Epoch 11411: train loss: 0.47551247477531433\n",
      "Epoch 11412: train loss: 0.4755123555660248\n",
      "Epoch 11413: train loss: 0.47551229596138\n",
      "Epoch 11414: train loss: 0.47551220655441284\n",
      "Epoch 11415: train loss: 0.47551214694976807\n",
      "Epoch 11416: train loss: 0.47551199793815613\n",
      "Epoch 11417: train loss: 0.47551199793815613\n",
      "Epoch 11418: train loss: 0.4755118787288666\n",
      "Epoch 11419: train loss: 0.4755117893218994\n",
      "Epoch 11420: train loss: 0.47551172971725464\n",
      "Epoch 11421: train loss: 0.4755116403102875\n",
      "Epoch 11422: train loss: 0.4755115807056427\n",
      "Epoch 11423: train loss: 0.47551146149635315\n",
      "Epoch 11424: train loss: 0.475511372089386\n",
      "Epoch 11425: train loss: 0.4755113124847412\n",
      "Epoch 11426: train loss: 0.47551122307777405\n",
      "Epoch 11427: train loss: 0.4755111038684845\n",
      "Epoch 11428: train loss: 0.4755110442638397\n",
      "Epoch 11429: train loss: 0.47551095485687256\n",
      "Epoch 11430: train loss: 0.4755108952522278\n",
      "Epoch 11431: train loss: 0.4755108058452606\n",
      "Epoch 11432: train loss: 0.47551074624061584\n",
      "Epoch 11433: train loss: 0.47551068663597107\n",
      "Epoch 11434: train loss: 0.4755105972290039\n",
      "Epoch 11435: train loss: 0.47551047801971436\n",
      "Epoch 11436: train loss: 0.4755104184150696\n",
      "Epoch 11437: train loss: 0.4755103886127472\n",
      "Epoch 11438: train loss: 0.47551020979881287\n",
      "Epoch 11439: train loss: 0.4755101203918457\n",
      "Epoch 11440: train loss: 0.4755100607872009\n",
      "Epoch 11441: train loss: 0.47550997138023376\n",
      "Epoch 11442: train loss: 0.475509911775589\n",
      "Epoch 11443: train loss: 0.4755098521709442\n",
      "Epoch 11444: train loss: 0.4755097031593323\n",
      "Epoch 11445: train loss: 0.4755096435546875\n",
      "Epoch 11446: train loss: 0.4755095839500427\n",
      "Epoch 11447: train loss: 0.47550949454307556\n",
      "Epoch 11448: train loss: 0.47550931572914124\n",
      "Epoch 11449: train loss: 0.47550931572914124\n",
      "Epoch 11450: train loss: 0.4755092263221741\n",
      "Epoch 11451: train loss: 0.4755091071128845\n",
      "Epoch 11452: train loss: 0.47550907731056213\n",
      "Epoch 11453: train loss: 0.47550901770591736\n",
      "Epoch 11454: train loss: 0.4755089581012726\n",
      "Epoch 11455: train loss: 0.47550880908966064\n",
      "Epoch 11456: train loss: 0.47550874948501587\n",
      "Epoch 11457: train loss: 0.4755086600780487\n",
      "Epoch 11458: train loss: 0.47550854086875916\n",
      "Epoch 11459: train loss: 0.4755084812641144\n",
      "Epoch 11460: train loss: 0.4755083918571472\n",
      "Epoch 11461: train loss: 0.47550833225250244\n",
      "Epoch 11462: train loss: 0.47550827264785767\n",
      "Epoch 11463: train loss: 0.4755081832408905\n",
      "Epoch 11464: train loss: 0.47550806403160095\n",
      "Epoch 11465: train loss: 0.47550803422927856\n",
      "Epoch 11466: train loss: 0.475507915019989\n",
      "Epoch 11467: train loss: 0.47550785541534424\n",
      "Epoch 11468: train loss: 0.47550782561302185\n",
      "Epoch 11469: train loss: 0.4755077064037323\n",
      "Epoch 11470: train loss: 0.47550761699676514\n",
      "Epoch 11471: train loss: 0.4755074977874756\n",
      "Epoch 11472: train loss: 0.4755074381828308\n",
      "Epoch 11473: train loss: 0.47550734877586365\n",
      "Epoch 11474: train loss: 0.4755072295665741\n",
      "Epoch 11475: train loss: 0.4755071997642517\n",
      "Epoch 11476: train loss: 0.47550714015960693\n",
      "Epoch 11477: train loss: 0.47550708055496216\n",
      "Epoch 11478: train loss: 0.4755069315433502\n",
      "Epoch 11479: train loss: 0.47550687193870544\n",
      "Epoch 11480: train loss: 0.4755067825317383\n",
      "Epoch 11481: train loss: 0.47550666332244873\n",
      "Epoch 11482: train loss: 0.47550660371780396\n",
      "Epoch 11483: train loss: 0.4755065143108368\n",
      "Epoch 11484: train loss: 0.47550639510154724\n",
      "Epoch 11485: train loss: 0.47550636529922485\n",
      "Epoch 11486: train loss: 0.4755062460899353\n",
      "Epoch 11487: train loss: 0.4755062460899353\n",
      "Epoch 11488: train loss: 0.47550609707832336\n",
      "Epoch 11489: train loss: 0.4755060374736786\n",
      "Epoch 11490: train loss: 0.4755059480667114\n",
      "Epoch 11491: train loss: 0.47550588846206665\n",
      "Epoch 11492: train loss: 0.4755057692527771\n",
      "Epoch 11493: train loss: 0.4755057096481323\n",
      "Epoch 11494: train loss: 0.47550562024116516\n",
      "Epoch 11495: train loss: 0.4755055606365204\n",
      "Epoch 11496: train loss: 0.4755054712295532\n",
      "Epoch 11497: train loss: 0.47550535202026367\n",
      "Epoch 11498: train loss: 0.4755052924156189\n",
      "Epoch 11499: train loss: 0.47550520300865173\n",
      "Epoch 11500: train loss: 0.47550514340400696\n",
      "Epoch 11501: train loss: 0.4755050539970398\n",
      "Epoch 11502: train loss: 0.475504994392395\n",
      "Epoch 11503: train loss: 0.47550493478775024\n",
      "Epoch 11504: train loss: 0.4755048453807831\n",
      "Epoch 11505: train loss: 0.47550472617149353\n",
      "Epoch 11506: train loss: 0.47550463676452637\n",
      "Epoch 11507: train loss: 0.4755045771598816\n",
      "Epoch 11508: train loss: 0.4755045175552368\n",
      "Epoch 11509: train loss: 0.47550442814826965\n",
      "Epoch 11510: train loss: 0.4755043089389801\n",
      "Epoch 11511: train loss: 0.47550421953201294\n",
      "Epoch 11512: train loss: 0.47550415992736816\n",
      "Epoch 11513: train loss: 0.4755041003227234\n",
      "Epoch 11514: train loss: 0.4755040109157562\n",
      "Epoch 11515: train loss: 0.4755038917064667\n",
      "Epoch 11516: train loss: 0.4755038022994995\n",
      "Epoch 11517: train loss: 0.47550374269485474\n",
      "Epoch 11518: train loss: 0.4755036234855652\n",
      "Epoch 11519: train loss: 0.4755035936832428\n",
      "Epoch 11520: train loss: 0.475503534078598\n",
      "Epoch 11521: train loss: 0.47550341486930847\n",
      "Epoch 11522: train loss: 0.4755033254623413\n",
      "Epoch 11523: train loss: 0.47550326585769653\n",
      "Epoch 11524: train loss: 0.47550320625305176\n",
      "Epoch 11525: train loss: 0.4755030572414398\n",
      "Epoch 11526: train loss: 0.47550299763679504\n",
      "Epoch 11527: train loss: 0.47550296783447266\n",
      "Epoch 11528: train loss: 0.4755028486251831\n",
      "Epoch 11529: train loss: 0.47550278902053833\n",
      "Epoch 11530: train loss: 0.47550269961357117\n",
      "Epoch 11531: train loss: 0.4755025804042816\n",
      "Epoch 11532: train loss: 0.47550255060195923\n",
      "Epoch 11533: train loss: 0.4755024313926697\n",
      "Epoch 11534: train loss: 0.4755023717880249\n",
      "Epoch 11535: train loss: 0.47550228238105774\n",
      "Epoch 11536: train loss: 0.4755021631717682\n",
      "Epoch 11537: train loss: 0.4755021333694458\n",
      "Epoch 11538: train loss: 0.475502073764801\n",
      "Epoch 11539: train loss: 0.4755018949508667\n",
      "Epoch 11540: train loss: 0.4755018651485443\n",
      "Epoch 11541: train loss: 0.47550180554389954\n",
      "Epoch 11542: train loss: 0.47550168633461\n",
      "Epoch 11543: train loss: 0.4755015969276428\n",
      "Epoch 11544: train loss: 0.47550153732299805\n",
      "Epoch 11545: train loss: 0.47550147771835327\n",
      "Epoch 11546: train loss: 0.4755013883113861\n",
      "Epoch 11547: train loss: 0.47550126910209656\n",
      "Epoch 11548: train loss: 0.47550123929977417\n",
      "Epoch 11549: train loss: 0.4755011796951294\n",
      "Epoch 11550: train loss: 0.47550106048583984\n",
      "Epoch 11551: train loss: 0.4755009710788727\n",
      "Epoch 11552: train loss: 0.4755009114742279\n",
      "Epoch 11553: train loss: 0.47550082206726074\n",
      "Epoch 11554: train loss: 0.4755006432533264\n",
      "Epoch 11555: train loss: 0.47550061345100403\n",
      "Epoch 11556: train loss: 0.47550061345100403\n",
      "Epoch 11557: train loss: 0.4755004346370697\n",
      "Epoch 11558: train loss: 0.4755004048347473\n",
      "Epoch 11559: train loss: 0.47550034523010254\n",
      "Epoch 11560: train loss: 0.475500226020813\n",
      "Epoch 11561: train loss: 0.4755001962184906\n",
      "Epoch 11562: train loss: 0.47550007700920105\n",
      "Epoch 11563: train loss: 0.4755000174045563\n",
      "Epoch 11564: train loss: 0.4754999279975891\n",
      "Epoch 11565: train loss: 0.47549980878829956\n",
      "Epoch 11566: train loss: 0.4754997789859772\n",
      "Epoch 11567: train loss: 0.4754996597766876\n",
      "Epoch 11568: train loss: 0.47549957036972046\n",
      "Epoch 11569: train loss: 0.4754995107650757\n",
      "Epoch 11570: train loss: 0.47549939155578613\n",
      "Epoch 11571: train loss: 0.47549930214881897\n",
      "Epoch 11572: train loss: 0.47549930214881897\n",
      "Epoch 11573: train loss: 0.4754991829395294\n",
      "Epoch 11574: train loss: 0.47549909353256226\n",
      "Epoch 11575: train loss: 0.4754990339279175\n",
      "Epoch 11576: train loss: 0.4754989445209503\n",
      "Epoch 11577: train loss: 0.47549888491630554\n",
      "Epoch 11578: train loss: 0.475498765707016\n",
      "Epoch 11579: train loss: 0.47549867630004883\n",
      "Epoch 11580: train loss: 0.4754985570907593\n",
      "Epoch 11581: train loss: 0.4754985272884369\n",
      "Epoch 11582: train loss: 0.4754984676837921\n",
      "Epoch 11583: train loss: 0.47549834847450256\n",
      "Epoch 11584: train loss: 0.4754983186721802\n",
      "Epoch 11585: train loss: 0.4754981994628906\n",
      "Epoch 11586: train loss: 0.4754980802536011\n",
      "Epoch 11587: train loss: 0.4754980504512787\n",
      "Epoch 11588: train loss: 0.47549793124198914\n",
      "Epoch 11589: train loss: 0.47549793124198914\n",
      "Epoch 11590: train loss: 0.475497841835022\n",
      "Epoch 11591: train loss: 0.4754977226257324\n",
      "Epoch 11592: train loss: 0.47549763321876526\n",
      "Epoch 11593: train loss: 0.4754975736141205\n",
      "Epoch 11594: train loss: 0.47549745440483093\n",
      "Epoch 11595: train loss: 0.47549742460250854\n",
      "Epoch 11596: train loss: 0.47549736499786377\n",
      "Epoch 11597: train loss: 0.4754972457885742\n",
      "Epoch 11598: train loss: 0.47549715638160706\n",
      "Epoch 11599: train loss: 0.4754970967769623\n",
      "Epoch 11600: train loss: 0.4754970073699951\n",
      "Epoch 11601: train loss: 0.47549694776535034\n",
      "Epoch 11602: train loss: 0.4754967987537384\n",
      "Epoch 11603: train loss: 0.47549673914909363\n",
      "Epoch 11604: train loss: 0.4754966199398041\n",
      "Epoch 11605: train loss: 0.4754965901374817\n",
      "Epoch 11606: train loss: 0.4754965305328369\n",
      "Epoch 11607: train loss: 0.47549641132354736\n",
      "Epoch 11608: train loss: 0.4754963219165802\n",
      "Epoch 11609: train loss: 0.4754962623119354\n",
      "Epoch 11610: train loss: 0.47549617290496826\n",
      "Epoch 11611: train loss: 0.4754960536956787\n",
      "Epoch 11612: train loss: 0.47549599409103394\n",
      "Epoch 11613: train loss: 0.4754959046840668\n",
      "Epoch 11614: train loss: 0.475495845079422\n",
      "Epoch 11615: train loss: 0.47549575567245483\n",
      "Epoch 11616: train loss: 0.47549569606781006\n",
      "Epoch 11617: train loss: 0.4754956364631653\n",
      "Epoch 11618: train loss: 0.4754955470561981\n",
      "Epoch 11619: train loss: 0.47549542784690857\n",
      "Epoch 11620: train loss: 0.4754953682422638\n",
      "Epoch 11621: train loss: 0.47549527883529663\n",
      "Epoch 11622: train loss: 0.47549521923065186\n",
      "Epoch 11623: train loss: 0.4754951298236847\n",
      "Epoch 11624: train loss: 0.4754950702190399\n",
      "Epoch 11625: train loss: 0.47549495100975037\n",
      "Epoch 11626: train loss: 0.475494921207428\n",
      "Epoch 11627: train loss: 0.4754948019981384\n",
      "Epoch 11628: train loss: 0.47549471259117126\n",
      "Epoch 11629: train loss: 0.4754946529865265\n",
      "Epoch 11630: train loss: 0.47549453377723694\n",
      "Epoch 11631: train loss: 0.47549450397491455\n",
      "Epoch 11632: train loss: 0.4754944443702698\n",
      "Epoch 11633: train loss: 0.4754943251609802\n",
      "Epoch 11634: train loss: 0.47549426555633545\n",
      "Epoch 11635: train loss: 0.4754941761493683\n",
      "Epoch 11636: train loss: 0.47549405694007874\n",
      "Epoch 11637: train loss: 0.47549402713775635\n",
      "Epoch 11638: train loss: 0.4754939079284668\n",
      "Epoch 11639: train loss: 0.475493848323822\n",
      "Epoch 11640: train loss: 0.47549375891685486\n",
      "Epoch 11641: train loss: 0.4754936993122101\n",
      "Epoch 11642: train loss: 0.4754936099052429\n",
      "Epoch 11643: train loss: 0.47549349069595337\n",
      "Epoch 11644: train loss: 0.4754934310913086\n",
      "Epoch 11645: train loss: 0.47549334168434143\n",
      "Epoch 11646: train loss: 0.47549328207969666\n",
      "Epoch 11647: train loss: 0.4754932224750519\n",
      "Epoch 11648: train loss: 0.4754931330680847\n",
      "Epoch 11649: train loss: 0.47549301385879517\n",
      "Epoch 11650: train loss: 0.475492924451828\n",
      "Epoch 11651: train loss: 0.4754928648471832\n",
      "Epoch 11652: train loss: 0.47549280524253845\n",
      "Epoch 11653: train loss: 0.4754927158355713\n",
      "Epoch 11654: train loss: 0.47549259662628174\n",
      "Epoch 11655: train loss: 0.4754925072193146\n",
      "Epoch 11656: train loss: 0.4754924476146698\n",
      "Epoch 11657: train loss: 0.475492388010025\n",
      "Epoch 11658: train loss: 0.47549229860305786\n",
      "Epoch 11659: train loss: 0.4754921793937683\n",
      "Epoch 11660: train loss: 0.4754921495914459\n",
      "Epoch 11661: train loss: 0.47549203038215637\n",
      "Epoch 11662: train loss: 0.4754919707775116\n",
      "Epoch 11663: train loss: 0.47549188137054443\n",
      "Epoch 11664: train loss: 0.47549182176589966\n",
      "Epoch 11665: train loss: 0.4754917323589325\n",
      "Epoch 11666: train loss: 0.4754916727542877\n",
      "Epoch 11667: train loss: 0.47549155354499817\n",
      "Epoch 11668: train loss: 0.4754915237426758\n",
      "Epoch 11669: train loss: 0.47549140453338623\n",
      "Epoch 11670: train loss: 0.47549134492874146\n",
      "Epoch 11671: train loss: 0.4754912555217743\n",
      "Epoch 11672: train loss: 0.47549113631248474\n",
      "Epoch 11673: train loss: 0.47549110651016235\n",
      "Epoch 11674: train loss: 0.4754910469055176\n",
      "Epoch 11675: train loss: 0.475490927696228\n",
      "Epoch 11676: train loss: 0.47549089789390564\n",
      "Epoch 11677: train loss: 0.4754907190799713\n",
      "Epoch 11678: train loss: 0.4754906892776489\n",
      "Epoch 11679: train loss: 0.47549062967300415\n",
      "Epoch 11680: train loss: 0.4754905104637146\n",
      "Epoch 11681: train loss: 0.4754904508590698\n",
      "Epoch 11682: train loss: 0.47549042105674744\n",
      "Epoch 11683: train loss: 0.4754903018474579\n",
      "Epoch 11684: train loss: 0.4754902422428131\n",
      "Epoch 11685: train loss: 0.47549009323120117\n",
      "Epoch 11686: train loss: 0.4754900336265564\n",
      "Epoch 11687: train loss: 0.475490003824234\n",
      "Epoch 11688: train loss: 0.47548988461494446\n",
      "Epoch 11689: train loss: 0.4754897952079773\n",
      "Epoch 11690: train loss: 0.4754897356033325\n",
      "Epoch 11691: train loss: 0.47548967599868774\n",
      "Epoch 11692: train loss: 0.4754895865917206\n",
      "Epoch 11693: train loss: 0.4754895269870758\n",
      "Epoch 11694: train loss: 0.47548940777778625\n",
      "Epoch 11695: train loss: 0.4754893183708191\n",
      "Epoch 11696: train loss: 0.4754892587661743\n",
      "Epoch 11697: train loss: 0.47548916935920715\n",
      "Epoch 11698: train loss: 0.4754890501499176\n",
      "Epoch 11699: train loss: 0.4754889905452728\n",
      "Epoch 11700: train loss: 0.47548896074295044\n",
      "Epoch 11701: train loss: 0.4754888415336609\n",
      "Epoch 11702: train loss: 0.4754887819290161\n",
      "Epoch 11703: train loss: 0.47548869252204895\n",
      "Epoch 11704: train loss: 0.4754885733127594\n",
      "Epoch 11705: train loss: 0.47548848390579224\n",
      "Epoch 11706: train loss: 0.47548842430114746\n",
      "Epoch 11707: train loss: 0.4754883646965027\n",
      "Epoch 11708: train loss: 0.4754883348941803\n",
      "Epoch 11709: train loss: 0.47548821568489075\n",
      "Epoch 11710: train loss: 0.4754881262779236\n",
      "Epoch 11711: train loss: 0.47548800706863403\n",
      "Epoch 11712: train loss: 0.47548794746398926\n",
      "Epoch 11713: train loss: 0.47548791766166687\n",
      "Epoch 11714: train loss: 0.4754877984523773\n",
      "Epoch 11715: train loss: 0.47548770904541016\n",
      "Epoch 11716: train loss: 0.4754876494407654\n",
      "Epoch 11717: train loss: 0.47548753023147583\n",
      "Epoch 11718: train loss: 0.47548750042915344\n",
      "Epoch 11719: train loss: 0.4754873812198639\n",
      "Epoch 11720: train loss: 0.4754873216152191\n",
      "Epoch 11721: train loss: 0.47548723220825195\n",
      "Epoch 11722: train loss: 0.4754871726036072\n",
      "Epoch 11723: train loss: 0.47548708319664\n",
      "Epoch 11724: train loss: 0.47548708319664\n",
      "Epoch 11725: train loss: 0.4754869043827057\n",
      "Epoch 11726: train loss: 0.4754868745803833\n",
      "Epoch 11727: train loss: 0.47548675537109375\n",
      "Epoch 11728: train loss: 0.4754866361618042\n",
      "Epoch 11729: train loss: 0.4754866361618042\n",
      "Epoch 11730: train loss: 0.47548648715019226\n",
      "Epoch 11731: train loss: 0.4754863977432251\n",
      "Epoch 11732: train loss: 0.4754863381385803\n",
      "Epoch 11733: train loss: 0.47548627853393555\n",
      "Epoch 11734: train loss: 0.47548621892929077\n",
      "Epoch 11735: train loss: 0.4754861295223236\n",
      "Epoch 11736: train loss: 0.47548601031303406\n",
      "Epoch 11737: train loss: 0.47548598051071167\n",
      "Epoch 11738: train loss: 0.4754859209060669\n",
      "Epoch 11739: train loss: 0.47548580169677734\n",
      "Epoch 11740: train loss: 0.4754857122898102\n",
      "Epoch 11741: train loss: 0.4754856526851654\n",
      "Epoch 11742: train loss: 0.47548559308052063\n",
      "Epoch 11743: train loss: 0.4754854440689087\n",
      "Epoch 11744: train loss: 0.4754853844642639\n",
      "Epoch 11745: train loss: 0.47548529505729675\n",
      "Epoch 11746: train loss: 0.47548529505729675\n",
      "Epoch 11747: train loss: 0.4754851758480072\n",
      "Epoch 11748: train loss: 0.47548508644104004\n",
      "Epoch 11749: train loss: 0.4754849672317505\n",
      "Epoch 11750: train loss: 0.4754848778247833\n",
      "Epoch 11751: train loss: 0.4754848778247833\n",
      "Epoch 11752: train loss: 0.4754847586154938\n",
      "Epoch 11753: train loss: 0.4754846692085266\n",
      "Epoch 11754: train loss: 0.47548460960388184\n",
      "Epoch 11755: train loss: 0.4754845201969147\n",
      "Epoch 11756: train loss: 0.4754844605922699\n",
      "Epoch 11757: train loss: 0.4754844009876251\n",
      "Epoch 11758: train loss: 0.4754842519760132\n",
      "Epoch 11759: train loss: 0.4754842519760132\n",
      "Epoch 11760: train loss: 0.47548413276672363\n",
      "Epoch 11761: train loss: 0.47548404335975647\n",
      "Epoch 11762: train loss: 0.47548404335975647\n",
      "Epoch 11763: train loss: 0.47548389434814453\n",
      "Epoch 11764: train loss: 0.475483775138855\n",
      "Epoch 11765: train loss: 0.4754837155342102\n",
      "Epoch 11766: train loss: 0.4754836857318878\n",
      "Epoch 11767: train loss: 0.4754835069179535\n",
      "Epoch 11768: train loss: 0.4754834771156311\n",
      "Epoch 11769: train loss: 0.47548341751098633\n",
      "Epoch 11770: train loss: 0.4754832983016968\n",
      "Epoch 11771: train loss: 0.4754832088947296\n",
      "Epoch 11772: train loss: 0.4754832088947296\n",
      "Epoch 11773: train loss: 0.47548308968544006\n",
      "Epoch 11774: train loss: 0.4754830002784729\n",
      "Epoch 11775: train loss: 0.4754829406738281\n",
      "Epoch 11776: train loss: 0.4754828214645386\n",
      "Epoch 11777: train loss: 0.4754827320575714\n",
      "Epoch 11778: train loss: 0.4754827320575714\n",
      "Epoch 11779: train loss: 0.4754825830459595\n",
      "Epoch 11780: train loss: 0.4754825234413147\n",
      "Epoch 11781: train loss: 0.4754824638366699\n",
      "Epoch 11782: train loss: 0.47548237442970276\n",
      "Epoch 11783: train loss: 0.4754822552204132\n",
      "Epoch 11784: train loss: 0.4754822552204132\n",
      "Epoch 11785: train loss: 0.47548210620880127\n",
      "Epoch 11786: train loss: 0.4754820466041565\n",
      "Epoch 11787: train loss: 0.47548195719718933\n",
      "Epoch 11788: train loss: 0.4754818379878998\n",
      "Epoch 11789: train loss: 0.4754818379878998\n",
      "Epoch 11790: train loss: 0.4754817485809326\n",
      "Epoch 11791: train loss: 0.47548162937164307\n",
      "Epoch 11792: train loss: 0.4754815697669983\n",
      "Epoch 11793: train loss: 0.47548148036003113\n",
      "Epoch 11794: train loss: 0.47548142075538635\n",
      "Epoch 11795: train loss: 0.4754813313484192\n",
      "Epoch 11796: train loss: 0.4754812717437744\n",
      "Epoch 11797: train loss: 0.47548121213912964\n",
      "Epoch 11798: train loss: 0.4754810631275177\n",
      "Epoch 11799: train loss: 0.4754810035228729\n",
      "Epoch 11800: train loss: 0.47548091411590576\n",
      "Epoch 11801: train loss: 0.475480854511261\n",
      "Epoch 11802: train loss: 0.4754807949066162\n",
      "Epoch 11803: train loss: 0.47548070549964905\n",
      "Epoch 11804: train loss: 0.4754806458950043\n",
      "Epoch 11805: train loss: 0.4754805266857147\n",
      "Epoch 11806: train loss: 0.47548043727874756\n",
      "Epoch 11807: train loss: 0.4754803776741028\n",
      "Epoch 11808: train loss: 0.475480318069458\n",
      "Epoch 11809: train loss: 0.47548016905784607\n",
      "Epoch 11810: train loss: 0.4754801094532013\n",
      "Epoch 11811: train loss: 0.47548002004623413\n",
      "Epoch 11812: train loss: 0.47548002004623413\n",
      "Epoch 11813: train loss: 0.4754799008369446\n",
      "Epoch 11814: train loss: 0.4754798710346222\n",
      "Epoch 11815: train loss: 0.47547975182533264\n",
      "Epoch 11816: train loss: 0.4754796624183655\n",
      "Epoch 11817: train loss: 0.4754795432090759\n",
      "Epoch 11818: train loss: 0.4754795432090759\n",
      "Epoch 11819: train loss: 0.475479394197464\n",
      "Epoch 11820: train loss: 0.475479394197464\n",
      "Epoch 11821: train loss: 0.47547924518585205\n",
      "Epoch 11822: train loss: 0.47547924518585205\n",
      "Epoch 11823: train loss: 0.4754791259765625\n",
      "Epoch 11824: train loss: 0.47547900676727295\n",
      "Epoch 11825: train loss: 0.47547897696495056\n",
      "Epoch 11826: train loss: 0.4754789173603058\n",
      "Epoch 11827: train loss: 0.47547879815101624\n",
      "Epoch 11828: train loss: 0.4754787087440491\n",
      "Epoch 11829: train loss: 0.4754786491394043\n",
      "Epoch 11830: train loss: 0.47547855973243713\n",
      "Epoch 11831: train loss: 0.47547850012779236\n",
      "Epoch 11832: train loss: 0.4754783809185028\n",
      "Epoch 11833: train loss: 0.47547829151153564\n",
      "Epoch 11834: train loss: 0.4754781723022461\n",
      "Epoch 11835: train loss: 0.4754781424999237\n",
      "Epoch 11836: train loss: 0.47547808289527893\n",
      "Epoch 11837: train loss: 0.47547802329063416\n",
      "Epoch 11838: train loss: 0.475477933883667\n",
      "Epoch 11839: train loss: 0.4754778742790222\n",
      "Epoch 11840: train loss: 0.47547775506973267\n",
      "Epoch 11841: train loss: 0.4754776656627655\n",
      "Epoch 11842: train loss: 0.4754776060581207\n",
      "Epoch 11843: train loss: 0.47547751665115356\n",
      "Epoch 11844: train loss: 0.4754774570465088\n",
      "Epoch 11845: train loss: 0.47547733783721924\n",
      "Epoch 11846: train loss: 0.47547730803489685\n",
      "Epoch 11847: train loss: 0.4754771888256073\n",
      "Epoch 11848: train loss: 0.4754771292209625\n",
      "Epoch 11849: train loss: 0.47547709941864014\n",
      "Epoch 11850: train loss: 0.47547703981399536\n",
      "Epoch 11851: train loss: 0.4754768908023834\n",
      "Epoch 11852: train loss: 0.47547683119773865\n",
      "Epoch 11853: train loss: 0.4754767119884491\n",
      "Epoch 11854: train loss: 0.4754766821861267\n",
      "Epoch 11855: train loss: 0.47547662258148193\n",
      "Epoch 11856: train loss: 0.4754765033721924\n",
      "Epoch 11857: train loss: 0.4754764139652252\n",
      "Epoch 11858: train loss: 0.47547635436058044\n",
      "Epoch 11859: train loss: 0.4754762649536133\n",
      "Epoch 11860: train loss: 0.4754762053489685\n",
      "Epoch 11861: train loss: 0.47547608613967896\n",
      "Epoch 11862: train loss: 0.4754759967327118\n",
      "Epoch 11863: train loss: 0.4754759967327118\n",
      "Epoch 11864: train loss: 0.47547587752342224\n",
      "Epoch 11865: train loss: 0.4754757881164551\n",
      "Epoch 11866: train loss: 0.4754757285118103\n",
      "Epoch 11867: train loss: 0.47547563910484314\n",
      "Epoch 11868: train loss: 0.47547557950019836\n",
      "Epoch 11869: train loss: 0.4754754602909088\n",
      "Epoch 11870: train loss: 0.4754754304885864\n",
      "Epoch 11871: train loss: 0.4754753112792969\n",
      "Epoch 11872: train loss: 0.4754751920700073\n",
      "Epoch 11873: train loss: 0.47547516226768494\n",
      "Epoch 11874: train loss: 0.4754750430583954\n",
      "Epoch 11875: train loss: 0.4754749834537506\n",
      "Epoch 11876: train loss: 0.47547489404678345\n",
      "Epoch 11877: train loss: 0.47547483444213867\n",
      "Epoch 11878: train loss: 0.4754747748374939\n",
      "Epoch 11879: train loss: 0.47547468543052673\n",
      "Epoch 11880: train loss: 0.4754745662212372\n",
      "Epoch 11881: train loss: 0.4754745364189148\n",
      "Epoch 11882: train loss: 0.47547441720962524\n",
      "Epoch 11883: train loss: 0.47547435760498047\n",
      "Epoch 11884: train loss: 0.4754743278026581\n",
      "Epoch 11885: train loss: 0.47547420859336853\n",
      "Epoch 11886: train loss: 0.47547411918640137\n",
      "Epoch 11887: train loss: 0.4754740595817566\n",
      "Epoch 11888: train loss: 0.4754739999771118\n",
      "Epoch 11889: train loss: 0.47547391057014465\n",
      "Epoch 11890: train loss: 0.4754737913608551\n",
      "Epoch 11891: train loss: 0.47547370195388794\n",
      "Epoch 11892: train loss: 0.47547364234924316\n",
      "Epoch 11893: train loss: 0.4754735827445984\n",
      "Epoch 11894: train loss: 0.4754734933376312\n",
      "Epoch 11895: train loss: 0.4754733741283417\n",
      "Epoch 11896: train loss: 0.4754733145236969\n",
      "Epoch 11897: train loss: 0.4754732847213745\n",
      "Epoch 11898: train loss: 0.47547316551208496\n",
      "Epoch 11899: train loss: 0.4754731059074402\n",
      "Epoch 11900: train loss: 0.475473016500473\n",
      "Epoch 11901: train loss: 0.47547295689582825\n",
      "Epoch 11902: train loss: 0.4754728674888611\n",
      "Epoch 11903: train loss: 0.4754728078842163\n",
      "Epoch 11904: train loss: 0.47547274827957153\n",
      "Epoch 11905: train loss: 0.47547265887260437\n",
      "Epoch 11906: train loss: 0.4754725396633148\n",
      "Epoch 11907: train loss: 0.47547245025634766\n",
      "Epoch 11908: train loss: 0.4754723906517029\n",
      "Epoch 11909: train loss: 0.4754723310470581\n",
      "Epoch 11910: train loss: 0.47547224164009094\n",
      "Epoch 11911: train loss: 0.47547218203544617\n",
      "Epoch 11912: train loss: 0.4754721224308014\n",
      "Epoch 11913: train loss: 0.47547203302383423\n",
      "Epoch 11914: train loss: 0.4754719138145447\n",
      "Epoch 11915: train loss: 0.4754718542098999\n",
      "Epoch 11916: train loss: 0.4754718244075775\n",
      "Epoch 11917: train loss: 0.4754716455936432\n",
      "Epoch 11918: train loss: 0.4754716157913208\n",
      "Epoch 11919: train loss: 0.475471556186676\n",
      "Epoch 11920: train loss: 0.4754714369773865\n",
      "Epoch 11921: train loss: 0.4754713773727417\n",
      "Epoch 11922: train loss: 0.47547128796577454\n",
      "Epoch 11923: train loss: 0.475471168756485\n",
      "Epoch 11924: train loss: 0.4754711389541626\n",
      "Epoch 11925: train loss: 0.47547101974487305\n",
      "Epoch 11926: train loss: 0.47547096014022827\n",
      "Epoch 11927: train loss: 0.4754708707332611\n",
      "Epoch 11928: train loss: 0.47547081112861633\n",
      "Epoch 11929: train loss: 0.47547075152397156\n",
      "Epoch 11930: train loss: 0.4754706621170044\n",
      "Epoch 11931: train loss: 0.47547054290771484\n",
      "Epoch 11932: train loss: 0.47547051310539246\n",
      "Epoch 11933: train loss: 0.4754704535007477\n",
      "Epoch 11934: train loss: 0.47547033429145813\n",
      "Epoch 11935: train loss: 0.47547024488449097\n",
      "Epoch 11936: train loss: 0.47547024488449097\n",
      "Epoch 11937: train loss: 0.4754701256752014\n",
      "Epoch 11938: train loss: 0.47547003626823425\n",
      "Epoch 11939: train loss: 0.4754699170589447\n",
      "Epoch 11940: train loss: 0.4754698872566223\n",
      "Epoch 11941: train loss: 0.47546982765197754\n",
      "Epoch 11942: train loss: 0.475469708442688\n",
      "Epoch 11943: train loss: 0.4754696786403656\n",
      "Epoch 11944: train loss: 0.4754694998264313\n",
      "Epoch 11945: train loss: 0.4754694700241089\n",
      "Epoch 11946: train loss: 0.4754694700241089\n",
      "Epoch 11947: train loss: 0.47546929121017456\n",
      "Epoch 11948: train loss: 0.4754692614078522\n",
      "Epoch 11949: train loss: 0.4754692018032074\n",
      "Epoch 11950: train loss: 0.47546908259391785\n",
      "Epoch 11951: train loss: 0.47546905279159546\n",
      "Epoch 11952: train loss: 0.4754689335823059\n",
      "Epoch 11953: train loss: 0.47546884417533875\n",
      "Epoch 11954: train loss: 0.4754687249660492\n",
      "Epoch 11955: train loss: 0.4754686653614044\n",
      "Epoch 11956: train loss: 0.4754686653614044\n",
      "Epoch 11957: train loss: 0.47546857595443726\n",
      "Epoch 11958: train loss: 0.4754684567451477\n",
      "Epoch 11959: train loss: 0.47546836733818054\n",
      "Epoch 11960: train loss: 0.47546830773353577\n",
      "Epoch 11961: train loss: 0.4754682183265686\n",
      "Epoch 11962: train loss: 0.47546815872192383\n",
      "Epoch 11963: train loss: 0.47546809911727905\n",
      "Epoch 11964: train loss: 0.4754680097103119\n",
      "Epoch 11965: train loss: 0.47546789050102234\n",
      "Epoch 11966: train loss: 0.47546783089637756\n",
      "Epoch 11967: train loss: 0.4754678010940552\n",
      "Epoch 11968: train loss: 0.4754676818847656\n",
      "Epoch 11969: train loss: 0.4754675626754761\n",
      "Epoch 11970: train loss: 0.4754675328731537\n",
      "Epoch 11971: train loss: 0.47546741366386414\n",
      "Epoch 11972: train loss: 0.47546741366386414\n",
      "Epoch 11973: train loss: 0.475467324256897\n",
      "Epoch 11974: train loss: 0.4754672050476074\n",
      "Epoch 11975: train loss: 0.47546714544296265\n",
      "Epoch 11976: train loss: 0.4754670560359955\n",
      "Epoch 11977: train loss: 0.4754669964313507\n",
      "Epoch 11978: train loss: 0.47546690702438354\n",
      "Epoch 11979: train loss: 0.47546684741973877\n",
      "Epoch 11980: train loss: 0.4754667282104492\n",
      "Epoch 11981: train loss: 0.47546669840812683\n",
      "Epoch 11982: train loss: 0.4754665791988373\n",
      "Epoch 11983: train loss: 0.4754665195941925\n",
      "Epoch 11984: train loss: 0.47546643018722534\n",
      "Epoch 11985: train loss: 0.47546637058258057\n",
      "Epoch 11986: train loss: 0.4754662811756134\n",
      "Epoch 11987: train loss: 0.47546622157096863\n",
      "Epoch 11988: train loss: 0.4754661023616791\n",
      "Epoch 11989: train loss: 0.4754660129547119\n",
      "Epoch 11990: train loss: 0.47546589374542236\n",
      "Epoch 11991: train loss: 0.47546589374542236\n",
      "Epoch 11992: train loss: 0.4754658043384552\n",
      "Epoch 11993: train loss: 0.4754657447338104\n",
      "Epoch 11994: train loss: 0.47546565532684326\n",
      "Epoch 11995: train loss: 0.4754655957221985\n",
      "Epoch 11996: train loss: 0.47546547651290894\n",
      "Epoch 11997: train loss: 0.47546544671058655\n",
      "Epoch 11998: train loss: 0.475465327501297\n",
      "Epoch 11999: train loss: 0.47546523809432983\n",
      "Epoch 12000: train loss: 0.47546517848968506\n",
      "Epoch 12001: train loss: 0.4754651188850403\n",
      "Epoch 12002: train loss: 0.4754650294780731\n",
      "Epoch 12003: train loss: 0.47546496987342834\n",
      "Epoch 12004: train loss: 0.4754648506641388\n",
      "Epoch 12005: train loss: 0.4754648208618164\n",
      "Epoch 12006: train loss: 0.47546470165252686\n",
      "Epoch 12007: train loss: 0.4754646420478821\n",
      "Epoch 12008: train loss: 0.4754646122455597\n",
      "Epoch 12009: train loss: 0.47546449303627014\n",
      "Epoch 12010: train loss: 0.475464403629303\n",
      "Epoch 12011: train loss: 0.4754643440246582\n",
      "Epoch 12012: train loss: 0.47546422481536865\n",
      "Epoch 12013: train loss: 0.47546419501304626\n",
      "Epoch 12014: train loss: 0.4754640758037567\n",
      "Epoch 12015: train loss: 0.47546401619911194\n",
      "Epoch 12016: train loss: 0.4754639267921448\n",
      "Epoch 12017: train loss: 0.4754638075828552\n",
      "Epoch 12018: train loss: 0.4754638075828552\n",
      "Epoch 12019: train loss: 0.47546371817588806\n",
      "Epoch 12020: train loss: 0.4754635989665985\n",
      "Epoch 12021: train loss: 0.47546353936195374\n",
      "Epoch 12022: train loss: 0.4754634499549866\n",
      "Epoch 12023: train loss: 0.4754633903503418\n",
      "Epoch 12024: train loss: 0.475463330745697\n",
      "Epoch 12025: train loss: 0.47546324133872986\n",
      "Epoch 12026: train loss: 0.4754631817340851\n",
      "Epoch 12027: train loss: 0.47546303272247314\n",
      "Epoch 12028: train loss: 0.47546303272247314\n",
      "Epoch 12029: train loss: 0.4754629135131836\n",
      "Epoch 12030: train loss: 0.47546282410621643\n",
      "Epoch 12031: train loss: 0.47546276450157166\n",
      "Epoch 12032: train loss: 0.4754627048969269\n",
      "Epoch 12033: train loss: 0.4754626154899597\n",
      "Epoch 12034: train loss: 0.47546249628067017\n",
      "Epoch 12035: train loss: 0.47546249628067017\n",
      "Epoch 12036: train loss: 0.475462406873703\n",
      "Epoch 12037: train loss: 0.47546228766441345\n",
      "Epoch 12038: train loss: 0.47546225786209106\n",
      "Epoch 12039: train loss: 0.4754621386528015\n",
      "Epoch 12040: train loss: 0.47546207904815674\n",
      "Epoch 12041: train loss: 0.47546204924583435\n",
      "Epoch 12042: train loss: 0.4754618704319\n",
      "Epoch 12043: train loss: 0.47546184062957764\n",
      "Epoch 12044: train loss: 0.47546178102493286\n",
      "Epoch 12045: train loss: 0.4754616618156433\n",
      "Epoch 12046: train loss: 0.47546157240867615\n",
      "Epoch 12047: train loss: 0.47546157240867615\n",
      "Epoch 12048: train loss: 0.4754614531993866\n",
      "Epoch 12049: train loss: 0.47546136379241943\n",
      "Epoch 12050: train loss: 0.47546130418777466\n",
      "Epoch 12051: train loss: 0.4754612147808075\n",
      "Epoch 12052: train loss: 0.4754611551761627\n",
      "Epoch 12053: train loss: 0.47546103596687317\n",
      "Epoch 12054: train loss: 0.475460946559906\n",
      "Epoch 12055: train loss: 0.47546088695526123\n",
      "Epoch 12056: train loss: 0.47546082735061646\n",
      "Epoch 12057: train loss: 0.4754607379436493\n",
      "Epoch 12058: train loss: 0.47546061873435974\n",
      "Epoch 12059: train loss: 0.47546061873435974\n",
      "Epoch 12060: train loss: 0.4754605293273926\n",
      "Epoch 12061: train loss: 0.475460410118103\n",
      "Epoch 12062: train loss: 0.475460410118103\n",
      "Epoch 12063: train loss: 0.4754602611064911\n",
      "Epoch 12064: train loss: 0.4754602015018463\n",
      "Epoch 12065: train loss: 0.47546011209487915\n",
      "Epoch 12066: train loss: 0.4754599928855896\n",
      "Epoch 12067: train loss: 0.4754599928855896\n",
      "Epoch 12068: train loss: 0.47545990347862244\n",
      "Epoch 12069: train loss: 0.4754597842693329\n",
      "Epoch 12070: train loss: 0.4754597246646881\n",
      "Epoch 12071: train loss: 0.47545963525772095\n",
      "Epoch 12072: train loss: 0.47545957565307617\n",
      "Epoch 12073: train loss: 0.475459486246109\n",
      "Epoch 12074: train loss: 0.47545942664146423\n",
      "Epoch 12075: train loss: 0.4754593074321747\n",
      "Epoch 12076: train loss: 0.4754592776298523\n",
      "Epoch 12077: train loss: 0.4754592180252075\n",
      "Epoch 12078: train loss: 0.47545909881591797\n",
      "Epoch 12079: train loss: 0.4754590690135956\n",
      "Epoch 12080: train loss: 0.47545894980430603\n",
      "Epoch 12081: train loss: 0.47545889019966125\n",
      "Epoch 12082: train loss: 0.4754588007926941\n",
      "Epoch 12083: train loss: 0.4754587411880493\n",
      "Epoch 12084: train loss: 0.47545865178108215\n",
      "Epoch 12085: train loss: 0.47545865178108215\n",
      "Epoch 12086: train loss: 0.4754584729671478\n",
      "Epoch 12087: train loss: 0.47545838356018066\n",
      "Epoch 12088: train loss: 0.4754583239555359\n",
      "Epoch 12089: train loss: 0.4754582643508911\n",
      "Epoch 12090: train loss: 0.4754582345485687\n",
      "Epoch 12091: train loss: 0.4754581153392792\n",
      "Epoch 12092: train loss: 0.475458025932312\n",
      "Epoch 12093: train loss: 0.47545796632766724\n",
      "Epoch 12094: train loss: 0.4754578471183777\n",
      "Epoch 12095: train loss: 0.4754578173160553\n",
      "Epoch 12096: train loss: 0.4754577577114105\n",
      "Epoch 12097: train loss: 0.47545763850212097\n",
      "Epoch 12098: train loss: 0.4754576086997986\n",
      "Epoch 12099: train loss: 0.47545748949050903\n",
      "Epoch 12100: train loss: 0.47545742988586426\n",
      "Epoch 12101: train loss: 0.47545740008354187\n",
      "Epoch 12102: train loss: 0.4754572808742523\n",
      "Epoch 12103: train loss: 0.47545719146728516\n",
      "Epoch 12104: train loss: 0.4754571318626404\n",
      "Epoch 12105: train loss: 0.47545701265335083\n",
      "Epoch 12106: train loss: 0.47545698285102844\n",
      "Epoch 12107: train loss: 0.4754568636417389\n",
      "Epoch 12108: train loss: 0.47545677423477173\n",
      "Epoch 12109: train loss: 0.47545671463012695\n",
      "Epoch 12110: train loss: 0.4754566550254822\n",
      "Epoch 12111: train loss: 0.475456565618515\n",
      "Epoch 12112: train loss: 0.47545650601387024\n",
      "Epoch 12113: train loss: 0.47545644640922546\n",
      "Epoch 12114: train loss: 0.4754563570022583\n",
      "Epoch 12115: train loss: 0.47545623779296875\n",
      "Epoch 12116: train loss: 0.475456178188324\n",
      "Epoch 12117: train loss: 0.4754561185836792\n",
      "Epoch 12118: train loss: 0.47545602917671204\n",
      "Epoch 12119: train loss: 0.47545596957206726\n",
      "Epoch 12120: train loss: 0.4754558801651001\n",
      "Epoch 12121: train loss: 0.4754558205604553\n",
      "Epoch 12122: train loss: 0.47545570135116577\n",
      "Epoch 12123: train loss: 0.4754556119441986\n",
      "Epoch 12124: train loss: 0.47545555233955383\n",
      "Epoch 12125: train loss: 0.47545549273490906\n",
      "Epoch 12126: train loss: 0.4754554033279419\n",
      "Epoch 12127: train loss: 0.4754553437232971\n",
      "Epoch 12128: train loss: 0.47545528411865234\n",
      "Epoch 12129: train loss: 0.4754551947116852\n",
      "Epoch 12130: train loss: 0.4754551351070404\n",
      "Epoch 12131: train loss: 0.47545504570007324\n",
      "Epoch 12132: train loss: 0.4754549264907837\n",
      "Epoch 12133: train loss: 0.4754548668861389\n",
      "Epoch 12134: train loss: 0.47545477747917175\n",
      "Epoch 12135: train loss: 0.475454717874527\n",
      "Epoch 12136: train loss: 0.4754546582698822\n",
      "Epoch 12137: train loss: 0.47545456886291504\n",
      "Epoch 12138: train loss: 0.47545450925827026\n",
      "Epoch 12139: train loss: 0.4754544198513031\n",
      "Epoch 12140: train loss: 0.4754543602466583\n",
      "Epoch 12141: train loss: 0.4754542410373688\n",
      "Epoch 12142: train loss: 0.4754542112350464\n",
      "Epoch 12143: train loss: 0.47545409202575684\n",
      "Epoch 12144: train loss: 0.47545403242111206\n",
      "Epoch 12145: train loss: 0.4754539430141449\n",
      "Epoch 12146: train loss: 0.47545382380485535\n",
      "Epoch 12147: train loss: 0.47545382380485535\n",
      "Epoch 12148: train loss: 0.4754537343978882\n",
      "Epoch 12149: train loss: 0.47545361518859863\n",
      "Epoch 12150: train loss: 0.47545358538627625\n",
      "Epoch 12151: train loss: 0.4754534661769867\n",
      "Epoch 12152: train loss: 0.4754534065723419\n",
      "Epoch 12153: train loss: 0.47545331716537476\n",
      "Epoch 12154: train loss: 0.47545325756073\n",
      "Epoch 12155: train loss: 0.4754531979560852\n",
      "Epoch 12156: train loss: 0.47545310854911804\n",
      "Epoch 12157: train loss: 0.47545304894447327\n",
      "Epoch 12158: train loss: 0.47545289993286133\n",
      "Epoch 12159: train loss: 0.47545289993286133\n",
      "Epoch 12160: train loss: 0.4754527807235718\n",
      "Epoch 12161: train loss: 0.4754527509212494\n",
      "Epoch 12162: train loss: 0.47545263171195984\n",
      "Epoch 12163: train loss: 0.47545257210731506\n",
      "Epoch 12164: train loss: 0.4754525423049927\n",
      "Epoch 12165: train loss: 0.4754524230957031\n",
      "Epoch 12166: train loss: 0.47545236349105835\n",
      "Epoch 12167: train loss: 0.4754522740840912\n",
      "Epoch 12168: train loss: 0.47545215487480164\n",
      "Epoch 12169: train loss: 0.47545209527015686\n",
      "Epoch 12170: train loss: 0.4754520058631897\n",
      "Epoch 12171: train loss: 0.4754519462585449\n",
      "Epoch 12172: train loss: 0.47545188665390015\n",
      "Epoch 12173: train loss: 0.47545185685157776\n",
      "Epoch 12174: train loss: 0.4754517376422882\n",
      "Epoch 12175: train loss: 0.47545164823532104\n",
      "Epoch 12176: train loss: 0.4754515290260315\n",
      "Epoch 12177: train loss: 0.4754514694213867\n",
      "Epoch 12178: train loss: 0.47545138001441956\n",
      "Epoch 12179: train loss: 0.4754513204097748\n",
      "Epoch 12180: train loss: 0.4754512310028076\n",
      "Epoch 12181: train loss: 0.47545117139816284\n",
      "Epoch 12182: train loss: 0.47545111179351807\n",
      "Epoch 12183: train loss: 0.4754510223865509\n",
      "Epoch 12184: train loss: 0.47545090317726135\n",
      "Epoch 12185: train loss: 0.4754508435726166\n",
      "Epoch 12186: train loss: 0.4754508137702942\n",
      "Epoch 12187: train loss: 0.47545069456100464\n",
      "Epoch 12188: train loss: 0.47545063495635986\n",
      "Epoch 12189: train loss: 0.4754505455493927\n",
      "Epoch 12190: train loss: 0.4754504859447479\n",
      "Epoch 12191: train loss: 0.47545039653778076\n",
      "Epoch 12192: train loss: 0.475450336933136\n",
      "Epoch 12193: train loss: 0.4754502773284912\n",
      "Epoch 12194: train loss: 0.47545018792152405\n",
      "Epoch 12195: train loss: 0.4754501283168793\n",
      "Epoch 12196: train loss: 0.4754500091075897\n",
      "Epoch 12197: train loss: 0.47544997930526733\n",
      "Epoch 12198: train loss: 0.4754498600959778\n",
      "Epoch 12199: train loss: 0.475449800491333\n",
      "Epoch 12200: train loss: 0.4754497706890106\n",
      "Epoch 12201: train loss: 0.47544965147972107\n",
      "Epoch 12202: train loss: 0.4754495918750763\n",
      "Epoch 12203: train loss: 0.47544950246810913\n",
      "Epoch 12204: train loss: 0.47544944286346436\n",
      "Epoch 12205: train loss: 0.4754493534564972\n",
      "Epoch 12206: train loss: 0.47544923424720764\n",
      "Epoch 12207: train loss: 0.47544917464256287\n",
      "Epoch 12208: train loss: 0.4754491448402405\n",
      "Epoch 12209: train loss: 0.4754490256309509\n",
      "Epoch 12210: train loss: 0.47544893622398376\n",
      "Epoch 12211: train loss: 0.475448876619339\n",
      "Epoch 12212: train loss: 0.4754488170146942\n",
      "Epoch 12213: train loss: 0.47544872760772705\n",
      "Epoch 12214: train loss: 0.4754486083984375\n",
      "Epoch 12215: train loss: 0.4754485487937927\n",
      "Epoch 12216: train loss: 0.47544848918914795\n",
      "Epoch 12217: train loss: 0.4754483997821808\n",
      "Epoch 12218: train loss: 0.475448340177536\n",
      "Epoch 12219: train loss: 0.47544828057289124\n",
      "Epoch 12220: train loss: 0.4754481911659241\n",
      "Epoch 12221: train loss: 0.4754481315612793\n",
      "Epoch 12222: train loss: 0.47544804215431213\n",
      "Epoch 12223: train loss: 0.47544798254966736\n",
      "Epoch 12224: train loss: 0.4754478633403778\n",
      "Epoch 12225: train loss: 0.47544777393341064\n",
      "Epoch 12226: train loss: 0.47544771432876587\n",
      "Epoch 12227: train loss: 0.4754476547241211\n",
      "Epoch 12228: train loss: 0.47544756531715393\n",
      "Epoch 12229: train loss: 0.47544750571250916\n",
      "Epoch 12230: train loss: 0.475447416305542\n",
      "Epoch 12231: train loss: 0.4754473567008972\n",
      "Epoch 12232: train loss: 0.47544729709625244\n",
      "Epoch 12233: train loss: 0.4754472076892853\n",
      "Epoch 12234: train loss: 0.4754471480846405\n",
      "Epoch 12235: train loss: 0.47544702887535095\n",
      "Epoch 12236: train loss: 0.4754469394683838\n",
      "Epoch 12237: train loss: 0.475446879863739\n",
      "Epoch 12238: train loss: 0.47544682025909424\n",
      "Epoch 12239: train loss: 0.47544679045677185\n",
      "Epoch 12240: train loss: 0.4754466712474823\n",
      "Epoch 12241: train loss: 0.4754466116428375\n",
      "Epoch 12242: train loss: 0.47544652223587036\n",
      "Epoch 12243: train loss: 0.4754464030265808\n",
      "Epoch 12244: train loss: 0.4754463732242584\n",
      "Epoch 12245: train loss: 0.47544631361961365\n",
      "Epoch 12246: train loss: 0.4754461944103241\n",
      "Epoch 12247: train loss: 0.47544610500335693\n",
      "Epoch 12248: train loss: 0.47544610500335693\n",
      "Epoch 12249: train loss: 0.4754459857940674\n",
      "Epoch 12250: train loss: 0.4754458963871002\n",
      "Epoch 12251: train loss: 0.47544577717781067\n",
      "Epoch 12252: train loss: 0.47544577717781067\n",
      "Epoch 12253: train loss: 0.4754457473754883\n",
      "Epoch 12254: train loss: 0.47544556856155396\n",
      "Epoch 12255: train loss: 0.47544553875923157\n",
      "Epoch 12256: train loss: 0.475445419549942\n",
      "Epoch 12257: train loss: 0.47544535994529724\n",
      "Epoch 12258: train loss: 0.47544533014297485\n",
      "Epoch 12259: train loss: 0.4754452109336853\n",
      "Epoch 12260: train loss: 0.4754451513290405\n",
      "Epoch 12261: train loss: 0.47544512152671814\n",
      "Epoch 12262: train loss: 0.4754449427127838\n",
      "Epoch 12263: train loss: 0.4754449129104614\n",
      "Epoch 12264: train loss: 0.47544485330581665\n",
      "Epoch 12265: train loss: 0.4754447340965271\n",
      "Epoch 12266: train loss: 0.47544464468955994\n",
      "Epoch 12267: train loss: 0.47544458508491516\n",
      "Epoch 12268: train loss: 0.4754445254802704\n",
      "Epoch 12269: train loss: 0.4754444658756256\n",
      "Epoch 12270: train loss: 0.4754444360733032\n",
      "Epoch 12271: train loss: 0.4754442572593689\n",
      "Epoch 12272: train loss: 0.4754442274570465\n",
      "Epoch 12273: train loss: 0.47544410824775696\n",
      "Epoch 12274: train loss: 0.4754440486431122\n",
      "Epoch 12275: train loss: 0.4754440188407898\n",
      "Epoch 12276: train loss: 0.475443959236145\n",
      "Epoch 12277: train loss: 0.47544384002685547\n",
      "Epoch 12278: train loss: 0.4754438102245331\n",
      "Epoch 12279: train loss: 0.47544369101524353\n",
      "Epoch 12280: train loss: 0.47544360160827637\n",
      "Epoch 12281: train loss: 0.47544360160827637\n",
      "Epoch 12282: train loss: 0.47544342279434204\n",
      "Epoch 12283: train loss: 0.47544342279434204\n",
      "Epoch 12284: train loss: 0.4754433333873749\n",
      "Epoch 12285: train loss: 0.4754432737827301\n",
      "Epoch 12286: train loss: 0.47544318437576294\n",
      "Epoch 12287: train loss: 0.4754430651664734\n",
      "Epoch 12288: train loss: 0.4754430055618286\n",
      "Epoch 12289: train loss: 0.4754429757595062\n",
      "Epoch 12290: train loss: 0.4754428565502167\n",
      "Epoch 12291: train loss: 0.4754427671432495\n",
      "Epoch 12292: train loss: 0.47544270753860474\n",
      "Epoch 12293: train loss: 0.4754425883293152\n",
      "Epoch 12294: train loss: 0.4754425883293152\n",
      "Epoch 12295: train loss: 0.47544243931770325\n",
      "Epoch 12296: train loss: 0.47544237971305847\n",
      "Epoch 12297: train loss: 0.4754422903060913\n",
      "Epoch 12298: train loss: 0.4754422903060913\n",
      "Epoch 12299: train loss: 0.47544217109680176\n",
      "Epoch 12300: train loss: 0.47544214129447937\n",
      "Epoch 12301: train loss: 0.47544196248054504\n",
      "Epoch 12302: train loss: 0.47544196248054504\n",
      "Epoch 12303: train loss: 0.4754418730735779\n",
      "Epoch 12304: train loss: 0.4754418134689331\n",
      "Epoch 12305: train loss: 0.47544166445732117\n",
      "Epoch 12306: train loss: 0.4754416048526764\n",
      "Epoch 12307: train loss: 0.4754415452480316\n",
      "Epoch 12308: train loss: 0.47544151544570923\n",
      "Epoch 12309: train loss: 0.4754413962364197\n",
      "Epoch 12310: train loss: 0.4754413068294525\n",
      "Epoch 12311: train loss: 0.4754413068294525\n",
      "Epoch 12312: train loss: 0.47544118762016296\n",
      "Epoch 12313: train loss: 0.4754410982131958\n",
      "Epoch 12314: train loss: 0.475441038608551\n",
      "Epoch 12315: train loss: 0.4754409193992615\n",
      "Epoch 12316: train loss: 0.4754408597946167\n",
      "Epoch 12317: train loss: 0.47544077038764954\n",
      "Epoch 12318: train loss: 0.47544071078300476\n",
      "Epoch 12319: train loss: 0.47544065117836\n",
      "Epoch 12320: train loss: 0.4754405617713928\n",
      "Epoch 12321: train loss: 0.47544050216674805\n",
      "Epoch 12322: train loss: 0.4754404127597809\n",
      "Epoch 12323: train loss: 0.4754403531551361\n",
      "Epoch 12324: train loss: 0.47544023394584656\n",
      "Epoch 12325: train loss: 0.47544020414352417\n",
      "Epoch 12326: train loss: 0.4754400849342346\n",
      "Epoch 12327: train loss: 0.47544002532958984\n",
      "Epoch 12328: train loss: 0.4754399359226227\n",
      "Epoch 12329: train loss: 0.4754398763179779\n",
      "Epoch 12330: train loss: 0.47543981671333313\n",
      "Epoch 12331: train loss: 0.47543972730636597\n",
      "Epoch 12332: train loss: 0.4754396677017212\n",
      "Epoch 12333: train loss: 0.47543957829475403\n",
      "Epoch 12334: train loss: 0.47543951869010925\n",
      "Epoch 12335: train loss: 0.4754394590854645\n",
      "Epoch 12336: train loss: 0.4754393696784973\n",
      "Epoch 12337: train loss: 0.47543931007385254\n",
      "Epoch 12338: train loss: 0.475439190864563\n",
      "Epoch 12339: train loss: 0.4754391610622406\n",
      "Epoch 12340: train loss: 0.47543904185295105\n",
      "Epoch 12341: train loss: 0.4754389822483063\n",
      "Epoch 12342: train loss: 0.4754389524459839\n",
      "Epoch 12343: train loss: 0.47543883323669434\n",
      "Epoch 12344: train loss: 0.4754387438297272\n",
      "Epoch 12345: train loss: 0.4754386842250824\n",
      "Epoch 12346: train loss: 0.4754386246204376\n",
      "Epoch 12347: train loss: 0.47543853521347046\n",
      "Epoch 12348: train loss: 0.4754384756088257\n",
      "Epoch 12349: train loss: 0.47543835639953613\n",
      "Epoch 12350: train loss: 0.47543826699256897\n",
      "Epoch 12351: train loss: 0.47543826699256897\n",
      "Epoch 12352: train loss: 0.4754382073879242\n",
      "Epoch 12353: train loss: 0.47543805837631226\n",
      "Epoch 12354: train loss: 0.47543805837631226\n",
      "Epoch 12355: train loss: 0.4754379391670227\n",
      "Epoch 12356: train loss: 0.47543784976005554\n",
      "Epoch 12357: train loss: 0.47543779015541077\n",
      "Epoch 12358: train loss: 0.4754377007484436\n",
      "Epoch 12359: train loss: 0.47543764114379883\n",
      "Epoch 12360: train loss: 0.47543758153915405\n",
      "Epoch 12361: train loss: 0.4754374921321869\n",
      "Epoch 12362: train loss: 0.4754374325275421\n",
      "Epoch 12363: train loss: 0.47543731331825256\n",
      "Epoch 12364: train loss: 0.4754372239112854\n",
      "Epoch 12365: train loss: 0.4754371643066406\n",
      "Epoch 12366: train loss: 0.47543710470199585\n",
      "Epoch 12367: train loss: 0.4754370152950287\n",
      "Epoch 12368: train loss: 0.4754369556903839\n",
      "Epoch 12369: train loss: 0.47543683648109436\n",
      "Epoch 12370: train loss: 0.475436806678772\n",
      "Epoch 12371: train loss: 0.4754366874694824\n",
      "Epoch 12372: train loss: 0.47543662786483765\n",
      "Epoch 12373: train loss: 0.4754365384578705\n",
      "Epoch 12374: train loss: 0.4754364788532257\n",
      "Epoch 12375: train loss: 0.47543641924858093\n",
      "Epoch 12376: train loss: 0.47543638944625854\n",
      "Epoch 12377: train loss: 0.4754362106323242\n",
      "Epoch 12378: train loss: 0.47543618083000183\n",
      "Epoch 12379: train loss: 0.47543612122535706\n",
      "Epoch 12380: train loss: 0.4754360616207123\n",
      "Epoch 12381: train loss: 0.4754359722137451\n",
      "Epoch 12382: train loss: 0.47543591260910034\n",
      "Epoch 12383: train loss: 0.47543585300445557\n",
      "Epoch 12384: train loss: 0.4754357635974884\n",
      "Epoch 12385: train loss: 0.47543564438819885\n",
      "Epoch 12386: train loss: 0.4754355549812317\n",
      "Epoch 12387: train loss: 0.4754354953765869\n",
      "Epoch 12388: train loss: 0.47543543577194214\n",
      "Epoch 12389: train loss: 0.475435346364975\n",
      "Epoch 12390: train loss: 0.4754352867603302\n",
      "Epoch 12391: train loss: 0.47543516755104065\n",
      "Epoch 12392: train loss: 0.47543513774871826\n",
      "Epoch 12393: train loss: 0.4754350185394287\n",
      "Epoch 12394: train loss: 0.47543495893478394\n",
      "Epoch 12395: train loss: 0.47543492913246155\n",
      "Epoch 12396: train loss: 0.475434809923172\n",
      "Epoch 12397: train loss: 0.4754347503185272\n",
      "Epoch 12398: train loss: 0.47543472051620483\n",
      "Epoch 12399: train loss: 0.47543466091156006\n",
      "Epoch 12400: train loss: 0.4754345118999481\n",
      "Epoch 12401: train loss: 0.4754345118999481\n",
      "Epoch 12402: train loss: 0.4754343330860138\n",
      "Epoch 12403: train loss: 0.4754343032836914\n",
      "Epoch 12404: train loss: 0.47543418407440186\n",
      "Epoch 12405: train loss: 0.4754341244697571\n",
      "Epoch 12406: train loss: 0.4754340946674347\n",
      "Epoch 12407: train loss: 0.47543397545814514\n",
      "Epoch 12408: train loss: 0.47543391585350037\n",
      "Epoch 12409: train loss: 0.475433886051178\n",
      "Epoch 12410: train loss: 0.4754337668418884\n",
      "Epoch 12411: train loss: 0.47543367743492126\n",
      "Epoch 12412: train loss: 0.47543367743492126\n",
      "Epoch 12413: train loss: 0.4754335582256317\n",
      "Epoch 12414: train loss: 0.47543349862098694\n",
      "Epoch 12415: train loss: 0.4754334092140198\n",
      "Epoch 12416: train loss: 0.475433349609375\n",
      "Epoch 12417: train loss: 0.47543323040008545\n",
      "Epoch 12418: train loss: 0.47543320059776306\n",
      "Epoch 12419: train loss: 0.4754330813884735\n",
      "Epoch 12420: train loss: 0.47543302178382874\n",
      "Epoch 12421: train loss: 0.4754329323768616\n",
      "Epoch 12422: train loss: 0.4754328727722168\n",
      "Epoch 12423: train loss: 0.47543278336524963\n",
      "Epoch 12424: train loss: 0.47543278336524963\n",
      "Epoch 12425: train loss: 0.4754326641559601\n",
      "Epoch 12426: train loss: 0.4754325747489929\n",
      "Epoch 12427: train loss: 0.47543245553970337\n",
      "Epoch 12428: train loss: 0.4754323959350586\n",
      "Epoch 12429: train loss: 0.4754323661327362\n",
      "Epoch 12430: train loss: 0.47543224692344666\n",
      "Epoch 12431: train loss: 0.4754321873188019\n",
      "Epoch 12432: train loss: 0.4754321575164795\n",
      "Epoch 12433: train loss: 0.47543203830718994\n",
      "Epoch 12434: train loss: 0.4754319489002228\n",
      "Epoch 12435: train loss: 0.475431889295578\n",
      "Epoch 12436: train loss: 0.4754318296909332\n",
      "Epoch 12437: train loss: 0.47543174028396606\n",
      "Epoch 12438: train loss: 0.4754316806793213\n",
      "Epoch 12439: train loss: 0.4754316210746765\n",
      "Epoch 12440: train loss: 0.47543156147003174\n",
      "Epoch 12441: train loss: 0.4754314124584198\n",
      "Epoch 12442: train loss: 0.475431352853775\n",
      "Epoch 12443: train loss: 0.475431352853775\n",
      "Epoch 12444: train loss: 0.4754312038421631\n",
      "Epoch 12445: train loss: 0.4754311442375183\n",
      "Epoch 12446: train loss: 0.4754311144351959\n",
      "Epoch 12447: train loss: 0.47543105483055115\n",
      "Epoch 12448: train loss: 0.4754309356212616\n",
      "Epoch 12449: train loss: 0.4754309058189392\n",
      "Epoch 12450: train loss: 0.47543078660964966\n",
      "Epoch 12451: train loss: 0.4754306972026825\n",
      "Epoch 12452: train loss: 0.4754306375980377\n",
      "Epoch 12453: train loss: 0.47543051838874817\n",
      "Epoch 12454: train loss: 0.4754304885864258\n",
      "Epoch 12455: train loss: 0.475430428981781\n",
      "Epoch 12456: train loss: 0.47543030977249146\n",
      "Epoch 12457: train loss: 0.47543027997016907\n",
      "Epoch 12458: train loss: 0.4754301607608795\n",
      "Epoch 12459: train loss: 0.47543010115623474\n",
      "Epoch 12460: train loss: 0.4754300117492676\n",
      "Epoch 12461: train loss: 0.4754299521446228\n",
      "Epoch 12462: train loss: 0.47542986273765564\n",
      "Epoch 12463: train loss: 0.47542980313301086\n",
      "Epoch 12464: train loss: 0.4754296839237213\n",
      "Epoch 12465: train loss: 0.4754296541213989\n",
      "Epoch 12466: train loss: 0.47542959451675415\n",
      "Epoch 12467: train loss: 0.4754294753074646\n",
      "Epoch 12468: train loss: 0.4754294157028198\n",
      "Epoch 12469: train loss: 0.47542932629585266\n",
      "Epoch 12470: train loss: 0.4754292666912079\n",
      "Epoch 12471: train loss: 0.4754292666912079\n",
      "Epoch 12472: train loss: 0.47542911767959595\n",
      "Epoch 12473: train loss: 0.47542905807495117\n",
      "Epoch 12474: train loss: 0.475428968667984\n",
      "Epoch 12475: train loss: 0.47542890906333923\n",
      "Epoch 12476: train loss: 0.47542884945869446\n",
      "Epoch 12477: train loss: 0.4754287600517273\n",
      "Epoch 12478: train loss: 0.4754287004470825\n",
      "Epoch 12479: train loss: 0.47542864084243774\n",
      "Epoch 12480: train loss: 0.4754285514354706\n",
      "Epoch 12481: train loss: 0.4754284918308258\n",
      "Epoch 12482: train loss: 0.47542843222618103\n",
      "Epoch 12483: train loss: 0.4754282832145691\n",
      "Epoch 12484: train loss: 0.4754282236099243\n",
      "Epoch 12485: train loss: 0.47542813420295715\n",
      "Epoch 12486: train loss: 0.47542813420295715\n",
      "Epoch 12487: train loss: 0.4754280149936676\n",
      "Epoch 12488: train loss: 0.47542792558670044\n",
      "Epoch 12489: train loss: 0.47542786598205566\n",
      "Epoch 12490: train loss: 0.4754277467727661\n",
      "Epoch 12491: train loss: 0.4754277169704437\n",
      "Epoch 12492: train loss: 0.4754275977611542\n",
      "Epoch 12493: train loss: 0.4754275381565094\n",
      "Epoch 12494: train loss: 0.47542744874954224\n",
      "Epoch 12495: train loss: 0.47542738914489746\n",
      "Epoch 12496: train loss: 0.47542738914489746\n",
      "Epoch 12497: train loss: 0.4754272997379303\n",
      "Epoch 12498: train loss: 0.47542718052864075\n",
      "Epoch 12499: train loss: 0.47542712092399597\n",
      "Epoch 12500: train loss: 0.4754270911216736\n",
      "Epoch 12501: train loss: 0.47542697191238403\n",
      "Epoch 12502: train loss: 0.47542691230773926\n",
      "Epoch 12503: train loss: 0.4754268229007721\n",
      "Epoch 12504: train loss: 0.4754267632961273\n",
      "Epoch 12505: train loss: 0.47542667388916016\n",
      "Epoch 12506: train loss: 0.4754266142845154\n",
      "Epoch 12507: train loss: 0.47542649507522583\n",
      "Epoch 12508: train loss: 0.47542646527290344\n",
      "Epoch 12509: train loss: 0.4754263460636139\n",
      "Epoch 12510: train loss: 0.47542625665664673\n",
      "Epoch 12511: train loss: 0.47542625665664673\n",
      "Epoch 12512: train loss: 0.4754261374473572\n",
      "Epoch 12513: train loss: 0.47542604804039\n",
      "Epoch 12514: train loss: 0.47542592883110046\n",
      "Epoch 12515: train loss: 0.47542592883110046\n",
      "Epoch 12516: train loss: 0.4754258692264557\n",
      "Epoch 12517: train loss: 0.4754258394241333\n",
      "Epoch 12518: train loss: 0.475425660610199\n",
      "Epoch 12519: train loss: 0.4754256010055542\n",
      "Epoch 12520: train loss: 0.47542551159858704\n",
      "Epoch 12521: train loss: 0.47542545199394226\n",
      "Epoch 12522: train loss: 0.4754253923892975\n",
      "Epoch 12523: train loss: 0.4754253625869751\n",
      "Epoch 12524: train loss: 0.47542524337768555\n",
      "Epoch 12525: train loss: 0.4754251539707184\n",
      "Epoch 12526: train loss: 0.4754251539707184\n",
      "Epoch 12527: train loss: 0.47542503476142883\n",
      "Epoch 12528: train loss: 0.47542497515678406\n",
      "Epoch 12529: train loss: 0.4754248857498169\n",
      "Epoch 12530: train loss: 0.47542476654052734\n",
      "Epoch 12531: train loss: 0.4754246771335602\n",
      "Epoch 12532: train loss: 0.4754246771335602\n",
      "Epoch 12533: train loss: 0.47542455792427063\n",
      "Epoch 12534: train loss: 0.47542446851730347\n",
      "Epoch 12535: train loss: 0.4754244089126587\n",
      "Epoch 12536: train loss: 0.4754243493080139\n",
      "Epoch 12537: train loss: 0.47542425990104675\n",
      "Epoch 12538: train loss: 0.4754241406917572\n",
      "Epoch 12539: train loss: 0.4754241108894348\n",
      "Epoch 12540: train loss: 0.47542405128479004\n",
      "Epoch 12541: train loss: 0.4754239320755005\n",
      "Epoch 12542: train loss: 0.4754239022731781\n",
      "Epoch 12543: train loss: 0.47542378306388855\n",
      "Epoch 12544: train loss: 0.4754237234592438\n",
      "Epoch 12545: train loss: 0.4754236340522766\n",
      "Epoch 12546: train loss: 0.47542357444763184\n",
      "Epoch 12547: train loss: 0.47542351484298706\n",
      "Epoch 12548: train loss: 0.4754234254360199\n",
      "Epoch 12549: train loss: 0.4754234254360199\n",
      "Epoch 12550: train loss: 0.47542327642440796\n",
      "Epoch 12551: train loss: 0.4754232168197632\n",
      "Epoch 12552: train loss: 0.4754231572151184\n",
      "Epoch 12553: train loss: 0.47542309761047363\n",
      "Epoch 12554: train loss: 0.47542300820350647\n",
      "Epoch 12555: train loss: 0.4754229485988617\n",
      "Epoch 12556: train loss: 0.4754228889942169\n",
      "Epoch 12557: train loss: 0.47542279958724976\n",
      "Epoch 12558: train loss: 0.475422739982605\n",
      "Epoch 12559: train loss: 0.4754226505756378\n",
      "Epoch 12560: train loss: 0.47542259097099304\n",
      "Epoch 12561: train loss: 0.47542253136634827\n",
      "Epoch 12562: train loss: 0.47542238235473633\n",
      "Epoch 12563: train loss: 0.47542238235473633\n",
      "Epoch 12564: train loss: 0.4754222631454468\n",
      "Epoch 12565: train loss: 0.4754221737384796\n",
      "Epoch 12566: train loss: 0.47542211413383484\n",
      "Epoch 12567: train loss: 0.4754220247268677\n",
      "Epoch 12568: train loss: 0.4754219651222229\n",
      "Epoch 12569: train loss: 0.4754219055175781\n",
      "Epoch 12570: train loss: 0.4754217863082886\n",
      "Epoch 12571: train loss: 0.4754217565059662\n",
      "Epoch 12572: train loss: 0.4754216969013214\n",
      "Epoch 12573: train loss: 0.47542157769203186\n",
      "Epoch 12574: train loss: 0.4754215478897095\n",
      "Epoch 12575: train loss: 0.4754214286804199\n",
      "Epoch 12576: train loss: 0.4754214286804199\n",
      "Epoch 12577: train loss: 0.47542133927345276\n",
      "Epoch 12578: train loss: 0.4754212200641632\n",
      "Epoch 12579: train loss: 0.47542116045951843\n",
      "Epoch 12580: train loss: 0.47542107105255127\n",
      "Epoch 12581: train loss: 0.4754210114479065\n",
      "Epoch 12582: train loss: 0.47542092204093933\n",
      "Epoch 12583: train loss: 0.4754208028316498\n",
      "Epoch 12584: train loss: 0.475420743227005\n",
      "Epoch 12585: train loss: 0.4754207134246826\n",
      "Epoch 12586: train loss: 0.47542059421539307\n",
      "Epoch 12587: train loss: 0.4754205346107483\n",
      "Epoch 12588: train loss: 0.4754205048084259\n",
      "Epoch 12589: train loss: 0.47542038559913635\n",
      "Epoch 12590: train loss: 0.4754203259944916\n",
      "Epoch 12591: train loss: 0.4754202961921692\n",
      "Epoch 12592: train loss: 0.47542017698287964\n",
      "Epoch 12593: train loss: 0.47542011737823486\n",
      "Epoch 12594: train loss: 0.4754200279712677\n",
      "Epoch 12595: train loss: 0.4754199683666229\n",
      "Epoch 12596: train loss: 0.47541987895965576\n",
      "Epoch 12597: train loss: 0.475419819355011\n",
      "Epoch 12598: train loss: 0.4754197597503662\n",
      "Epoch 12599: train loss: 0.47541970014572144\n",
      "Epoch 12600: train loss: 0.4754196107387543\n",
      "Epoch 12601: train loss: 0.4754194915294647\n",
      "Epoch 12602: train loss: 0.47541946172714233\n",
      "Epoch 12603: train loss: 0.47541940212249756\n",
      "Epoch 12604: train loss: 0.475419282913208\n",
      "Epoch 12605: train loss: 0.4754192531108856\n",
      "Epoch 12606: train loss: 0.47541913390159607\n",
      "Epoch 12607: train loss: 0.4754190444946289\n",
      "Epoch 12608: train loss: 0.4754190444946289\n",
      "Epoch 12609: train loss: 0.47541892528533936\n",
      "Epoch 12610: train loss: 0.4754188656806946\n",
      "Epoch 12611: train loss: 0.4754187762737274\n",
      "Epoch 12612: train loss: 0.47541871666908264\n",
      "Epoch 12613: train loss: 0.47541865706443787\n",
      "Epoch 12614: train loss: 0.4754185676574707\n",
      "Epoch 12615: train loss: 0.4754185080528259\n",
      "Epoch 12616: train loss: 0.47541841864585876\n",
      "Epoch 12617: train loss: 0.475418359041214\n",
      "Epoch 12618: train loss: 0.4754182994365692\n",
      "Epoch 12619: train loss: 0.47541821002960205\n",
      "Epoch 12620: train loss: 0.4754181504249573\n",
      "Epoch 12621: train loss: 0.4754180908203125\n",
      "Epoch 12622: train loss: 0.47541794180870056\n",
      "Epoch 12623: train loss: 0.4754178822040558\n",
      "Epoch 12624: train loss: 0.475417822599411\n",
      "Epoch 12625: train loss: 0.47541773319244385\n",
      "Epoch 12626: train loss: 0.4754176735877991\n",
      "Epoch 12627: train loss: 0.4754175543785095\n",
      "Epoch 12628: train loss: 0.47541752457618713\n",
      "Epoch 12629: train loss: 0.4754174053668976\n",
      "Epoch 12630: train loss: 0.4754173457622528\n",
      "Epoch 12631: train loss: 0.4754173159599304\n",
      "Epoch 12632: train loss: 0.47541719675064087\n",
      "Epoch 12633: train loss: 0.47541719675064087\n",
      "Epoch 12634: train loss: 0.47541704773902893\n",
      "Epoch 12635: train loss: 0.47541698813438416\n",
      "Epoch 12636: train loss: 0.4754169285297394\n",
      "Epoch 12637: train loss: 0.4754168391227722\n",
      "Epoch 12638: train loss: 0.4754168391227722\n",
      "Epoch 12639: train loss: 0.47541671991348267\n",
      "Epoch 12640: train loss: 0.4754166305065155\n",
      "Epoch 12641: train loss: 0.4754165709018707\n",
      "Epoch 12642: train loss: 0.47541651129722595\n",
      "Epoch 12643: train loss: 0.4754164218902588\n",
      "Epoch 12644: train loss: 0.47541630268096924\n",
      "Epoch 12645: train loss: 0.47541627287864685\n",
      "Epoch 12646: train loss: 0.4754162132740021\n",
      "Epoch 12647: train loss: 0.4754160940647125\n",
      "Epoch 12648: train loss: 0.47541606426239014\n",
      "Epoch 12649: train loss: 0.47541600465774536\n",
      "Epoch 12650: train loss: 0.4754158854484558\n",
      "Epoch 12651: train loss: 0.47541579604148865\n",
      "Epoch 12652: train loss: 0.47541573643684387\n",
      "Epoch 12653: train loss: 0.4754156768321991\n",
      "Epoch 12654: train loss: 0.47541558742523193\n",
      "Epoch 12655: train loss: 0.47541552782058716\n",
      "Epoch 12656: train loss: 0.4754154682159424\n",
      "Epoch 12657: train loss: 0.4754153788089752\n",
      "Epoch 12658: train loss: 0.47541531920433044\n",
      "Epoch 12659: train loss: 0.4754152297973633\n",
      "Epoch 12660: train loss: 0.4754151701927185\n",
      "Epoch 12661: train loss: 0.47541511058807373\n",
      "Epoch 12662: train loss: 0.47541502118110657\n",
      "Epoch 12663: train loss: 0.4754149615764618\n",
      "Epoch 12664: train loss: 0.47541484236717224\n",
      "Epoch 12665: train loss: 0.4754147529602051\n",
      "Epoch 12666: train loss: 0.4754146933555603\n",
      "Epoch 12667: train loss: 0.4754146337509155\n",
      "Epoch 12668: train loss: 0.47541460394859314\n",
      "Epoch 12669: train loss: 0.4754144847393036\n",
      "Epoch 12670: train loss: 0.4754144251346588\n",
      "Epoch 12671: train loss: 0.47541433572769165\n",
      "Epoch 12672: train loss: 0.4754142165184021\n",
      "Epoch 12673: train loss: 0.4754142165184021\n",
      "Epoch 12674: train loss: 0.47541412711143494\n",
      "Epoch 12675: train loss: 0.47541412711143494\n",
      "Epoch 12676: train loss: 0.4754140079021454\n",
      "Epoch 12677: train loss: 0.4754139184951782\n",
      "Epoch 12678: train loss: 0.47541385889053345\n",
      "Epoch 12679: train loss: 0.4754137396812439\n",
      "Epoch 12680: train loss: 0.4754137098789215\n",
      "Epoch 12681: train loss: 0.47541359066963196\n",
      "Epoch 12682: train loss: 0.4754135310649872\n",
      "Epoch 12683: train loss: 0.4754135012626648\n",
      "Epoch 12684: train loss: 0.47541338205337524\n",
      "Epoch 12685: train loss: 0.47541332244873047\n",
      "Epoch 12686: train loss: 0.4754132330417633\n",
      "Epoch 12687: train loss: 0.47541317343711853\n",
      "Epoch 12688: train loss: 0.47541308403015137\n",
      "Epoch 12689: train loss: 0.4754130244255066\n",
      "Epoch 12690: train loss: 0.4754129648208618\n",
      "Epoch 12691: train loss: 0.47541287541389465\n",
      "Epoch 12692: train loss: 0.4754128158092499\n",
      "Epoch 12693: train loss: 0.4754127562046051\n",
      "Epoch 12694: train loss: 0.47541266679763794\n",
      "Epoch 12695: train loss: 0.47541260719299316\n",
      "Epoch 12696: train loss: 0.4754124581813812\n",
      "Epoch 12697: train loss: 0.4754124581813812\n",
      "Epoch 12698: train loss: 0.47541239857673645\n",
      "Epoch 12699: train loss: 0.4754122793674469\n",
      "Epoch 12700: train loss: 0.4754122495651245\n",
      "Epoch 12701: train loss: 0.47541213035583496\n",
      "Epoch 12702: train loss: 0.4754120409488678\n",
      "Epoch 12703: train loss: 0.4754120409488678\n",
      "Epoch 12704: train loss: 0.47541192173957825\n",
      "Epoch 12705: train loss: 0.47541186213493347\n",
      "Epoch 12706: train loss: 0.4754117727279663\n",
      "Epoch 12707: train loss: 0.47541165351867676\n",
      "Epoch 12708: train loss: 0.47541162371635437\n",
      "Epoch 12709: train loss: 0.4754115045070648\n",
      "Epoch 12710: train loss: 0.4754115045070648\n",
      "Epoch 12711: train loss: 0.47541141510009766\n",
      "Epoch 12712: train loss: 0.4754113554954529\n",
      "Epoch 12713: train loss: 0.47541120648384094\n",
      "Epoch 12714: train loss: 0.47541120648384094\n",
      "Epoch 12715: train loss: 0.47541114687919617\n",
      "Epoch 12716: train loss: 0.4754110276699066\n",
      "Epoch 12717: train loss: 0.47541099786758423\n",
      "Epoch 12718: train loss: 0.47541093826293945\n",
      "Epoch 12719: train loss: 0.4754108190536499\n",
      "Epoch 12720: train loss: 0.4754107892513275\n",
      "Epoch 12721: train loss: 0.47541067004203796\n",
      "Epoch 12722: train loss: 0.4754105806350708\n",
      "Epoch 12723: train loss: 0.475410521030426\n",
      "Epoch 12724: train loss: 0.47541046142578125\n",
      "Epoch 12725: train loss: 0.4754103422164917\n",
      "Epoch 12726: train loss: 0.4754103124141693\n",
      "Epoch 12727: train loss: 0.47541019320487976\n",
      "Epoch 12728: train loss: 0.475410133600235\n",
      "Epoch 12729: train loss: 0.4754100441932678\n",
      "Epoch 12730: train loss: 0.47540992498397827\n",
      "Epoch 12731: train loss: 0.47540992498397827\n",
      "Epoch 12732: train loss: 0.4754098951816559\n",
      "Epoch 12733: train loss: 0.47540977597236633\n",
      "Epoch 12734: train loss: 0.47540971636772156\n",
      "Epoch 12735: train loss: 0.47540968656539917\n",
      "Epoch 12736: train loss: 0.4754095673561096\n",
      "Epoch 12737: train loss: 0.47540947794914246\n",
      "Epoch 12738: train loss: 0.4754094183444977\n",
      "Epoch 12739: train loss: 0.4754093587398529\n",
      "Epoch 12740: train loss: 0.47540926933288574\n",
      "Epoch 12741: train loss: 0.4754091501235962\n",
      "Epoch 12742: train loss: 0.4754091501235962\n",
      "Epoch 12743: train loss: 0.47540906071662903\n",
      "Epoch 12744: train loss: 0.4754089415073395\n",
      "Epoch 12745: train loss: 0.4754089415073395\n",
      "Epoch 12746: train loss: 0.4754088521003723\n",
      "Epoch 12747: train loss: 0.47540879249572754\n",
      "Epoch 12748: train loss: 0.475408673286438\n",
      "Epoch 12749: train loss: 0.4754085838794708\n",
      "Epoch 12750: train loss: 0.47540852427482605\n",
      "Epoch 12751: train loss: 0.4754084646701813\n",
      "Epoch 12752: train loss: 0.4754084348678589\n",
      "Epoch 12753: train loss: 0.47540831565856934\n",
      "Epoch 12754: train loss: 0.47540825605392456\n",
      "Epoch 12755: train loss: 0.4754081666469574\n",
      "Epoch 12756: train loss: 0.47540804743766785\n",
      "Epoch 12757: train loss: 0.47540804743766785\n",
      "Epoch 12758: train loss: 0.4754079580307007\n",
      "Epoch 12759: train loss: 0.4754078984260559\n",
      "Epoch 12760: train loss: 0.47540780901908875\n",
      "Epoch 12761: train loss: 0.47540774941444397\n",
      "Epoch 12762: train loss: 0.4754076898097992\n",
      "Epoch 12763: train loss: 0.47540760040283203\n",
      "Epoch 12764: train loss: 0.47540754079818726\n",
      "Epoch 12765: train loss: 0.4754074811935425\n",
      "Epoch 12766: train loss: 0.4754073917865753\n",
      "Epoch 12767: train loss: 0.47540727257728577\n",
      "Epoch 12768: train loss: 0.475407212972641\n",
      "Epoch 12769: train loss: 0.4754071831703186\n",
      "Epoch 12770: train loss: 0.47540706396102905\n",
      "Epoch 12771: train loss: 0.4754070043563843\n",
      "Epoch 12772: train loss: 0.4754069745540619\n",
      "Epoch 12773: train loss: 0.47540685534477234\n",
      "Epoch 12774: train loss: 0.47540679574012756\n",
      "Epoch 12775: train loss: 0.4754067063331604\n",
      "Epoch 12776: train loss: 0.47540658712387085\n",
      "Epoch 12777: train loss: 0.47540658712387085\n",
      "Epoch 12778: train loss: 0.4754065275192261\n",
      "Epoch 12779: train loss: 0.47540637850761414\n",
      "Epoch 12780: train loss: 0.47540631890296936\n",
      "Epoch 12781: train loss: 0.475406289100647\n",
      "Epoch 12782: train loss: 0.4754062294960022\n",
      "Epoch 12783: train loss: 0.47540611028671265\n",
      "Epoch 12784: train loss: 0.47540608048439026\n",
      "Epoch 12785: train loss: 0.4754059612751007\n",
      "Epoch 12786: train loss: 0.47540590167045593\n",
      "Epoch 12787: train loss: 0.47540587186813354\n",
      "Epoch 12788: train loss: 0.475405752658844\n",
      "Epoch 12789: train loss: 0.47540566325187683\n",
      "Epoch 12790: train loss: 0.47540566325187683\n",
      "Epoch 12791: train loss: 0.4754055440425873\n",
      "Epoch 12792: train loss: 0.4754054546356201\n",
      "Epoch 12793: train loss: 0.4754054546356201\n",
      "Epoch 12794: train loss: 0.47540533542633057\n",
      "Epoch 12795: train loss: 0.4754052758216858\n",
      "Epoch 12796: train loss: 0.47540518641471863\n",
      "Epoch 12797: train loss: 0.47540512681007385\n",
      "Epoch 12798: train loss: 0.4754050672054291\n",
      "Epoch 12799: train loss: 0.47540491819381714\n",
      "Epoch 12800: train loss: 0.47540491819381714\n",
      "Epoch 12801: train loss: 0.47540482878685\n",
      "Epoch 12802: train loss: 0.4754047691822052\n",
      "Epoch 12803: train loss: 0.4754047095775604\n",
      "Epoch 12804: train loss: 0.47540462017059326\n",
      "Epoch 12805: train loss: 0.4754045009613037\n",
      "Epoch 12806: train loss: 0.47540444135665894\n",
      "Epoch 12807: train loss: 0.47540441155433655\n",
      "Epoch 12808: train loss: 0.475404292345047\n",
      "Epoch 12809: train loss: 0.4754042327404022\n",
      "Epoch 12810: train loss: 0.47540420293807983\n",
      "Epoch 12811: train loss: 0.4754040837287903\n",
      "Epoch 12812: train loss: 0.4754040241241455\n",
      "Epoch 12813: train loss: 0.4754039943218231\n",
      "Epoch 12814: train loss: 0.47540387511253357\n",
      "Epoch 12815: train loss: 0.4754037857055664\n",
      "Epoch 12816: train loss: 0.4754037857055664\n",
      "Epoch 12817: train loss: 0.47540366649627686\n",
      "Epoch 12818: train loss: 0.4754035770893097\n",
      "Epoch 12819: train loss: 0.4754035174846649\n",
      "Epoch 12820: train loss: 0.47540345788002014\n",
      "Epoch 12821: train loss: 0.475403368473053\n",
      "Epoch 12822: train loss: 0.4754033088684082\n",
      "Epoch 12823: train loss: 0.47540318965911865\n",
      "Epoch 12824: train loss: 0.47540318965911865\n",
      "Epoch 12825: train loss: 0.4754030406475067\n",
      "Epoch 12826: train loss: 0.47540298104286194\n",
      "Epoch 12827: train loss: 0.47540295124053955\n",
      "Epoch 12828: train loss: 0.47540283203125\n",
      "Epoch 12829: train loss: 0.4754027724266052\n",
      "Epoch 12830: train loss: 0.47540268301963806\n",
      "Epoch 12831: train loss: 0.4754025638103485\n",
      "Epoch 12832: train loss: 0.4754025638103485\n",
      "Epoch 12833: train loss: 0.47540250420570374\n",
      "Epoch 12834: train loss: 0.4754024147987366\n",
      "Epoch 12835: train loss: 0.4754023551940918\n",
      "Epoch 12836: train loss: 0.47540226578712463\n",
      "Epoch 12837: train loss: 0.47540220618247986\n",
      "Epoch 12838: train loss: 0.4754020869731903\n",
      "Epoch 12839: train loss: 0.4754020571708679\n",
      "Epoch 12840: train loss: 0.47540193796157837\n",
      "Epoch 12841: train loss: 0.4754018783569336\n",
      "Epoch 12842: train loss: 0.47540178894996643\n",
      "Epoch 12843: train loss: 0.47540178894996643\n",
      "Epoch 12844: train loss: 0.4754016399383545\n",
      "Epoch 12845: train loss: 0.4754016399383545\n",
      "Epoch 12846: train loss: 0.4754015803337097\n",
      "Epoch 12847: train loss: 0.47540146112442017\n",
      "Epoch 12848: train loss: 0.4754014313220978\n",
      "Epoch 12849: train loss: 0.475401371717453\n",
      "Epoch 12850: train loss: 0.47540125250816345\n",
      "Epoch 12851: train loss: 0.4754011631011963\n",
      "Epoch 12852: train loss: 0.4754011034965515\n",
      "Epoch 12853: train loss: 0.47540101408958435\n",
      "Epoch 12854: train loss: 0.4754009544849396\n",
      "Epoch 12855: train loss: 0.47540083527565\n",
      "Epoch 12856: train loss: 0.47540083527565\n",
      "Epoch 12857: train loss: 0.47540074586868286\n",
      "Epoch 12858: train loss: 0.4754006266593933\n",
      "Epoch 12859: train loss: 0.4754005968570709\n",
      "Epoch 12860: train loss: 0.47540053725242615\n",
      "Epoch 12861: train loss: 0.4754004180431366\n",
      "Epoch 12862: train loss: 0.4754003882408142\n",
      "Epoch 12863: train loss: 0.47540032863616943\n",
      "Epoch 12864: train loss: 0.4754002094268799\n",
      "Epoch 12865: train loss: 0.4754001796245575\n",
      "Epoch 12866: train loss: 0.4754001200199127\n",
      "Epoch 12867: train loss: 0.47540000081062317\n",
      "Epoch 12868: train loss: 0.475399911403656\n",
      "Epoch 12869: train loss: 0.47539985179901123\n",
      "Epoch 12870: train loss: 0.47539979219436646\n",
      "Epoch 12871: train loss: 0.47539976239204407\n",
      "Epoch 12872: train loss: 0.4753996431827545\n",
      "Epoch 12873: train loss: 0.4753996431827545\n",
      "Epoch 12874: train loss: 0.4753994941711426\n",
      "Epoch 12875: train loss: 0.4753994345664978\n",
      "Epoch 12876: train loss: 0.475399374961853\n",
      "Epoch 12877: train loss: 0.47539928555488586\n",
      "Epoch 12878: train loss: 0.4753991663455963\n",
      "Epoch 12879: train loss: 0.4753991663455963\n",
      "Epoch 12880: train loss: 0.47539907693862915\n",
      "Epoch 12881: train loss: 0.4753990173339844\n",
      "Epoch 12882: train loss: 0.47539886832237244\n",
      "Epoch 12883: train loss: 0.47539886832237244\n",
      "Epoch 12884: train loss: 0.4753987491130829\n",
      "Epoch 12885: train loss: 0.4753986895084381\n",
      "Epoch 12886: train loss: 0.4753986597061157\n",
      "Epoch 12887: train loss: 0.47539860010147095\n",
      "Epoch 12888: train loss: 0.4753984808921814\n",
      "Epoch 12889: train loss: 0.475398451089859\n",
      "Epoch 12890: train loss: 0.47539833188056946\n",
      "Epoch 12891: train loss: 0.4753982722759247\n",
      "Epoch 12892: train loss: 0.4753982424736023\n",
      "Epoch 12893: train loss: 0.47539812326431274\n",
      "Epoch 12894: train loss: 0.47539806365966797\n",
      "Epoch 12895: train loss: 0.4753979742527008\n",
      "Epoch 12896: train loss: 0.47539791464805603\n",
      "Epoch 12897: train loss: 0.47539782524108887\n",
      "Epoch 12898: train loss: 0.47539782524108887\n",
      "Epoch 12899: train loss: 0.4753977060317993\n",
      "Epoch 12900: train loss: 0.47539761662483215\n",
      "Epoch 12901: train loss: 0.4753975570201874\n",
      "Epoch 12902: train loss: 0.4753974974155426\n",
      "Epoch 12903: train loss: 0.4753974378108978\n",
      "Epoch 12904: train loss: 0.47539734840393066\n",
      "Epoch 12905: train loss: 0.4753972291946411\n",
      "Epoch 12906: train loss: 0.4753971993923187\n",
      "Epoch 12907: train loss: 0.47539713978767395\n",
      "Epoch 12908: train loss: 0.4753970801830292\n",
      "Epoch 12909: train loss: 0.475396990776062\n",
      "Epoch 12910: train loss: 0.47539687156677246\n",
      "Epoch 12911: train loss: 0.4753968119621277\n",
      "Epoch 12912: train loss: 0.4753967821598053\n",
      "Epoch 12913: train loss: 0.4753967225551605\n",
      "Epoch 12914: train loss: 0.47539660334587097\n",
      "Epoch 12915: train loss: 0.4753965735435486\n",
      "Epoch 12916: train loss: 0.47539645433425903\n",
      "Epoch 12917: train loss: 0.47539639472961426\n",
      "Epoch 12918: train loss: 0.47539639472961426\n",
      "Epoch 12919: train loss: 0.4753962457180023\n",
      "Epoch 12920: train loss: 0.47539615631103516\n",
      "Epoch 12921: train loss: 0.4753960967063904\n",
      "Epoch 12922: train loss: 0.4753960371017456\n",
      "Epoch 12923: train loss: 0.47539594769477844\n",
      "Epoch 12924: train loss: 0.47539588809013367\n",
      "Epoch 12925: train loss: 0.4753958284854889\n",
      "Epoch 12926: train loss: 0.4753957688808441\n",
      "Epoch 12927: train loss: 0.47539567947387695\n",
      "Epoch 12928: train loss: 0.47539567947387695\n",
      "Epoch 12929: train loss: 0.475395530462265\n",
      "Epoch 12930: train loss: 0.47539547085762024\n",
      "Epoch 12931: train loss: 0.4753953516483307\n",
      "Epoch 12932: train loss: 0.4753953218460083\n",
      "Epoch 12933: train loss: 0.4753952622413635\n",
      "Epoch 12934: train loss: 0.475395143032074\n",
      "Epoch 12935: train loss: 0.4753950834274292\n",
      "Epoch 12936: train loss: 0.47539499402046204\n",
      "Epoch 12937: train loss: 0.47539493441581726\n",
      "Epoch 12938: train loss: 0.4753948748111725\n",
      "Epoch 12939: train loss: 0.4753947854042053\n",
      "Epoch 12940: train loss: 0.4753947854042053\n",
      "Epoch 12941: train loss: 0.47539466619491577\n",
      "Epoch 12942: train loss: 0.4753946363925934\n",
      "Epoch 12943: train loss: 0.47539451718330383\n",
      "Epoch 12944: train loss: 0.47539445757865906\n",
      "Epoch 12945: train loss: 0.4753943681716919\n",
      "Epoch 12946: train loss: 0.4753943085670471\n",
      "Epoch 12947: train loss: 0.47539424896240234\n",
      "Epoch 12948: train loss: 0.4753941595554352\n",
      "Epoch 12949: train loss: 0.47539404034614563\n",
      "Epoch 12950: train loss: 0.47539401054382324\n",
      "Epoch 12951: train loss: 0.47539395093917847\n",
      "Epoch 12952: train loss: 0.4753938913345337\n",
      "Epoch 12953: train loss: 0.47539380192756653\n",
      "Epoch 12954: train loss: 0.475393682718277\n",
      "Epoch 12955: train loss: 0.4753936231136322\n",
      "Epoch 12956: train loss: 0.47539353370666504\n",
      "Epoch 12957: train loss: 0.47539347410202026\n",
      "Epoch 12958: train loss: 0.4753934144973755\n",
      "Epoch 12959: train loss: 0.4753933846950531\n",
      "Epoch 12960: train loss: 0.4753933250904083\n",
      "Epoch 12961: train loss: 0.4753932058811188\n",
      "Epoch 12962: train loss: 0.4753931164741516\n",
      "Epoch 12963: train loss: 0.4753931164741516\n",
      "Epoch 12964: train loss: 0.47539299726486206\n",
      "Epoch 12965: train loss: 0.4753929078578949\n",
      "Epoch 12966: train loss: 0.4753928482532501\n",
      "Epoch 12967: train loss: 0.4753928482532501\n",
      "Epoch 12968: train loss: 0.4753926992416382\n",
      "Epoch 12969: train loss: 0.47539258003234863\n",
      "Epoch 12970: train loss: 0.47539258003234863\n",
      "Epoch 12971: train loss: 0.47539249062538147\n",
      "Epoch 12972: train loss: 0.4753924310207367\n",
      "Epoch 12973: train loss: 0.4753923714160919\n",
      "Epoch 12974: train loss: 0.47539234161376953\n",
      "Epoch 12975: train loss: 0.47539222240448\n",
      "Epoch 12976: train loss: 0.4753921329975128\n",
      "Epoch 12977: train loss: 0.47539207339286804\n",
      "Epoch 12978: train loss: 0.47539201378822327\n",
      "Epoch 12979: train loss: 0.4753919541835785\n",
      "Epoch 12980: train loss: 0.47539180517196655\n",
      "Epoch 12981: train loss: 0.4753917455673218\n",
      "Epoch 12982: train loss: 0.4753917157649994\n",
      "Epoch 12983: train loss: 0.4753916561603546\n",
      "Epoch 12984: train loss: 0.47539153695106506\n",
      "Epoch 12985: train loss: 0.4753915071487427\n",
      "Epoch 12986: train loss: 0.4753914475440979\n",
      "Epoch 12987: train loss: 0.47539132833480835\n",
      "Epoch 12988: train loss: 0.4753912687301636\n",
      "Epoch 12989: train loss: 0.4753912389278412\n",
      "Epoch 12990: train loss: 0.47539111971855164\n",
      "Epoch 12991: train loss: 0.4753910303115845\n",
      "Epoch 12992: train loss: 0.4753909707069397\n",
      "Epoch 12993: train loss: 0.4753909111022949\n",
      "Epoch 12994: train loss: 0.47539085149765015\n",
      "Epoch 12995: train loss: 0.475390762090683\n",
      "Epoch 12996: train loss: 0.4753907024860382\n",
      "Epoch 12997: train loss: 0.47539061307907104\n",
      "Epoch 12998: train loss: 0.47539055347442627\n",
      "Epoch 12999: train loss: 0.4753904938697815\n",
      "Epoch 13000: train loss: 0.47539040446281433\n",
      "Epoch 13001: train loss: 0.47539034485816956\n",
      "Epoch 13002: train loss: 0.47539022564888\n",
      "Epoch 13003: train loss: 0.4753901958465576\n",
      "Epoch 13004: train loss: 0.47539013624191284\n",
      "Epoch 13005: train loss: 0.47539007663726807\n",
      "Epoch 13006: train loss: 0.4753899872303009\n",
      "Epoch 13007: train loss: 0.47538986802101135\n",
      "Epoch 13008: train loss: 0.4753898084163666\n",
      "Epoch 13009: train loss: 0.4753897786140442\n",
      "Epoch 13010: train loss: 0.4753897190093994\n",
      "Epoch 13011: train loss: 0.47538959980010986\n",
      "Epoch 13012: train loss: 0.4753895699977875\n",
      "Epoch 13013: train loss: 0.4753894507884979\n",
      "Epoch 13014: train loss: 0.47538936138153076\n",
      "Epoch 13015: train loss: 0.47538936138153076\n",
      "Epoch 13016: train loss: 0.4753892421722412\n",
      "Epoch 13017: train loss: 0.47538918256759644\n",
      "Epoch 13018: train loss: 0.47538915276527405\n",
      "Epoch 13019: train loss: 0.4753890335559845\n",
      "Epoch 13020: train loss: 0.4753889739513397\n",
      "Epoch 13021: train loss: 0.47538888454437256\n",
      "Epoch 13022: train loss: 0.4753888249397278\n",
      "Epoch 13023: train loss: 0.475388765335083\n",
      "Epoch 13024: train loss: 0.4753887355327606\n",
      "Epoch 13025: train loss: 0.47538867592811584\n",
      "Epoch 13026: train loss: 0.4753885567188263\n",
      "Epoch 13027: train loss: 0.4753885269165039\n",
      "Epoch 13028: train loss: 0.47538840770721436\n",
      "Epoch 13029: train loss: 0.4753883481025696\n",
      "Epoch 13030: train loss: 0.4753882586956024\n",
      "Epoch 13031: train loss: 0.47538819909095764\n",
      "Epoch 13032: train loss: 0.4753881096839905\n",
      "Epoch 13033: train loss: 0.4753880500793457\n",
      "Epoch 13034: train loss: 0.4753879904747009\n",
      "Epoch 13035: train loss: 0.475387841463089\n",
      "Epoch 13036: train loss: 0.47538790106773376\n",
      "Epoch 13037: train loss: 0.47538772225379944\n",
      "Epoch 13038: train loss: 0.47538772225379944\n",
      "Epoch 13039: train loss: 0.4753876328468323\n",
      "Epoch 13040: train loss: 0.4753875732421875\n",
      "Epoch 13041: train loss: 0.47538745403289795\n",
      "Epoch 13042: train loss: 0.47538742423057556\n",
      "Epoch 13043: train loss: 0.47538724541664124\n",
      "Epoch 13044: train loss: 0.47538724541664124\n",
      "Epoch 13045: train loss: 0.47538724541664124\n",
      "Epoch 13046: train loss: 0.4753871560096741\n",
      "Epoch 13047: train loss: 0.4753870368003845\n",
      "Epoch 13048: train loss: 0.47538694739341736\n",
      "Epoch 13049: train loss: 0.4753868877887726\n",
      "Epoch 13050: train loss: 0.4753868281841278\n",
      "Epoch 13051: train loss: 0.47538673877716064\n",
      "Epoch 13052: train loss: 0.4753866195678711\n",
      "Epoch 13053: train loss: 0.4753866195678711\n",
      "Epoch 13054: train loss: 0.47538653016090393\n",
      "Epoch 13055: train loss: 0.47538647055625916\n",
      "Epoch 13056: train loss: 0.4753864109516144\n",
      "Epoch 13057: train loss: 0.4753863215446472\n",
      "Epoch 13058: train loss: 0.47538620233535767\n",
      "Epoch 13059: train loss: 0.47538620233535767\n",
      "Epoch 13060: train loss: 0.4753861129283905\n",
      "Epoch 13061: train loss: 0.4753860533237457\n",
      "Epoch 13062: train loss: 0.47538596391677856\n",
      "Epoch 13063: train loss: 0.4753859043121338\n",
      "Epoch 13064: train loss: 0.475385844707489\n",
      "Epoch 13065: train loss: 0.47538575530052185\n",
      "Epoch 13066: train loss: 0.4753856360912323\n",
      "Epoch 13067: train loss: 0.4753855764865875\n",
      "Epoch 13068: train loss: 0.47538554668426514\n",
      "Epoch 13069: train loss: 0.47538548707962036\n",
      "Epoch 13070: train loss: 0.4753853678703308\n",
      "Epoch 13071: train loss: 0.4753853380680084\n",
      "Epoch 13072: train loss: 0.47538527846336365\n",
      "Epoch 13073: train loss: 0.4753851592540741\n",
      "Epoch 13074: train loss: 0.47538506984710693\n",
      "Epoch 13075: train loss: 0.47538506984710693\n",
      "Epoch 13076: train loss: 0.47538501024246216\n",
      "Epoch 13077: train loss: 0.475384920835495\n",
      "Epoch 13078: train loss: 0.4753848612308502\n",
      "Epoch 13079: train loss: 0.47538480162620544\n",
      "Epoch 13080: train loss: 0.4753846526145935\n",
      "Epoch 13081: train loss: 0.4753846526145935\n",
      "Epoch 13082: train loss: 0.47538453340530396\n",
      "Epoch 13083: train loss: 0.47538450360298157\n",
      "Epoch 13084: train loss: 0.4753844439983368\n",
      "Epoch 13085: train loss: 0.47538432478904724\n",
      "Epoch 13086: train loss: 0.47538432478904724\n",
      "Epoch 13087: train loss: 0.4753841757774353\n",
      "Epoch 13088: train loss: 0.4753841161727905\n",
      "Epoch 13089: train loss: 0.47538402676582336\n",
      "Epoch 13090: train loss: 0.4753839671611786\n",
      "Epoch 13091: train loss: 0.4753839075565338\n",
      "Epoch 13092: train loss: 0.47538381814956665\n",
      "Epoch 13093: train loss: 0.4753837585449219\n",
      "Epoch 13094: train loss: 0.4753836989402771\n",
      "Epoch 13095: train loss: 0.47538360953330994\n",
      "Epoch 13096: train loss: 0.47538354992866516\n",
      "Epoch 13097: train loss: 0.4753834903240204\n",
      "Epoch 13098: train loss: 0.4753834009170532\n",
      "Epoch 13099: train loss: 0.47538334131240845\n",
      "Epoch 13100: train loss: 0.4753832221031189\n",
      "Epoch 13101: train loss: 0.4753831923007965\n",
      "Epoch 13102: train loss: 0.47538313269615173\n",
      "Epoch 13103: train loss: 0.47538307309150696\n",
      "Epoch 13104: train loss: 0.475382924079895\n",
      "Epoch 13105: train loss: 0.475382924079895\n",
      "Epoch 13106: train loss: 0.47538280487060547\n",
      "Epoch 13107: train loss: 0.4753827750682831\n",
      "Epoch 13108: train loss: 0.47538265585899353\n",
      "Epoch 13109: train loss: 0.47538259625434875\n",
      "Epoch 13110: train loss: 0.47538256645202637\n",
      "Epoch 13111: train loss: 0.4753824472427368\n",
      "Epoch 13112: train loss: 0.47538238763809204\n",
      "Epoch 13113: train loss: 0.47538235783576965\n",
      "Epoch 13114: train loss: 0.4753822386264801\n",
      "Epoch 13115: train loss: 0.4753821790218353\n",
      "Epoch 13116: train loss: 0.47538214921951294\n",
      "Epoch 13117: train loss: 0.4753820300102234\n",
      "Epoch 13118: train loss: 0.4753819704055786\n",
      "Epoch 13119: train loss: 0.47538188099861145\n",
      "Epoch 13120: train loss: 0.4753818213939667\n",
      "Epoch 13121: train loss: 0.4753817319869995\n",
      "Epoch 13122: train loss: 0.4753817319869995\n",
      "Epoch 13123: train loss: 0.47538167238235474\n",
      "Epoch 13124: train loss: 0.4753815233707428\n",
      "Epoch 13125: train loss: 0.475381463766098\n",
      "Epoch 13126: train loss: 0.47538140416145325\n",
      "Epoch 13127: train loss: 0.4753813147544861\n",
      "Epoch 13128: train loss: 0.4753812551498413\n",
      "Epoch 13129: train loss: 0.47538119554519653\n",
      "Epoch 13130: train loss: 0.47538110613822937\n",
      "Epoch 13131: train loss: 0.4753810465335846\n",
      "Epoch 13132: train loss: 0.4753809869289398\n",
      "Epoch 13133: train loss: 0.47538089752197266\n",
      "Epoch 13134: train loss: 0.4753808379173279\n",
      "Epoch 13135: train loss: 0.47538071870803833\n",
      "Epoch 13136: train loss: 0.47538068890571594\n",
      "Epoch 13137: train loss: 0.47538062930107117\n",
      "Epoch 13138: train loss: 0.4753805100917816\n",
      "Epoch 13139: train loss: 0.4753805100917816\n",
      "Epoch 13140: train loss: 0.4753803610801697\n",
      "Epoch 13141: train loss: 0.4753803014755249\n",
      "Epoch 13142: train loss: 0.4753802716732025\n",
      "Epoch 13143: train loss: 0.47538021206855774\n",
      "Epoch 13144: train loss: 0.47538015246391296\n",
      "Epoch 13145: train loss: 0.4753800630569458\n",
      "Epoch 13146: train loss: 0.475380003452301\n",
      "Epoch 13147: train loss: 0.47537994384765625\n",
      "Epoch 13148: train loss: 0.4753798246383667\n",
      "Epoch 13149: train loss: 0.4753797948360443\n",
      "Epoch 13150: train loss: 0.47537973523139954\n",
      "Epoch 13151: train loss: 0.47537961602211\n",
      "Epoch 13152: train loss: 0.4753795862197876\n",
      "Epoch 13153: train loss: 0.47537946701049805\n",
      "Epoch 13154: train loss: 0.47537940740585327\n",
      "Epoch 13155: train loss: 0.4753793179988861\n",
      "Epoch 13156: train loss: 0.47537925839424133\n",
      "Epoch 13157: train loss: 0.47537919878959656\n",
      "Epoch 13158: train loss: 0.4753791093826294\n",
      "Epoch 13159: train loss: 0.4753791093826294\n",
      "Epoch 13160: train loss: 0.47537899017333984\n",
      "Epoch 13161: train loss: 0.4753789007663727\n",
      "Epoch 13162: train loss: 0.4753788411617279\n",
      "Epoch 13163: train loss: 0.47537878155708313\n",
      "Epoch 13164: train loss: 0.47537869215011597\n",
      "Epoch 13165: train loss: 0.4753786325454712\n",
      "Epoch 13166: train loss: 0.4753785729408264\n",
      "Epoch 13167: train loss: 0.47537848353385925\n",
      "Epoch 13168: train loss: 0.4753784239292145\n",
      "Epoch 13169: train loss: 0.4753783643245697\n",
      "Epoch 13170: train loss: 0.4753783345222473\n",
      "Epoch 13171: train loss: 0.475378155708313\n",
      "Epoch 13172: train loss: 0.4753781259059906\n",
      "Epoch 13173: train loss: 0.47537800669670105\n",
      "Epoch 13174: train loss: 0.4753779470920563\n",
      "Epoch 13175: train loss: 0.4753779470920563\n",
      "Epoch 13176: train loss: 0.4753778576850891\n",
      "Epoch 13177: train loss: 0.47537773847579956\n",
      "Epoch 13178: train loss: 0.4753777086734772\n",
      "Epoch 13179: train loss: 0.4753776490688324\n",
      "Epoch 13180: train loss: 0.4753775894641876\n",
      "Epoch 13181: train loss: 0.47537750005722046\n",
      "Epoch 13182: train loss: 0.4753774404525757\n",
      "Epoch 13183: train loss: 0.47537732124328613\n",
      "Epoch 13184: train loss: 0.47537723183631897\n",
      "Epoch 13185: train loss: 0.4753771722316742\n",
      "Epoch 13186: train loss: 0.4753771126270294\n",
      "Epoch 13187: train loss: 0.47537708282470703\n",
      "Epoch 13188: train loss: 0.4753769636154175\n",
      "Epoch 13189: train loss: 0.4753769040107727\n",
      "Epoch 13190: train loss: 0.4753768742084503\n",
      "Epoch 13191: train loss: 0.47537681460380554\n",
      "Epoch 13192: train loss: 0.475376695394516\n",
      "Epoch 13193: train loss: 0.4753766655921936\n",
      "Epoch 13194: train loss: 0.47537660598754883\n",
      "Epoch 13195: train loss: 0.4753764867782593\n",
      "Epoch 13196: train loss: 0.4753763973712921\n",
      "Epoch 13197: train loss: 0.4753763973712921\n",
      "Epoch 13198: train loss: 0.47537627816200256\n",
      "Epoch 13199: train loss: 0.4753761887550354\n",
      "Epoch 13200: train loss: 0.4753761291503906\n",
      "Epoch 13201: train loss: 0.47537606954574585\n",
      "Epoch 13202: train loss: 0.4753760099411011\n",
      "Epoch 13203: train loss: 0.4753759801387787\n",
      "Epoch 13204: train loss: 0.47537586092948914\n",
      "Epoch 13205: train loss: 0.47537580132484436\n",
      "Epoch 13206: train loss: 0.4753757119178772\n",
      "Epoch 13207: train loss: 0.4753756523132324\n",
      "Epoch 13208: train loss: 0.47537556290626526\n",
      "Epoch 13209: train loss: 0.47537556290626526\n",
      "Epoch 13210: train loss: 0.4753754436969757\n",
      "Epoch 13211: train loss: 0.47537538409233093\n",
      "Epoch 13212: train loss: 0.47537529468536377\n",
      "Epoch 13213: train loss: 0.475375235080719\n",
      "Epoch 13214: train loss: 0.47537514567375183\n",
      "Epoch 13215: train loss: 0.47537508606910706\n",
      "Epoch 13216: train loss: 0.4753750264644623\n",
      "Epoch 13217: train loss: 0.4753749370574951\n",
      "Epoch 13218: train loss: 0.47537487745285034\n",
      "Epoch 13219: train loss: 0.4753747582435608\n",
      "Epoch 13220: train loss: 0.4753747284412384\n",
      "Epoch 13221: train loss: 0.47537466883659363\n",
      "Epoch 13222: train loss: 0.47537460923194885\n",
      "Epoch 13223: train loss: 0.4753745198249817\n",
      "Epoch 13224: train loss: 0.4753744602203369\n",
      "Epoch 13225: train loss: 0.47537440061569214\n",
      "Epoch 13226: train loss: 0.475374311208725\n",
      "Epoch 13227: train loss: 0.4753742516040802\n",
      "Epoch 13228: train loss: 0.47537413239479065\n",
      "Epoch 13229: train loss: 0.47537410259246826\n",
      "Epoch 13230: train loss: 0.4753740429878235\n",
      "Epoch 13231: train loss: 0.4753739833831787\n",
      "Epoch 13232: train loss: 0.4753738343715668\n",
      "Epoch 13233: train loss: 0.475373774766922\n",
      "Epoch 13234: train loss: 0.475373774766922\n",
      "Epoch 13235: train loss: 0.47537368535995483\n",
      "Epoch 13236: train loss: 0.4753735661506653\n",
      "Epoch 13237: train loss: 0.4753735661506653\n",
      "Epoch 13238: train loss: 0.4753734767436981\n",
      "Epoch 13239: train loss: 0.47537335753440857\n",
      "Epoch 13240: train loss: 0.4753732979297638\n",
      "Epoch 13241: train loss: 0.4753732681274414\n",
      "Epoch 13242: train loss: 0.47537314891815186\n",
      "Epoch 13243: train loss: 0.47537314891815186\n",
      "Epoch 13244: train loss: 0.4753730595111847\n",
      "Epoch 13245: train loss: 0.4753729999065399\n",
      "Epoch 13246: train loss: 0.47537294030189514\n",
      "Epoch 13247: train loss: 0.475372850894928\n",
      "Epoch 13248: train loss: 0.4753727912902832\n",
      "Epoch 13249: train loss: 0.47537267208099365\n",
      "Epoch 13250: train loss: 0.47537264227867126\n",
      "Epoch 13251: train loss: 0.4753725826740265\n",
      "Epoch 13252: train loss: 0.47537246346473694\n",
      "Epoch 13253: train loss: 0.47537243366241455\n",
      "Epoch 13254: train loss: 0.4753723740577698\n",
      "Epoch 13255: train loss: 0.4753722548484802\n",
      "Epoch 13256: train loss: 0.47537219524383545\n",
      "Epoch 13257: train loss: 0.4753721058368683\n",
      "Epoch 13258: train loss: 0.47537198662757874\n",
      "Epoch 13259: train loss: 0.47537198662757874\n",
      "Epoch 13260: train loss: 0.47537195682525635\n",
      "Epoch 13261: train loss: 0.475371778011322\n",
      "Epoch 13262: train loss: 0.475371778011322\n",
      "Epoch 13263: train loss: 0.47537168860435486\n",
      "Epoch 13264: train loss: 0.47537168860435486\n",
      "Epoch 13265: train loss: 0.4753715395927429\n",
      "Epoch 13266: train loss: 0.47537147998809814\n",
      "Epoch 13267: train loss: 0.47537142038345337\n",
      "Epoch 13268: train loss: 0.4753713309764862\n",
      "Epoch 13269: train loss: 0.4753713309764862\n",
      "Epoch 13270: train loss: 0.47537121176719666\n",
      "Epoch 13271: train loss: 0.4753711223602295\n",
      "Epoch 13272: train loss: 0.4753710627555847\n",
      "Epoch 13273: train loss: 0.47537100315093994\n",
      "Epoch 13274: train loss: 0.47537094354629517\n",
      "Epoch 13275: train loss: 0.475370854139328\n",
      "Epoch 13276: train loss: 0.47537073493003845\n",
      "Epoch 13277: train loss: 0.47537070512771606\n",
      "Epoch 13278: train loss: 0.4753706455230713\n",
      "Epoch 13279: train loss: 0.47537052631378174\n",
      "Epoch 13280: train loss: 0.47537049651145935\n",
      "Epoch 13281: train loss: 0.4753704369068146\n",
      "Epoch 13282: train loss: 0.475370317697525\n",
      "Epoch 13283: train loss: 0.47537028789520264\n",
      "Epoch 13284: train loss: 0.47537022829055786\n",
      "Epoch 13285: train loss: 0.4753701686859131\n",
      "Epoch 13286: train loss: 0.47537001967430115\n",
      "Epoch 13287: train loss: 0.47537001967430115\n",
      "Epoch 13288: train loss: 0.47536996006965637\n",
      "Epoch 13289: train loss: 0.4753698706626892\n",
      "Epoch 13290: train loss: 0.47536981105804443\n",
      "Epoch 13291: train loss: 0.4753696918487549\n",
      "Epoch 13292: train loss: 0.4753696620464325\n",
      "Epoch 13293: train loss: 0.4753696024417877\n",
      "Epoch 13294: train loss: 0.47536948323249817\n",
      "Epoch 13295: train loss: 0.4753694534301758\n",
      "Epoch 13296: train loss: 0.475369393825531\n",
      "Epoch 13297: train loss: 0.47536927461624146\n",
      "Epoch 13298: train loss: 0.47536924481391907\n",
      "Epoch 13299: train loss: 0.4753691852092743\n",
      "Epoch 13300: train loss: 0.4753691256046295\n",
      "Epoch 13301: train loss: 0.4753689765930176\n",
      "Epoch 13302: train loss: 0.4753689765930176\n",
      "Epoch 13303: train loss: 0.4753689169883728\n",
      "Epoch 13304: train loss: 0.47536882758140564\n",
      "Epoch 13305: train loss: 0.47536876797676086\n",
      "Epoch 13306: train loss: 0.4753686487674713\n",
      "Epoch 13307: train loss: 0.4753686487674713\n",
      "Epoch 13308: train loss: 0.4753684997558594\n",
      "Epoch 13309: train loss: 0.4753684401512146\n",
      "Epoch 13310: train loss: 0.47536835074424744\n",
      "Epoch 13311: train loss: 0.47536835074424744\n",
      "Epoch 13312: train loss: 0.4753682315349579\n",
      "Epoch 13313: train loss: 0.4753681719303131\n",
      "Epoch 13314: train loss: 0.4753681421279907\n",
      "Epoch 13315: train loss: 0.47536802291870117\n",
      "Epoch 13316: train loss: 0.475367933511734\n",
      "Epoch 13317: train loss: 0.475367933511734\n",
      "Epoch 13318: train loss: 0.47536781430244446\n",
      "Epoch 13319: train loss: 0.4753677248954773\n",
      "Epoch 13320: train loss: 0.4753676652908325\n",
      "Epoch 13321: train loss: 0.47536760568618774\n",
      "Epoch 13322: train loss: 0.47536754608154297\n",
      "Epoch 13323: train loss: 0.4753674566745758\n",
      "Epoch 13324: train loss: 0.47536739706993103\n",
      "Epoch 13325: train loss: 0.47536733746528625\n",
      "Epoch 13326: train loss: 0.47536730766296387\n",
      "Epoch 13327: train loss: 0.4753671884536743\n",
      "Epoch 13328: train loss: 0.47536712884902954\n",
      "Epoch 13329: train loss: 0.47536709904670715\n",
      "Epoch 13330: train loss: 0.4753669798374176\n",
      "Epoch 13331: train loss: 0.4753669202327728\n",
      "Epoch 13332: train loss: 0.47536683082580566\n",
      "Epoch 13333: train loss: 0.47536683082580566\n",
      "Epoch 13334: train loss: 0.4753667116165161\n",
      "Epoch 13335: train loss: 0.47536662220954895\n",
      "Epoch 13336: train loss: 0.4753665626049042\n",
      "Epoch 13337: train loss: 0.4753665030002594\n",
      "Epoch 13338: train loss: 0.47536641359329224\n",
      "Epoch 13339: train loss: 0.47536635398864746\n",
      "Epoch 13340: train loss: 0.4753662645816803\n",
      "Epoch 13341: train loss: 0.47536614537239075\n",
      "Epoch 13342: train loss: 0.47536614537239075\n",
      "Epoch 13343: train loss: 0.4753660559654236\n",
      "Epoch 13344: train loss: 0.4753659963607788\n",
      "Epoch 13345: train loss: 0.47536593675613403\n",
      "Epoch 13346: train loss: 0.47536587715148926\n",
      "Epoch 13347: train loss: 0.47536584734916687\n",
      "Epoch 13348: train loss: 0.47536566853523254\n",
      "Epoch 13349: train loss: 0.47536563873291016\n",
      "Epoch 13350: train loss: 0.4753655791282654\n",
      "Epoch 13351: train loss: 0.4753655195236206\n",
      "Epoch 13352: train loss: 0.47536543011665344\n",
      "Epoch 13353: train loss: 0.47536543011665344\n",
      "Epoch 13354: train loss: 0.4753653109073639\n",
      "Epoch 13355: train loss: 0.4753652513027191\n",
      "Epoch 13356: train loss: 0.47536522150039673\n",
      "Epoch 13357: train loss: 0.4753651022911072\n",
      "Epoch 13358: train loss: 0.4753650426864624\n",
      "Epoch 13359: train loss: 0.47536495327949524\n",
      "Epoch 13360: train loss: 0.47536489367485046\n",
      "Epoch 13361: train loss: 0.4753648042678833\n",
      "Epoch 13362: train loss: 0.4753647446632385\n",
      "Epoch 13363: train loss: 0.475364625453949\n",
      "Epoch 13364: train loss: 0.4753645658493042\n",
      "Epoch 13365: train loss: 0.4753645658493042\n",
      "Epoch 13366: train loss: 0.47536447644233704\n",
      "Epoch 13367: train loss: 0.4753643572330475\n",
      "Epoch 13368: train loss: 0.4753643274307251\n",
      "Epoch 13369: train loss: 0.4753642678260803\n",
      "Epoch 13370: train loss: 0.47536414861679077\n",
      "Epoch 13371: train loss: 0.4753641188144684\n",
      "Epoch 13372: train loss: 0.47536399960517883\n",
      "Epoch 13373: train loss: 0.47536394000053406\n",
      "Epoch 13374: train loss: 0.47536391019821167\n",
      "Epoch 13375: train loss: 0.4753638505935669\n",
      "Epoch 13376: train loss: 0.4753637909889221\n",
      "Epoch 13377: train loss: 0.47536370158195496\n",
      "Epoch 13378: train loss: 0.4753635823726654\n",
      "Epoch 13379: train loss: 0.47536352276802063\n",
      "Epoch 13380: train loss: 0.47536349296569824\n",
      "Epoch 13381: train loss: 0.47536343336105347\n",
      "Epoch 13382: train loss: 0.4753633141517639\n",
      "Epoch 13383: train loss: 0.47536328434944153\n",
      "Epoch 13384: train loss: 0.475363165140152\n",
      "Epoch 13385: train loss: 0.4753631055355072\n",
      "Epoch 13386: train loss: 0.47536301612854004\n",
      "Epoch 13387: train loss: 0.47536301612854004\n",
      "Epoch 13388: train loss: 0.4753628969192505\n",
      "Epoch 13389: train loss: 0.4753628969192505\n",
      "Epoch 13390: train loss: 0.47536274790763855\n",
      "Epoch 13391: train loss: 0.47536274790763855\n",
      "Epoch 13392: train loss: 0.4753626585006714\n",
      "Epoch 13393: train loss: 0.4753625988960266\n",
      "Epoch 13394: train loss: 0.47536247968673706\n",
      "Epoch 13395: train loss: 0.4753624498844147\n",
      "Epoch 13396: train loss: 0.4753623902797699\n",
      "Epoch 13397: train loss: 0.47536227107048035\n",
      "Epoch 13398: train loss: 0.47536224126815796\n",
      "Epoch 13399: train loss: 0.4753621816635132\n",
      "Epoch 13400: train loss: 0.4753621220588684\n",
      "Epoch 13401: train loss: 0.47536197304725647\n",
      "Epoch 13402: train loss: 0.47536203265190125\n",
      "Epoch 13403: train loss: 0.4753618538379669\n",
      "Epoch 13404: train loss: 0.4753618538379669\n",
      "Epoch 13405: train loss: 0.47536176443099976\n",
      "Epoch 13406: train loss: 0.4753616452217102\n",
      "Epoch 13407: train loss: 0.4753616154193878\n",
      "Epoch 13408: train loss: 0.47536149621009827\n",
      "Epoch 13409: train loss: 0.4753614366054535\n",
      "Epoch 13410: train loss: 0.4753614068031311\n",
      "Epoch 13411: train loss: 0.47536134719848633\n",
      "Epoch 13412: train loss: 0.4753612279891968\n",
      "Epoch 13413: train loss: 0.4753612279891968\n",
      "Epoch 13414: train loss: 0.4753611385822296\n",
      "Epoch 13415: train loss: 0.47536101937294006\n",
      "Epoch 13416: train loss: 0.4753609895706177\n",
      "Epoch 13417: train loss: 0.4753608703613281\n",
      "Epoch 13418: train loss: 0.47536081075668335\n",
      "Epoch 13419: train loss: 0.4753607511520386\n",
      "Epoch 13420: train loss: 0.4753607213497162\n",
      "Epoch 13421: train loss: 0.47536060214042664\n",
      "Epoch 13422: train loss: 0.47536054253578186\n",
      "Epoch 13423: train loss: 0.4753605127334595\n",
      "Epoch 13424: train loss: 0.4753603935241699\n",
      "Epoch 13425: train loss: 0.47536033391952515\n",
      "Epoch 13426: train loss: 0.47536030411720276\n",
      "Epoch 13427: train loss: 0.4753601849079132\n",
      "Epoch 13428: train loss: 0.47536012530326843\n",
      "Epoch 13429: train loss: 0.47536003589630127\n",
      "Epoch 13430: train loss: 0.4753599762916565\n",
      "Epoch 13431: train loss: 0.4753599166870117\n",
      "Epoch 13432: train loss: 0.47535988688468933\n",
      "Epoch 13433: train loss: 0.4753597676753998\n",
      "Epoch 13434: train loss: 0.475359708070755\n",
      "Epoch 13435: train loss: 0.4753596782684326\n",
      "Epoch 13436: train loss: 0.47535961866378784\n",
      "Epoch 13437: train loss: 0.4753594994544983\n",
      "Epoch 13438: train loss: 0.47535941004753113\n",
      "Epoch 13439: train loss: 0.47535935044288635\n",
      "Epoch 13440: train loss: 0.4753592908382416\n",
      "Epoch 13441: train loss: 0.4753592014312744\n",
      "Epoch 13442: train loss: 0.47535914182662964\n",
      "Epoch 13443: train loss: 0.47535908222198486\n",
      "Epoch 13444: train loss: 0.4753590524196625\n",
      "Epoch 13445: train loss: 0.4753589332103729\n",
      "Epoch 13446: train loss: 0.47535887360572815\n",
      "Epoch 13447: train loss: 0.47535884380340576\n",
      "Epoch 13448: train loss: 0.4753587245941162\n",
      "Epoch 13449: train loss: 0.4753587245941162\n",
      "Epoch 13450: train loss: 0.4753585755825043\n",
      "Epoch 13451: train loss: 0.4753585755825043\n",
      "Epoch 13452: train loss: 0.4753584563732147\n",
      "Epoch 13453: train loss: 0.47535842657089233\n",
      "Epoch 13454: train loss: 0.4753583073616028\n",
      "Epoch 13455: train loss: 0.4753582179546356\n",
      "Epoch 13456: train loss: 0.4753582179546356\n",
      "Epoch 13457: train loss: 0.47535809874534607\n",
      "Epoch 13458: train loss: 0.4753580093383789\n",
      "Epoch 13459: train loss: 0.47535794973373413\n",
      "Epoch 13460: train loss: 0.47535794973373413\n",
      "Epoch 13461: train loss: 0.4753578305244446\n",
      "Epoch 13462: train loss: 0.4753578007221222\n",
      "Epoch 13463: train loss: 0.47535768151283264\n",
      "Epoch 13464: train loss: 0.47535762190818787\n",
      "Epoch 13465: train loss: 0.4753575325012207\n",
      "Epoch 13466: train loss: 0.4753575325012207\n",
      "Epoch 13467: train loss: 0.47535738348960876\n",
      "Epoch 13468: train loss: 0.47535738348960876\n",
      "Epoch 13469: train loss: 0.4753572642803192\n",
      "Epoch 13470: train loss: 0.4753572642803192\n",
      "Epoch 13471: train loss: 0.47535717487335205\n",
      "Epoch 13472: train loss: 0.4753570556640625\n",
      "Epoch 13473: train loss: 0.4753569960594177\n",
      "Epoch 13474: train loss: 0.47535693645477295\n",
      "Epoch 13475: train loss: 0.4753568470478058\n",
      "Epoch 13476: train loss: 0.475356787443161\n",
      "Epoch 13477: train loss: 0.47535672783851624\n",
      "Epoch 13478: train loss: 0.4753566384315491\n",
      "Epoch 13479: train loss: 0.4753565192222595\n",
      "Epoch 13480: train loss: 0.4753565192222595\n",
      "Epoch 13481: train loss: 0.47535642981529236\n",
      "Epoch 13482: train loss: 0.4753563702106476\n",
      "Epoch 13483: train loss: 0.4753562808036804\n",
      "Epoch 13484: train loss: 0.4753562808036804\n",
      "Epoch 13485: train loss: 0.47535616159439087\n",
      "Epoch 13486: train loss: 0.4753561019897461\n",
      "Epoch 13487: train loss: 0.47535601258277893\n",
      "Epoch 13488: train loss: 0.47535601258277893\n",
      "Epoch 13489: train loss: 0.4753558933734894\n",
      "Epoch 13490: train loss: 0.4753558039665222\n",
      "Epoch 13491: train loss: 0.4753558039665222\n",
      "Epoch 13492: train loss: 0.47535568475723267\n",
      "Epoch 13493: train loss: 0.4753555953502655\n",
      "Epoch 13494: train loss: 0.4753555357456207\n",
      "Epoch 13495: train loss: 0.47535547614097595\n",
      "Epoch 13496: train loss: 0.4753553867340088\n",
      "Epoch 13497: train loss: 0.4753553867340088\n",
      "Epoch 13498: train loss: 0.47535526752471924\n",
      "Epoch 13499: train loss: 0.4753551781177521\n",
      "Epoch 13500: train loss: 0.4753551185131073\n",
      "Epoch 13501: train loss: 0.47535502910614014\n",
      "Epoch 13502: train loss: 0.47535496950149536\n",
      "Epoch 13503: train loss: 0.4753549098968506\n",
      "Epoch 13504: train loss: 0.4753548502922058\n",
      "Epoch 13505: train loss: 0.47535476088523865\n",
      "Epoch 13506: train loss: 0.4753546416759491\n",
      "Epoch 13507: train loss: 0.4753546416759491\n",
      "Epoch 13508: train loss: 0.47535455226898193\n",
      "Epoch 13509: train loss: 0.47535455226898193\n",
      "Epoch 13510: train loss: 0.4753544330596924\n",
      "Epoch 13511: train loss: 0.4753543436527252\n",
      "Epoch 13512: train loss: 0.47535422444343567\n",
      "Epoch 13513: train loss: 0.4753541946411133\n",
      "Epoch 13514: train loss: 0.4753541350364685\n",
      "Epoch 13515: train loss: 0.4753541350364685\n",
      "Epoch 13516: train loss: 0.47535401582717896\n",
      "Epoch 13517: train loss: 0.47535398602485657\n",
      "Epoch 13518: train loss: 0.475353866815567\n",
      "Epoch 13519: train loss: 0.47535380721092224\n",
      "Epoch 13520: train loss: 0.47535377740859985\n",
      "Epoch 13521: train loss: 0.4753536581993103\n",
      "Epoch 13522: train loss: 0.47535356879234314\n",
      "Epoch 13523: train loss: 0.47535350918769836\n",
      "Epoch 13524: train loss: 0.47535350918769836\n",
      "Epoch 13525: train loss: 0.4753533601760864\n",
      "Epoch 13526: train loss: 0.47535330057144165\n",
      "Epoch 13527: train loss: 0.4753532409667969\n",
      "Epoch 13528: train loss: 0.4753532409667969\n",
      "Epoch 13529: train loss: 0.4753531217575073\n",
      "Epoch 13530: train loss: 0.47535303235054016\n",
      "Epoch 13531: train loss: 0.4753529727458954\n",
      "Epoch 13532: train loss: 0.4753529131412506\n",
      "Epoch 13533: train loss: 0.4753528833389282\n",
      "Epoch 13534: train loss: 0.47535276412963867\n",
      "Epoch 13535: train loss: 0.4753526747226715\n",
      "Epoch 13536: train loss: 0.47535261511802673\n",
      "Epoch 13537: train loss: 0.47535255551338196\n",
      "Epoch 13538: train loss: 0.4753524959087372\n",
      "Epoch 13539: train loss: 0.47535240650177\n",
      "Epoch 13540: train loss: 0.47535234689712524\n",
      "Epoch 13541: train loss: 0.47535228729248047\n",
      "Epoch 13542: train loss: 0.4753522574901581\n",
      "Epoch 13543: train loss: 0.47535213828086853\n",
      "Epoch 13544: train loss: 0.47535204887390137\n",
      "Epoch 13545: train loss: 0.47535204887390137\n",
      "Epoch 13546: train loss: 0.4753519296646118\n",
      "Epoch 13547: train loss: 0.47535187005996704\n",
      "Epoch 13548: train loss: 0.4753517806529999\n",
      "Epoch 13549: train loss: 0.4753517210483551\n",
      "Epoch 13550: train loss: 0.4753516614437103\n",
      "Epoch 13551: train loss: 0.47535157203674316\n",
      "Epoch 13552: train loss: 0.4753515124320984\n",
      "Epoch 13553: train loss: 0.4753514528274536\n",
      "Epoch 13554: train loss: 0.4753514230251312\n",
      "Epoch 13555: train loss: 0.4753513038158417\n",
      "Epoch 13556: train loss: 0.4753512442111969\n",
      "Epoch 13557: train loss: 0.47535115480422974\n",
      "Epoch 13558: train loss: 0.47535109519958496\n",
      "Epoch 13559: train loss: 0.4753510057926178\n",
      "Epoch 13560: train loss: 0.4753510057926178\n",
      "Epoch 13561: train loss: 0.47535088658332825\n",
      "Epoch 13562: train loss: 0.47535082697868347\n",
      "Epoch 13563: train loss: 0.4753507971763611\n",
      "Epoch 13564: train loss: 0.47535067796707153\n",
      "Epoch 13565: train loss: 0.47535061836242676\n",
      "Epoch 13566: train loss: 0.4753505289554596\n",
      "Epoch 13567: train loss: 0.4753505289554596\n",
      "Epoch 13568: train loss: 0.4753504693508148\n",
      "Epoch 13569: train loss: 0.4753503203392029\n",
      "Epoch 13570: train loss: 0.4753502607345581\n",
      "Epoch 13571: train loss: 0.47535020112991333\n",
      "Epoch 13572: train loss: 0.47535011172294617\n",
      "Epoch 13573: train loss: 0.47535011172294617\n",
      "Epoch 13574: train loss: 0.47534996271133423\n",
      "Epoch 13575: train loss: 0.47534996271133423\n",
      "Epoch 13576: train loss: 0.4753498435020447\n",
      "Epoch 13577: train loss: 0.4753497838973999\n",
      "Epoch 13578: train loss: 0.4753497540950775\n",
      "Epoch 13579: train loss: 0.47534969449043274\n",
      "Epoch 13580: train loss: 0.4753495752811432\n",
      "Epoch 13581: train loss: 0.4753495454788208\n",
      "Epoch 13582: train loss: 0.475349485874176\n",
      "Epoch 13583: train loss: 0.47534942626953125\n",
      "Epoch 13584: train loss: 0.4753493666648865\n",
      "Epoch 13585: train loss: 0.47534921765327454\n",
      "Epoch 13586: train loss: 0.47534915804862976\n",
      "Epoch 13587: train loss: 0.475349098443985\n",
      "Epoch 13588: train loss: 0.4753490686416626\n",
      "Epoch 13589: train loss: 0.47534894943237305\n",
      "Epoch 13590: train loss: 0.47534888982772827\n",
      "Epoch 13591: train loss: 0.4753488004207611\n",
      "Epoch 13592: train loss: 0.4753488004207611\n",
      "Epoch 13593: train loss: 0.47534868121147156\n",
      "Epoch 13594: train loss: 0.4753485918045044\n",
      "Epoch 13595: train loss: 0.4753485321998596\n",
      "Epoch 13596: train loss: 0.47534847259521484\n",
      "Epoch 13597: train loss: 0.47534844279289246\n",
      "Epoch 13598: train loss: 0.4753483831882477\n",
      "Epoch 13599: train loss: 0.47534826397895813\n",
      "Epoch 13600: train loss: 0.47534823417663574\n",
      "Epoch 13601: train loss: 0.47534817457199097\n",
      "Epoch 13602: train loss: 0.4753480553627014\n",
      "Epoch 13603: train loss: 0.47534802556037903\n",
      "Epoch 13604: train loss: 0.4753479063510895\n",
      "Epoch 13605: train loss: 0.4753479063510895\n",
      "Epoch 13606: train loss: 0.4753478169441223\n",
      "Epoch 13607: train loss: 0.47534775733947754\n",
      "Epoch 13608: train loss: 0.47534769773483276\n",
      "Epoch 13609: train loss: 0.4753476083278656\n",
      "Epoch 13610: train loss: 0.4753475487232208\n",
      "Epoch 13611: train loss: 0.4753474295139313\n",
      "Epoch 13612: train loss: 0.4753473997116089\n",
      "Epoch 13613: train loss: 0.47534728050231934\n",
      "Epoch 13614: train loss: 0.47534728050231934\n",
      "Epoch 13615: train loss: 0.47534722089767456\n",
      "Epoch 13616: train loss: 0.4753471314907074\n",
      "Epoch 13617: train loss: 0.4753470718860626\n",
      "Epoch 13618: train loss: 0.47534701228141785\n",
      "Epoch 13619: train loss: 0.4753469228744507\n",
      "Epoch 13620: train loss: 0.4753468632698059\n",
      "Epoch 13621: train loss: 0.47534677386283875\n",
      "Epoch 13622: train loss: 0.47534671425819397\n",
      "Epoch 13623: train loss: 0.4753465950489044\n",
      "Epoch 13624: train loss: 0.47534656524658203\n",
      "Epoch 13625: train loss: 0.47534650564193726\n",
      "Epoch 13626: train loss: 0.47534650564193726\n",
      "Epoch 13627: train loss: 0.4753463864326477\n",
      "Epoch 13628: train loss: 0.47534629702568054\n",
      "Epoch 13629: train loss: 0.47534623742103577\n",
      "Epoch 13630: train loss: 0.475346177816391\n",
      "Epoch 13631: train loss: 0.47534608840942383\n",
      "Epoch 13632: train loss: 0.47534602880477905\n",
      "Epoch 13633: train loss: 0.4753459393978119\n",
      "Epoch 13634: train loss: 0.4753458797931671\n",
      "Epoch 13635: train loss: 0.4753458797931671\n",
      "Epoch 13636: train loss: 0.4753457307815552\n",
      "Epoch 13637: train loss: 0.4753457307815552\n",
      "Epoch 13638: train loss: 0.4753456115722656\n",
      "Epoch 13639: train loss: 0.4753454923629761\n",
      "Epoch 13640: train loss: 0.4753454625606537\n",
      "Epoch 13641: train loss: 0.4753454029560089\n",
      "Epoch 13642: train loss: 0.47534534335136414\n",
      "Epoch 13643: train loss: 0.475345253944397\n",
      "Epoch 13644: train loss: 0.4753451943397522\n",
      "Epoch 13645: train loss: 0.4753451347351074\n",
      "Epoch 13646: train loss: 0.47534507513046265\n",
      "Epoch 13647: train loss: 0.4753449857234955\n",
      "Epoch 13648: train loss: 0.4753449261188507\n",
      "Epoch 13649: train loss: 0.47534483671188354\n",
      "Epoch 13650: train loss: 0.47534477710723877\n",
      "Epoch 13651: train loss: 0.475344717502594\n",
      "Epoch 13652: train loss: 0.47534462809562683\n",
      "Epoch 13653: train loss: 0.47534456849098206\n",
      "Epoch 13654: train loss: 0.4753445088863373\n",
      "Epoch 13655: train loss: 0.4753444492816925\n",
      "Epoch 13656: train loss: 0.47534435987472534\n",
      "Epoch 13657: train loss: 0.4753442406654358\n",
      "Epoch 13658: train loss: 0.4753442108631134\n",
      "Epoch 13659: train loss: 0.4753442108631134\n",
      "Epoch 13660: train loss: 0.47534409165382385\n",
      "Epoch 13661: train loss: 0.4753440022468567\n",
      "Epoch 13662: train loss: 0.4753439426422119\n",
      "Epoch 13663: train loss: 0.47534388303756714\n",
      "Epoch 13664: train loss: 0.4753437936306\n",
      "Epoch 13665: train loss: 0.4753437936306\n",
      "Epoch 13666: train loss: 0.4753436744213104\n",
      "Epoch 13667: train loss: 0.47534361481666565\n",
      "Epoch 13668: train loss: 0.4753435254096985\n",
      "Epoch 13669: train loss: 0.4753435254096985\n",
      "Epoch 13670: train loss: 0.47534340620040894\n",
      "Epoch 13671: train loss: 0.47534337639808655\n",
      "Epoch 13672: train loss: 0.475343257188797\n",
      "Epoch 13673: train loss: 0.47534316778182983\n",
      "Epoch 13674: train loss: 0.47534310817718506\n",
      "Epoch 13675: train loss: 0.4753430485725403\n",
      "Epoch 13676: train loss: 0.4753429889678955\n",
      "Epoch 13677: train loss: 0.4753429591655731\n",
      "Epoch 13678: train loss: 0.47534283995628357\n",
      "Epoch 13679: train loss: 0.4753427803516388\n",
      "Epoch 13680: train loss: 0.4753427505493164\n",
      "Epoch 13681: train loss: 0.47534263134002686\n",
      "Epoch 13682: train loss: 0.4753425717353821\n",
      "Epoch 13683: train loss: 0.4753425419330597\n",
      "Epoch 13684: train loss: 0.4753424823284149\n",
      "Epoch 13685: train loss: 0.47534236311912537\n",
      "Epoch 13686: train loss: 0.4753422737121582\n",
      "Epoch 13687: train loss: 0.4753422141075134\n",
      "Epoch 13688: train loss: 0.47534215450286865\n",
      "Epoch 13689: train loss: 0.47534212470054626\n",
      "Epoch 13690: train loss: 0.4753420054912567\n",
      "Epoch 13691: train loss: 0.47534194588661194\n",
      "Epoch 13692: train loss: 0.47534191608428955\n",
      "Epoch 13693: train loss: 0.4753418564796448\n",
      "Epoch 13694: train loss: 0.4753417372703552\n",
      "Epoch 13695: train loss: 0.47534167766571045\n",
      "Epoch 13696: train loss: 0.47534164786338806\n",
      "Epoch 13697: train loss: 0.4753415286540985\n",
      "Epoch 13698: train loss: 0.47534143924713135\n",
      "Epoch 13699: train loss: 0.47534143924713135\n",
      "Epoch 13700: train loss: 0.4753413796424866\n",
      "Epoch 13701: train loss: 0.475341260433197\n",
      "Epoch 13702: train loss: 0.47534123063087463\n",
      "Epoch 13703: train loss: 0.47534117102622986\n",
      "Epoch 13704: train loss: 0.4753410518169403\n",
      "Epoch 13705: train loss: 0.47534096240997314\n",
      "Epoch 13706: train loss: 0.47534096240997314\n",
      "Epoch 13707: train loss: 0.4753408432006836\n",
      "Epoch 13708: train loss: 0.4753408133983612\n",
      "Epoch 13709: train loss: 0.47534069418907166\n",
      "Epoch 13710: train loss: 0.4753406345844269\n",
      "Epoch 13711: train loss: 0.4753406345844269\n",
      "Epoch 13712: train loss: 0.4753405451774597\n",
      "Epoch 13713: train loss: 0.47534048557281494\n",
      "Epoch 13714: train loss: 0.47534042596817017\n",
      "Epoch 13715: train loss: 0.475340336561203\n",
      "Epoch 13716: train loss: 0.4753402769565582\n",
      "Epoch 13717: train loss: 0.47534018754959106\n",
      "Epoch 13718: train loss: 0.4753401279449463\n",
      "Epoch 13719: train loss: 0.47534000873565674\n",
      "Epoch 13720: train loss: 0.47533997893333435\n",
      "Epoch 13721: train loss: 0.4753399193286896\n",
      "Epoch 13722: train loss: 0.4753398597240448\n",
      "Epoch 13723: train loss: 0.47533977031707764\n",
      "Epoch 13724: train loss: 0.47533971071243286\n",
      "Epoch 13725: train loss: 0.4753395915031433\n",
      "Epoch 13726: train loss: 0.4753395617008209\n",
      "Epoch 13727: train loss: 0.47533950209617615\n",
      "Epoch 13728: train loss: 0.47533944249153137\n",
      "Epoch 13729: train loss: 0.4753393828868866\n",
      "Epoch 13730: train loss: 0.4753393530845642\n",
      "Epoch 13731: train loss: 0.47533923387527466\n",
      "Epoch 13732: train loss: 0.4753391742706299\n",
      "Epoch 13733: train loss: 0.4753391444683075\n",
      "Epoch 13734: train loss: 0.47533902525901794\n",
      "Epoch 13735: train loss: 0.47533896565437317\n",
      "Epoch 13736: train loss: 0.475338876247406\n",
      "Epoch 13737: train loss: 0.47533881664276123\n",
      "Epoch 13738: train loss: 0.47533872723579407\n",
      "Epoch 13739: train loss: 0.4753386676311493\n",
      "Epoch 13740: train loss: 0.4753386676311493\n",
      "Epoch 13741: train loss: 0.47533854842185974\n",
      "Epoch 13742: train loss: 0.4753384590148926\n",
      "Epoch 13743: train loss: 0.4753384590148926\n",
      "Epoch 13744: train loss: 0.475338339805603\n",
      "Epoch 13745: train loss: 0.47533825039863586\n",
      "Epoch 13746: train loss: 0.47533825039863586\n",
      "Epoch 13747: train loss: 0.4753381311893463\n",
      "Epoch 13748: train loss: 0.47533804178237915\n",
      "Epoch 13749: train loss: 0.47533804178237915\n",
      "Epoch 13750: train loss: 0.4753379225730896\n",
      "Epoch 13751: train loss: 0.4753378629684448\n",
      "Epoch 13752: train loss: 0.47533783316612244\n",
      "Epoch 13753: train loss: 0.4753377139568329\n",
      "Epoch 13754: train loss: 0.4753376543521881\n",
      "Epoch 13755: train loss: 0.4753376245498657\n",
      "Epoch 13756: train loss: 0.47533750534057617\n",
      "Epoch 13757: train loss: 0.4753374457359314\n",
      "Epoch 13758: train loss: 0.475337415933609\n",
      "Epoch 13759: train loss: 0.47533729672431946\n",
      "Epoch 13760: train loss: 0.4753372371196747\n",
      "Epoch 13761: train loss: 0.4753372073173523\n",
      "Epoch 13762: train loss: 0.47533708810806274\n",
      "Epoch 13763: train loss: 0.47533702850341797\n",
      "Epoch 13764: train loss: 0.47533702850341797\n",
      "Epoch 13765: train loss: 0.4753369390964508\n",
      "Epoch 13766: train loss: 0.47533681988716125\n",
      "Epoch 13767: train loss: 0.47533679008483887\n",
      "Epoch 13768: train loss: 0.4753367304801941\n",
      "Epoch 13769: train loss: 0.47533661127090454\n",
      "Epoch 13770: train loss: 0.47533661127090454\n",
      "Epoch 13771: train loss: 0.4753365218639374\n",
      "Epoch 13772: train loss: 0.4753364622592926\n",
      "Epoch 13773: train loss: 0.47533637285232544\n",
      "Epoch 13774: train loss: 0.47533631324768066\n",
      "Epoch 13775: train loss: 0.4753361940383911\n",
      "Epoch 13776: train loss: 0.4753361642360687\n",
      "Epoch 13777: train loss: 0.47533610463142395\n",
      "Epoch 13778: train loss: 0.4753360450267792\n",
      "Epoch 13779: train loss: 0.475335955619812\n",
      "Epoch 13780: train loss: 0.47533589601516724\n",
      "Epoch 13781: train loss: 0.47533583641052246\n",
      "Epoch 13782: train loss: 0.4753357470035553\n",
      "Epoch 13783: train loss: 0.4753357470035553\n",
      "Epoch 13784: train loss: 0.47533562779426575\n",
      "Epoch 13785: train loss: 0.47533556818962097\n",
      "Epoch 13786: train loss: 0.4753354787826538\n",
      "Epoch 13787: train loss: 0.47533541917800903\n",
      "Epoch 13788: train loss: 0.47533535957336426\n",
      "Epoch 13789: train loss: 0.47533532977104187\n",
      "Epoch 13790: train loss: 0.4753352105617523\n",
      "Epoch 13791: train loss: 0.47533515095710754\n",
      "Epoch 13792: train loss: 0.47533512115478516\n",
      "Epoch 13793: train loss: 0.4753350019454956\n",
      "Epoch 13794: train loss: 0.47533491253852844\n",
      "Epoch 13795: train loss: 0.47533491253852844\n",
      "Epoch 13796: train loss: 0.47533485293388367\n",
      "Epoch 13797: train loss: 0.4753347337245941\n",
      "Epoch 13798: train loss: 0.47533464431762695\n",
      "Epoch 13799: train loss: 0.47533464431762695\n",
      "Epoch 13800: train loss: 0.4753345251083374\n",
      "Epoch 13801: train loss: 0.475334495306015\n",
      "Epoch 13802: train loss: 0.47533437609672546\n",
      "Epoch 13803: train loss: 0.4753343164920807\n",
      "Epoch 13804: train loss: 0.4753342866897583\n",
      "Epoch 13805: train loss: 0.47533416748046875\n",
      "Epoch 13806: train loss: 0.475334107875824\n",
      "Epoch 13807: train loss: 0.475334107875824\n",
      "Epoch 13808: train loss: 0.4753340184688568\n",
      "Epoch 13809: train loss: 0.47533395886421204\n",
      "Epoch 13810: train loss: 0.47533389925956726\n",
      "Epoch 13811: train loss: 0.4753338098526001\n",
      "Epoch 13812: train loss: 0.47533369064331055\n",
      "Epoch 13813: train loss: 0.47533363103866577\n",
      "Epoch 13814: train loss: 0.4753335416316986\n",
      "Epoch 13815: train loss: 0.47533348202705383\n",
      "Epoch 13816: train loss: 0.47533348202705383\n",
      "Epoch 13817: train loss: 0.47533342242240906\n",
      "Epoch 13818: train loss: 0.4753332734107971\n",
      "Epoch 13819: train loss: 0.4753332734107971\n",
      "Epoch 13820: train loss: 0.47533318400382996\n",
      "Epoch 13821: train loss: 0.4753330647945404\n",
      "Epoch 13822: train loss: 0.4753330647945404\n",
      "Epoch 13823: train loss: 0.47533297538757324\n",
      "Epoch 13824: train loss: 0.47533291578292847\n",
      "Epoch 13825: train loss: 0.4753327965736389\n",
      "Epoch 13826: train loss: 0.47533276677131653\n",
      "Epoch 13827: train loss: 0.47533270716667175\n",
      "Epoch 13828: train loss: 0.4753325879573822\n",
      "Epoch 13829: train loss: 0.4753325879573822\n",
      "Epoch 13830: train loss: 0.47533249855041504\n",
      "Epoch 13831: train loss: 0.47533249855041504\n",
      "Epoch 13832: train loss: 0.4753323793411255\n",
      "Epoch 13833: train loss: 0.4753322899341583\n",
      "Epoch 13834: train loss: 0.4753321707248688\n",
      "Epoch 13835: train loss: 0.4753321409225464\n",
      "Epoch 13836: train loss: 0.4753320813179016\n",
      "Epoch 13837: train loss: 0.47533202171325684\n",
      "Epoch 13838: train loss: 0.47533196210861206\n",
      "Epoch 13839: train loss: 0.4753318727016449\n",
      "Epoch 13840: train loss: 0.4753318727016449\n",
      "Epoch 13841: train loss: 0.47533175349235535\n",
      "Epoch 13842: train loss: 0.4753316640853882\n",
      "Epoch 13843: train loss: 0.4753316640853882\n",
      "Epoch 13844: train loss: 0.47533154487609863\n",
      "Epoch 13845: train loss: 0.47533145546913147\n",
      "Epoch 13846: train loss: 0.47533145546913147\n",
      "Epoch 13847: train loss: 0.4753313362598419\n",
      "Epoch 13848: train loss: 0.47533124685287476\n",
      "Epoch 13849: train loss: 0.47533124685287476\n",
      "Epoch 13850: train loss: 0.4753311276435852\n",
      "Epoch 13851: train loss: 0.47533103823661804\n",
      "Epoch 13852: train loss: 0.47533103823661804\n",
      "Epoch 13853: train loss: 0.4753309190273285\n",
      "Epoch 13854: train loss: 0.4753308892250061\n",
      "Epoch 13855: train loss: 0.47533077001571655\n",
      "Epoch 13856: train loss: 0.4753307104110718\n",
      "Epoch 13857: train loss: 0.4753306806087494\n",
      "Epoch 13858: train loss: 0.4753306210041046\n",
      "Epoch 13859: train loss: 0.47533050179481506\n",
      "Epoch 13860: train loss: 0.47533050179481506\n",
      "Epoch 13861: train loss: 0.4753304123878479\n",
      "Epoch 13862: train loss: 0.47533029317855835\n",
      "Epoch 13863: train loss: 0.4753302335739136\n",
      "Epoch 13864: train loss: 0.4753302037715912\n",
      "Epoch 13865: train loss: 0.47533008456230164\n",
      "Epoch 13866: train loss: 0.47533008456230164\n",
      "Epoch 13867: train loss: 0.4753299951553345\n",
      "Epoch 13868: train loss: 0.4753299355506897\n",
      "Epoch 13869: train loss: 0.47532981634140015\n",
      "Epoch 13870: train loss: 0.47532978653907776\n",
      "Epoch 13871: train loss: 0.47532978653907776\n",
      "Epoch 13872: train loss: 0.4753296673297882\n",
      "Epoch 13873: train loss: 0.47532957792282104\n",
      "Epoch 13874: train loss: 0.47532951831817627\n",
      "Epoch 13875: train loss: 0.4753294587135315\n",
      "Epoch 13876: train loss: 0.4753294587135315\n",
      "Epoch 13877: train loss: 0.47532930970191956\n",
      "Epoch 13878: train loss: 0.4753292500972748\n",
      "Epoch 13879: train loss: 0.4753291606903076\n",
      "Epoch 13880: train loss: 0.47532910108566284\n",
      "Epoch 13881: train loss: 0.47532904148101807\n",
      "Epoch 13882: train loss: 0.4753289520740509\n",
      "Epoch 13883: train loss: 0.47532889246940613\n",
      "Epoch 13884: train loss: 0.47532883286476135\n",
      "Epoch 13885: train loss: 0.4753287732601166\n",
      "Epoch 13886: train loss: 0.4753287434577942\n",
      "Epoch 13887: train loss: 0.4753286838531494\n",
      "Epoch 13888: train loss: 0.47532856464385986\n",
      "Epoch 13889: train loss: 0.4753285348415375\n",
      "Epoch 13890: train loss: 0.4753284752368927\n",
      "Epoch 13891: train loss: 0.47532835602760315\n",
      "Epoch 13892: train loss: 0.47532832622528076\n",
      "Epoch 13893: train loss: 0.4753282070159912\n",
      "Epoch 13894: train loss: 0.47532814741134644\n",
      "Epoch 13895: train loss: 0.47532811760902405\n",
      "Epoch 13896: train loss: 0.4753280580043793\n",
      "Epoch 13897: train loss: 0.4753279387950897\n",
      "Epoch 13898: train loss: 0.47532790899276733\n",
      "Epoch 13899: train loss: 0.47532784938812256\n",
      "Epoch 13900: train loss: 0.475327730178833\n",
      "Epoch 13901: train loss: 0.475327730178833\n",
      "Epoch 13902: train loss: 0.47532764077186584\n",
      "Epoch 13903: train loss: 0.47532758116722107\n",
      "Epoch 13904: train loss: 0.4753274917602539\n",
      "Epoch 13905: train loss: 0.47532743215560913\n",
      "Epoch 13906: train loss: 0.47532737255096436\n",
      "Epoch 13907: train loss: 0.4753272831439972\n",
      "Epoch 13908: train loss: 0.4753272235393524\n",
      "Epoch 13909: train loss: 0.47532716393470764\n",
      "Epoch 13910: train loss: 0.47532710433006287\n",
      "Epoch 13911: train loss: 0.4753270745277405\n",
      "Epoch 13912: train loss: 0.4753269553184509\n",
      "Epoch 13913: train loss: 0.4753269553184509\n",
      "Epoch 13914: train loss: 0.475326806306839\n",
      "Epoch 13915: train loss: 0.4753267467021942\n",
      "Epoch 13916: train loss: 0.47532665729522705\n",
      "Epoch 13917: train loss: 0.47532665729522705\n",
      "Epoch 13918: train loss: 0.4753265380859375\n",
      "Epoch 13919: train loss: 0.4753264784812927\n",
      "Epoch 13920: train loss: 0.47532641887664795\n",
      "Epoch 13921: train loss: 0.4753263294696808\n",
      "Epoch 13922: train loss: 0.475326269865036\n",
      "Epoch 13923: train loss: 0.47532621026039124\n",
      "Epoch 13924: train loss: 0.4753261208534241\n",
      "Epoch 13925: train loss: 0.4753261208534241\n",
      "Epoch 13926: train loss: 0.4753260016441345\n",
      "Epoch 13927: train loss: 0.47532591223716736\n",
      "Epoch 13928: train loss: 0.4753258526325226\n",
      "Epoch 13929: train loss: 0.4753257930278778\n",
      "Epoch 13930: train loss: 0.4753257632255554\n",
      "Epoch 13931: train loss: 0.47532564401626587\n",
      "Epoch 13932: train loss: 0.4753255844116211\n",
      "Epoch 13933: train loss: 0.4753255546092987\n",
      "Epoch 13934: train loss: 0.47532549500465393\n",
      "Epoch 13935: train loss: 0.4753253757953644\n",
      "Epoch 13936: train loss: 0.475325345993042\n",
      "Epoch 13937: train loss: 0.4753252863883972\n",
      "Epoch 13938: train loss: 0.47532516717910767\n",
      "Epoch 13939: train loss: 0.4753251373767853\n",
      "Epoch 13940: train loss: 0.4753250181674957\n",
      "Epoch 13941: train loss: 0.47532495856285095\n",
      "Epoch 13942: train loss: 0.47532492876052856\n",
      "Epoch 13943: train loss: 0.4753248691558838\n",
      "Epoch 13944: train loss: 0.47532474994659424\n",
      "Epoch 13945: train loss: 0.47532472014427185\n",
      "Epoch 13946: train loss: 0.4753246605396271\n",
      "Epoch 13947: train loss: 0.4753246009349823\n",
      "Epoch 13948: train loss: 0.47532451152801514\n",
      "Epoch 13949: train loss: 0.47532445192337036\n",
      "Epoch 13950: train loss: 0.4753243923187256\n",
      "Epoch 13951: train loss: 0.4753243327140808\n",
      "Epoch 13952: train loss: 0.47532424330711365\n",
      "Epoch 13953: train loss: 0.47532418370246887\n",
      "Epoch 13954: train loss: 0.4753240942955017\n",
      "Epoch 13955: train loss: 0.47532403469085693\n",
      "Epoch 13956: train loss: 0.47532397508621216\n",
      "Epoch 13957: train loss: 0.4753239154815674\n",
      "Epoch 13958: train loss: 0.4753238260746002\n",
      "Epoch 13959: train loss: 0.47532376646995544\n",
      "Epoch 13960: train loss: 0.47532370686531067\n",
      "Epoch 13961: train loss: 0.4753236174583435\n",
      "Epoch 13962: train loss: 0.4753236174583435\n",
      "Epoch 13963: train loss: 0.47532349824905396\n",
      "Epoch 13964: train loss: 0.47532349824905396\n",
      "Epoch 13965: train loss: 0.4753234088420868\n",
      "Epoch 13966: train loss: 0.475323349237442\n",
      "Epoch 13967: train loss: 0.4753232002258301\n",
      "Epoch 13968: train loss: 0.4753232002258301\n",
      "Epoch 13969: train loss: 0.4753231406211853\n",
      "Epoch 13970: train loss: 0.47532305121421814\n",
      "Epoch 13971: train loss: 0.47532299160957336\n",
      "Epoch 13972: train loss: 0.4753229320049286\n",
      "Epoch 13973: train loss: 0.4753228724002838\n",
      "Epoch 13974: train loss: 0.47532278299331665\n",
      "Epoch 13975: train loss: 0.4753226637840271\n",
      "Epoch 13976: train loss: 0.4753226637840271\n",
      "Epoch 13977: train loss: 0.47532257437705994\n",
      "Epoch 13978: train loss: 0.47532251477241516\n",
      "Epoch 13979: train loss: 0.4753224551677704\n",
      "Epoch 13980: train loss: 0.4753223657608032\n",
      "Epoch 13981: train loss: 0.47532230615615845\n",
      "Epoch 13982: train loss: 0.47532224655151367\n",
      "Epoch 13983: train loss: 0.4753221869468689\n",
      "Epoch 13984: train loss: 0.47532203793525696\n",
      "Epoch 13985: train loss: 0.47532203793525696\n",
      "Epoch 13986: train loss: 0.4753219783306122\n",
      "Epoch 13987: train loss: 0.475321888923645\n",
      "Epoch 13988: train loss: 0.47532182931900024\n",
      "Epoch 13989: train loss: 0.4753217399120331\n",
      "Epoch 13990: train loss: 0.4753216803073883\n",
      "Epoch 13991: train loss: 0.47532162070274353\n",
      "Epoch 13992: train loss: 0.47532153129577637\n",
      "Epoch 13993: train loss: 0.4753214716911316\n",
      "Epoch 13994: train loss: 0.4753214716911316\n",
      "Epoch 13995: train loss: 0.47532135248184204\n",
      "Epoch 13996: train loss: 0.4753212630748749\n",
      "Epoch 13997: train loss: 0.4753212034702301\n",
      "Epoch 13998: train loss: 0.4753211438655853\n",
      "Epoch 13999: train loss: 0.47532111406326294\n",
      "Epoch 14000: train loss: 0.4753209948539734\n",
      "Epoch 14001: train loss: 0.4753209352493286\n",
      "Epoch 14002: train loss: 0.4753209054470062\n",
      "Epoch 14003: train loss: 0.47532084584236145\n",
      "Epoch 14004: train loss: 0.4753207862377167\n",
      "Epoch 14005: train loss: 0.4753206968307495\n",
      "Epoch 14006: train loss: 0.47532063722610474\n",
      "Epoch 14007: train loss: 0.47532057762145996\n",
      "Epoch 14008: train loss: 0.4753204882144928\n",
      "Epoch 14009: train loss: 0.47532036900520325\n",
      "Epoch 14010: train loss: 0.47532030940055847\n",
      "Epoch 14011: train loss: 0.4753202795982361\n",
      "Epoch 14012: train loss: 0.4753202199935913\n",
      "Epoch 14013: train loss: 0.47532016038894653\n",
      "Epoch 14014: train loss: 0.47532010078430176\n",
      "Epoch 14015: train loss: 0.47532007098197937\n",
      "Epoch 14016: train loss: 0.4753199517726898\n",
      "Epoch 14017: train loss: 0.47531986236572266\n",
      "Epoch 14018: train loss: 0.4753198027610779\n",
      "Epoch 14019: train loss: 0.4753197431564331\n",
      "Epoch 14020: train loss: 0.47531968355178833\n",
      "Epoch 14021: train loss: 0.47531959414482117\n",
      "Epoch 14022: train loss: 0.4753195345401764\n",
      "Epoch 14023: train loss: 0.4753194749355316\n",
      "Epoch 14024: train loss: 0.47531938552856445\n",
      "Epoch 14025: train loss: 0.4753193259239197\n",
      "Epoch 14026: train loss: 0.4753192663192749\n",
      "Epoch 14027: train loss: 0.4753192365169525\n",
      "Epoch 14028: train loss: 0.47531911730766296\n",
      "Epoch 14029: train loss: 0.4753190577030182\n",
      "Epoch 14030: train loss: 0.4753190279006958\n",
      "Epoch 14031: train loss: 0.47531890869140625\n",
      "Epoch 14032: train loss: 0.4753188490867615\n",
      "Epoch 14033: train loss: 0.4753187894821167\n",
      "Epoch 14034: train loss: 0.4753187596797943\n",
      "Epoch 14035: train loss: 0.47531864047050476\n",
      "Epoch 14036: train loss: 0.47531858086586\n",
      "Epoch 14037: train loss: 0.4753185510635376\n",
      "Epoch 14038: train loss: 0.4753184914588928\n",
      "Epoch 14039: train loss: 0.47531837224960327\n",
      "Epoch 14040: train loss: 0.4753183424472809\n",
      "Epoch 14041: train loss: 0.47531822323799133\n",
      "Epoch 14042: train loss: 0.47531816363334656\n",
      "Epoch 14043: train loss: 0.47531813383102417\n",
      "Epoch 14044: train loss: 0.4753180742263794\n",
      "Epoch 14045: train loss: 0.4753180146217346\n",
      "Epoch 14046: train loss: 0.47531795501708984\n",
      "Epoch 14047: train loss: 0.4753178656101227\n",
      "Epoch 14048: train loss: 0.4753178060054779\n",
      "Epoch 14049: train loss: 0.47531771659851074\n",
      "Epoch 14050: train loss: 0.47531765699386597\n",
      "Epoch 14051: train loss: 0.4753175973892212\n",
      "Epoch 14052: train loss: 0.4753175377845764\n",
      "Epoch 14053: train loss: 0.47531744837760925\n",
      "Epoch 14054: train loss: 0.47531744837760925\n",
      "Epoch 14055: train loss: 0.4753173291683197\n",
      "Epoch 14056: train loss: 0.4753172993659973\n",
      "Epoch 14057: train loss: 0.47531718015670776\n",
      "Epoch 14058: train loss: 0.475317120552063\n",
      "Epoch 14059: train loss: 0.4753170311450958\n",
      "Epoch 14060: train loss: 0.4753170311450958\n",
      "Epoch 14061: train loss: 0.4753169119358063\n",
      "Epoch 14062: train loss: 0.4753168821334839\n",
      "Epoch 14063: train loss: 0.4753168225288391\n",
      "Epoch 14064: train loss: 0.47531670331954956\n",
      "Epoch 14065: train loss: 0.4753166735172272\n",
      "Epoch 14066: train loss: 0.4753165543079376\n",
      "Epoch 14067: train loss: 0.47531649470329285\n",
      "Epoch 14068: train loss: 0.47531646490097046\n",
      "Epoch 14069: train loss: 0.4753163456916809\n",
      "Epoch 14070: train loss: 0.47531628608703613\n",
      "Epoch 14071: train loss: 0.47531628608703613\n",
      "Epoch 14072: train loss: 0.47531619668006897\n",
      "Epoch 14073: train loss: 0.4753161370754242\n",
      "Epoch 14074: train loss: 0.4753160774707794\n",
      "Epoch 14075: train loss: 0.47531598806381226\n",
      "Epoch 14076: train loss: 0.4753159284591675\n",
      "Epoch 14077: train loss: 0.4753158688545227\n",
      "Epoch 14078: train loss: 0.47531577944755554\n",
      "Epoch 14079: train loss: 0.47531571984291077\n",
      "Epoch 14080: train loss: 0.475315660238266\n",
      "Epoch 14081: train loss: 0.47531557083129883\n",
      "Epoch 14082: train loss: 0.47531557083129883\n",
      "Epoch 14083: train loss: 0.4753154516220093\n",
      "Epoch 14084: train loss: 0.4753154218196869\n",
      "Epoch 14085: train loss: 0.4753153622150421\n",
      "Epoch 14086: train loss: 0.47531524300575256\n",
      "Epoch 14087: train loss: 0.4753151535987854\n",
      "Epoch 14088: train loss: 0.4753151535987854\n",
      "Epoch 14089: train loss: 0.47531503438949585\n",
      "Epoch 14090: train loss: 0.4753149449825287\n",
      "Epoch 14091: train loss: 0.4753149449825287\n",
      "Epoch 14092: train loss: 0.47531482577323914\n",
      "Epoch 14093: train loss: 0.47531476616859436\n",
      "Epoch 14094: train loss: 0.475314736366272\n",
      "Epoch 14095: train loss: 0.4753146767616272\n",
      "Epoch 14096: train loss: 0.47531455755233765\n",
      "Epoch 14097: train loss: 0.47531452775001526\n",
      "Epoch 14098: train loss: 0.4753144681453705\n",
      "Epoch 14099: train loss: 0.4753144085407257\n",
      "Epoch 14100: train loss: 0.47531431913375854\n",
      "Epoch 14101: train loss: 0.47531425952911377\n",
      "Epoch 14102: train loss: 0.475314199924469\n",
      "Epoch 14103: train loss: 0.4753141403198242\n",
      "Epoch 14104: train loss: 0.47531405091285706\n",
      "Epoch 14105: train loss: 0.4753139913082123\n",
      "Epoch 14106: train loss: 0.4753139019012451\n",
      "Epoch 14107: train loss: 0.47531384229660034\n",
      "Epoch 14108: train loss: 0.47531378269195557\n",
      "Epoch 14109: train loss: 0.4753136932849884\n",
      "Epoch 14110: train loss: 0.47531363368034363\n",
      "Epoch 14111: train loss: 0.47531357407569885\n",
      "Epoch 14112: train loss: 0.4753135144710541\n",
      "Epoch 14113: train loss: 0.4753134250640869\n",
      "Epoch 14114: train loss: 0.47531336545944214\n",
      "Epoch 14115: train loss: 0.47531330585479736\n",
      "Epoch 14116: train loss: 0.4753132164478302\n",
      "Epoch 14117: train loss: 0.4753132164478302\n",
      "Epoch 14118: train loss: 0.47531309723854065\n",
      "Epoch 14119: train loss: 0.47531306743621826\n",
      "Epoch 14120: train loss: 0.4753129482269287\n",
      "Epoch 14121: train loss: 0.47531288862228394\n",
      "Epoch 14122: train loss: 0.47531285881996155\n",
      "Epoch 14123: train loss: 0.475312739610672\n",
      "Epoch 14124: train loss: 0.475312739610672\n",
      "Epoch 14125: train loss: 0.4753126800060272\n",
      "Epoch 14126: train loss: 0.47531259059906006\n",
      "Epoch 14127: train loss: 0.4753125309944153\n",
      "Epoch 14128: train loss: 0.4753124415874481\n",
      "Epoch 14129: train loss: 0.47531238198280334\n",
      "Epoch 14130: train loss: 0.47531232237815857\n",
      "Epoch 14131: train loss: 0.4753122627735138\n",
      "Epoch 14132: train loss: 0.47531217336654663\n",
      "Epoch 14133: train loss: 0.47531211376190186\n",
      "Epoch 14134: train loss: 0.4753120243549347\n",
      "Epoch 14135: train loss: 0.4753120243549347\n",
      "Epoch 14136: train loss: 0.47531184554100037\n",
      "Epoch 14137: train loss: 0.47531184554100037\n",
      "Epoch 14138: train loss: 0.475311815738678\n",
      "Epoch 14139: train loss: 0.4753116965293884\n",
      "Epoch 14140: train loss: 0.47531163692474365\n",
      "Epoch 14141: train loss: 0.4753115475177765\n",
      "Epoch 14142: train loss: 0.4753115475177765\n",
      "Epoch 14143: train loss: 0.47531142830848694\n",
      "Epoch 14144: train loss: 0.47531142830848694\n",
      "Epoch 14145: train loss: 0.475311279296875\n",
      "Epoch 14146: train loss: 0.475311279296875\n",
      "Epoch 14147: train loss: 0.4753112196922302\n",
      "Epoch 14148: train loss: 0.47531113028526306\n",
      "Epoch 14149: train loss: 0.4753110706806183\n",
      "Epoch 14150: train loss: 0.47531095147132874\n",
      "Epoch 14151: train loss: 0.47531092166900635\n",
      "Epoch 14152: train loss: 0.4753108620643616\n",
      "Epoch 14153: train loss: 0.4753108024597168\n",
      "Epoch 14154: train loss: 0.475310742855072\n",
      "Epoch 14155: train loss: 0.47531065344810486\n",
      "Epoch 14156: train loss: 0.4753105938434601\n",
      "Epoch 14157: train loss: 0.4753105342388153\n",
      "Epoch 14158: train loss: 0.47531044483184814\n",
      "Epoch 14159: train loss: 0.47531038522720337\n",
      "Epoch 14160: train loss: 0.4753102958202362\n",
      "Epoch 14161: train loss: 0.47531023621559143\n",
      "Epoch 14162: train loss: 0.4753101170063019\n",
      "Epoch 14163: train loss: 0.4753101170063019\n",
      "Epoch 14164: train loss: 0.4753100275993347\n",
      "Epoch 14165: train loss: 0.47530996799468994\n",
      "Epoch 14166: train loss: 0.47530990839004517\n",
      "Epoch 14167: train loss: 0.4753098785877228\n",
      "Epoch 14168: train loss: 0.475309818983078\n",
      "Epoch 14169: train loss: 0.47530969977378845\n",
      "Epoch 14170: train loss: 0.47530966997146606\n",
      "Epoch 14171: train loss: 0.4753096103668213\n",
      "Epoch 14172: train loss: 0.47530949115753174\n",
      "Epoch 14173: train loss: 0.47530946135520935\n",
      "Epoch 14174: train loss: 0.4753094017505646\n",
      "Epoch 14175: train loss: 0.4753093421459198\n",
      "Epoch 14176: train loss: 0.47530925273895264\n",
      "Epoch 14177: train loss: 0.47530919313430786\n",
      "Epoch 14178: train loss: 0.4753091335296631\n",
      "Epoch 14179: train loss: 0.4753090739250183\n",
      "Epoch 14180: train loss: 0.47530898451805115\n",
      "Epoch 14181: train loss: 0.47530892491340637\n",
      "Epoch 14182: train loss: 0.4753088653087616\n",
      "Epoch 14183: train loss: 0.4753088355064392\n",
      "Epoch 14184: train loss: 0.47530871629714966\n",
      "Epoch 14185: train loss: 0.4753086566925049\n",
      "Epoch 14186: train loss: 0.4753086268901825\n",
      "Epoch 14187: train loss: 0.47530850768089294\n",
      "Epoch 14188: train loss: 0.47530844807624817\n",
      "Epoch 14189: train loss: 0.475308358669281\n",
      "Epoch 14190: train loss: 0.47530829906463623\n",
      "Epoch 14191: train loss: 0.47530823945999146\n",
      "Epoch 14192: train loss: 0.47530823945999146\n",
      "Epoch 14193: train loss: 0.4753081500530243\n",
      "Epoch 14194: train loss: 0.47530803084373474\n",
      "Epoch 14195: train loss: 0.47530800104141235\n",
      "Epoch 14196: train loss: 0.4753079414367676\n",
      "Epoch 14197: train loss: 0.475307822227478\n",
      "Epoch 14198: train loss: 0.47530779242515564\n",
      "Epoch 14199: train loss: 0.47530773282051086\n",
      "Epoch 14200: train loss: 0.4753076732158661\n",
      "Epoch 14201: train loss: 0.4753076136112213\n",
      "Epoch 14202: train loss: 0.47530752420425415\n",
      "Epoch 14203: train loss: 0.4753074645996094\n",
      "Epoch 14204: train loss: 0.4753074049949646\n",
      "Epoch 14205: train loss: 0.4753073453903198\n",
      "Epoch 14206: train loss: 0.47530725598335266\n",
      "Epoch 14207: train loss: 0.4753071963787079\n",
      "Epoch 14208: train loss: 0.4753071069717407\n",
      "Epoch 14209: train loss: 0.47530704736709595\n",
      "Epoch 14210: train loss: 0.47530698776245117\n",
      "Epoch 14211: train loss: 0.475306898355484\n",
      "Epoch 14212: train loss: 0.475306898355484\n",
      "Epoch 14213: train loss: 0.47530677914619446\n",
      "Epoch 14214: train loss: 0.4753066897392273\n",
      "Epoch 14215: train loss: 0.4753066897392273\n",
      "Epoch 14216: train loss: 0.4753066301345825\n",
      "Epoch 14217: train loss: 0.47530651092529297\n",
      "Epoch 14218: train loss: 0.47530651092529297\n",
      "Epoch 14219: train loss: 0.47530636191368103\n",
      "Epoch 14220: train loss: 0.47530630230903625\n",
      "Epoch 14221: train loss: 0.47530627250671387\n",
      "Epoch 14222: train loss: 0.4753061532974243\n",
      "Epoch 14223: train loss: 0.47530609369277954\n",
      "Epoch 14224: train loss: 0.47530606389045715\n",
      "Epoch 14225: train loss: 0.4753059446811676\n",
      "Epoch 14226: train loss: 0.4753059446811676\n",
      "Epoch 14227: train loss: 0.4753058850765228\n",
      "Epoch 14228: train loss: 0.47530579566955566\n",
      "Epoch 14229: train loss: 0.4753057360649109\n",
      "Epoch 14230: train loss: 0.4753056466579437\n",
      "Epoch 14231: train loss: 0.47530558705329895\n",
      "Epoch 14232: train loss: 0.4753055274486542\n",
      "Epoch 14233: train loss: 0.4753054678440094\n",
      "Epoch 14234: train loss: 0.475305438041687\n",
      "Epoch 14235: train loss: 0.47530531883239746\n",
      "Epoch 14236: train loss: 0.4753052592277527\n",
      "Epoch 14237: train loss: 0.4753051698207855\n",
      "Epoch 14238: train loss: 0.47530511021614075\n",
      "Epoch 14239: train loss: 0.47530505061149597\n",
      "Epoch 14240: train loss: 0.4753050208091736\n",
      "Epoch 14241: train loss: 0.47530490159988403\n",
      "Epoch 14242: train loss: 0.47530484199523926\n",
      "Epoch 14243: train loss: 0.47530481219291687\n",
      "Epoch 14244: train loss: 0.4753046929836273\n",
      "Epoch 14245: train loss: 0.47530463337898254\n",
      "Epoch 14246: train loss: 0.47530460357666016\n",
      "Epoch 14247: train loss: 0.4753045439720154\n",
      "Epoch 14248: train loss: 0.4753044843673706\n",
      "Epoch 14249: train loss: 0.47530439496040344\n",
      "Epoch 14250: train loss: 0.47530433535575867\n",
      "Epoch 14251: train loss: 0.4753042757511139\n",
      "Epoch 14252: train loss: 0.47530418634414673\n",
      "Epoch 14253: train loss: 0.47530412673950195\n",
      "Epoch 14254: train loss: 0.4753040671348572\n",
      "Epoch 14255: train loss: 0.47530397772789\n",
      "Epoch 14256: train loss: 0.47530391812324524\n",
      "Epoch 14257: train loss: 0.47530385851860046\n",
      "Epoch 14258: train loss: 0.4753037989139557\n",
      "Epoch 14259: train loss: 0.4753037691116333\n",
      "Epoch 14260: train loss: 0.47530364990234375\n",
      "Epoch 14261: train loss: 0.47530364990234375\n",
      "Epoch 14262: train loss: 0.4753035306930542\n",
      "Epoch 14263: train loss: 0.4753035008907318\n",
      "Epoch 14264: train loss: 0.47530338168144226\n",
      "Epoch 14265: train loss: 0.47530338168144226\n",
      "Epoch 14266: train loss: 0.4753032922744751\n",
      "Epoch 14267: train loss: 0.4753032326698303\n",
      "Epoch 14268: train loss: 0.4753030836582184\n",
      "Epoch 14269: train loss: 0.4753030836582184\n",
      "Epoch 14270: train loss: 0.4753030240535736\n",
      "Epoch 14271: train loss: 0.47530290484428406\n",
      "Epoch 14272: train loss: 0.47530290484428406\n",
      "Epoch 14273: train loss: 0.4753028154373169\n",
      "Epoch 14274: train loss: 0.4753027558326721\n",
      "Epoch 14275: train loss: 0.47530266642570496\n",
      "Epoch 14276: train loss: 0.4753026068210602\n",
      "Epoch 14277: train loss: 0.47530248761177063\n",
      "Epoch 14278: train loss: 0.47530248761177063\n",
      "Epoch 14279: train loss: 0.47530239820480347\n",
      "Epoch 14280: train loss: 0.4753023386001587\n",
      "Epoch 14281: train loss: 0.47530224919319153\n",
      "Epoch 14282: train loss: 0.47530224919319153\n",
      "Epoch 14283: train loss: 0.475302129983902\n",
      "Epoch 14284: train loss: 0.4753020703792572\n",
      "Epoch 14285: train loss: 0.4753020405769348\n",
      "Epoch 14286: train loss: 0.47530198097229004\n",
      "Epoch 14287: train loss: 0.4753018617630005\n",
      "Epoch 14288: train loss: 0.4753018617630005\n",
      "Epoch 14289: train loss: 0.4753017723560333\n",
      "Epoch 14290: train loss: 0.4753016531467438\n",
      "Epoch 14291: train loss: 0.4753016233444214\n",
      "Epoch 14292: train loss: 0.4753015637397766\n",
      "Epoch 14293: train loss: 0.47530150413513184\n",
      "Epoch 14294: train loss: 0.47530144453048706\n",
      "Epoch 14295: train loss: 0.4753013551235199\n",
      "Epoch 14296: train loss: 0.4753012955188751\n",
      "Epoch 14297: train loss: 0.47530120611190796\n",
      "Epoch 14298: train loss: 0.4753011465072632\n",
      "Epoch 14299: train loss: 0.47530102729797363\n",
      "Epoch 14300: train loss: 0.47530102729797363\n",
      "Epoch 14301: train loss: 0.47530099749565125\n",
      "Epoch 14302: train loss: 0.4753008782863617\n",
      "Epoch 14303: train loss: 0.4753008186817169\n",
      "Epoch 14304: train loss: 0.47530078887939453\n",
      "Epoch 14305: train loss: 0.47530072927474976\n",
      "Epoch 14306: train loss: 0.4753006100654602\n",
      "Epoch 14307: train loss: 0.4753005802631378\n",
      "Epoch 14308: train loss: 0.47530052065849304\n",
      "Epoch 14309: train loss: 0.4753004014492035\n",
      "Epoch 14310: train loss: 0.4753004014492035\n",
      "Epoch 14311: train loss: 0.47530031204223633\n",
      "Epoch 14312: train loss: 0.4753001928329468\n",
      "Epoch 14313: train loss: 0.4753001630306244\n",
      "Epoch 14314: train loss: 0.4753001034259796\n",
      "Epoch 14315: train loss: 0.47530004382133484\n",
      "Epoch 14316: train loss: 0.4752999544143677\n",
      "Epoch 14317: train loss: 0.4752998948097229\n",
      "Epoch 14318: train loss: 0.4752998352050781\n",
      "Epoch 14319: train loss: 0.47529977560043335\n",
      "Epoch 14320: train loss: 0.4752996861934662\n",
      "Epoch 14321: train loss: 0.4752996265888214\n",
      "Epoch 14322: train loss: 0.47529956698417664\n",
      "Epoch 14323: train loss: 0.47529950737953186\n",
      "Epoch 14324: train loss: 0.4752994179725647\n",
      "Epoch 14325: train loss: 0.4752993583679199\n",
      "Epoch 14326: train loss: 0.47529926896095276\n",
      "Epoch 14327: train loss: 0.47529926896095276\n",
      "Epoch 14328: train loss: 0.475299209356308\n",
      "Epoch 14329: train loss: 0.47529909014701843\n",
      "Epoch 14330: train loss: 0.47529900074005127\n",
      "Epoch 14331: train loss: 0.47529906034469604\n",
      "Epoch 14332: train loss: 0.4752989411354065\n",
      "Epoch 14333: train loss: 0.47529885172843933\n",
      "Epoch 14334: train loss: 0.47529879212379456\n",
      "Epoch 14335: train loss: 0.475298672914505\n",
      "Epoch 14336: train loss: 0.475298672914505\n",
      "Epoch 14337: train loss: 0.47529858350753784\n",
      "Epoch 14338: train loss: 0.4752984642982483\n",
      "Epoch 14339: train loss: 0.4752984642982483\n",
      "Epoch 14340: train loss: 0.47529837489128113\n",
      "Epoch 14341: train loss: 0.47529831528663635\n",
      "Epoch 14342: train loss: 0.4752982258796692\n",
      "Epoch 14343: train loss: 0.4752981662750244\n",
      "Epoch 14344: train loss: 0.47529810667037964\n",
      "Epoch 14345: train loss: 0.47529804706573486\n",
      "Epoch 14346: train loss: 0.4752980172634125\n",
      "Epoch 14347: train loss: 0.4752979576587677\n",
      "Epoch 14348: train loss: 0.4752978980541229\n",
      "Epoch 14349: train loss: 0.47529780864715576\n",
      "Epoch 14350: train loss: 0.475297749042511\n",
      "Epoch 14351: train loss: 0.4752976894378662\n",
      "Epoch 14352: train loss: 0.47529760003089905\n",
      "Epoch 14353: train loss: 0.4752975404262543\n",
      "Epoch 14354: train loss: 0.4752974808216095\n",
      "Epoch 14355: train loss: 0.47529739141464233\n",
      "Epoch 14356: train loss: 0.47529733180999756\n",
      "Epoch 14357: train loss: 0.4752972722053528\n",
      "Epoch 14358: train loss: 0.475297212600708\n",
      "Epoch 14359: train loss: 0.47529712319374084\n",
      "Epoch 14360: train loss: 0.47529706358909607\n",
      "Epoch 14361: train loss: 0.4752970039844513\n",
      "Epoch 14362: train loss: 0.47529691457748413\n",
      "Epoch 14363: train loss: 0.47529685497283936\n",
      "Epoch 14364: train loss: 0.47529685497283936\n",
      "Epoch 14365: train loss: 0.4752967655658722\n",
      "Epoch 14366: train loss: 0.4752967059612274\n",
      "Epoch 14367: train loss: 0.47529664635658264\n",
      "Epoch 14368: train loss: 0.4752965569496155\n",
      "Epoch 14369: train loss: 0.4752964973449707\n",
      "Epoch 14370: train loss: 0.47529637813568115\n",
      "Epoch 14371: train loss: 0.47529634833335876\n",
      "Epoch 14372: train loss: 0.475296288728714\n",
      "Epoch 14373: train loss: 0.4752962291240692\n",
      "Epoch 14374: train loss: 0.47529613971710205\n",
      "Epoch 14375: train loss: 0.4752960801124573\n",
      "Epoch 14376: train loss: 0.4752960205078125\n",
      "Epoch 14377: train loss: 0.47529590129852295\n",
      "Epoch 14378: train loss: 0.47529587149620056\n",
      "Epoch 14379: train loss: 0.4752958118915558\n",
      "Epoch 14380: train loss: 0.475295752286911\n",
      "Epoch 14381: train loss: 0.47529569268226624\n",
      "Epoch 14382: train loss: 0.47529566287994385\n",
      "Epoch 14383: train loss: 0.4752954840660095\n",
      "Epoch 14384: train loss: 0.47529545426368713\n",
      "Epoch 14385: train loss: 0.47529545426368713\n",
      "Epoch 14386: train loss: 0.4752953350543976\n",
      "Epoch 14387: train loss: 0.4752952754497528\n",
      "Epoch 14388: train loss: 0.4752952456474304\n",
      "Epoch 14389: train loss: 0.47529518604278564\n",
      "Epoch 14390: train loss: 0.47529512643814087\n",
      "Epoch 14391: train loss: 0.4752950370311737\n",
      "Epoch 14392: train loss: 0.47529497742652893\n",
      "Epoch 14393: train loss: 0.47529491782188416\n",
      "Epoch 14394: train loss: 0.4752948582172394\n",
      "Epoch 14395: train loss: 0.47529470920562744\n",
      "Epoch 14396: train loss: 0.47529470920562744\n",
      "Epoch 14397: train loss: 0.47529464960098267\n",
      "Epoch 14398: train loss: 0.4752945601940155\n",
      "Epoch 14399: train loss: 0.4752945005893707\n",
      "Epoch 14400: train loss: 0.47529444098472595\n",
      "Epoch 14401: train loss: 0.47529441118240356\n",
      "Epoch 14402: train loss: 0.475294291973114\n",
      "Epoch 14403: train loss: 0.475294291973114\n",
      "Epoch 14404: train loss: 0.47529420256614685\n",
      "Epoch 14405: train loss: 0.4752940833568573\n",
      "Epoch 14406: train loss: 0.4752940237522125\n",
      "Epoch 14407: train loss: 0.47529399394989014\n",
      "Epoch 14408: train loss: 0.47529393434524536\n",
      "Epoch 14409: train loss: 0.4752938151359558\n",
      "Epoch 14410: train loss: 0.4752937853336334\n",
      "Epoch 14411: train loss: 0.47529372572898865\n",
      "Epoch 14412: train loss: 0.4752936065196991\n",
      "Epoch 14413: train loss: 0.4752936065196991\n",
      "Epoch 14414: train loss: 0.4752935767173767\n",
      "Epoch 14415: train loss: 0.4752933979034424\n",
      "Epoch 14416: train loss: 0.47529336810112\n",
      "Epoch 14417: train loss: 0.4752933084964752\n",
      "Epoch 14418: train loss: 0.47529324889183044\n",
      "Epoch 14419: train loss: 0.4752931594848633\n",
      "Epoch 14420: train loss: 0.4752930998802185\n",
      "Epoch 14421: train loss: 0.47529298067092896\n",
      "Epoch 14422: train loss: 0.47529298067092896\n",
      "Epoch 14423: train loss: 0.47529295086860657\n",
      "Epoch 14424: train loss: 0.4752928912639618\n",
      "Epoch 14425: train loss: 0.47529277205467224\n",
      "Epoch 14426: train loss: 0.47529274225234985\n",
      "Epoch 14427: train loss: 0.4752926826477051\n",
      "Epoch 14428: train loss: 0.4752925634384155\n",
      "Epoch 14429: train loss: 0.47529253363609314\n",
      "Epoch 14430: train loss: 0.47529247403144836\n",
      "Epoch 14431: train loss: 0.4752923548221588\n",
      "Epoch 14432: train loss: 0.4752923250198364\n",
      "Epoch 14433: train loss: 0.47529226541519165\n",
      "Epoch 14434: train loss: 0.4752922058105469\n",
      "Epoch 14435: train loss: 0.4752921462059021\n",
      "Epoch 14436: train loss: 0.4752920866012573\n",
      "Epoch 14437: train loss: 0.4752919375896454\n",
      "Epoch 14438: train loss: 0.4752919375896454\n",
      "Epoch 14439: train loss: 0.4752918481826782\n",
      "Epoch 14440: train loss: 0.47529172897338867\n",
      "Epoch 14441: train loss: 0.47529172897338867\n",
      "Epoch 14442: train loss: 0.47529172897338867\n",
      "Epoch 14443: train loss: 0.4752916395664215\n",
      "Epoch 14444: train loss: 0.47529152035713196\n",
      "Epoch 14445: train loss: 0.4752914607524872\n",
      "Epoch 14446: train loss: 0.4752914309501648\n",
      "Epoch 14447: train loss: 0.47529131174087524\n",
      "Epoch 14448: train loss: 0.47529125213623047\n",
      "Epoch 14449: train loss: 0.4752912223339081\n",
      "Epoch 14450: train loss: 0.4752911627292633\n",
      "Epoch 14451: train loss: 0.47529110312461853\n",
      "Epoch 14452: train loss: 0.47529101371765137\n",
      "Epoch 14453: train loss: 0.4752909541130066\n",
      "Epoch 14454: train loss: 0.47529083490371704\n",
      "Epoch 14455: train loss: 0.47529080510139465\n",
      "Epoch 14456: train loss: 0.47529080510139465\n",
      "Epoch 14457: train loss: 0.4752906858921051\n",
      "Epoch 14458: train loss: 0.4752906262874603\n",
      "Epoch 14459: train loss: 0.47529059648513794\n",
      "Epoch 14460: train loss: 0.47529053688049316\n",
      "Epoch 14461: train loss: 0.4752904176712036\n",
      "Epoch 14462: train loss: 0.47529032826423645\n",
      "Epoch 14463: train loss: 0.47529032826423645\n",
      "Epoch 14464: train loss: 0.4752902686595917\n",
      "Epoch 14465: train loss: 0.4752901792526245\n",
      "Epoch 14466: train loss: 0.47529011964797974\n",
      "Epoch 14467: train loss: 0.47529006004333496\n",
      "Epoch 14468: train loss: 0.4752899706363678\n",
      "Epoch 14469: train loss: 0.475289911031723\n",
      "Epoch 14470: train loss: 0.47528979182243347\n",
      "Epoch 14471: train loss: 0.47528979182243347\n",
      "Epoch 14472: train loss: 0.4752897024154663\n",
      "Epoch 14473: train loss: 0.47528958320617676\n",
      "Epoch 14474: train loss: 0.47528955340385437\n",
      "Epoch 14475: train loss: 0.47528955340385437\n",
      "Epoch 14476: train loss: 0.4752894341945648\n",
      "Epoch 14477: train loss: 0.47528937458992004\n",
      "Epoch 14478: train loss: 0.47528934478759766\n",
      "Epoch 14479: train loss: 0.4752892255783081\n",
      "Epoch 14480: train loss: 0.47528916597366333\n",
      "Epoch 14481: train loss: 0.47528913617134094\n",
      "Epoch 14482: train loss: 0.4752890169620514\n",
      "Epoch 14483: train loss: 0.4752890169620514\n",
      "Epoch 14484: train loss: 0.47528892755508423\n",
      "Epoch 14485: train loss: 0.47528886795043945\n",
      "Epoch 14486: train loss: 0.4752888083457947\n",
      "Epoch 14487: train loss: 0.4752887189388275\n",
      "Epoch 14488: train loss: 0.47528865933418274\n",
      "Epoch 14489: train loss: 0.47528859972953796\n",
      "Epoch 14490: train loss: 0.4752885103225708\n",
      "Epoch 14491: train loss: 0.475288450717926\n",
      "Epoch 14492: train loss: 0.47528839111328125\n",
      "Epoch 14493: train loss: 0.4752883315086365\n",
      "Epoch 14494: train loss: 0.4752882719039917\n",
      "Epoch 14495: train loss: 0.4752882421016693\n",
      "Epoch 14496: train loss: 0.47528812289237976\n",
      "Epoch 14497: train loss: 0.475288063287735\n",
      "Epoch 14498: train loss: 0.4752879738807678\n",
      "Epoch 14499: train loss: 0.4752879738807678\n",
      "Epoch 14500: train loss: 0.47528785467147827\n",
      "Epoch 14501: train loss: 0.4752877652645111\n",
      "Epoch 14502: train loss: 0.4752877652645111\n",
      "Epoch 14503: train loss: 0.47528764605522156\n",
      "Epoch 14504: train loss: 0.4752875566482544\n",
      "Epoch 14505: train loss: 0.4752875566482544\n",
      "Epoch 14506: train loss: 0.4752874970436096\n",
      "Epoch 14507: train loss: 0.47528740763664246\n",
      "Epoch 14508: train loss: 0.4752873480319977\n",
      "Epoch 14509: train loss: 0.4752872884273529\n",
      "Epoch 14510: train loss: 0.47528722882270813\n",
      "Epoch 14511: train loss: 0.47528713941574097\n",
      "Epoch 14512: train loss: 0.4752870798110962\n",
      "Epoch 14513: train loss: 0.4752870202064514\n",
      "Epoch 14514: train loss: 0.47528693079948425\n",
      "Epoch 14515: train loss: 0.47528693079948425\n",
      "Epoch 14516: train loss: 0.4752868115901947\n",
      "Epoch 14517: train loss: 0.4752867817878723\n",
      "Epoch 14518: train loss: 0.47528666257858276\n",
      "Epoch 14519: train loss: 0.475286602973938\n",
      "Epoch 14520: train loss: 0.4752865731716156\n",
      "Epoch 14521: train loss: 0.4752865135669708\n",
      "Epoch 14522: train loss: 0.4752863943576813\n",
      "Epoch 14523: train loss: 0.4752863943576813\n",
      "Epoch 14524: train loss: 0.47528624534606934\n",
      "Epoch 14525: train loss: 0.47528618574142456\n",
      "Epoch 14526: train loss: 0.4752861559391022\n",
      "Epoch 14527: train loss: 0.4752860963344574\n",
      "Epoch 14528: train loss: 0.4752860367298126\n",
      "Epoch 14529: train loss: 0.47528597712516785\n",
      "Epoch 14530: train loss: 0.4752858877182007\n",
      "Epoch 14531: train loss: 0.4752858281135559\n",
      "Epoch 14532: train loss: 0.47528576850891113\n",
      "Epoch 14533: train loss: 0.47528567910194397\n",
      "Epoch 14534: train loss: 0.4752856194972992\n",
      "Epoch 14535: train loss: 0.4752856194972992\n",
      "Epoch 14536: train loss: 0.47528553009033203\n",
      "Epoch 14537: train loss: 0.47528547048568726\n",
      "Epoch 14538: train loss: 0.4752853512763977\n",
      "Epoch 14539: train loss: 0.47528526186943054\n",
      "Epoch 14540: train loss: 0.47528526186943054\n",
      "Epoch 14541: train loss: 0.475285142660141\n",
      "Epoch 14542: train loss: 0.4752851128578186\n",
      "Epoch 14543: train loss: 0.47528505325317383\n",
      "Epoch 14544: train loss: 0.47528499364852905\n",
      "Epoch 14545: train loss: 0.4752849340438843\n",
      "Epoch 14546: train loss: 0.4752849042415619\n",
      "Epoch 14547: train loss: 0.47528478503227234\n",
      "Epoch 14548: train loss: 0.47528472542762756\n",
      "Epoch 14549: train loss: 0.4752846360206604\n",
      "Epoch 14550: train loss: 0.4752846360206604\n",
      "Epoch 14551: train loss: 0.47528451681137085\n",
      "Epoch 14552: train loss: 0.4752844572067261\n",
      "Epoch 14553: train loss: 0.4752844572067261\n",
      "Epoch 14554: train loss: 0.4752843677997589\n",
      "Epoch 14555: train loss: 0.47528424859046936\n",
      "Epoch 14556: train loss: 0.475284218788147\n",
      "Epoch 14557: train loss: 0.4752841591835022\n",
      "Epoch 14558: train loss: 0.4752840995788574\n",
      "Epoch 14559: train loss: 0.47528401017189026\n",
      "Epoch 14560: train loss: 0.4752839505672455\n",
      "Epoch 14561: train loss: 0.47528383135795593\n",
      "Epoch 14562: train loss: 0.47528383135795593\n",
      "Epoch 14563: train loss: 0.47528374195098877\n",
      "Epoch 14564: train loss: 0.475283682346344\n",
      "Epoch 14565: train loss: 0.4752836227416992\n",
      "Epoch 14566: train loss: 0.47528359293937683\n",
      "Epoch 14567: train loss: 0.4752834737300873\n",
      "Epoch 14568: train loss: 0.4752834141254425\n",
      "Epoch 14569: train loss: 0.47528326511383057\n",
      "Epoch 14570: train loss: 0.47528332471847534\n",
      "Epoch 14571: train loss: 0.4752832055091858\n",
      "Epoch 14572: train loss: 0.4752831757068634\n",
      "Epoch 14573: train loss: 0.47528311610221863\n",
      "Epoch 14574: train loss: 0.47528305649757385\n",
      "Epoch 14575: train loss: 0.4752829670906067\n",
      "Epoch 14576: train loss: 0.47528284788131714\n",
      "Epoch 14577: train loss: 0.47528284788131714\n",
      "Epoch 14578: train loss: 0.47528278827667236\n",
      "Epoch 14579: train loss: 0.47528275847435\n",
      "Epoch 14580: train loss: 0.4752826392650604\n",
      "Epoch 14581: train loss: 0.47528257966041565\n",
      "Epoch 14582: train loss: 0.4752824902534485\n",
      "Epoch 14583: train loss: 0.4752824306488037\n",
      "Epoch 14584: train loss: 0.47528237104415894\n",
      "Epoch 14585: train loss: 0.47528234124183655\n",
      "Epoch 14586: train loss: 0.475282222032547\n",
      "Epoch 14587: train loss: 0.4752821624279022\n",
      "Epoch 14588: train loss: 0.47528213262557983\n",
      "Epoch 14589: train loss: 0.47528207302093506\n",
      "Epoch 14590: train loss: 0.4752820134162903\n",
      "Epoch 14591: train loss: 0.4752819538116455\n",
      "Epoch 14592: train loss: 0.47528186440467834\n",
      "Epoch 14593: train loss: 0.47528180480003357\n",
      "Epoch 14594: train loss: 0.4752817153930664\n",
      "Epoch 14595: train loss: 0.47528165578842163\n",
      "Epoch 14596: train loss: 0.47528159618377686\n",
      "Epoch 14597: train loss: 0.4752815365791321\n",
      "Epoch 14598: train loss: 0.4752814471721649\n",
      "Epoch 14599: train loss: 0.4752814471721649\n",
      "Epoch 14600: train loss: 0.47528132796287537\n",
      "Epoch 14601: train loss: 0.4752812385559082\n",
      "Epoch 14602: train loss: 0.4752812385559082\n",
      "Epoch 14603: train loss: 0.47528111934661865\n",
      "Epoch 14604: train loss: 0.47528108954429626\n",
      "Epoch 14605: train loss: 0.4752810299396515\n",
      "Epoch 14606: train loss: 0.47528091073036194\n",
      "Epoch 14607: train loss: 0.47528088092803955\n",
      "Epoch 14608: train loss: 0.4752808213233948\n",
      "Epoch 14609: train loss: 0.47528076171875\n",
      "Epoch 14610: train loss: 0.47528064250946045\n",
      "Epoch 14611: train loss: 0.47528064250946045\n",
      "Epoch 14612: train loss: 0.4752805531024933\n",
      "Epoch 14613: train loss: 0.47528043389320374\n",
      "Epoch 14614: train loss: 0.47528043389320374\n",
      "Epoch 14615: train loss: 0.4752803444862366\n",
      "Epoch 14616: train loss: 0.4752803444862366\n",
      "Epoch 14617: train loss: 0.475280225276947\n",
      "Epoch 14618: train loss: 0.47528013586997986\n",
      "Epoch 14619: train loss: 0.4752800762653351\n",
      "Epoch 14620: train loss: 0.4752800166606903\n",
      "Epoch 14621: train loss: 0.47527992725372314\n",
      "Epoch 14622: train loss: 0.47527992725372314\n",
      "Epoch 14623: train loss: 0.4752798080444336\n",
      "Epoch 14624: train loss: 0.4752797782421112\n",
      "Epoch 14625: train loss: 0.47527971863746643\n",
      "Epoch 14626: train loss: 0.4752795994281769\n",
      "Epoch 14627: train loss: 0.4752795994281769\n",
      "Epoch 14628: train loss: 0.4752795100212097\n",
      "Epoch 14629: train loss: 0.47527945041656494\n",
      "Epoch 14630: train loss: 0.4752793610095978\n",
      "Epoch 14631: train loss: 0.475279301404953\n",
      "Epoch 14632: train loss: 0.4752792418003082\n",
      "Epoch 14633: train loss: 0.47527918219566345\n",
      "Epoch 14634: train loss: 0.47527915239334106\n",
      "Epoch 14635: train loss: 0.4752790331840515\n",
      "Epoch 14636: train loss: 0.47527897357940674\n",
      "Epoch 14637: train loss: 0.47527894377708435\n",
      "Epoch 14638: train loss: 0.4752788841724396\n",
      "Epoch 14639: train loss: 0.47527876496315\n",
      "Epoch 14640: train loss: 0.47527873516082764\n",
      "Epoch 14641: train loss: 0.47527867555618286\n",
      "Epoch 14642: train loss: 0.4752786159515381\n",
      "Epoch 14643: train loss: 0.4752785265445709\n",
      "Epoch 14644: train loss: 0.47527846693992615\n",
      "Epoch 14645: train loss: 0.47527840733528137\n",
      "Epoch 14646: train loss: 0.4752783477306366\n",
      "Epoch 14647: train loss: 0.47527825832366943\n",
      "Epoch 14648: train loss: 0.47527819871902466\n",
      "Epoch 14649: train loss: 0.4752781391143799\n",
      "Epoch 14650: train loss: 0.4752781093120575\n",
      "Epoch 14651: train loss: 0.4752780497074127\n",
      "Epoch 14652: train loss: 0.47527793049812317\n",
      "Epoch 14653: train loss: 0.4752779006958008\n",
      "Epoch 14654: train loss: 0.475277841091156\n",
      "Epoch 14655: train loss: 0.47527772188186646\n",
      "Epoch 14656: train loss: 0.47527772188186646\n",
      "Epoch 14657: train loss: 0.4752776324748993\n",
      "Epoch 14658: train loss: 0.4752775728702545\n",
      "Epoch 14659: train loss: 0.47527751326560974\n",
      "Epoch 14660: train loss: 0.47527748346328735\n",
      "Epoch 14661: train loss: 0.4752773642539978\n",
      "Epoch 14662: train loss: 0.475277304649353\n",
      "Epoch 14663: train loss: 0.47527727484703064\n",
      "Epoch 14664: train loss: 0.47527721524238586\n",
      "Epoch 14665: train loss: 0.4752770960330963\n",
      "Epoch 14666: train loss: 0.4752770960330963\n",
      "Epoch 14667: train loss: 0.47527700662612915\n",
      "Epoch 14668: train loss: 0.4752768874168396\n",
      "Epoch 14669: train loss: 0.47527679800987244\n",
      "Epoch 14670: train loss: 0.47527679800987244\n",
      "Epoch 14671: train loss: 0.4752766788005829\n",
      "Epoch 14672: train loss: 0.4752766191959381\n",
      "Epoch 14673: train loss: 0.4752765893936157\n",
      "Epoch 14674: train loss: 0.47527652978897095\n",
      "Epoch 14675: train loss: 0.47527647018432617\n",
      "Epoch 14676: train loss: 0.475276380777359\n",
      "Epoch 14677: train loss: 0.47527632117271423\n",
      "Epoch 14678: train loss: 0.4752762019634247\n",
      "Epoch 14679: train loss: 0.4752762019634247\n",
      "Epoch 14680: train loss: 0.4752761125564575\n",
      "Epoch 14681: train loss: 0.47527599334716797\n",
      "Epoch 14682: train loss: 0.47527599334716797\n",
      "Epoch 14683: train loss: 0.4752759039402008\n",
      "Epoch 14684: train loss: 0.47527584433555603\n",
      "Epoch 14685: train loss: 0.47527578473091125\n",
      "Epoch 14686: train loss: 0.47527575492858887\n",
      "Epoch 14687: train loss: 0.4752756953239441\n",
      "Epoch 14688: train loss: 0.47527557611465454\n",
      "Epoch 14689: train loss: 0.47527554631233215\n",
      "Epoch 14690: train loss: 0.47527554631233215\n",
      "Epoch 14691: train loss: 0.4752754271030426\n",
      "Epoch 14692: train loss: 0.47527533769607544\n",
      "Epoch 14693: train loss: 0.47527533769607544\n",
      "Epoch 14694: train loss: 0.4752752184867859\n",
      "Epoch 14695: train loss: 0.4752751290798187\n",
      "Epoch 14696: train loss: 0.47527506947517395\n",
      "Epoch 14697: train loss: 0.4752750098705292\n",
      "Epoch 14698: train loss: 0.4752749502658844\n",
      "Epoch 14699: train loss: 0.475274920463562\n",
      "Epoch 14700: train loss: 0.47527486085891724\n",
      "Epoch 14701: train loss: 0.4752747416496277\n",
      "Epoch 14702: train loss: 0.4752747118473053\n",
      "Epoch 14703: train loss: 0.4752746522426605\n",
      "Epoch 14704: train loss: 0.47527459263801575\n",
      "Epoch 14705: train loss: 0.47527453303337097\n",
      "Epoch 14706: train loss: 0.4752744436264038\n",
      "Epoch 14707: train loss: 0.47527432441711426\n",
      "Epoch 14708: train loss: 0.47527429461479187\n",
      "Epoch 14709: train loss: 0.4752742350101471\n",
      "Epoch 14710: train loss: 0.4752741754055023\n",
      "Epoch 14711: train loss: 0.47527411580085754\n",
      "Epoch 14712: train loss: 0.47527408599853516\n",
      "Epoch 14713: train loss: 0.4752739667892456\n",
      "Epoch 14714: train loss: 0.4752739667892456\n",
      "Epoch 14715: train loss: 0.47527387738227844\n",
      "Epoch 14716: train loss: 0.47527381777763367\n",
      "Epoch 14717: train loss: 0.4752736985683441\n",
      "Epoch 14718: train loss: 0.47527366876602173\n",
      "Epoch 14719: train loss: 0.47527360916137695\n",
      "Epoch 14720: train loss: 0.4752735495567322\n",
      "Epoch 14721: train loss: 0.475273460149765\n",
      "Epoch 14722: train loss: 0.47527340054512024\n",
      "Epoch 14723: train loss: 0.47527334094047546\n",
      "Epoch 14724: train loss: 0.4752732515335083\n",
      "Epoch 14725: train loss: 0.4752731919288635\n",
      "Epoch 14726: train loss: 0.47527313232421875\n",
      "Epoch 14727: train loss: 0.475273072719574\n",
      "Epoch 14728: train loss: 0.4752730131149292\n",
      "Epoch 14729: train loss: 0.47527292370796204\n",
      "Epoch 14730: train loss: 0.47527286410331726\n",
      "Epoch 14731: train loss: 0.4752728044986725\n",
      "Epoch 14732: train loss: 0.4752727746963501\n",
      "Epoch 14733: train loss: 0.4752727150917053\n",
      "Epoch 14734: train loss: 0.47527259588241577\n",
      "Epoch 14735: train loss: 0.47527259588241577\n",
      "Epoch 14736: train loss: 0.4752725064754486\n",
      "Epoch 14737: train loss: 0.47527244687080383\n",
      "Epoch 14738: train loss: 0.47527238726615906\n",
      "Epoch 14739: train loss: 0.4752722978591919\n",
      "Epoch 14740: train loss: 0.4752722382545471\n",
      "Epoch 14741: train loss: 0.47527214884757996\n",
      "Epoch 14742: train loss: 0.4752720892429352\n",
      "Epoch 14743: train loss: 0.4752720892429352\n",
      "Epoch 14744: train loss: 0.47527197003364563\n",
      "Epoch 14745: train loss: 0.47527188062667847\n",
      "Epoch 14746: train loss: 0.47527188062667847\n",
      "Epoch 14747: train loss: 0.4752717614173889\n",
      "Epoch 14748: train loss: 0.47527167201042175\n",
      "Epoch 14749: train loss: 0.475271612405777\n",
      "Epoch 14750: train loss: 0.4752715528011322\n",
      "Epoch 14751: train loss: 0.4752715528011322\n",
      "Epoch 14752: train loss: 0.47527146339416504\n",
      "Epoch 14753: train loss: 0.47527140378952026\n",
      "Epoch 14754: train loss: 0.4752713441848755\n",
      "Epoch 14755: train loss: 0.4752712547779083\n",
      "Epoch 14756: train loss: 0.47527119517326355\n",
      "Epoch 14757: train loss: 0.4752711355686188\n",
      "Epoch 14758: train loss: 0.4752710461616516\n",
      "Epoch 14759: train loss: 0.4752710461616516\n",
      "Epoch 14760: train loss: 0.47527092695236206\n",
      "Epoch 14761: train loss: 0.4752708971500397\n",
      "Epoch 14762: train loss: 0.4752708375453949\n",
      "Epoch 14763: train loss: 0.4752707779407501\n",
      "Epoch 14764: train loss: 0.47527071833610535\n",
      "Epoch 14765: train loss: 0.4752706289291382\n",
      "Epoch 14766: train loss: 0.47527050971984863\n",
      "Epoch 14767: train loss: 0.47527047991752625\n",
      "Epoch 14768: train loss: 0.47527047991752625\n",
      "Epoch 14769: train loss: 0.47527042031288147\n",
      "Epoch 14770: train loss: 0.47527027130126953\n",
      "Epoch 14771: train loss: 0.47527027130126953\n",
      "Epoch 14772: train loss: 0.47527021169662476\n",
      "Epoch 14773: train loss: 0.4752700626850128\n",
      "Epoch 14774: train loss: 0.4752700626850128\n",
      "Epoch 14775: train loss: 0.47526994347572327\n",
      "Epoch 14776: train loss: 0.4752698838710785\n",
      "Epoch 14777: train loss: 0.4752698540687561\n",
      "Epoch 14778: train loss: 0.47526979446411133\n",
      "Epoch 14779: train loss: 0.4752696752548218\n",
      "Epoch 14780: train loss: 0.4752696752548218\n",
      "Epoch 14781: train loss: 0.4752695858478546\n",
      "Epoch 14782: train loss: 0.47526952624320984\n",
      "Epoch 14783: train loss: 0.47526946663856506\n",
      "Epoch 14784: train loss: 0.4752693772315979\n",
      "Epoch 14785: train loss: 0.4752693176269531\n",
      "Epoch 14786: train loss: 0.47526925802230835\n",
      "Epoch 14787: train loss: 0.4752691984176636\n",
      "Epoch 14788: train loss: 0.4752691090106964\n",
      "Epoch 14789: train loss: 0.47526904940605164\n",
      "Epoch 14790: train loss: 0.47526898980140686\n",
      "Epoch 14791: train loss: 0.4752689003944397\n",
      "Epoch 14792: train loss: 0.4752688407897949\n",
      "Epoch 14793: train loss: 0.47526878118515015\n",
      "Epoch 14794: train loss: 0.47526875138282776\n",
      "Epoch 14795: train loss: 0.4752686321735382\n",
      "Epoch 14796: train loss: 0.47526857256889343\n",
      "Epoch 14797: train loss: 0.47526854276657104\n",
      "Epoch 14798: train loss: 0.47526848316192627\n",
      "Epoch 14799: train loss: 0.4752684235572815\n",
      "Epoch 14800: train loss: 0.4752683639526367\n",
      "Epoch 14801: train loss: 0.47526827454566956\n",
      "Epoch 14802: train loss: 0.4752682149410248\n",
      "Epoch 14803: train loss: 0.4752682149410248\n",
      "Epoch 14804: train loss: 0.4752681255340576\n",
      "Epoch 14805: train loss: 0.47526806592941284\n",
      "Epoch 14806: train loss: 0.4752679467201233\n",
      "Epoch 14807: train loss: 0.4752679169178009\n",
      "Epoch 14808: train loss: 0.47526785731315613\n",
      "Epoch 14809: train loss: 0.4752677381038666\n",
      "Epoch 14810: train loss: 0.4752677083015442\n",
      "Epoch 14811: train loss: 0.4752676486968994\n",
      "Epoch 14812: train loss: 0.47526758909225464\n",
      "Epoch 14813: train loss: 0.47526752948760986\n",
      "Epoch 14814: train loss: 0.4752674996852875\n",
      "Epoch 14815: train loss: 0.4752674400806427\n",
      "Epoch 14816: train loss: 0.47526729106903076\n",
      "Epoch 14817: train loss: 0.47526729106903076\n",
      "Epoch 14818: train loss: 0.4752671718597412\n",
      "Epoch 14819: train loss: 0.47526711225509644\n",
      "Epoch 14820: train loss: 0.47526708245277405\n",
      "Epoch 14821: train loss: 0.4752669632434845\n",
      "Epoch 14822: train loss: 0.4752669632434845\n",
      "Epoch 14823: train loss: 0.47526687383651733\n",
      "Epoch 14824: train loss: 0.47526681423187256\n",
      "Epoch 14825: train loss: 0.4752667546272278\n",
      "Epoch 14826: train loss: 0.4752666652202606\n",
      "Epoch 14827: train loss: 0.47526660561561584\n",
      "Epoch 14828: train loss: 0.47526654601097107\n",
      "Epoch 14829: train loss: 0.4752664864063263\n",
      "Epoch 14830: train loss: 0.47526639699935913\n",
      "Epoch 14831: train loss: 0.47526639699935913\n",
      "Epoch 14832: train loss: 0.4752662777900696\n",
      "Epoch 14833: train loss: 0.4752662479877472\n",
      "Epoch 14834: train loss: 0.47526612877845764\n",
      "Epoch 14835: train loss: 0.47526612877845764\n",
      "Epoch 14836: train loss: 0.4752660393714905\n",
      "Epoch 14837: train loss: 0.4752659797668457\n",
      "Epoch 14838: train loss: 0.4752659201622009\n",
      "Epoch 14839: train loss: 0.47526583075523376\n",
      "Epoch 14840: train loss: 0.475265771150589\n",
      "Epoch 14841: train loss: 0.4752657115459442\n",
      "Epoch 14842: train loss: 0.47526565194129944\n",
      "Epoch 14843: train loss: 0.4752655625343323\n",
      "Epoch 14844: train loss: 0.4752655029296875\n",
      "Epoch 14845: train loss: 0.47526538372039795\n",
      "Epoch 14846: train loss: 0.47526538372039795\n",
      "Epoch 14847: train loss: 0.4752652943134308\n",
      "Epoch 14848: train loss: 0.475265234708786\n",
      "Epoch 14849: train loss: 0.47526517510414124\n",
      "Epoch 14850: train loss: 0.47526514530181885\n",
      "Epoch 14851: train loss: 0.4752650856971741\n",
      "Epoch 14852: train loss: 0.4752649664878845\n",
      "Epoch 14853: train loss: 0.4752649664878845\n",
      "Epoch 14854: train loss: 0.47526487708091736\n",
      "Epoch 14855: train loss: 0.4752647578716278\n",
      "Epoch 14856: train loss: 0.4752647280693054\n",
      "Epoch 14857: train loss: 0.47526466846466064\n",
      "Epoch 14858: train loss: 0.47526466846466064\n",
      "Epoch 14859: train loss: 0.4752645194530487\n",
      "Epoch 14860: train loss: 0.47526445984840393\n",
      "Epoch 14861: train loss: 0.47526445984840393\n",
      "Epoch 14862: train loss: 0.47526440024375916\n",
      "Epoch 14863: train loss: 0.475264310836792\n",
      "Epoch 14864: train loss: 0.4752642512321472\n",
      "Epoch 14865: train loss: 0.47526419162750244\n",
      "Epoch 14866: train loss: 0.4752641022205353\n",
      "Epoch 14867: train loss: 0.4752640426158905\n",
      "Epoch 14868: train loss: 0.4752639830112457\n",
      "Epoch 14869: train loss: 0.47526389360427856\n",
      "Epoch 14870: train loss: 0.4752638339996338\n",
      "Epoch 14871: train loss: 0.475263774394989\n",
      "Epoch 14872: train loss: 0.47526371479034424\n",
      "Epoch 14873: train loss: 0.47526368498802185\n",
      "Epoch 14874: train loss: 0.4752635657787323\n",
      "Epoch 14875: train loss: 0.4752635061740875\n",
      "Epoch 14876: train loss: 0.4752635061740875\n",
      "Epoch 14877: train loss: 0.4752633571624756\n",
      "Epoch 14878: train loss: 0.4752632677555084\n",
      "Epoch 14879: train loss: 0.4752632677555084\n",
      "Epoch 14880: train loss: 0.47526320815086365\n",
      "Epoch 14881: train loss: 0.4752630889415741\n",
      "Epoch 14882: train loss: 0.4752630591392517\n",
      "Epoch 14883: train loss: 0.47526299953460693\n",
      "Epoch 14884: train loss: 0.47526293992996216\n",
      "Epoch 14885: train loss: 0.4752628803253174\n",
      "Epoch 14886: train loss: 0.4752627909183502\n",
      "Epoch 14887: train loss: 0.4752627909183502\n",
      "Epoch 14888: train loss: 0.47526267170906067\n",
      "Epoch 14889: train loss: 0.4752625823020935\n",
      "Epoch 14890: train loss: 0.47526252269744873\n",
      "Epoch 14891: train loss: 0.47526246309280396\n",
      "Epoch 14892: train loss: 0.47526243329048157\n",
      "Epoch 14893: train loss: 0.4752623736858368\n",
      "Epoch 14894: train loss: 0.47526225447654724\n",
      "Epoch 14895: train loss: 0.47526222467422485\n",
      "Epoch 14896: train loss: 0.4752621650695801\n",
      "Epoch 14897: train loss: 0.4752620458602905\n",
      "Epoch 14898: train loss: 0.4752620458602905\n",
      "Epoch 14899: train loss: 0.47526195645332336\n",
      "Epoch 14900: train loss: 0.4752618968486786\n",
      "Epoch 14901: train loss: 0.4752618372440338\n",
      "Epoch 14902: train loss: 0.4752618074417114\n",
      "Epoch 14903: train loss: 0.47526174783706665\n",
      "Epoch 14904: train loss: 0.4752616286277771\n",
      "Epoch 14905: train loss: 0.4752615690231323\n",
      "Epoch 14906: train loss: 0.47526153922080994\n",
      "Epoch 14907: train loss: 0.4752614200115204\n",
      "Epoch 14908: train loss: 0.4752613306045532\n",
      "Epoch 14909: train loss: 0.4752613306045532\n",
      "Epoch 14910: train loss: 0.47526127099990845\n",
      "Epoch 14911: train loss: 0.47526121139526367\n",
      "Epoch 14912: train loss: 0.4752611219882965\n",
      "Epoch 14913: train loss: 0.47526106238365173\n",
      "Epoch 14914: train loss: 0.47526100277900696\n",
      "Epoch 14915: train loss: 0.4752609431743622\n",
      "Epoch 14916: train loss: 0.4752609133720398\n",
      "Epoch 14917: train loss: 0.47526079416275024\n",
      "Epoch 14918: train loss: 0.47526073455810547\n",
      "Epoch 14919: train loss: 0.4752607047557831\n",
      "Epoch 14920: train loss: 0.4752606451511383\n",
      "Epoch 14921: train loss: 0.47526052594184875\n",
      "Epoch 14922: train loss: 0.47526049613952637\n",
      "Epoch 14923: train loss: 0.4752603769302368\n",
      "Epoch 14924: train loss: 0.47526031732559204\n",
      "Epoch 14925: train loss: 0.47526028752326965\n",
      "Epoch 14926: train loss: 0.4752602279186249\n",
      "Epoch 14927: train loss: 0.4752601683139801\n",
      "Epoch 14928: train loss: 0.47526007890701294\n",
      "Epoch 14929: train loss: 0.47526007890701294\n",
      "Epoch 14930: train loss: 0.4752599596977234\n",
      "Epoch 14931: train loss: 0.4752599000930786\n",
      "Epoch 14932: train loss: 0.4752598702907562\n",
      "Epoch 14933: train loss: 0.4752597510814667\n",
      "Epoch 14934: train loss: 0.4752596616744995\n",
      "Epoch 14935: train loss: 0.4752596616744995\n",
      "Epoch 14936: train loss: 0.47525954246520996\n",
      "Epoch 14937: train loss: 0.47525954246520996\n",
      "Epoch 14938: train loss: 0.4752594828605652\n",
      "Epoch 14939: train loss: 0.475259393453598\n",
      "Epoch 14940: train loss: 0.47525933384895325\n",
      "Epoch 14941: train loss: 0.47525927424430847\n",
      "Epoch 14942: train loss: 0.4752591848373413\n",
      "Epoch 14943: train loss: 0.4752591848373413\n",
      "Epoch 14944: train loss: 0.47525906562805176\n",
      "Epoch 14945: train loss: 0.47525903582572937\n",
      "Epoch 14946: train loss: 0.4752589762210846\n",
      "Epoch 14947: train loss: 0.47525882720947266\n",
      "Epoch 14948: train loss: 0.47525882720947266\n",
      "Epoch 14949: train loss: 0.4752587676048279\n",
      "Epoch 14950: train loss: 0.4752587080001831\n",
      "Epoch 14951: train loss: 0.47525861859321594\n",
      "Epoch 14952: train loss: 0.47525855898857117\n",
      "Epoch 14953: train loss: 0.4752584993839264\n",
      "Epoch 14954: train loss: 0.4752584397792816\n",
      "Epoch 14955: train loss: 0.47525835037231445\n",
      "Epoch 14956: train loss: 0.4752582907676697\n",
      "Epoch 14957: train loss: 0.4752582907676697\n",
      "Epoch 14958: train loss: 0.4752582013607025\n",
      "Epoch 14959: train loss: 0.47525808215141296\n",
      "Epoch 14960: train loss: 0.47525808215141296\n",
      "Epoch 14961: train loss: 0.4752579927444458\n",
      "Epoch 14962: train loss: 0.475257933139801\n",
      "Epoch 14963: train loss: 0.47525787353515625\n",
      "Epoch 14964: train loss: 0.4752577543258667\n",
      "Epoch 14965: train loss: 0.4752577245235443\n",
      "Epoch 14966: train loss: 0.47525766491889954\n",
      "Epoch 14967: train loss: 0.47525754570961\n",
      "Epoch 14968: train loss: 0.47525754570961\n",
      "Epoch 14969: train loss: 0.4752574563026428\n",
      "Epoch 14970: train loss: 0.47525739669799805\n",
      "Epoch 14971: train loss: 0.4752573072910309\n",
      "Epoch 14972: train loss: 0.4752572476863861\n",
      "Epoch 14973: train loss: 0.4752572476863861\n",
      "Epoch 14974: train loss: 0.47525709867477417\n",
      "Epoch 14975: train loss: 0.4752570390701294\n",
      "Epoch 14976: train loss: 0.4752570390701294\n",
      "Epoch 14977: train loss: 0.47525691986083984\n",
      "Epoch 14978: train loss: 0.47525689005851746\n",
      "Epoch 14979: train loss: 0.4752568304538727\n",
      "Epoch 14980: train loss: 0.47525671124458313\n",
      "Epoch 14981: train loss: 0.47525668144226074\n",
      "Epoch 14982: train loss: 0.47525668144226074\n",
      "Epoch 14983: train loss: 0.4752565026283264\n",
      "Epoch 14984: train loss: 0.47525647282600403\n",
      "Epoch 14985: train loss: 0.47525647282600403\n",
      "Epoch 14986: train loss: 0.47525641322135925\n",
      "Epoch 14987: train loss: 0.4752562940120697\n",
      "Epoch 14988: train loss: 0.4752562642097473\n",
      "Epoch 14989: train loss: 0.47525620460510254\n",
      "Epoch 14990: train loss: 0.47525614500045776\n",
      "Epoch 14991: train loss: 0.4752560555934906\n",
      "Epoch 14992: train loss: 0.4752559959888458\n",
      "Epoch 14993: train loss: 0.47525593638420105\n",
      "Epoch 14994: train loss: 0.4752558767795563\n",
      "Epoch 14995: train loss: 0.4752557873725891\n",
      "Epoch 14996: train loss: 0.47525572776794434\n",
      "Epoch 14997: train loss: 0.47525566816329956\n",
      "Epoch 14998: train loss: 0.4752555787563324\n",
      "Epoch 14999: train loss: 0.4752555787563324\n",
      "Epoch 15000: train loss: 0.4752555191516876\n",
      "Epoch 15001: train loss: 0.4752553701400757\n",
      "Epoch 15002: train loss: 0.4752553105354309\n",
      "Epoch 15003: train loss: 0.4752553105354309\n",
      "Epoch 15004: train loss: 0.47525522112846375\n",
      "Epoch 15005: train loss: 0.47525522112846375\n",
      "Epoch 15006: train loss: 0.4752551019191742\n",
      "Epoch 15007: train loss: 0.4752550423145294\n",
      "Epoch 15008: train loss: 0.47525495290756226\n",
      "Epoch 15009: train loss: 0.47525495290756226\n",
      "Epoch 15010: train loss: 0.4752548336982727\n",
      "Epoch 15011: train loss: 0.47525474429130554\n",
      "Epoch 15012: train loss: 0.47525474429130554\n",
      "Epoch 15013: train loss: 0.475254625082016\n",
      "Epoch 15014: train loss: 0.4752545952796936\n",
      "Epoch 15015: train loss: 0.47525453567504883\n",
      "Epoch 15016: train loss: 0.4752544164657593\n",
      "Epoch 15017: train loss: 0.4752544164657593\n",
      "Epoch 15018: train loss: 0.4752543270587921\n",
      "Epoch 15019: train loss: 0.47525426745414734\n",
      "Epoch 15020: train loss: 0.4752541780471802\n",
      "Epoch 15021: train loss: 0.4752541184425354\n",
      "Epoch 15022: train loss: 0.4752540588378906\n",
      "Epoch 15023: train loss: 0.47525399923324585\n",
      "Epoch 15024: train loss: 0.4752539396286011\n",
      "Epoch 15025: train loss: 0.4752539098262787\n",
      "Epoch 15026: train loss: 0.4752538502216339\n",
      "Epoch 15027: train loss: 0.47525373101234436\n",
      "Epoch 15028: train loss: 0.475253701210022\n",
      "Epoch 15029: train loss: 0.4752536416053772\n",
      "Epoch 15030: train loss: 0.47525352239608765\n",
      "Epoch 15031: train loss: 0.47525349259376526\n",
      "Epoch 15032: train loss: 0.4752534329891205\n",
      "Epoch 15033: train loss: 0.4752533733844757\n",
      "Epoch 15034: train loss: 0.47525328397750854\n",
      "Epoch 15035: train loss: 0.47525322437286377\n",
      "Epoch 15036: train loss: 0.475253164768219\n",
      "Epoch 15037: train loss: 0.4752531051635742\n",
      "Epoch 15038: train loss: 0.47525307536125183\n",
      "Epoch 15039: train loss: 0.47525301575660706\n",
      "Epoch 15040: train loss: 0.4752528667449951\n",
      "Epoch 15041: train loss: 0.4752528965473175\n",
      "Epoch 15042: train loss: 0.47525280714035034\n",
      "Epoch 15043: train loss: 0.47525274753570557\n",
      "Epoch 15044: train loss: 0.4752526581287384\n",
      "Epoch 15045: train loss: 0.4752526581287384\n",
      "Epoch 15046: train loss: 0.4752524793148041\n",
      "Epoch 15047: train loss: 0.4752524793148041\n",
      "Epoch 15048: train loss: 0.4752523899078369\n",
      "Epoch 15049: train loss: 0.47525233030319214\n",
      "Epoch 15050: train loss: 0.47525227069854736\n",
      "Epoch 15051: train loss: 0.4752521812915802\n",
      "Epoch 15052: train loss: 0.4752521812915802\n",
      "Epoch 15053: train loss: 0.4752521216869354\n",
      "Epoch 15054: train loss: 0.47525203227996826\n",
      "Epoch 15055: train loss: 0.4752519726753235\n",
      "Epoch 15056: train loss: 0.4752519130706787\n",
      "Epoch 15057: train loss: 0.47525182366371155\n",
      "Epoch 15058: train loss: 0.4752517640590668\n",
      "Epoch 15059: train loss: 0.475251704454422\n",
      "Epoch 15060: train loss: 0.4752516448497772\n",
      "Epoch 15061: train loss: 0.47525155544281006\n",
      "Epoch 15062: train loss: 0.4752514958381653\n",
      "Epoch 15063: train loss: 0.4752514362335205\n",
      "Epoch 15064: train loss: 0.4752514064311981\n",
      "Epoch 15065: train loss: 0.47525134682655334\n",
      "Epoch 15066: train loss: 0.4752512276172638\n",
      "Epoch 15067: train loss: 0.47525113821029663\n",
      "Epoch 15068: train loss: 0.47525113821029663\n",
      "Epoch 15069: train loss: 0.4752510190010071\n",
      "Epoch 15070: train loss: 0.4752509891986847\n",
      "Epoch 15071: train loss: 0.4752509295940399\n",
      "Epoch 15072: train loss: 0.47525086998939514\n",
      "Epoch 15073: train loss: 0.475250780582428\n",
      "Epoch 15074: train loss: 0.4752507209777832\n",
      "Epoch 15075: train loss: 0.4752506613731384\n",
      "Epoch 15076: train loss: 0.47525060176849365\n",
      "Epoch 15077: train loss: 0.47525057196617126\n",
      "Epoch 15078: train loss: 0.4752504527568817\n",
      "Epoch 15079: train loss: 0.4752504527568817\n",
      "Epoch 15080: train loss: 0.47525036334991455\n",
      "Epoch 15081: train loss: 0.4752503037452698\n",
      "Epoch 15082: train loss: 0.4752501845359802\n",
      "Epoch 15083: train loss: 0.4752501845359802\n",
      "Epoch 15084: train loss: 0.47525009512901306\n",
      "Epoch 15085: train loss: 0.4752500355243683\n",
      "Epoch 15086: train loss: 0.47524991631507874\n",
      "Epoch 15087: train loss: 0.47524991631507874\n",
      "Epoch 15088: train loss: 0.4752498269081116\n",
      "Epoch 15089: train loss: 0.475249707698822\n",
      "Epoch 15090: train loss: 0.475249707698822\n",
      "Epoch 15091: train loss: 0.47524961829185486\n",
      "Epoch 15092: train loss: 0.4752494990825653\n",
      "Epoch 15093: train loss: 0.4752494990825653\n",
      "Epoch 15094: train loss: 0.4752494692802429\n",
      "Epoch 15095: train loss: 0.47524940967559814\n",
      "Epoch 15096: train loss: 0.4752492904663086\n",
      "Epoch 15097: train loss: 0.4752492606639862\n",
      "Epoch 15098: train loss: 0.47524914145469666\n",
      "Epoch 15099: train loss: 0.4752490818500519\n",
      "Epoch 15100: train loss: 0.4752490520477295\n",
      "Epoch 15101: train loss: 0.4752489924430847\n",
      "Epoch 15102: train loss: 0.47524893283843994\n",
      "Epoch 15103: train loss: 0.47524887323379517\n",
      "Epoch 15104: train loss: 0.4752488434314728\n",
      "Epoch 15105: train loss: 0.4752487242221832\n",
      "Epoch 15106: train loss: 0.47524866461753845\n",
      "Epoch 15107: train loss: 0.47524863481521606\n",
      "Epoch 15108: train loss: 0.4752485752105713\n",
      "Epoch 15109: train loss: 0.47524845600128174\n",
      "Epoch 15110: train loss: 0.47524842619895935\n",
      "Epoch 15111: train loss: 0.4752483665943146\n",
      "Epoch 15112: train loss: 0.47524821758270264\n",
      "Epoch 15113: train loss: 0.47524821758270264\n",
      "Epoch 15114: train loss: 0.47524815797805786\n",
      "Epoch 15115: train loss: 0.4752480983734131\n",
      "Epoch 15116: train loss: 0.4752480387687683\n",
      "Epoch 15117: train loss: 0.47524794936180115\n",
      "Epoch 15118: train loss: 0.47524788975715637\n",
      "Epoch 15119: train loss: 0.4752478301525116\n",
      "Epoch 15120: train loss: 0.47524774074554443\n",
      "Epoch 15121: train loss: 0.47524768114089966\n",
      "Epoch 15122: train loss: 0.4752476215362549\n",
      "Epoch 15123: train loss: 0.4752475917339325\n",
      "Epoch 15124: train loss: 0.4752475321292877\n",
      "Epoch 15125: train loss: 0.47524741291999817\n",
      "Epoch 15126: train loss: 0.47524741291999817\n",
      "Epoch 15127: train loss: 0.475247323513031\n",
      "Epoch 15128: train loss: 0.47524726390838623\n",
      "Epoch 15129: train loss: 0.47524720430374146\n",
      "Epoch 15130: train loss: 0.4752471148967743\n",
      "Epoch 15131: train loss: 0.4752470552921295\n",
      "Epoch 15132: train loss: 0.47524699568748474\n",
      "Epoch 15133: train loss: 0.47524696588516235\n",
      "Epoch 15134: train loss: 0.4752468466758728\n",
      "Epoch 15135: train loss: 0.475246787071228\n",
      "Epoch 15136: train loss: 0.47524675726890564\n",
      "Epoch 15137: train loss: 0.47524669766426086\n",
      "Epoch 15138: train loss: 0.4752466380596161\n",
      "Epoch 15139: train loss: 0.4752465784549713\n",
      "Epoch 15140: train loss: 0.47524648904800415\n",
      "Epoch 15141: train loss: 0.47524648904800415\n",
      "Epoch 15142: train loss: 0.4752463102340698\n",
      "Epoch 15143: train loss: 0.47524628043174744\n",
      "Epoch 15144: train loss: 0.47524628043174744\n",
      "Epoch 15145: train loss: 0.4752461612224579\n",
      "Epoch 15146: train loss: 0.4752461016178131\n",
      "Epoch 15147: train loss: 0.47524601221084595\n",
      "Epoch 15148: train loss: 0.47524595260620117\n",
      "Epoch 15149: train loss: 0.47524595260620117\n",
      "Epoch 15150: train loss: 0.475245863199234\n",
      "Epoch 15151: train loss: 0.47524580359458923\n",
      "Epoch 15152: train loss: 0.47524574398994446\n",
      "Epoch 15153: train loss: 0.4752456843852997\n",
      "Epoch 15154: train loss: 0.4752456545829773\n",
      "Epoch 15155: train loss: 0.47524553537368774\n",
      "Epoch 15156: train loss: 0.47524547576904297\n",
      "Epoch 15157: train loss: 0.4752454459667206\n",
      "Epoch 15158: train loss: 0.47524532675743103\n",
      "Epoch 15159: train loss: 0.47524523735046387\n",
      "Epoch 15160: train loss: 0.47524523735046387\n",
      "Epoch 15161: train loss: 0.4752451777458191\n",
      "Epoch 15162: train loss: 0.4752451181411743\n",
      "Epoch 15163: train loss: 0.47524502873420715\n",
      "Epoch 15164: train loss: 0.4752449095249176\n",
      "Epoch 15165: train loss: 0.4752449095249176\n",
      "Epoch 15166: train loss: 0.4752448499202728\n",
      "Epoch 15167: train loss: 0.47524476051330566\n",
      "Epoch 15168: train loss: 0.4752447009086609\n",
      "Epoch 15169: train loss: 0.4752446413040161\n",
      "Epoch 15170: train loss: 0.47524455189704895\n",
      "Epoch 15171: train loss: 0.47524455189704895\n",
      "Epoch 15172: train loss: 0.4752444326877594\n",
      "Epoch 15173: train loss: 0.475244402885437\n",
      "Epoch 15174: train loss: 0.47524428367614746\n",
      "Epoch 15175: train loss: 0.47524428367614746\n",
      "Epoch 15176: train loss: 0.4752441942691803\n",
      "Epoch 15177: train loss: 0.4752441346645355\n",
      "Epoch 15178: train loss: 0.47524401545524597\n",
      "Epoch 15179: train loss: 0.4752439856529236\n",
      "Epoch 15180: train loss: 0.4752439260482788\n",
      "Epoch 15181: train loss: 0.47524386644363403\n",
      "Epoch 15182: train loss: 0.47524380683898926\n",
      "Epoch 15183: train loss: 0.4752437174320221\n",
      "Epoch 15184: train loss: 0.4752436578273773\n",
      "Epoch 15185: train loss: 0.47524359822273254\n",
      "Epoch 15186: train loss: 0.47524356842041016\n",
      "Epoch 15187: train loss: 0.4752435088157654\n",
      "Epoch 15188: train loss: 0.47524338960647583\n",
      "Epoch 15189: train loss: 0.47524335980415344\n",
      "Epoch 15190: train loss: 0.47524335980415344\n",
      "Epoch 15191: train loss: 0.4752432405948639\n",
      "Epoch 15192: train loss: 0.4752431809902191\n",
      "Epoch 15193: train loss: 0.47524309158325195\n",
      "Epoch 15194: train loss: 0.4752430319786072\n",
      "Epoch 15195: train loss: 0.4752429723739624\n",
      "Epoch 15196: train loss: 0.47524288296699524\n",
      "Epoch 15197: train loss: 0.47524282336235046\n",
      "Epoch 15198: train loss: 0.4752427637577057\n",
      "Epoch 15199: train loss: 0.4752427339553833\n",
      "Epoch 15200: train loss: 0.4752426743507385\n",
      "Epoch 15201: train loss: 0.47524261474609375\n",
      "Epoch 15202: train loss: 0.475242555141449\n",
      "Epoch 15203: train loss: 0.4752424657344818\n",
      "Epoch 15204: train loss: 0.47524240612983704\n",
      "Epoch 15205: train loss: 0.47524234652519226\n",
      "Epoch 15206: train loss: 0.4752422869205475\n",
      "Epoch 15207: train loss: 0.4752421975135803\n",
      "Epoch 15208: train loss: 0.47524213790893555\n",
      "Epoch 15209: train loss: 0.47524207830429077\n",
      "Epoch 15210: train loss: 0.4752419888973236\n",
      "Epoch 15211: train loss: 0.4752419888973236\n",
      "Epoch 15212: train loss: 0.47524186968803406\n",
      "Epoch 15213: train loss: 0.47524183988571167\n",
      "Epoch 15214: train loss: 0.4752417206764221\n",
      "Epoch 15215: train loss: 0.47524166107177734\n",
      "Epoch 15216: train loss: 0.47524163126945496\n",
      "Epoch 15217: train loss: 0.4752415716648102\n",
      "Epoch 15218: train loss: 0.4752415120601654\n",
      "Epoch 15219: train loss: 0.47524145245552063\n",
      "Epoch 15220: train loss: 0.47524136304855347\n",
      "Epoch 15221: train loss: 0.4752413034439087\n",
      "Epoch 15222: train loss: 0.4752412438392639\n",
      "Epoch 15223: train loss: 0.47524121403694153\n",
      "Epoch 15224: train loss: 0.47524115443229675\n",
      "Epoch 15225: train loss: 0.4752410352230072\n",
      "Epoch 15226: train loss: 0.4752410054206848\n",
      "Epoch 15227: train loss: 0.47524094581604004\n",
      "Epoch 15228: train loss: 0.47524088621139526\n",
      "Epoch 15229: train loss: 0.4752408266067505\n",
      "Epoch 15230: train loss: 0.4752407371997833\n",
      "Epoch 15231: train loss: 0.47524067759513855\n",
      "Epoch 15232: train loss: 0.4752406179904938\n",
      "Epoch 15233: train loss: 0.4752405881881714\n",
      "Epoch 15234: train loss: 0.4752405285835266\n",
      "Epoch 15235: train loss: 0.47524040937423706\n",
      "Epoch 15236: train loss: 0.4752403795719147\n",
      "Epoch 15237: train loss: 0.4752402603626251\n",
      "Epoch 15238: train loss: 0.47524020075798035\n",
      "Epoch 15239: train loss: 0.47524017095565796\n",
      "Epoch 15240: train loss: 0.47524017095565796\n",
      "Epoch 15241: train loss: 0.47523999214172363\n",
      "Epoch 15242: train loss: 0.47523999214172363\n",
      "Epoch 15243: train loss: 0.47523990273475647\n",
      "Epoch 15244: train loss: 0.4752398431301117\n",
      "Epoch 15245: train loss: 0.4752398431301117\n",
      "Epoch 15246: train loss: 0.47523975372314453\n",
      "Epoch 15247: train loss: 0.47523969411849976\n",
      "Epoch 15248: train loss: 0.4752395749092102\n",
      "Epoch 15249: train loss: 0.4752395451068878\n",
      "Epoch 15250: train loss: 0.47523948550224304\n",
      "Epoch 15251: train loss: 0.47523942589759827\n",
      "Epoch 15252: train loss: 0.4752393364906311\n",
      "Epoch 15253: train loss: 0.47523927688598633\n",
      "Epoch 15254: train loss: 0.4752391576766968\n",
      "Epoch 15255: train loss: 0.4752391576766968\n",
      "Epoch 15256: train loss: 0.4752390682697296\n",
      "Epoch 15257: train loss: 0.47523900866508484\n",
      "Epoch 15258: train loss: 0.47523894906044006\n",
      "Epoch 15259: train loss: 0.4752388596534729\n",
      "Epoch 15260: train loss: 0.4752388596534729\n",
      "Epoch 15261: train loss: 0.47523874044418335\n",
      "Epoch 15262: train loss: 0.4752386510372162\n",
      "Epoch 15263: train loss: 0.4752385914325714\n",
      "Epoch 15264: train loss: 0.4752385914325714\n",
      "Epoch 15265: train loss: 0.47523853182792664\n",
      "Epoch 15266: train loss: 0.47523847222328186\n",
      "Epoch 15267: train loss: 0.4752383232116699\n",
      "Epoch 15268: train loss: 0.4752383232116699\n",
      "Epoch 15269: train loss: 0.47523826360702515\n",
      "Epoch 15270: train loss: 0.475238174200058\n",
      "Epoch 15271: train loss: 0.4752381145954132\n",
      "Epoch 15272: train loss: 0.47523805499076843\n",
      "Epoch 15273: train loss: 0.47523802518844604\n",
      "Epoch 15274: train loss: 0.4752379059791565\n",
      "Epoch 15275: train loss: 0.4752379059791565\n",
      "Epoch 15276: train loss: 0.47523781657218933\n",
      "Epoch 15277: train loss: 0.4752376973628998\n",
      "Epoch 15278: train loss: 0.4752376973628998\n",
      "Epoch 15279: train loss: 0.4752376079559326\n",
      "Epoch 15280: train loss: 0.47523754835128784\n",
      "Epoch 15281: train loss: 0.47523748874664307\n",
      "Epoch 15282: train loss: 0.4752374291419983\n",
      "Epoch 15283: train loss: 0.4752373993396759\n",
      "Epoch 15284: train loss: 0.47523728013038635\n",
      "Epoch 15285: train loss: 0.4752372205257416\n",
      "Epoch 15286: train loss: 0.4752371907234192\n",
      "Epoch 15287: train loss: 0.47523707151412964\n",
      "Epoch 15288: train loss: 0.47523701190948486\n",
      "Epoch 15289: train loss: 0.4752369821071625\n",
      "Epoch 15290: train loss: 0.4752368628978729\n",
      "Epoch 15291: train loss: 0.47523680329322815\n",
      "Epoch 15292: train loss: 0.47523677349090576\n",
      "Epoch 15293: train loss: 0.47523677349090576\n",
      "Epoch 15294: train loss: 0.4752366542816162\n",
      "Epoch 15295: train loss: 0.47523659467697144\n",
      "Epoch 15296: train loss: 0.47523656487464905\n",
      "Epoch 15297: train loss: 0.4752364456653595\n",
      "Epoch 15298: train loss: 0.4752364456653595\n",
      "Epoch 15299: train loss: 0.47523629665374756\n",
      "Epoch 15300: train loss: 0.47523629665374756\n",
      "Epoch 15301: train loss: 0.475236177444458\n",
      "Epoch 15302: train loss: 0.4752361476421356\n",
      "Epoch 15303: train loss: 0.47523608803749084\n",
      "Epoch 15304: train loss: 0.47523602843284607\n",
      "Epoch 15305: train loss: 0.4752359390258789\n",
      "Epoch 15306: train loss: 0.4752359390258789\n",
      "Epoch 15307: train loss: 0.47523581981658936\n",
      "Epoch 15308: train loss: 0.47523581981658936\n",
      "Epoch 15309: train loss: 0.4752357304096222\n",
      "Epoch 15310: train loss: 0.47523561120033264\n",
      "Epoch 15311: train loss: 0.47523561120033264\n",
      "Epoch 15312: train loss: 0.4752355217933655\n",
      "Epoch 15313: train loss: 0.4752354025840759\n",
      "Epoch 15314: train loss: 0.4752354025840759\n",
      "Epoch 15315: train loss: 0.47523531317710876\n",
      "Epoch 15316: train loss: 0.475235253572464\n",
      "Epoch 15317: train loss: 0.4752351939678192\n",
      "Epoch 15318: train loss: 0.47523513436317444\n",
      "Epoch 15319: train loss: 0.47523510456085205\n",
      "Epoch 15320: train loss: 0.4752350449562073\n",
      "Epoch 15321: train loss: 0.4752349257469177\n",
      "Epoch 15322: train loss: 0.47523483633995056\n",
      "Epoch 15323: train loss: 0.47523483633995056\n",
      "Epoch 15324: train loss: 0.4752347767353058\n",
      "Epoch 15325: train loss: 0.47523465752601624\n",
      "Epoch 15326: train loss: 0.47523462772369385\n",
      "Epoch 15327: train loss: 0.4752345085144043\n",
      "Epoch 15328: train loss: 0.4752344489097595\n",
      "Epoch 15329: train loss: 0.47523441910743713\n",
      "Epoch 15330: train loss: 0.47523435950279236\n",
      "Epoch 15331: train loss: 0.4752342998981476\n",
      "Epoch 15332: train loss: 0.4752342104911804\n",
      "Epoch 15333: train loss: 0.4752342104911804\n",
      "Epoch 15334: train loss: 0.47523415088653564\n",
      "Epoch 15335: train loss: 0.4752340316772461\n",
      "Epoch 15336: train loss: 0.4752340018749237\n",
      "Epoch 15337: train loss: 0.47523394227027893\n",
      "Epoch 15338: train loss: 0.475233793258667\n",
      "Epoch 15339: train loss: 0.4752338230609894\n",
      "Epoch 15340: train loss: 0.4752337336540222\n",
      "Epoch 15341: train loss: 0.4752337336540222\n",
      "Epoch 15342: train loss: 0.47523361444473267\n",
      "Epoch 15343: train loss: 0.4752335250377655\n",
      "Epoch 15344: train loss: 0.4752334654331207\n",
      "Epoch 15345: train loss: 0.47523340582847595\n",
      "Epoch 15346: train loss: 0.47523337602615356\n",
      "Epoch 15347: train loss: 0.4752333164215088\n",
      "Epoch 15348: train loss: 0.47523319721221924\n",
      "Epoch 15349: train loss: 0.47523316740989685\n",
      "Epoch 15350: train loss: 0.4752331078052521\n",
      "Epoch 15351: train loss: 0.4752330482006073\n",
      "Epoch 15352: train loss: 0.47523295879364014\n",
      "Epoch 15353: train loss: 0.47523289918899536\n",
      "Epoch 15354: train loss: 0.4752328395843506\n",
      "Epoch 15355: train loss: 0.4752327799797058\n",
      "Epoch 15356: train loss: 0.4752327501773834\n",
      "Epoch 15357: train loss: 0.47523269057273865\n",
      "Epoch 15358: train loss: 0.4752325713634491\n",
      "Epoch 15359: train loss: 0.47523248195648193\n",
      "Epoch 15360: train loss: 0.47523242235183716\n",
      "Epoch 15361: train loss: 0.47523248195648193\n",
      "Epoch 15362: train loss: 0.4752323627471924\n",
      "Epoch 15363: train loss: 0.4752322733402252\n",
      "Epoch 15364: train loss: 0.47523221373558044\n",
      "Epoch 15365: train loss: 0.4752321243286133\n",
      "Epoch 15366: train loss: 0.4752320647239685\n",
      "Epoch 15367: train loss: 0.47523200511932373\n",
      "Epoch 15368: train loss: 0.47523194551467896\n",
      "Epoch 15369: train loss: 0.4752318561077118\n",
      "Epoch 15370: train loss: 0.4752318561077118\n",
      "Epoch 15371: train loss: 0.47523173689842224\n",
      "Epoch 15372: train loss: 0.47523170709609985\n",
      "Epoch 15373: train loss: 0.4752316474914551\n",
      "Epoch 15374: train loss: 0.4752315878868103\n",
      "Epoch 15375: train loss: 0.47523149847984314\n",
      "Epoch 15376: train loss: 0.47523149847984314\n",
      "Epoch 15377: train loss: 0.4752313792705536\n",
      "Epoch 15378: train loss: 0.4752313196659088\n",
      "Epoch 15379: train loss: 0.4752312898635864\n",
      "Epoch 15380: train loss: 0.4752311706542969\n",
      "Epoch 15381: train loss: 0.4752311706542969\n",
      "Epoch 15382: train loss: 0.4752310514450073\n",
      "Epoch 15383: train loss: 0.47523096203804016\n",
      "Epoch 15384: train loss: 0.4752309024333954\n",
      "Epoch 15385: train loss: 0.4752309024333954\n",
      "Epoch 15386: train loss: 0.4752308130264282\n",
      "Epoch 15387: train loss: 0.47523075342178345\n",
      "Epoch 15388: train loss: 0.47523069381713867\n",
      "Epoch 15389: train loss: 0.4752306044101715\n",
      "Epoch 15390: train loss: 0.4752306044101715\n",
      "Epoch 15391: train loss: 0.47523048520088196\n",
      "Epoch 15392: train loss: 0.4752304255962372\n",
      "Epoch 15393: train loss: 0.47523033618927\n",
      "Epoch 15394: train loss: 0.47523027658462524\n",
      "Epoch 15395: train loss: 0.47523021697998047\n",
      "Epoch 15396: train loss: 0.47523021697998047\n",
      "Epoch 15397: train loss: 0.4752301275730133\n",
      "Epoch 15398: train loss: 0.47523000836372375\n",
      "Epoch 15399: train loss: 0.47522997856140137\n",
      "Epoch 15400: train loss: 0.4752299189567566\n",
      "Epoch 15401: train loss: 0.4752298593521118\n",
      "Epoch 15402: train loss: 0.47522979974746704\n",
      "Epoch 15403: train loss: 0.4752297103404999\n",
      "Epoch 15404: train loss: 0.4752297103404999\n",
      "Epoch 15405: train loss: 0.4752296507358551\n",
      "Epoch 15406: train loss: 0.47522950172424316\n",
      "Epoch 15407: train loss: 0.47522950172424316\n",
      "Epoch 15408: train loss: 0.4752294421195984\n",
      "Epoch 15409: train loss: 0.4752293527126312\n",
      "Epoch 15410: train loss: 0.47522929310798645\n",
      "Epoch 15411: train loss: 0.4752292335033417\n",
      "Epoch 15412: train loss: 0.4752291738986969\n",
      "Epoch 15413: train loss: 0.4752291440963745\n",
      "Epoch 15414: train loss: 0.47522902488708496\n",
      "Epoch 15415: train loss: 0.47522902488708496\n",
      "Epoch 15416: train loss: 0.4752289354801178\n",
      "Epoch 15417: train loss: 0.47522881627082825\n",
      "Epoch 15418: train loss: 0.47522881627082825\n",
      "Epoch 15419: train loss: 0.4752287268638611\n",
      "Epoch 15420: train loss: 0.4752286672592163\n",
      "Epoch 15421: train loss: 0.47522854804992676\n",
      "Epoch 15422: train loss: 0.47522851824760437\n",
      "Epoch 15423: train loss: 0.4752284586429596\n",
      "Epoch 15424: train loss: 0.4752284586429596\n",
      "Epoch 15425: train loss: 0.47522830963134766\n",
      "Epoch 15426: train loss: 0.4752282500267029\n",
      "Epoch 15427: train loss: 0.47522813081741333\n",
      "Epoch 15428: train loss: 0.47522813081741333\n",
      "Epoch 15429: train loss: 0.47522810101509094\n",
      "Epoch 15430: train loss: 0.47522804141044617\n",
      "Epoch 15431: train loss: 0.4752279818058014\n",
      "Epoch 15432: train loss: 0.47522789239883423\n",
      "Epoch 15433: train loss: 0.47522783279418945\n",
      "Epoch 15434: train loss: 0.4752277731895447\n",
      "Epoch 15435: train loss: 0.4752277135848999\n",
      "Epoch 15436: train loss: 0.47522762417793274\n",
      "Epoch 15437: train loss: 0.47522756457328796\n",
      "Epoch 15438: train loss: 0.4752275049686432\n",
      "Epoch 15439: train loss: 0.475227415561676\n",
      "Epoch 15440: train loss: 0.47522735595703125\n",
      "Epoch 15441: train loss: 0.47522735595703125\n",
      "Epoch 15442: train loss: 0.4752272367477417\n",
      "Epoch 15443: train loss: 0.4752272367477417\n",
      "Epoch 15444: train loss: 0.47522708773612976\n",
      "Epoch 15445: train loss: 0.47522708773612976\n",
      "Epoch 15446: train loss: 0.475227028131485\n",
      "Epoch 15447: train loss: 0.4752269387245178\n",
      "Epoch 15448: train loss: 0.47522687911987305\n",
      "Epoch 15449: train loss: 0.47522681951522827\n",
      "Epoch 15450: train loss: 0.4752267301082611\n",
      "Epoch 15451: train loss: 0.47522667050361633\n",
      "Epoch 15452: train loss: 0.47522661089897156\n",
      "Epoch 15453: train loss: 0.47522658109664917\n",
      "Epoch 15454: train loss: 0.47522658109664917\n",
      "Epoch 15455: train loss: 0.47522640228271484\n",
      "Epoch 15456: train loss: 0.47522640228271484\n",
      "Epoch 15457: train loss: 0.4752263128757477\n",
      "Epoch 15458: train loss: 0.47522619366645813\n",
      "Epoch 15459: train loss: 0.47522619366645813\n",
      "Epoch 15460: train loss: 0.47522610425949097\n",
      "Epoch 15461: train loss: 0.4752260446548462\n",
      "Epoch 15462: train loss: 0.4752259850502014\n",
      "Epoch 15463: train loss: 0.47522589564323425\n",
      "Epoch 15464: train loss: 0.47522589564323425\n",
      "Epoch 15465: train loss: 0.4752257764339447\n",
      "Epoch 15466: train loss: 0.4752257466316223\n",
      "Epoch 15467: train loss: 0.47522568702697754\n",
      "Epoch 15468: train loss: 0.475225567817688\n",
      "Epoch 15469: train loss: 0.475225567817688\n",
      "Epoch 15470: train loss: 0.4752254784107208\n",
      "Epoch 15471: train loss: 0.47522541880607605\n",
      "Epoch 15472: train loss: 0.4752253592014313\n",
      "Epoch 15473: train loss: 0.4752253293991089\n",
      "Epoch 15474: train loss: 0.47522521018981934\n",
      "Epoch 15475: train loss: 0.47522515058517456\n",
      "Epoch 15476: train loss: 0.4752251207828522\n",
      "Epoch 15477: train loss: 0.4752250611782074\n",
      "Epoch 15478: train loss: 0.4752250015735626\n",
      "Epoch 15479: train loss: 0.47522491216659546\n",
      "Epoch 15480: train loss: 0.4752248525619507\n",
      "Epoch 15481: train loss: 0.4752247929573059\n",
      "Epoch 15482: train loss: 0.47522470355033875\n",
      "Epoch 15483: train loss: 0.47522464394569397\n",
      "Epoch 15484: train loss: 0.47522464394569397\n",
      "Epoch 15485: train loss: 0.4752245247364044\n",
      "Epoch 15486: train loss: 0.47522449493408203\n",
      "Epoch 15487: train loss: 0.47522443532943726\n",
      "Epoch 15488: train loss: 0.4752243161201477\n",
      "Epoch 15489: train loss: 0.4752243161201477\n",
      "Epoch 15490: train loss: 0.47522422671318054\n",
      "Epoch 15491: train loss: 0.47522416710853577\n",
      "Epoch 15492: train loss: 0.475224107503891\n",
      "Epoch 15493: train loss: 0.4752240777015686\n",
      "Epoch 15494: train loss: 0.47522395849227905\n",
      "Epoch 15495: train loss: 0.4752238988876343\n",
      "Epoch 15496: train loss: 0.4752238690853119\n",
      "Epoch 15497: train loss: 0.4752238094806671\n",
      "Epoch 15498: train loss: 0.47522369027137756\n",
      "Epoch 15499: train loss: 0.4752236604690552\n",
      "Epoch 15500: train loss: 0.4752236008644104\n",
      "Epoch 15501: train loss: 0.4752236008644104\n",
      "Epoch 15502: train loss: 0.47522348165512085\n",
      "Epoch 15503: train loss: 0.4752233922481537\n",
      "Epoch 15504: train loss: 0.4752233326435089\n",
      "Epoch 15505: train loss: 0.47522327303886414\n",
      "Epoch 15506: train loss: 0.475223183631897\n",
      "Epoch 15507: train loss: 0.475223183631897\n",
      "Epoch 15508: train loss: 0.4752230644226074\n",
      "Epoch 15509: train loss: 0.47522300481796265\n",
      "Epoch 15510: train loss: 0.47522297501564026\n",
      "Epoch 15511: train loss: 0.4752229154109955\n",
      "Epoch 15512: train loss: 0.4752228558063507\n",
      "Epoch 15513: train loss: 0.47522276639938354\n",
      "Epoch 15514: train loss: 0.47522270679473877\n",
      "Epoch 15515: train loss: 0.475222647190094\n",
      "Epoch 15516: train loss: 0.4752225875854492\n",
      "Epoch 15517: train loss: 0.47522255778312683\n",
      "Epoch 15518: train loss: 0.4752224385738373\n",
      "Epoch 15519: train loss: 0.4752223789691925\n",
      "Epoch 15520: train loss: 0.4752223491668701\n",
      "Epoch 15521: train loss: 0.47522222995758057\n",
      "Epoch 15522: train loss: 0.47522222995758057\n",
      "Epoch 15523: train loss: 0.4752221405506134\n",
      "Epoch 15524: train loss: 0.47522208094596863\n",
      "Epoch 15525: train loss: 0.47522202134132385\n",
      "Epoch 15526: train loss: 0.4752219319343567\n",
      "Epoch 15527: train loss: 0.4752219319343567\n",
      "Epoch 15528: train loss: 0.47522181272506714\n",
      "Epoch 15529: train loss: 0.47522175312042236\n",
      "Epoch 15530: train loss: 0.4752216637134552\n",
      "Epoch 15531: train loss: 0.4752216041088104\n",
      "Epoch 15532: train loss: 0.47522154450416565\n",
      "Epoch 15533: train loss: 0.47522151470184326\n",
      "Epoch 15534: train loss: 0.4752214550971985\n",
      "Epoch 15535: train loss: 0.47522133588790894\n",
      "Epoch 15536: train loss: 0.47522133588790894\n",
      "Epoch 15537: train loss: 0.4752212464809418\n",
      "Epoch 15538: train loss: 0.475221186876297\n",
      "Epoch 15539: train loss: 0.47522109746932983\n",
      "Epoch 15540: train loss: 0.47522109746932983\n",
      "Epoch 15541: train loss: 0.47522103786468506\n",
      "Epoch 15542: train loss: 0.4752208888530731\n",
      "Epoch 15543: train loss: 0.4752208888530731\n",
      "Epoch 15544: train loss: 0.47522076964378357\n",
      "Epoch 15545: train loss: 0.47522076964378357\n",
      "Epoch 15546: train loss: 0.4752206802368164\n",
      "Epoch 15547: train loss: 0.47522062063217163\n",
      "Epoch 15548: train loss: 0.47522056102752686\n",
      "Epoch 15549: train loss: 0.4752205014228821\n",
      "Epoch 15550: train loss: 0.4752204120159149\n",
      "Epoch 15551: train loss: 0.47522035241127014\n",
      "Epoch 15552: train loss: 0.47522029280662537\n",
      "Epoch 15553: train loss: 0.475220263004303\n",
      "Epoch 15554: train loss: 0.4752202033996582\n",
      "Epoch 15555: train loss: 0.4752201437950134\n",
      "Epoch 15556: train loss: 0.47522008419036865\n",
      "Epoch 15557: train loss: 0.4752199947834015\n",
      "Epoch 15558: train loss: 0.4752199351787567\n",
      "Epoch 15559: train loss: 0.47521987557411194\n",
      "Epoch 15560: train loss: 0.4752197861671448\n",
      "Epoch 15561: train loss: 0.4752197265625\n",
      "Epoch 15562: train loss: 0.4752196669578552\n",
      "Epoch 15563: train loss: 0.47521957755088806\n",
      "Epoch 15564: train loss: 0.47521957755088806\n",
      "Epoch 15565: train loss: 0.4752194583415985\n",
      "Epoch 15566: train loss: 0.47521939873695374\n",
      "Epoch 15567: train loss: 0.47521936893463135\n",
      "Epoch 15568: train loss: 0.4752193093299866\n",
      "Epoch 15569: train loss: 0.475219190120697\n",
      "Epoch 15570: train loss: 0.47521916031837463\n",
      "Epoch 15571: train loss: 0.47521910071372986\n",
      "Epoch 15572: train loss: 0.4752190411090851\n",
      "Epoch 15573: train loss: 0.4752189815044403\n",
      "Epoch 15574: train loss: 0.47521889209747314\n",
      "Epoch 15575: train loss: 0.47521883249282837\n",
      "Epoch 15576: train loss: 0.4752187728881836\n",
      "Epoch 15577: train loss: 0.4752187430858612\n",
      "Epoch 15578: train loss: 0.47521868348121643\n",
      "Epoch 15579: train loss: 0.47521862387657166\n",
      "Epoch 15580: train loss: 0.4752185344696045\n",
      "Epoch 15581: train loss: 0.4752184748649597\n",
      "Epoch 15582: train loss: 0.47521835565567017\n",
      "Epoch 15583: train loss: 0.47521835565567017\n",
      "Epoch 15584: train loss: 0.475218266248703\n",
      "Epoch 15585: train loss: 0.475218266248703\n",
      "Epoch 15586: train loss: 0.47521814703941345\n",
      "Epoch 15587: train loss: 0.4752180576324463\n",
      "Epoch 15588: train loss: 0.4752179980278015\n",
      "Epoch 15589: train loss: 0.4752179980278015\n",
      "Epoch 15590: train loss: 0.47521790862083435\n",
      "Epoch 15591: train loss: 0.4752178490161896\n",
      "Epoch 15592: train loss: 0.4752177298069\n",
      "Epoch 15593: train loss: 0.4752177298069\n",
      "Epoch 15594: train loss: 0.47521770000457764\n",
      "Epoch 15595: train loss: 0.4752175807952881\n",
      "Epoch 15596: train loss: 0.4752175211906433\n",
      "Epoch 15597: train loss: 0.4752174913883209\n",
      "Epoch 15598: train loss: 0.47521743178367615\n",
      "Epoch 15599: train loss: 0.47521737217903137\n",
      "Epoch 15600: train loss: 0.4752172827720642\n",
      "Epoch 15601: train loss: 0.47521722316741943\n",
      "Epoch 15602: train loss: 0.47521716356277466\n",
      "Epoch 15603: train loss: 0.47521716356277466\n",
      "Epoch 15604: train loss: 0.4752170741558075\n",
      "Epoch 15605: train loss: 0.47521695494651794\n",
      "Epoch 15606: train loss: 0.47521689534187317\n",
      "Epoch 15607: train loss: 0.4752168655395508\n",
      "Epoch 15608: train loss: 0.475216805934906\n",
      "Epoch 15609: train loss: 0.47521674633026123\n",
      "Epoch 15610: train loss: 0.47521665692329407\n",
      "Epoch 15611: train loss: 0.47521665692329407\n",
      "Epoch 15612: train loss: 0.4752165377140045\n",
      "Epoch 15613: train loss: 0.47521644830703735\n",
      "Epoch 15614: train loss: 0.4752163887023926\n",
      "Epoch 15615: train loss: 0.4752163887023926\n",
      "Epoch 15616: train loss: 0.475216269493103\n",
      "Epoch 15617: train loss: 0.47521618008613586\n",
      "Epoch 15618: train loss: 0.4752161204814911\n",
      "Epoch 15619: train loss: 0.4752160608768463\n",
      "Epoch 15620: train loss: 0.4752160310745239\n",
      "Epoch 15621: train loss: 0.47521597146987915\n",
      "Epoch 15622: train loss: 0.4752158522605896\n",
      "Epoch 15623: train loss: 0.4752158522605896\n",
      "Epoch 15624: train loss: 0.4752157926559448\n",
      "Epoch 15625: train loss: 0.47521570324897766\n",
      "Epoch 15626: train loss: 0.4752155840396881\n",
      "Epoch 15627: train loss: 0.4752155840396881\n",
      "Epoch 15628: train loss: 0.4752155542373657\n",
      "Epoch 15629: train loss: 0.47521543502807617\n",
      "Epoch 15630: train loss: 0.4752153754234314\n",
      "Epoch 15631: train loss: 0.475215345621109\n",
      "Epoch 15632: train loss: 0.47521522641181946\n",
      "Epoch 15633: train loss: 0.47521522641181946\n",
      "Epoch 15634: train loss: 0.4752151370048523\n",
      "Epoch 15635: train loss: 0.4752150774002075\n",
      "Epoch 15636: train loss: 0.47521501779556274\n",
      "Epoch 15637: train loss: 0.47521495819091797\n",
      "Epoch 15638: train loss: 0.4752149283885956\n",
      "Epoch 15639: train loss: 0.47521480917930603\n",
      "Epoch 15640: train loss: 0.47521474957466125\n",
      "Epoch 15641: train loss: 0.47521471977233887\n",
      "Epoch 15642: train loss: 0.4752146005630493\n",
      "Epoch 15643: train loss: 0.4752146005630493\n",
      "Epoch 15644: train loss: 0.47521451115608215\n",
      "Epoch 15645: train loss: 0.4752144515514374\n",
      "Epoch 15646: train loss: 0.4752143919467926\n",
      "Epoch 15647: train loss: 0.47521430253982544\n",
      "Epoch 15648: train loss: 0.47521430253982544\n",
      "Epoch 15649: train loss: 0.4752141833305359\n",
      "Epoch 15650: train loss: 0.4752141237258911\n",
      "Epoch 15651: train loss: 0.4752140939235687\n",
      "Epoch 15652: train loss: 0.4752139747142792\n",
      "Epoch 15653: train loss: 0.4752139151096344\n",
      "Epoch 15654: train loss: 0.475213885307312\n",
      "Epoch 15655: train loss: 0.47521376609802246\n",
      "Epoch 15656: train loss: 0.47521376609802246\n",
      "Epoch 15657: train loss: 0.4752137064933777\n",
      "Epoch 15658: train loss: 0.4752136170864105\n",
      "Epoch 15659: train loss: 0.47521355748176575\n",
      "Epoch 15660: train loss: 0.47521349787712097\n",
      "Epoch 15661: train loss: 0.4752134680747986\n",
      "Epoch 15662: train loss: 0.47521334886550903\n",
      "Epoch 15663: train loss: 0.47521334886550903\n",
      "Epoch 15664: train loss: 0.47521328926086426\n",
      "Epoch 15665: train loss: 0.47521325945854187\n",
      "Epoch 15666: train loss: 0.4752131402492523\n",
      "Epoch 15667: train loss: 0.47521305084228516\n",
      "Epoch 15668: train loss: 0.4752129912376404\n",
      "Epoch 15669: train loss: 0.4752129316329956\n",
      "Epoch 15670: train loss: 0.47521287202835083\n",
      "Epoch 15671: train loss: 0.47521284222602844\n",
      "Epoch 15672: train loss: 0.47521278262138367\n",
      "Epoch 15673: train loss: 0.4752127230167389\n",
      "Epoch 15674: train loss: 0.47521263360977173\n",
      "Epoch 15675: train loss: 0.47521257400512695\n",
      "Epoch 15676: train loss: 0.4752125144004822\n",
      "Epoch 15677: train loss: 0.475212424993515\n",
      "Epoch 15678: train loss: 0.47521236538887024\n",
      "Epoch 15679: train loss: 0.47521230578422546\n",
      "Epoch 15680: train loss: 0.4752122461795807\n",
      "Epoch 15681: train loss: 0.4752122163772583\n",
      "Epoch 15682: train loss: 0.4752121567726135\n",
      "Epoch 15683: train loss: 0.475212037563324\n",
      "Epoch 15684: train loss: 0.475212037563324\n",
      "Epoch 15685: train loss: 0.4752119779586792\n",
      "Epoch 15686: train loss: 0.47521188855171204\n",
      "Epoch 15687: train loss: 0.47521182894706726\n",
      "Epoch 15688: train loss: 0.4752117693424225\n",
      "Epoch 15689: train loss: 0.4752116799354553\n",
      "Epoch 15690: train loss: 0.47521162033081055\n",
      "Epoch 15691: train loss: 0.47521156072616577\n",
      "Epoch 15692: train loss: 0.4752115309238434\n",
      "Epoch 15693: train loss: 0.47521141171455383\n",
      "Epoch 15694: train loss: 0.47521135210990906\n",
      "Epoch 15695: train loss: 0.47521132230758667\n",
      "Epoch 15696: train loss: 0.4752112627029419\n",
      "Epoch 15697: train loss: 0.4752112030982971\n",
      "Epoch 15698: train loss: 0.47521111369132996\n",
      "Epoch 15699: train loss: 0.4752110540866852\n",
      "Epoch 15700: train loss: 0.4752109944820404\n",
      "Epoch 15701: train loss: 0.47521093487739563\n",
      "Epoch 15702: train loss: 0.47521090507507324\n",
      "Epoch 15703: train loss: 0.4752107858657837\n",
      "Epoch 15704: train loss: 0.4752107262611389\n",
      "Epoch 15705: train loss: 0.47521063685417175\n",
      "Epoch 15706: train loss: 0.47521063685417175\n",
      "Epoch 15707: train loss: 0.475210577249527\n",
      "Epoch 15708: train loss: 0.4752105176448822\n",
      "Epoch 15709: train loss: 0.47521042823791504\n",
      "Epoch 15710: train loss: 0.47521042823791504\n",
      "Epoch 15711: train loss: 0.4752103090286255\n",
      "Epoch 15712: train loss: 0.4752102196216583\n",
      "Epoch 15713: train loss: 0.4752102196216583\n",
      "Epoch 15714: train loss: 0.47521016001701355\n",
      "Epoch 15715: train loss: 0.4752100706100464\n",
      "Epoch 15716: train loss: 0.4752100110054016\n",
      "Epoch 15717: train loss: 0.47520989179611206\n",
      "Epoch 15718: train loss: 0.4752098619937897\n",
      "Epoch 15719: train loss: 0.4752098023891449\n",
      "Epoch 15720: train loss: 0.4752097427845001\n",
      "Epoch 15721: train loss: 0.47520965337753296\n",
      "Epoch 15722: train loss: 0.47520965337753296\n",
      "Epoch 15723: train loss: 0.4752095341682434\n",
      "Epoch 15724: train loss: 0.47520947456359863\n",
      "Epoch 15725: train loss: 0.47520944476127625\n",
      "Epoch 15726: train loss: 0.47520938515663147\n",
      "Epoch 15727: train loss: 0.4752092659473419\n",
      "Epoch 15728: train loss: 0.47520923614501953\n",
      "Epoch 15729: train loss: 0.47520917654037476\n",
      "Epoch 15730: train loss: 0.47520917654037476\n",
      "Epoch 15731: train loss: 0.4752090573310852\n",
      "Epoch 15732: train loss: 0.47520896792411804\n",
      "Epoch 15733: train loss: 0.4752088487148285\n",
      "Epoch 15734: train loss: 0.4752088487148285\n",
      "Epoch 15735: train loss: 0.4752088189125061\n",
      "Epoch 15736: train loss: 0.47520875930786133\n",
      "Epoch 15737: train loss: 0.4752086400985718\n",
      "Epoch 15738: train loss: 0.4752086400985718\n",
      "Epoch 15739: train loss: 0.4752085506916046\n",
      "Epoch 15740: train loss: 0.47520849108695984\n",
      "Epoch 15741: train loss: 0.4752084016799927\n",
      "Epoch 15742: train loss: 0.4752084016799927\n",
      "Epoch 15743: train loss: 0.4752083420753479\n",
      "Epoch 15744: train loss: 0.47520822286605835\n",
      "Epoch 15745: train loss: 0.4752081632614136\n",
      "Epoch 15746: train loss: 0.4752081334590912\n",
      "Epoch 15747: train loss: 0.4752080738544464\n",
      "Epoch 15748: train loss: 0.47520801424980164\n",
      "Epoch 15749: train loss: 0.4752079248428345\n",
      "Epoch 15750: train loss: 0.4752078652381897\n",
      "Epoch 15751: train loss: 0.4752078652381897\n",
      "Epoch 15752: train loss: 0.47520771622657776\n",
      "Epoch 15753: train loss: 0.47520771622657776\n",
      "Epoch 15754: train loss: 0.4752075970172882\n",
      "Epoch 15755: train loss: 0.47520753741264343\n",
      "Epoch 15756: train loss: 0.47520750761032104\n",
      "Epoch 15757: train loss: 0.47520744800567627\n",
      "Epoch 15758: train loss: 0.4752073884010315\n",
      "Epoch 15759: train loss: 0.4752073287963867\n",
      "Epoch 15760: train loss: 0.47520723938941956\n",
      "Epoch 15761: train loss: 0.4752071797847748\n",
      "Epoch 15762: train loss: 0.4752070903778076\n",
      "Epoch 15763: train loss: 0.4752070903778076\n",
      "Epoch 15764: train loss: 0.47520697116851807\n",
      "Epoch 15765: train loss: 0.4752069115638733\n",
      "Epoch 15766: train loss: 0.4752068817615509\n",
      "Epoch 15767: train loss: 0.47520682215690613\n",
      "Epoch 15768: train loss: 0.47520676255226135\n",
      "Epoch 15769: train loss: 0.4752066731452942\n",
      "Epoch 15770: train loss: 0.47520655393600464\n",
      "Epoch 15771: train loss: 0.47520655393600464\n",
      "Epoch 15772: train loss: 0.4752064645290375\n",
      "Epoch 15773: train loss: 0.4752064645290375\n",
      "Epoch 15774: train loss: 0.4752063453197479\n",
      "Epoch 15775: train loss: 0.4752063453197479\n",
      "Epoch 15776: train loss: 0.47520625591278076\n",
      "Epoch 15777: train loss: 0.4752061367034912\n",
      "Epoch 15778: train loss: 0.4752061367034912\n",
      "Epoch 15779: train loss: 0.47520604729652405\n",
      "Epoch 15780: train loss: 0.47520604729652405\n",
      "Epoch 15781: train loss: 0.4752059280872345\n",
      "Epoch 15782: train loss: 0.4752058684825897\n",
      "Epoch 15783: train loss: 0.47520583868026733\n",
      "Epoch 15784: train loss: 0.4752057194709778\n",
      "Epoch 15785: train loss: 0.475205659866333\n",
      "Epoch 15786: train loss: 0.4752056300640106\n",
      "Epoch 15787: train loss: 0.47520557045936584\n",
      "Epoch 15788: train loss: 0.47520551085472107\n",
      "Epoch 15789: train loss: 0.4752054214477539\n",
      "Epoch 15790: train loss: 0.47520536184310913\n",
      "Epoch 15791: train loss: 0.47520530223846436\n",
      "Epoch 15792: train loss: 0.4752052128314972\n",
      "Epoch 15793: train loss: 0.4752051532268524\n",
      "Epoch 15794: train loss: 0.47520509362220764\n",
      "Epoch 15795: train loss: 0.47520503401756287\n",
      "Epoch 15796: train loss: 0.4752050042152405\n",
      "Epoch 15797: train loss: 0.4752049446105957\n",
      "Epoch 15798: train loss: 0.4752048850059509\n",
      "Epoch 15799: train loss: 0.47520482540130615\n",
      "Epoch 15800: train loss: 0.475204735994339\n",
      "Epoch 15801: train loss: 0.4752046763896942\n",
      "Epoch 15802: train loss: 0.47520458698272705\n",
      "Epoch 15803: train loss: 0.47520458698272705\n",
      "Epoch 15804: train loss: 0.4752044677734375\n",
      "Epoch 15805: train loss: 0.4752044081687927\n",
      "Epoch 15806: train loss: 0.47520434856414795\n",
      "Epoch 15807: train loss: 0.47520431876182556\n",
      "Epoch 15808: train loss: 0.4752042591571808\n",
      "Epoch 15809: train loss: 0.47520413994789124\n",
      "Epoch 15810: train loss: 0.47520411014556885\n",
      "Epoch 15811: train loss: 0.4752040505409241\n",
      "Epoch 15812: train loss: 0.4752039909362793\n",
      "Epoch 15813: train loss: 0.4752039313316345\n",
      "Epoch 15814: train loss: 0.47520390152931213\n",
      "Epoch 15815: train loss: 0.4752037823200226\n",
      "Epoch 15816: train loss: 0.4752037227153778\n",
      "Epoch 15817: train loss: 0.4752036929130554\n",
      "Epoch 15818: train loss: 0.47520363330841064\n",
      "Epoch 15819: train loss: 0.47520357370376587\n",
      "Epoch 15820: train loss: 0.4752035140991211\n",
      "Epoch 15821: train loss: 0.47520342469215393\n",
      "Epoch 15822: train loss: 0.47520342469215393\n",
      "Epoch 15823: train loss: 0.4752033054828644\n",
      "Epoch 15824: train loss: 0.4752032160758972\n",
      "Epoch 15825: train loss: 0.4752032160758972\n",
      "Epoch 15826: train loss: 0.47520315647125244\n",
      "Epoch 15827: train loss: 0.4752030074596405\n",
      "Epoch 15828: train loss: 0.4752029478549957\n",
      "Epoch 15829: train loss: 0.47520288825035095\n",
      "Epoch 15830: train loss: 0.47520285844802856\n",
      "Epoch 15831: train loss: 0.4752027988433838\n",
      "Epoch 15832: train loss: 0.475202739238739\n",
      "Epoch 15833: train loss: 0.47520267963409424\n",
      "Epoch 15834: train loss: 0.47520264983177185\n",
      "Epoch 15835: train loss: 0.4752025306224823\n",
      "Epoch 15836: train loss: 0.4752024710178375\n",
      "Epoch 15837: train loss: 0.47520244121551514\n",
      "Epoch 15838: train loss: 0.4752023220062256\n",
      "Epoch 15839: train loss: 0.4752023220062256\n",
      "Epoch 15840: train loss: 0.4752022624015808\n",
      "Epoch 15841: train loss: 0.47520217299461365\n",
      "Epoch 15842: train loss: 0.47520211338996887\n",
      "Epoch 15843: train loss: 0.4752020537853241\n",
      "Epoch 15844: train loss: 0.47520196437835693\n",
      "Epoch 15845: train loss: 0.47520190477371216\n",
      "Epoch 15846: train loss: 0.4752018451690674\n",
      "Epoch 15847: train loss: 0.475201815366745\n",
      "Epoch 15848: train loss: 0.4752017557621002\n",
      "Epoch 15849: train loss: 0.47520169615745544\n",
      "Epoch 15850: train loss: 0.47520163655281067\n",
      "Epoch 15851: train loss: 0.4752015471458435\n",
      "Epoch 15852: train loss: 0.47520148754119873\n",
      "Epoch 15853: train loss: 0.47520139813423157\n",
      "Epoch 15854: train loss: 0.4752013385295868\n",
      "Epoch 15855: train loss: 0.4752013385295868\n",
      "Epoch 15856: train loss: 0.47520121932029724\n",
      "Epoch 15857: train loss: 0.47520118951797485\n",
      "Epoch 15858: train loss: 0.4752011299133301\n",
      "Epoch 15859: train loss: 0.4752010703086853\n",
      "Epoch 15860: train loss: 0.4752010107040405\n",
      "Epoch 15861: train loss: 0.47520092129707336\n",
      "Epoch 15862: train loss: 0.47520092129707336\n",
      "Epoch 15863: train loss: 0.4752008020877838\n",
      "Epoch 15864: train loss: 0.4752007722854614\n",
      "Epoch 15865: train loss: 0.47520071268081665\n",
      "Epoch 15866: train loss: 0.4752005934715271\n",
      "Epoch 15867: train loss: 0.4752005338668823\n",
      "Epoch 15868: train loss: 0.47520044445991516\n",
      "Epoch 15869: train loss: 0.4752003848552704\n",
      "Epoch 15870: train loss: 0.4752003848552704\n",
      "Epoch 15871: train loss: 0.4752002954483032\n",
      "Epoch 15872: train loss: 0.4752002954483032\n",
      "Epoch 15873: train loss: 0.47520017623901367\n",
      "Epoch 15874: train loss: 0.4752001166343689\n",
      "Epoch 15875: train loss: 0.4752000868320465\n",
      "Epoch 15876: train loss: 0.47519996762275696\n",
      "Epoch 15877: train loss: 0.4751999080181122\n",
      "Epoch 15878: train loss: 0.4751998782157898\n",
      "Epoch 15879: train loss: 0.475199818611145\n",
      "Epoch 15880: train loss: 0.47519975900650024\n",
      "Epoch 15881: train loss: 0.4751996695995331\n",
      "Epoch 15882: train loss: 0.4751996099948883\n",
      "Epoch 15883: train loss: 0.47519955039024353\n",
      "Epoch 15884: train loss: 0.47519949078559875\n",
      "Epoch 15885: train loss: 0.47519946098327637\n",
      "Epoch 15886: train loss: 0.4751993417739868\n",
      "Epoch 15887: train loss: 0.47519925236701965\n",
      "Epoch 15888: train loss: 0.47519925236701965\n",
      "Epoch 15889: train loss: 0.4751991927623749\n",
      "Epoch 15890: train loss: 0.4751990735530853\n",
      "Epoch 15891: train loss: 0.4751990735530853\n",
      "Epoch 15892: train loss: 0.47519898414611816\n",
      "Epoch 15893: train loss: 0.4751989245414734\n",
      "Epoch 15894: train loss: 0.4751988649368286\n",
      "Epoch 15895: train loss: 0.4751988351345062\n",
      "Epoch 15896: train loss: 0.47519877552986145\n",
      "Epoch 15897: train loss: 0.4751986563205719\n",
      "Epoch 15898: train loss: 0.4751986265182495\n",
      "Epoch 15899: train loss: 0.47519856691360474\n",
      "Epoch 15900: train loss: 0.47519850730895996\n",
      "Epoch 15901: train loss: 0.4751984477043152\n",
      "Epoch 15902: train loss: 0.4751984179019928\n",
      "Epoch 15903: train loss: 0.47519829869270325\n",
      "Epoch 15904: train loss: 0.47519823908805847\n",
      "Epoch 15905: train loss: 0.4751982092857361\n",
      "Epoch 15906: train loss: 0.4751981496810913\n",
      "Epoch 15907: train loss: 0.47519809007644653\n",
      "Epoch 15908: train loss: 0.47519800066947937\n",
      "Epoch 15909: train loss: 0.47519800066947937\n",
      "Epoch 15910: train loss: 0.4751978814601898\n",
      "Epoch 15911: train loss: 0.47519782185554504\n",
      "Epoch 15912: train loss: 0.47519779205322266\n",
      "Epoch 15913: train loss: 0.4751976728439331\n",
      "Epoch 15914: train loss: 0.47519761323928833\n",
      "Epoch 15915: train loss: 0.47519758343696594\n",
      "Epoch 15916: train loss: 0.4751974642276764\n",
      "Epoch 15917: train loss: 0.4751974046230316\n",
      "Epoch 15918: train loss: 0.4751974046230316\n",
      "Epoch 15919: train loss: 0.47519731521606445\n",
      "Epoch 15920: train loss: 0.4751972556114197\n",
      "Epoch 15921: train loss: 0.4751971662044525\n",
      "Epoch 15922: train loss: 0.47519710659980774\n",
      "Epoch 15923: train loss: 0.47519704699516296\n",
      "Epoch 15924: train loss: 0.4751969575881958\n",
      "Epoch 15925: train loss: 0.4751969575881958\n",
      "Epoch 15926: train loss: 0.475196897983551\n",
      "Epoch 15927: train loss: 0.47519683837890625\n",
      "Epoch 15928: train loss: 0.4751967191696167\n",
      "Epoch 15929: train loss: 0.4751966893672943\n",
      "Epoch 15930: train loss: 0.4751966893672943\n",
      "Epoch 15931: train loss: 0.47519651055336\n",
      "Epoch 15932: train loss: 0.47519651055336\n",
      "Epoch 15933: train loss: 0.4751964807510376\n",
      "Epoch 15934: train loss: 0.4751964211463928\n",
      "Epoch 15935: train loss: 0.47519630193710327\n",
      "Epoch 15936: train loss: 0.4751962721347809\n",
      "Epoch 15937: train loss: 0.4751962125301361\n",
      "Epoch 15938: train loss: 0.47519609332084656\n",
      "Epoch 15939: train loss: 0.47519606351852417\n",
      "Epoch 15940: train loss: 0.4751960039138794\n",
      "Epoch 15941: train loss: 0.4751959443092346\n",
      "Epoch 15942: train loss: 0.47519588470458984\n",
      "Epoch 15943: train loss: 0.4751957952976227\n",
      "Epoch 15944: train loss: 0.4751957952976227\n",
      "Epoch 15945: train loss: 0.47519567608833313\n",
      "Epoch 15946: train loss: 0.47519564628601074\n",
      "Epoch 15947: train loss: 0.47519558668136597\n",
      "Epoch 15948: train loss: 0.4751954674720764\n",
      "Epoch 15949: train loss: 0.4751954674720764\n",
      "Epoch 15950: train loss: 0.47519537806510925\n",
      "Epoch 15951: train loss: 0.4751952588558197\n",
      "Epoch 15952: train loss: 0.4751952588558197\n",
      "Epoch 15953: train loss: 0.4751952290534973\n",
      "Epoch 15954: train loss: 0.47519510984420776\n",
      "Epoch 15955: train loss: 0.47519510984420776\n",
      "Epoch 15956: train loss: 0.4751950204372406\n",
      "Epoch 15957: train loss: 0.4751949608325958\n",
      "Epoch 15958: train loss: 0.4751948416233063\n",
      "Epoch 15959: train loss: 0.4751948416233063\n",
      "Epoch 15960: train loss: 0.4751947522163391\n",
      "Epoch 15961: train loss: 0.47519469261169434\n",
      "Epoch 15962: train loss: 0.4751946032047272\n",
      "Epoch 15963: train loss: 0.4751946032047272\n",
      "Epoch 15964: train loss: 0.4751944839954376\n",
      "Epoch 15965: train loss: 0.47519442439079285\n",
      "Epoch 15966: train loss: 0.47519439458847046\n",
      "Epoch 15967: train loss: 0.4751943349838257\n",
      "Epoch 15968: train loss: 0.4751942753791809\n",
      "Epoch 15969: train loss: 0.47519421577453613\n",
      "Epoch 15970: train loss: 0.47519418597221375\n",
      "Epoch 15971: train loss: 0.47519412636756897\n",
      "Epoch 15972: train loss: 0.47519397735595703\n",
      "Epoch 15973: train loss: 0.47519397735595703\n",
      "Epoch 15974: train loss: 0.47519391775131226\n",
      "Epoch 15975: train loss: 0.4751938581466675\n",
      "Epoch 15976: train loss: 0.4751937687397003\n",
      "Epoch 15977: train loss: 0.4751937687397003\n",
      "Epoch 15978: train loss: 0.47519364953041077\n",
      "Epoch 15979: train loss: 0.475193589925766\n",
      "Epoch 15980: train loss: 0.4751935601234436\n",
      "Epoch 15981: train loss: 0.47519350051879883\n",
      "Epoch 15982: train loss: 0.4751933813095093\n",
      "Epoch 15983: train loss: 0.4751933515071869\n",
      "Epoch 15984: train loss: 0.4751932919025421\n",
      "Epoch 15985: train loss: 0.47519317269325256\n",
      "Epoch 15986: train loss: 0.4751931428909302\n",
      "Epoch 15987: train loss: 0.4751930832862854\n",
      "Epoch 15988: train loss: 0.4751930236816406\n",
      "Epoch 15989: train loss: 0.47519296407699585\n",
      "Epoch 15990: train loss: 0.4751928746700287\n",
      "Epoch 15991: train loss: 0.4751928746700287\n",
      "Epoch 15992: train loss: 0.47519275546073914\n",
      "Epoch 15993: train loss: 0.47519269585609436\n",
      "Epoch 15994: train loss: 0.475192666053772\n",
      "Epoch 15995: train loss: 0.4751925468444824\n",
      "Epoch 15996: train loss: 0.47519248723983765\n",
      "Epoch 15997: train loss: 0.47519245743751526\n",
      "Epoch 15998: train loss: 0.4751923978328705\n",
      "Epoch 15999: train loss: 0.4751923382282257\n",
      "Epoch 16000: train loss: 0.47519227862358093\n",
      "Epoch 16001: train loss: 0.47519218921661377\n",
      "Epoch 16002: train loss: 0.475192129611969\n",
      "Epoch 16003: train loss: 0.4751920700073242\n",
      "Epoch 16004: train loss: 0.47519198060035706\n",
      "Epoch 16005: train loss: 0.47519198060035706\n",
      "Epoch 16006: train loss: 0.4751919209957123\n",
      "Epoch 16007: train loss: 0.4751918613910675\n",
      "Epoch 16008: train loss: 0.47519177198410034\n",
      "Epoch 16009: train loss: 0.47519171237945557\n",
      "Epoch 16010: train loss: 0.4751916527748108\n",
      "Epoch 16011: train loss: 0.47519156336784363\n",
      "Epoch 16012: train loss: 0.47519156336784363\n",
      "Epoch 16013: train loss: 0.4751914441585541\n",
      "Epoch 16014: train loss: 0.4751914441585541\n",
      "Epoch 16015: train loss: 0.4751913547515869\n",
      "Epoch 16016: train loss: 0.47519129514694214\n",
      "Epoch 16017: train loss: 0.47519123554229736\n",
      "Epoch 16018: train loss: 0.4751911461353302\n",
      "Epoch 16019: train loss: 0.4751910865306854\n",
      "Epoch 16020: train loss: 0.47519102692604065\n",
      "Epoch 16021: train loss: 0.4751909375190735\n",
      "Epoch 16022: train loss: 0.4751908779144287\n",
      "Epoch 16023: train loss: 0.4751908779144287\n",
      "Epoch 16024: train loss: 0.47519078850746155\n",
      "Epoch 16025: train loss: 0.47519078850746155\n",
      "Epoch 16026: train loss: 0.475190669298172\n",
      "Epoch 16027: train loss: 0.47519057989120483\n",
      "Epoch 16028: train loss: 0.47519052028656006\n",
      "Epoch 16029: train loss: 0.4751904606819153\n",
      "Epoch 16030: train loss: 0.4751903712749481\n",
      "Epoch 16031: train loss: 0.4751904010772705\n",
      "Epoch 16032: train loss: 0.47519025206565857\n",
      "Epoch 16033: train loss: 0.47519025206565857\n",
      "Epoch 16034: train loss: 0.4751901626586914\n",
      "Epoch 16035: train loss: 0.47519004344940186\n",
      "Epoch 16036: train loss: 0.47519004344940186\n",
      "Epoch 16037: train loss: 0.4751899838447571\n",
      "Epoch 16038: train loss: 0.4751898944377899\n",
      "Epoch 16039: train loss: 0.47518983483314514\n",
      "Epoch 16040: train loss: 0.47518977522850037\n",
      "Epoch 16041: train loss: 0.475189745426178\n",
      "Epoch 16042: train loss: 0.475189745426178\n",
      "Epoch 16043: train loss: 0.4751896262168884\n",
      "Epoch 16044: train loss: 0.47518956661224365\n",
      "Epoch 16045: train loss: 0.47518953680992126\n",
      "Epoch 16046: train loss: 0.4751894176006317\n",
      "Epoch 16047: train loss: 0.47518935799598694\n",
      "Epoch 16048: train loss: 0.4751892685890198\n",
      "Epoch 16049: train loss: 0.475189208984375\n",
      "Epoch 16050: train loss: 0.475189208984375\n",
      "Epoch 16051: train loss: 0.4751891493797302\n",
      "Epoch 16052: train loss: 0.47518905997276306\n",
      "Epoch 16053: train loss: 0.4751890003681183\n",
      "Epoch 16054: train loss: 0.4751889407634735\n",
      "Epoch 16055: train loss: 0.47518888115882874\n",
      "Epoch 16056: train loss: 0.4751887917518616\n",
      "Epoch 16057: train loss: 0.4751887321472168\n",
      "Epoch 16058: train loss: 0.475188672542572\n",
      "Epoch 16059: train loss: 0.47518864274024963\n",
      "Epoch 16060: train loss: 0.47518858313560486\n",
      "Epoch 16061: train loss: 0.4751884639263153\n",
      "Epoch 16062: train loss: 0.4751884341239929\n",
      "Epoch 16063: train loss: 0.47518837451934814\n",
      "Epoch 16064: train loss: 0.47518831491470337\n",
      "Epoch 16065: train loss: 0.4751882553100586\n",
      "Epoch 16066: train loss: 0.4751882255077362\n",
      "Epoch 16067: train loss: 0.47518810629844666\n",
      "Epoch 16068: train loss: 0.4751880466938019\n",
      "Epoch 16069: train loss: 0.4751880168914795\n",
      "Epoch 16070: train loss: 0.4751879572868347\n",
      "Epoch 16071: train loss: 0.47518783807754517\n",
      "Epoch 16072: train loss: 0.47518783807754517\n",
      "Epoch 16073: train loss: 0.475187748670578\n",
      "Epoch 16074: train loss: 0.475187748670578\n",
      "Epoch 16075: train loss: 0.47518762946128845\n",
      "Epoch 16076: train loss: 0.4751875400543213\n",
      "Epoch 16077: train loss: 0.4751874804496765\n",
      "Epoch 16078: train loss: 0.47518742084503174\n",
      "Epoch 16079: train loss: 0.47518739104270935\n",
      "Epoch 16080: train loss: 0.4751873314380646\n",
      "Epoch 16081: train loss: 0.4751872718334198\n",
      "Epoch 16082: train loss: 0.47518718242645264\n",
      "Epoch 16083: train loss: 0.47518712282180786\n",
      "Epoch 16084: train loss: 0.4751870632171631\n",
      "Epoch 16085: train loss: 0.4751870036125183\n",
      "Epoch 16086: train loss: 0.47518691420555115\n",
      "Epoch 16087: train loss: 0.47518691420555115\n",
      "Epoch 16088: train loss: 0.4751867949962616\n",
      "Epoch 16089: train loss: 0.47518670558929443\n",
      "Epoch 16090: train loss: 0.47518670558929443\n",
      "Epoch 16091: train loss: 0.47518664598464966\n",
      "Epoch 16092: train loss: 0.4751865565776825\n",
      "Epoch 16093: train loss: 0.4751864969730377\n",
      "Epoch 16094: train loss: 0.47518643736839294\n",
      "Epoch 16095: train loss: 0.47518637776374817\n",
      "Epoch 16096: train loss: 0.475186288356781\n",
      "Epoch 16097: train loss: 0.475186288356781\n",
      "Epoch 16098: train loss: 0.47518616914749146\n",
      "Epoch 16099: train loss: 0.47518616914749146\n",
      "Epoch 16100: train loss: 0.4751860797405243\n",
      "Epoch 16101: train loss: 0.4751860201358795\n",
      "Epoch 16102: train loss: 0.47518593072891235\n",
      "Epoch 16103: train loss: 0.4751858711242676\n",
      "Epoch 16104: train loss: 0.4751858115196228\n",
      "Epoch 16105: train loss: 0.475185751914978\n",
      "Epoch 16106: train loss: 0.47518572211265564\n",
      "Epoch 16107: train loss: 0.47518566250801086\n",
      "Epoch 16108: train loss: 0.4751855432987213\n",
      "Epoch 16109: train loss: 0.4751855134963989\n",
      "Epoch 16110: train loss: 0.47518545389175415\n",
      "Epoch 16111: train loss: 0.4751853346824646\n",
      "Epoch 16112: train loss: 0.4751853346824646\n",
      "Epoch 16113: train loss: 0.4751852750778198\n",
      "Epoch 16114: train loss: 0.47518518567085266\n",
      "Epoch 16115: train loss: 0.4751851260662079\n",
      "Epoch 16116: train loss: 0.4751851260662079\n",
      "Epoch 16117: train loss: 0.4751850366592407\n",
      "Epoch 16118: train loss: 0.47518497705459595\n",
      "Epoch 16119: train loss: 0.47518491744995117\n",
      "Epoch 16120: train loss: 0.475184828042984\n",
      "Epoch 16121: train loss: 0.47518476843833923\n",
      "Epoch 16122: train loss: 0.47518470883369446\n",
      "Epoch 16123: train loss: 0.4751846194267273\n",
      "Epoch 16124: train loss: 0.4751846194267273\n",
      "Epoch 16125: train loss: 0.47518450021743774\n",
      "Epoch 16126: train loss: 0.47518444061279297\n",
      "Epoch 16127: train loss: 0.4751844108104706\n",
      "Epoch 16128: train loss: 0.4751843512058258\n",
      "Epoch 16129: train loss: 0.47518429160118103\n",
      "Epoch 16130: train loss: 0.47518423199653625\n",
      "Epoch 16131: train loss: 0.4751841425895691\n",
      "Epoch 16132: train loss: 0.4751840829849243\n",
      "Epoch 16133: train loss: 0.47518402338027954\n",
      "Epoch 16134: train loss: 0.4751839339733124\n",
      "Epoch 16135: train loss: 0.4751838743686676\n",
      "Epoch 16136: train loss: 0.4751838743686676\n",
      "Epoch 16137: train loss: 0.47518378496170044\n",
      "Epoch 16138: train loss: 0.47518372535705566\n",
      "Epoch 16139: train loss: 0.4751836657524109\n",
      "Epoch 16140: train loss: 0.4751836061477661\n",
      "Epoch 16141: train loss: 0.47518351674079895\n",
      "Epoch 16142: train loss: 0.47518351674079895\n",
      "Epoch 16143: train loss: 0.475183367729187\n",
      "Epoch 16144: train loss: 0.4751833975315094\n",
      "Epoch 16145: train loss: 0.47518330812454224\n",
      "Epoch 16146: train loss: 0.47518324851989746\n",
      "Epoch 16147: train loss: 0.4751831591129303\n",
      "Epoch 16148: train loss: 0.4751830995082855\n",
      "Epoch 16149: train loss: 0.47518303990364075\n",
      "Epoch 16150: train loss: 0.47518298029899597\n",
      "Epoch 16151: train loss: 0.4751828908920288\n",
      "Epoch 16152: train loss: 0.47518283128738403\n",
      "Epoch 16153: train loss: 0.47518283128738403\n",
      "Epoch 16154: train loss: 0.47518274188041687\n",
      "Epoch 16155: train loss: 0.4751826226711273\n",
      "Epoch 16156: train loss: 0.4751826226711273\n",
      "Epoch 16157: train loss: 0.47518253326416016\n",
      "Epoch 16158: train loss: 0.4751824736595154\n",
      "Epoch 16159: train loss: 0.47518235445022583\n",
      "Epoch 16160: train loss: 0.47518235445022583\n",
      "Epoch 16161: train loss: 0.47518232464790344\n",
      "Epoch 16162: train loss: 0.4751822054386139\n",
      "Epoch 16163: train loss: 0.4751821458339691\n",
      "Epoch 16164: train loss: 0.47518211603164673\n",
      "Epoch 16165: train loss: 0.47518211603164673\n",
      "Epoch 16166: train loss: 0.4751819968223572\n",
      "Epoch 16167: train loss: 0.4751819372177124\n",
      "Epoch 16168: train loss: 0.47518184781074524\n",
      "Epoch 16169: train loss: 0.47518178820610046\n",
      "Epoch 16170: train loss: 0.4751817286014557\n",
      "Epoch 16171: train loss: 0.4751816987991333\n",
      "Epoch 16172: train loss: 0.4751816391944885\n",
      "Epoch 16173: train loss: 0.47518157958984375\n",
      "Epoch 16174: train loss: 0.475181519985199\n",
      "Epoch 16175: train loss: 0.4751814603805542\n",
      "Epoch 16176: train loss: 0.47518137097358704\n",
      "Epoch 16177: train loss: 0.47518137097358704\n",
      "Epoch 16178: train loss: 0.4751812219619751\n",
      "Epoch 16179: train loss: 0.4751811623573303\n",
      "Epoch 16180: train loss: 0.4751811623573303\n",
      "Epoch 16181: train loss: 0.47518110275268555\n",
      "Epoch 16182: train loss: 0.4751810133457184\n",
      "Epoch 16183: train loss: 0.4751809537410736\n",
      "Epoch 16184: train loss: 0.47518083453178406\n",
      "Epoch 16185: train loss: 0.47518080472946167\n",
      "Epoch 16186: train loss: 0.4751807451248169\n",
      "Epoch 16187: train loss: 0.4751806855201721\n",
      "Epoch 16188: train loss: 0.47518062591552734\n",
      "Epoch 16189: train loss: 0.47518059611320496\n",
      "Epoch 16190: train loss: 0.4751804769039154\n",
      "Epoch 16191: train loss: 0.4751804769039154\n",
      "Epoch 16192: train loss: 0.47518038749694824\n",
      "Epoch 16193: train loss: 0.47518032789230347\n",
      "Epoch 16194: train loss: 0.4751802682876587\n",
      "Epoch 16195: train loss: 0.4751802086830139\n",
      "Epoch 16196: train loss: 0.47518011927604675\n",
      "Epoch 16197: train loss: 0.475180059671402\n",
      "Epoch 16198: train loss: 0.4751800000667572\n",
      "Epoch 16199: train loss: 0.4751799702644348\n",
      "Epoch 16200: train loss: 0.47517985105514526\n",
      "Epoch 16201: train loss: 0.4751797914505005\n",
      "Epoch 16202: train loss: 0.4751797616481781\n",
      "Epoch 16203: train loss: 0.47517964243888855\n",
      "Epoch 16204: train loss: 0.47517964243888855\n",
      "Epoch 16205: train loss: 0.4751795828342438\n",
      "Epoch 16206: train loss: 0.4751794934272766\n",
      "Epoch 16207: train loss: 0.4751794934272766\n",
      "Epoch 16208: train loss: 0.47517937421798706\n",
      "Epoch 16209: train loss: 0.4751793444156647\n",
      "Epoch 16210: train loss: 0.4751792848110199\n",
      "Epoch 16211: train loss: 0.4751792252063751\n",
      "Epoch 16212: train loss: 0.47517913579940796\n",
      "Epoch 16213: train loss: 0.4751790761947632\n",
      "Epoch 16214: train loss: 0.4751790165901184\n",
      "Epoch 16215: train loss: 0.47517895698547363\n",
      "Epoch 16216: train loss: 0.47517892718315125\n",
      "Epoch 16217: train loss: 0.4751788079738617\n",
      "Epoch 16218: train loss: 0.4751788079738617\n",
      "Epoch 16219: train loss: 0.47517871856689453\n",
      "Epoch 16220: train loss: 0.47517865896224976\n",
      "Epoch 16221: train loss: 0.475178599357605\n",
      "Epoch 16222: train loss: 0.4751785099506378\n",
      "Epoch 16223: train loss: 0.47517845034599304\n",
      "Epoch 16224: train loss: 0.47517845034599304\n",
      "Epoch 16225: train loss: 0.4751783311367035\n",
      "Epoch 16226: train loss: 0.4751783013343811\n",
      "Epoch 16227: train loss: 0.47517824172973633\n",
      "Epoch 16228: train loss: 0.47517818212509155\n",
      "Epoch 16229: train loss: 0.4751781225204468\n",
      "Epoch 16230: train loss: 0.4751780927181244\n",
      "Epoch 16231: train loss: 0.4751780331134796\n",
      "Epoch 16232: train loss: 0.47517791390419006\n",
      "Epoch 16233: train loss: 0.4751778841018677\n",
      "Epoch 16234: train loss: 0.4751778244972229\n",
      "Epoch 16235: train loss: 0.47517770528793335\n",
      "Epoch 16236: train loss: 0.4751776456832886\n",
      "Epoch 16237: train loss: 0.4751776158809662\n",
      "Epoch 16238: train loss: 0.47517749667167664\n",
      "Epoch 16239: train loss: 0.47517743706703186\n",
      "Epoch 16240: train loss: 0.4751774072647095\n",
      "Epoch 16241: train loss: 0.4751773476600647\n",
      "Epoch 16242: train loss: 0.4751772880554199\n",
      "Epoch 16243: train loss: 0.47517722845077515\n",
      "Epoch 16244: train loss: 0.47517719864845276\n",
      "Epoch 16245: train loss: 0.475177139043808\n",
      "Epoch 16246: train loss: 0.4751770794391632\n",
      "Epoch 16247: train loss: 0.47517699003219604\n",
      "Epoch 16248: train loss: 0.47517693042755127\n",
      "Epoch 16249: train loss: 0.4751768708229065\n",
      "Epoch 16250: train loss: 0.4751768112182617\n",
      "Epoch 16251: train loss: 0.47517672181129456\n",
      "Epoch 16252: train loss: 0.4751766622066498\n",
      "Epoch 16253: train loss: 0.4751766622066498\n",
      "Epoch 16254: train loss: 0.4751765727996826\n",
      "Epoch 16255: train loss: 0.47517645359039307\n",
      "Epoch 16256: train loss: 0.47517645359039307\n",
      "Epoch 16257: train loss: 0.4751763641834259\n",
      "Epoch 16258: train loss: 0.47517624497413635\n",
      "Epoch 16259: train loss: 0.47517624497413635\n",
      "Epoch 16260: train loss: 0.4751761853694916\n",
      "Epoch 16261: train loss: 0.4751761555671692\n",
      "Epoch 16262: train loss: 0.4751760959625244\n",
      "Epoch 16263: train loss: 0.47517597675323486\n",
      "Epoch 16264: train loss: 0.4751759469509125\n",
      "Epoch 16265: train loss: 0.4751758873462677\n",
      "Epoch 16266: train loss: 0.4751758277416229\n",
      "Epoch 16267: train loss: 0.47517576813697815\n",
      "Epoch 16268: train loss: 0.475175678730011\n",
      "Epoch 16269: train loss: 0.4751756191253662\n",
      "Epoch 16270: train loss: 0.47517555952072144\n",
      "Epoch 16271: train loss: 0.47517552971839905\n",
      "Epoch 16272: train loss: 0.4751754105091095\n",
      "Epoch 16273: train loss: 0.4751753509044647\n",
      "Epoch 16274: train loss: 0.47517532110214233\n",
      "Epoch 16275: train loss: 0.47517526149749756\n",
      "Epoch 16276: train loss: 0.47517526149749756\n",
      "Epoch 16277: train loss: 0.475175142288208\n",
      "Epoch 16278: train loss: 0.4751751124858856\n",
      "Epoch 16279: train loss: 0.47517499327659607\n",
      "Epoch 16280: train loss: 0.4751749336719513\n",
      "Epoch 16281: train loss: 0.4751749038696289\n",
      "Epoch 16282: train loss: 0.4751749038696289\n",
      "Epoch 16283: train loss: 0.47517478466033936\n",
      "Epoch 16284: train loss: 0.4751746952533722\n",
      "Epoch 16285: train loss: 0.4751746356487274\n",
      "Epoch 16286: train loss: 0.47517457604408264\n",
      "Epoch 16287: train loss: 0.47517451643943787\n",
      "Epoch 16288: train loss: 0.4751744866371155\n",
      "Epoch 16289: train loss: 0.4751744270324707\n",
      "Epoch 16290: train loss: 0.47517430782318115\n",
      "Epoch 16291: train loss: 0.47517430782318115\n",
      "Epoch 16292: train loss: 0.475174218416214\n",
      "Epoch 16293: train loss: 0.4751741588115692\n",
      "Epoch 16294: train loss: 0.47517409920692444\n",
      "Epoch 16295: train loss: 0.4751740097999573\n",
      "Epoch 16296: train loss: 0.4751740097999573\n",
      "Epoch 16297: train loss: 0.4751739501953125\n",
      "Epoch 16298: train loss: 0.47517383098602295\n",
      "Epoch 16299: train loss: 0.47517380118370056\n",
      "Epoch 16300: train loss: 0.4751737415790558\n",
      "Epoch 16301: train loss: 0.47517362236976624\n",
      "Epoch 16302: train loss: 0.47517362236976624\n",
      "Epoch 16303: train loss: 0.4751735329627991\n",
      "Epoch 16304: train loss: 0.4751734733581543\n",
      "Epoch 16305: train loss: 0.4751734137535095\n",
      "Epoch 16306: train loss: 0.47517338395118713\n",
      "Epoch 16307: train loss: 0.4751732647418976\n",
      "Epoch 16308: train loss: 0.4751732051372528\n",
      "Epoch 16309: train loss: 0.4751731753349304\n",
      "Epoch 16310: train loss: 0.47517305612564087\n",
      "Epoch 16311: train loss: 0.47517305612564087\n",
      "Epoch 16312: train loss: 0.4751729965209961\n",
      "Epoch 16313: train loss: 0.47517290711402893\n",
      "Epoch 16314: train loss: 0.47517290711402893\n",
      "Epoch 16315: train loss: 0.4751727879047394\n",
      "Epoch 16316: train loss: 0.4751726984977722\n",
      "Epoch 16317: train loss: 0.4751726984977722\n",
      "Epoch 16318: train loss: 0.47517263889312744\n",
      "Epoch 16319: train loss: 0.47517257928848267\n",
      "Epoch 16320: train loss: 0.4751724898815155\n",
      "Epoch 16321: train loss: 0.47517237067222595\n",
      "Epoch 16322: train loss: 0.47517237067222595\n",
      "Epoch 16323: train loss: 0.4751722812652588\n",
      "Epoch 16324: train loss: 0.475172221660614\n",
      "Epoch 16325: train loss: 0.47517216205596924\n",
      "Epoch 16326: train loss: 0.47517216205596924\n",
      "Epoch 16327: train loss: 0.47517213225364685\n",
      "Epoch 16328: train loss: 0.4751720130443573\n",
      "Epoch 16329: train loss: 0.47517192363739014\n",
      "Epoch 16330: train loss: 0.47517186403274536\n",
      "Epoch 16331: train loss: 0.4751718044281006\n",
      "Epoch 16332: train loss: 0.4751717448234558\n",
      "Epoch 16333: train loss: 0.47517165541648865\n",
      "Epoch 16334: train loss: 0.47517159581184387\n",
      "Epoch 16335: train loss: 0.47517159581184387\n",
      "Epoch 16336: train loss: 0.4751715362071991\n",
      "Epoch 16337: train loss: 0.47517144680023193\n",
      "Epoch 16338: train loss: 0.47517138719558716\n",
      "Epoch 16339: train loss: 0.4751713275909424\n",
      "Epoch 16340: train loss: 0.47517129778862\n",
      "Epoch 16341: train loss: 0.4751712381839752\n",
      "Epoch 16342: train loss: 0.47517111897468567\n",
      "Epoch 16343: train loss: 0.4751710891723633\n",
      "Epoch 16344: train loss: 0.4751710295677185\n",
      "Epoch 16345: train loss: 0.47517091035842896\n",
      "Epoch 16346: train loss: 0.47517091035842896\n",
      "Epoch 16347: train loss: 0.47517088055610657\n",
      "Epoch 16348: train loss: 0.475170761346817\n",
      "Epoch 16349: train loss: 0.47517070174217224\n",
      "Epoch 16350: train loss: 0.47517070174217224\n",
      "Epoch 16351: train loss: 0.4751705527305603\n",
      "Epoch 16352: train loss: 0.4751705527305603\n",
      "Epoch 16353: train loss: 0.4751704931259155\n",
      "Epoch 16354: train loss: 0.47517046332359314\n",
      "Epoch 16355: train loss: 0.47517040371894836\n",
      "Epoch 16356: train loss: 0.4751702845096588\n",
      "Epoch 16357: train loss: 0.47517019510269165\n",
      "Epoch 16358: train loss: 0.4751701354980469\n",
      "Epoch 16359: train loss: 0.4751700758934021\n",
      "Epoch 16360: train loss: 0.4751700162887573\n",
      "Epoch 16361: train loss: 0.47516998648643494\n",
      "Epoch 16362: train loss: 0.47516992688179016\n",
      "Epoch 16363: train loss: 0.4751698672771454\n",
      "Epoch 16364: train loss: 0.4751697778701782\n",
      "Epoch 16365: train loss: 0.4751697778701782\n",
      "Epoch 16366: train loss: 0.47516965866088867\n",
      "Epoch 16367: train loss: 0.4751695990562439\n",
      "Epoch 16368: train loss: 0.4751695692539215\n",
      "Epoch 16369: train loss: 0.47516945004463196\n",
      "Epoch 16370: train loss: 0.4751693904399872\n",
      "Epoch 16371: train loss: 0.4751693606376648\n",
      "Epoch 16372: train loss: 0.4751693606376648\n",
      "Epoch 16373: train loss: 0.47516924142837524\n",
      "Epoch 16374: train loss: 0.4751691520214081\n",
      "Epoch 16375: train loss: 0.4751691520214081\n",
      "Epoch 16376: train loss: 0.4751690924167633\n",
      "Epoch 16377: train loss: 0.47516897320747375\n",
      "Epoch 16378: train loss: 0.47516894340515137\n",
      "Epoch 16379: train loss: 0.47516894340515137\n",
      "Epoch 16380: train loss: 0.47516876459121704\n",
      "Epoch 16381: train loss: 0.47516876459121704\n",
      "Epoch 16382: train loss: 0.47516873478889465\n",
      "Epoch 16383: train loss: 0.4751686155796051\n",
      "Epoch 16384: train loss: 0.4751685559749603\n",
      "Epoch 16385: train loss: 0.47516852617263794\n",
      "Epoch 16386: train loss: 0.47516846656799316\n",
      "Epoch 16387: train loss: 0.4751683473587036\n",
      "Epoch 16388: train loss: 0.4751683473587036\n",
      "Epoch 16389: train loss: 0.47516825795173645\n",
      "Epoch 16390: train loss: 0.4751681387424469\n",
      "Epoch 16391: train loss: 0.4751681983470917\n",
      "Epoch 16392: train loss: 0.47516804933547974\n",
      "Epoch 16393: train loss: 0.47516804933547974\n",
      "Epoch 16394: train loss: 0.4751679301261902\n",
      "Epoch 16395: train loss: 0.4751679003238678\n",
      "Epoch 16396: train loss: 0.475167840719223\n",
      "Epoch 16397: train loss: 0.47516778111457825\n",
      "Epoch 16398: train loss: 0.4751676917076111\n",
      "Epoch 16399: train loss: 0.4751676321029663\n",
      "Epoch 16400: train loss: 0.47516757249832153\n",
      "Epoch 16401: train loss: 0.47516751289367676\n",
      "Epoch 16402: train loss: 0.47516748309135437\n",
      "Epoch 16403: train loss: 0.4751674234867096\n",
      "Epoch 16404: train loss: 0.47516730427742004\n",
      "Epoch 16405: train loss: 0.47516727447509766\n",
      "Epoch 16406: train loss: 0.4751672148704529\n",
      "Epoch 16407: train loss: 0.4751672148704529\n",
      "Epoch 16408: train loss: 0.47516709566116333\n",
      "Epoch 16409: train loss: 0.47516706585884094\n",
      "Epoch 16410: train loss: 0.4751669466495514\n",
      "Epoch 16411: train loss: 0.4751668870449066\n",
      "Epoch 16412: train loss: 0.47516685724258423\n",
      "Epoch 16413: train loss: 0.47516679763793945\n",
      "Epoch 16414: train loss: 0.4751667380332947\n",
      "Epoch 16415: train loss: 0.4751666784286499\n",
      "Epoch 16416: train loss: 0.4751666486263275\n",
      "Epoch 16417: train loss: 0.47516652941703796\n",
      "Epoch 16418: train loss: 0.4751664698123932\n",
      "Epoch 16419: train loss: 0.4751664400100708\n",
      "Epoch 16420: train loss: 0.475166380405426\n",
      "Epoch 16421: train loss: 0.47516632080078125\n",
      "Epoch 16422: train loss: 0.4751662611961365\n",
      "Epoch 16423: train loss: 0.4751661717891693\n",
      "Epoch 16424: train loss: 0.47516611218452454\n",
      "Epoch 16425: train loss: 0.47516605257987976\n",
      "Epoch 16426: train loss: 0.475165992975235\n",
      "Epoch 16427: train loss: 0.4751659631729126\n",
      "Epoch 16428: train loss: 0.47516584396362305\n",
      "Epoch 16429: train loss: 0.47516584396362305\n",
      "Epoch 16430: train loss: 0.47516578435897827\n",
      "Epoch 16431: train loss: 0.4751656949520111\n",
      "Epoch 16432: train loss: 0.47516557574272156\n",
      "Epoch 16433: train loss: 0.47516557574272156\n",
      "Epoch 16434: train loss: 0.47516554594039917\n",
      "Epoch 16435: train loss: 0.4751654267311096\n",
      "Epoch 16436: train loss: 0.47516536712646484\n",
      "Epoch 16437: train loss: 0.4751652777194977\n",
      "Epoch 16438: train loss: 0.4751652777194977\n",
      "Epoch 16439: train loss: 0.47516515851020813\n",
      "Epoch 16440: train loss: 0.47516512870788574\n",
      "Epoch 16441: train loss: 0.47516506910324097\n",
      "Epoch 16442: train loss: 0.4751649498939514\n",
      "Epoch 16443: train loss: 0.47516492009162903\n",
      "Epoch 16444: train loss: 0.47516492009162903\n",
      "Epoch 16445: train loss: 0.47516486048698425\n",
      "Epoch 16446: train loss: 0.4751647412776947\n",
      "Epoch 16447: train loss: 0.4751647114753723\n",
      "Epoch 16448: train loss: 0.47516465187072754\n",
      "Epoch 16449: train loss: 0.47516459226608276\n",
      "Epoch 16450: train loss: 0.4751645028591156\n",
      "Epoch 16451: train loss: 0.4751644432544708\n",
      "Epoch 16452: train loss: 0.47516438364982605\n",
      "Epoch 16453: train loss: 0.4751643240451813\n",
      "Epoch 16454: train loss: 0.4751642346382141\n",
      "Epoch 16455: train loss: 0.4751642942428589\n",
      "Epoch 16456: train loss: 0.47516417503356934\n",
      "Epoch 16457: train loss: 0.4751640856266022\n",
      "Epoch 16458: train loss: 0.4751640856266022\n",
      "Epoch 16459: train loss: 0.4751639664173126\n",
      "Epoch 16460: train loss: 0.47516387701034546\n",
      "Epoch 16461: train loss: 0.47516387701034546\n",
      "Epoch 16462: train loss: 0.4751637578010559\n",
      "Epoch 16463: train loss: 0.47516369819641113\n",
      "Epoch 16464: train loss: 0.47516366839408875\n",
      "Epoch 16465: train loss: 0.47516360878944397\n",
      "Epoch 16466: train loss: 0.4751635491847992\n",
      "Epoch 16467: train loss: 0.4751634895801544\n",
      "Epoch 16468: train loss: 0.47516345977783203\n",
      "Epoch 16469: train loss: 0.4751633405685425\n",
      "Epoch 16470: train loss: 0.4751632809638977\n",
      "Epoch 16471: train loss: 0.4751632511615753\n",
      "Epoch 16472: train loss: 0.47516319155693054\n",
      "Epoch 16473: train loss: 0.47516313195228577\n",
      "Epoch 16474: train loss: 0.4751630425453186\n",
      "Epoch 16475: train loss: 0.47516298294067383\n",
      "Epoch 16476: train loss: 0.47516292333602905\n",
      "Epoch 16477: train loss: 0.4751628637313843\n",
      "Epoch 16478: train loss: 0.4751628339290619\n",
      "Epoch 16479: train loss: 0.4751627743244171\n",
      "Epoch 16480: train loss: 0.47516265511512756\n",
      "Epoch 16481: train loss: 0.4751626253128052\n",
      "Epoch 16482: train loss: 0.4751625657081604\n",
      "Epoch 16483: train loss: 0.47516244649887085\n",
      "Epoch 16484: train loss: 0.47516244649887085\n",
      "Epoch 16485: train loss: 0.4751623570919037\n",
      "Epoch 16486: train loss: 0.4751623570919037\n",
      "Epoch 16487: train loss: 0.47516223788261414\n",
      "Epoch 16488: train loss: 0.47516217827796936\n",
      "Epoch 16489: train loss: 0.475162148475647\n",
      "Epoch 16490: train loss: 0.4751620292663574\n",
      "Epoch 16491: train loss: 0.4751620292663574\n",
      "Epoch 16492: train loss: 0.47516196966171265\n",
      "Epoch 16493: train loss: 0.4751618802547455\n",
      "Epoch 16494: train loss: 0.4751618206501007\n",
      "Epoch 16495: train loss: 0.47516176104545593\n",
      "Epoch 16496: train loss: 0.47516167163848877\n",
      "Epoch 16497: train loss: 0.47516167163848877\n",
      "Epoch 16498: train loss: 0.475161612033844\n",
      "Epoch 16499: train loss: 0.47516152262687683\n",
      "Epoch 16500: train loss: 0.47516146302223206\n",
      "Epoch 16501: train loss: 0.4751613438129425\n",
      "Epoch 16502: train loss: 0.4751613438129425\n",
      "Epoch 16503: train loss: 0.47516125440597534\n",
      "Epoch 16504: train loss: 0.47516125440597534\n",
      "Epoch 16505: train loss: 0.4751611351966858\n",
      "Epoch 16506: train loss: 0.4751611053943634\n",
      "Epoch 16507: train loss: 0.47516098618507385\n",
      "Epoch 16508: train loss: 0.4751609265804291\n",
      "Epoch 16509: train loss: 0.4751609265804291\n",
      "Epoch 16510: train loss: 0.4751608371734619\n",
      "Epoch 16511: train loss: 0.47516077756881714\n",
      "Epoch 16512: train loss: 0.47516071796417236\n",
      "Epoch 16513: train loss: 0.47516068816185\n",
      "Epoch 16514: train loss: 0.4751606285572052\n",
      "Epoch 16515: train loss: 0.47516050934791565\n",
      "Epoch 16516: train loss: 0.47516047954559326\n",
      "Epoch 16517: train loss: 0.47516047954559326\n",
      "Epoch 16518: train loss: 0.4751603603363037\n",
      "Epoch 16519: train loss: 0.47516030073165894\n",
      "Epoch 16520: train loss: 0.47516027092933655\n",
      "Epoch 16521: train loss: 0.475160151720047\n",
      "Epoch 16522: train loss: 0.4751600921154022\n",
      "Epoch 16523: train loss: 0.4751600921154022\n",
      "Epoch 16524: train loss: 0.47516000270843506\n",
      "Epoch 16525: train loss: 0.4751599431037903\n",
      "Epoch 16526: train loss: 0.4751598536968231\n",
      "Epoch 16527: train loss: 0.47515979409217834\n",
      "Epoch 16528: train loss: 0.47515973448753357\n",
      "Epoch 16529: train loss: 0.4751596748828888\n",
      "Epoch 16530: train loss: 0.4751596450805664\n",
      "Epoch 16531: train loss: 0.47515952587127686\n",
      "Epoch 16532: train loss: 0.47515952587127686\n",
      "Epoch 16533: train loss: 0.4751594662666321\n",
      "Epoch 16534: train loss: 0.4751594364643097\n",
      "Epoch 16535: train loss: 0.47515925765037537\n",
      "Epoch 16536: train loss: 0.475159227848053\n",
      "Epoch 16537: train loss: 0.475159227848053\n",
      "Epoch 16538: train loss: 0.4751591086387634\n",
      "Epoch 16539: train loss: 0.4751591086387634\n",
      "Epoch 16540: train loss: 0.47515901923179626\n",
      "Epoch 16541: train loss: 0.4751589596271515\n",
      "Epoch 16542: train loss: 0.47515884041786194\n",
      "Epoch 16543: train loss: 0.47515881061553955\n",
      "Epoch 16544: train loss: 0.47515881061553955\n",
      "Epoch 16545: train loss: 0.4751587510108948\n",
      "Epoch 16546: train loss: 0.4751586318016052\n",
      "Epoch 16547: train loss: 0.47515857219696045\n",
      "Epoch 16548: train loss: 0.47515854239463806\n",
      "Epoch 16549: train loss: 0.4751584827899933\n",
      "Epoch 16550: train loss: 0.47515836358070374\n",
      "Epoch 16551: train loss: 0.47515833377838135\n",
      "Epoch 16552: train loss: 0.4751582741737366\n",
      "Epoch 16553: train loss: 0.4751582145690918\n",
      "Epoch 16554: train loss: 0.475158154964447\n",
      "Epoch 16555: train loss: 0.47515806555747986\n",
      "Epoch 16556: train loss: 0.47515806555747986\n",
      "Epoch 16557: train loss: 0.4751580059528351\n",
      "Epoch 16558: train loss: 0.4751579165458679\n",
      "Epoch 16559: train loss: 0.47515785694122314\n",
      "Epoch 16560: train loss: 0.4751577377319336\n",
      "Epoch 16561: train loss: 0.4751577377319336\n",
      "Epoch 16562: train loss: 0.47515764832496643\n",
      "Epoch 16563: train loss: 0.47515758872032166\n",
      "Epoch 16564: train loss: 0.4751575291156769\n",
      "Epoch 16565: train loss: 0.4751574993133545\n",
      "Epoch 16566: train loss: 0.4751574397087097\n",
      "Epoch 16567: train loss: 0.47515738010406494\n",
      "Epoch 16568: train loss: 0.4751572906970978\n",
      "Epoch 16569: train loss: 0.475157231092453\n",
      "Epoch 16570: train loss: 0.4751571714878082\n",
      "Epoch 16571: train loss: 0.47515711188316345\n",
      "Epoch 16572: train loss: 0.4751570224761963\n",
      "Epoch 16573: train loss: 0.4751570224761963\n",
      "Epoch 16574: train loss: 0.47515690326690674\n",
      "Epoch 16575: train loss: 0.47515690326690674\n",
      "Epoch 16576: train loss: 0.47515687346458435\n",
      "Epoch 16577: train loss: 0.4751567542552948\n",
      "Epoch 16578: train loss: 0.47515666484832764\n",
      "Epoch 16579: train loss: 0.47515660524368286\n",
      "Epoch 16580: train loss: 0.47515660524368286\n",
      "Epoch 16581: train loss: 0.4751564860343933\n",
      "Epoch 16582: train loss: 0.4751564860343933\n",
      "Epoch 16583: train loss: 0.47515639662742615\n",
      "Epoch 16584: train loss: 0.47515633702278137\n",
      "Epoch 16585: train loss: 0.4751562774181366\n",
      "Epoch 16586: train loss: 0.47515618801116943\n",
      "Epoch 16587: train loss: 0.47515612840652466\n",
      "Epoch 16588: train loss: 0.4751560688018799\n",
      "Epoch 16589: train loss: 0.4751560389995575\n",
      "Epoch 16590: train loss: 0.4751559793949127\n",
      "Epoch 16591: train loss: 0.47515586018562317\n",
      "Epoch 16592: train loss: 0.4751558303833008\n",
      "Epoch 16593: train loss: 0.475155770778656\n",
      "Epoch 16594: train loss: 0.47515571117401123\n",
      "Epoch 16595: train loss: 0.47515562176704407\n",
      "Epoch 16596: train loss: 0.4751555621623993\n",
      "Epoch 16597: train loss: 0.4751555621623993\n",
      "Epoch 16598: train loss: 0.47515544295310974\n",
      "Epoch 16599: train loss: 0.47515541315078735\n",
      "Epoch 16600: train loss: 0.4751553535461426\n",
      "Epoch 16601: train loss: 0.475155234336853\n",
      "Epoch 16602: train loss: 0.47515520453453064\n",
      "Epoch 16603: train loss: 0.47515520453453064\n",
      "Epoch 16604: train loss: 0.4751550853252411\n",
      "Epoch 16605: train loss: 0.4751550853252411\n",
      "Epoch 16606: train loss: 0.47515493631362915\n",
      "Epoch 16607: train loss: 0.47515493631362915\n",
      "Epoch 16608: train loss: 0.4751548767089844\n",
      "Epoch 16609: train loss: 0.4751548171043396\n",
      "Epoch 16610: train loss: 0.47515472769737244\n",
      "Epoch 16611: train loss: 0.47515466809272766\n",
      "Epoch 16612: train loss: 0.4751545488834381\n",
      "Epoch 16613: train loss: 0.4751545190811157\n",
      "Epoch 16614: train loss: 0.4751545190811157\n",
      "Epoch 16615: train loss: 0.47515439987182617\n",
      "Epoch 16616: train loss: 0.4751543402671814\n",
      "Epoch 16617: train loss: 0.475154310464859\n",
      "Epoch 16618: train loss: 0.475154310464859\n",
      "Epoch 16619: train loss: 0.47515419125556946\n",
      "Epoch 16620: train loss: 0.4751541316509247\n",
      "Epoch 16621: train loss: 0.4751540422439575\n",
      "Epoch 16622: train loss: 0.4751540422439575\n",
      "Epoch 16623: train loss: 0.47515398263931274\n",
      "Epoch 16624: train loss: 0.4751538932323456\n",
      "Epoch 16625: train loss: 0.4751538336277008\n",
      "Epoch 16626: train loss: 0.47515377402305603\n",
      "Epoch 16627: train loss: 0.47515371441841125\n",
      "Epoch 16628: train loss: 0.47515368461608887\n",
      "Epoch 16629: train loss: 0.4751535654067993\n",
      "Epoch 16630: train loss: 0.47515350580215454\n",
      "Epoch 16631: train loss: 0.47515347599983215\n",
      "Epoch 16632: train loss: 0.4751534163951874\n",
      "Epoch 16633: train loss: 0.4751533567905426\n",
      "Epoch 16634: train loss: 0.4751532971858978\n",
      "Epoch 16635: train loss: 0.47515320777893066\n",
      "Epoch 16636: train loss: 0.4751531481742859\n",
      "Epoch 16637: train loss: 0.4751530885696411\n",
      "Epoch 16638: train loss: 0.4751530587673187\n",
      "Epoch 16639: train loss: 0.47515299916267395\n",
      "Epoch 16640: train loss: 0.4751529395580292\n",
      "Epoch 16641: train loss: 0.475152850151062\n",
      "Epoch 16642: train loss: 0.47515279054641724\n",
      "Epoch 16643: train loss: 0.47515273094177246\n",
      "Epoch 16644: train loss: 0.4751526415348053\n",
      "Epoch 16645: train loss: 0.4751525819301605\n",
      "Epoch 16646: train loss: 0.47515252232551575\n",
      "Epoch 16647: train loss: 0.47515246272087097\n",
      "Epoch 16648: train loss: 0.47515246272087097\n",
      "Epoch 16649: train loss: 0.4751523733139038\n",
      "Epoch 16650: train loss: 0.47515231370925903\n",
      "Epoch 16651: train loss: 0.47515225410461426\n",
      "Epoch 16652: train loss: 0.47515222430229187\n",
      "Epoch 16653: train loss: 0.4751521646976471\n",
      "Epoch 16654: train loss: 0.47515204548835754\n",
      "Epoch 16655: train loss: 0.47515201568603516\n",
      "Epoch 16656: train loss: 0.47515201568603516\n",
      "Epoch 16657: train loss: 0.4751518964767456\n",
      "Epoch 16658: train loss: 0.47515180706977844\n",
      "Epoch 16659: train loss: 0.47515174746513367\n",
      "Epoch 16660: train loss: 0.4751516878604889\n",
      "Epoch 16661: train loss: 0.4751516282558441\n",
      "Epoch 16662: train loss: 0.47515159845352173\n",
      "Epoch 16663: train loss: 0.47515153884887695\n",
      "Epoch 16664: train loss: 0.4751514792442322\n",
      "Epoch 16665: train loss: 0.475151389837265\n",
      "Epoch 16666: train loss: 0.47515133023262024\n",
      "Epoch 16667: train loss: 0.47515127062797546\n",
      "Epoch 16668: train loss: 0.4751511812210083\n",
      "Epoch 16669: train loss: 0.4751511812210083\n",
      "Epoch 16670: train loss: 0.4751511216163635\n",
      "Epoch 16671: train loss: 0.47515106201171875\n",
      "Epoch 16672: train loss: 0.4751509428024292\n",
      "Epoch 16673: train loss: 0.4751509130001068\n",
      "Epoch 16674: train loss: 0.47515085339546204\n",
      "Epoch 16675: train loss: 0.47515079379081726\n",
      "Epoch 16676: train loss: 0.4751507043838501\n",
      "Epoch 16677: train loss: 0.4751506447792053\n",
      "Epoch 16678: train loss: 0.47515058517456055\n",
      "Epoch 16679: train loss: 0.47515052556991577\n",
      "Epoch 16680: train loss: 0.4751504957675934\n",
      "Epoch 16681: train loss: 0.47515037655830383\n",
      "Epoch 16682: train loss: 0.47515031695365906\n",
      "Epoch 16683: train loss: 0.47515031695365906\n",
      "Epoch 16684: train loss: 0.4751502275466919\n",
      "Epoch 16685: train loss: 0.4751501679420471\n",
      "Epoch 16686: train loss: 0.47515010833740234\n",
      "Epoch 16687: train loss: 0.4751500189304352\n",
      "Epoch 16688: train loss: 0.4751500189304352\n",
      "Epoch 16689: train loss: 0.4751499593257904\n",
      "Epoch 16690: train loss: 0.47514986991882324\n",
      "Epoch 16691: train loss: 0.47514981031417847\n",
      "Epoch 16692: train loss: 0.4751497507095337\n",
      "Epoch 16693: train loss: 0.4751496911048889\n",
      "Epoch 16694: train loss: 0.47514966130256653\n",
      "Epoch 16695: train loss: 0.475149542093277\n",
      "Epoch 16696: train loss: 0.4751494824886322\n",
      "Epoch 16697: train loss: 0.4751494824886322\n",
      "Epoch 16698: train loss: 0.47514939308166504\n",
      "Epoch 16699: train loss: 0.47514939308166504\n",
      "Epoch 16700: train loss: 0.4751492440700531\n",
      "Epoch 16701: train loss: 0.4751491844654083\n",
      "Epoch 16702: train loss: 0.47514912486076355\n",
      "Epoch 16703: train loss: 0.4751490652561188\n",
      "Epoch 16704: train loss: 0.4751490354537964\n",
      "Epoch 16705: train loss: 0.4751489758491516\n",
      "Epoch 16706: train loss: 0.47514891624450684\n",
      "Epoch 16707: train loss: 0.4751488268375397\n",
      "Epoch 16708: train loss: 0.4751487672328949\n",
      "Epoch 16709: train loss: 0.4751487076282501\n",
      "Epoch 16710: train loss: 0.47514864802360535\n",
      "Epoch 16711: train loss: 0.4751485586166382\n",
      "Epoch 16712: train loss: 0.4751485586166382\n",
      "Epoch 16713: train loss: 0.4751484990119934\n",
      "Epoch 16714: train loss: 0.47514840960502625\n",
      "Epoch 16715: train loss: 0.47514835000038147\n",
      "Epoch 16716: train loss: 0.4751482903957367\n",
      "Epoch 16717: train loss: 0.4751482307910919\n",
      "Epoch 16718: train loss: 0.4751482307910919\n",
      "Epoch 16719: train loss: 0.47514814138412476\n",
      "Epoch 16720: train loss: 0.4751480221748352\n",
      "Epoch 16721: train loss: 0.4751479923725128\n",
      "Epoch 16722: train loss: 0.47514793276786804\n",
      "Epoch 16723: train loss: 0.47514787316322327\n",
      "Epoch 16724: train loss: 0.4751478135585785\n",
      "Epoch 16725: train loss: 0.47514772415161133\n",
      "Epoch 16726: train loss: 0.47514772415161133\n",
      "Epoch 16727: train loss: 0.47514766454696655\n",
      "Epoch 16728: train loss: 0.4751475155353546\n",
      "Epoch 16729: train loss: 0.47514745593070984\n",
      "Epoch 16730: train loss: 0.47514745593070984\n",
      "Epoch 16731: train loss: 0.47514739632606506\n",
      "Epoch 16732: train loss: 0.4751473069190979\n",
      "Epoch 16733: train loss: 0.4751472473144531\n",
      "Epoch 16734: train loss: 0.47514718770980835\n",
      "Epoch 16735: train loss: 0.4751470983028412\n",
      "Epoch 16736: train loss: 0.4751470983028412\n",
      "Epoch 16737: train loss: 0.4751470386981964\n",
      "Epoch 16738: train loss: 0.47514691948890686\n",
      "Epoch 16739: train loss: 0.4751468896865845\n",
      "Epoch 16740: train loss: 0.4751468300819397\n",
      "Epoch 16741: train loss: 0.4751467704772949\n",
      "Epoch 16742: train loss: 0.47514671087265015\n",
      "Epoch 16743: train loss: 0.47514668107032776\n",
      "Epoch 16744: train loss: 0.475146621465683\n",
      "Epoch 16745: train loss: 0.4751465618610382\n",
      "Epoch 16746: train loss: 0.47514647245407104\n",
      "Epoch 16747: train loss: 0.47514641284942627\n",
      "Epoch 16748: train loss: 0.4751463532447815\n",
      "Epoch 16749: train loss: 0.47514626383781433\n",
      "Epoch 16750: train loss: 0.47514620423316956\n",
      "Epoch 16751: train loss: 0.47514620423316956\n",
      "Epoch 16752: train loss: 0.47514608502388\n",
      "Epoch 16753: train loss: 0.4751460552215576\n",
      "Epoch 16754: train loss: 0.47514599561691284\n",
      "Epoch 16755: train loss: 0.47514593601226807\n",
      "Epoch 16756: train loss: 0.4751458764076233\n",
      "Epoch 16757: train loss: 0.47514578700065613\n",
      "Epoch 16758: train loss: 0.47514572739601135\n",
      "Epoch 16759: train loss: 0.47514572739601135\n",
      "Epoch 16760: train loss: 0.4751456677913666\n",
      "Epoch 16761: train loss: 0.47514551877975464\n",
      "Epoch 16762: train loss: 0.47514551877975464\n",
      "Epoch 16763: train loss: 0.47514545917510986\n",
      "Epoch 16764: train loss: 0.4751454293727875\n",
      "Epoch 16765: train loss: 0.4751453101634979\n",
      "Epoch 16766: train loss: 0.47514525055885315\n",
      "Epoch 16767: train loss: 0.47514522075653076\n",
      "Epoch 16768: train loss: 0.475145161151886\n",
      "Epoch 16769: train loss: 0.4751451015472412\n",
      "Epoch 16770: train loss: 0.47514504194259644\n",
      "Epoch 16771: train loss: 0.4751449525356293\n",
      "Epoch 16772: train loss: 0.4751448929309845\n",
      "Epoch 16773: train loss: 0.4751448333263397\n",
      "Epoch 16774: train loss: 0.47514480352401733\n",
      "Epoch 16775: train loss: 0.47514474391937256\n",
      "Epoch 16776: train loss: 0.475144624710083\n",
      "Epoch 16777: train loss: 0.4751445949077606\n",
      "Epoch 16778: train loss: 0.4751445949077606\n",
      "Epoch 16779: train loss: 0.47514447569847107\n",
      "Epoch 16780: train loss: 0.4751444160938263\n",
      "Epoch 16781: train loss: 0.4751443862915039\n",
      "Epoch 16782: train loss: 0.47514432668685913\n",
      "Epoch 16783: train loss: 0.47514426708221436\n",
      "Epoch 16784: train loss: 0.4751441776752472\n",
      "Epoch 16785: train loss: 0.4751441776752472\n",
      "Epoch 16786: train loss: 0.47514405846595764\n",
      "Epoch 16787: train loss: 0.47514399886131287\n",
      "Epoch 16788: train loss: 0.4751439690589905\n",
      "Epoch 16789: train loss: 0.4751438498497009\n",
      "Epoch 16790: train loss: 0.4751438498497009\n",
      "Epoch 16791: train loss: 0.47514376044273376\n",
      "Epoch 16792: train loss: 0.475143700838089\n",
      "Epoch 16793: train loss: 0.4751436412334442\n",
      "Epoch 16794: train loss: 0.47514358162879944\n",
      "Epoch 16795: train loss: 0.4751434922218323\n",
      "Epoch 16796: train loss: 0.4751434922218323\n",
      "Epoch 16797: train loss: 0.4751433730125427\n",
      "Epoch 16798: train loss: 0.47514331340789795\n",
      "Epoch 16799: train loss: 0.47514328360557556\n",
      "Epoch 16800: train loss: 0.47514328360557556\n",
      "Epoch 16801: train loss: 0.47514310479164124\n",
      "Epoch 16802: train loss: 0.47514310479164124\n",
      "Epoch 16803: train loss: 0.47514307498931885\n",
      "Epoch 16804: train loss: 0.4751429557800293\n",
      "Epoch 16805: train loss: 0.4751428961753845\n",
      "Epoch 16806: train loss: 0.47514286637306213\n",
      "Epoch 16807: train loss: 0.47514280676841736\n",
      "Epoch 16808: train loss: 0.4751426875591278\n",
      "Epoch 16809: train loss: 0.4751426875591278\n",
      "Epoch 16810: train loss: 0.47514259815216064\n",
      "Epoch 16811: train loss: 0.47514253854751587\n",
      "Epoch 16812: train loss: 0.4751424491405487\n",
      "Epoch 16813: train loss: 0.47514238953590393\n",
      "Epoch 16814: train loss: 0.47514232993125916\n",
      "Epoch 16815: train loss: 0.47514232993125916\n",
      "Epoch 16816: train loss: 0.475142240524292\n",
      "Epoch 16817: train loss: 0.4751421809196472\n",
      "Epoch 16818: train loss: 0.47514212131500244\n",
      "Epoch 16819: train loss: 0.47514206171035767\n",
      "Epoch 16820: train loss: 0.4751420319080353\n",
      "Epoch 16821: train loss: 0.4751419126987457\n",
      "Epoch 16822: train loss: 0.4751419126987457\n",
      "Epoch 16823: train loss: 0.47514182329177856\n",
      "Epoch 16824: train loss: 0.475141704082489\n",
      "Epoch 16825: train loss: 0.47514164447784424\n",
      "Epoch 16826: train loss: 0.47514164447784424\n",
      "Epoch 16827: train loss: 0.4751415550708771\n",
      "Epoch 16828: train loss: 0.4751414954662323\n",
      "Epoch 16829: train loss: 0.4751414954662323\n",
      "Epoch 16830: train loss: 0.47514140605926514\n",
      "Epoch 16831: train loss: 0.4751412868499756\n",
      "Epoch 16832: train loss: 0.4751412868499756\n",
      "Epoch 16833: train loss: 0.4751411974430084\n",
      "Epoch 16834: train loss: 0.47514113783836365\n",
      "Epoch 16835: train loss: 0.47514113783836365\n",
      "Epoch 16836: train loss: 0.4751410186290741\n",
      "Epoch 16837: train loss: 0.4751409888267517\n",
      "Epoch 16838: train loss: 0.47514086961746216\n",
      "Epoch 16839: train loss: 0.47514086961746216\n",
      "Epoch 16840: train loss: 0.475140780210495\n",
      "Epoch 16841: train loss: 0.4751407206058502\n",
      "Epoch 16842: train loss: 0.47514066100120544\n",
      "Epoch 16843: train loss: 0.47514060139656067\n",
      "Epoch 16844: train loss: 0.4751405715942383\n",
      "Epoch 16845: train loss: 0.4751405119895935\n",
      "Epoch 16846: train loss: 0.47514039278030396\n",
      "Epoch 16847: train loss: 0.47514039278030396\n",
      "Epoch 16848: train loss: 0.4751403033733368\n",
      "Epoch 16849: train loss: 0.475140243768692\n",
      "Epoch 16850: train loss: 0.47514018416404724\n",
      "Epoch 16851: train loss: 0.47514015436172485\n",
      "Epoch 16852: train loss: 0.4751400351524353\n",
      "Epoch 16853: train loss: 0.4751399755477905\n",
      "Epoch 16854: train loss: 0.4751399755477905\n",
      "Epoch 16855: train loss: 0.47513988614082336\n",
      "Epoch 16856: train loss: 0.4751398265361786\n",
      "Epoch 16857: train loss: 0.4751397669315338\n",
      "Epoch 16858: train loss: 0.4751397371292114\n",
      "Epoch 16859: train loss: 0.47513967752456665\n",
      "Epoch 16860: train loss: 0.4751395583152771\n",
      "Epoch 16861: train loss: 0.4751395583152771\n",
      "Epoch 16862: train loss: 0.47513946890830994\n",
      "Epoch 16863: train loss: 0.47513940930366516\n",
      "Epoch 16864: train loss: 0.4751393496990204\n",
      "Epoch 16865: train loss: 0.4751392602920532\n",
      "Epoch 16866: train loss: 0.47513920068740845\n",
      "Epoch 16867: train loss: 0.47513914108276367\n",
      "Epoch 16868: train loss: 0.4751390516757965\n",
      "Epoch 16869: train loss: 0.4751390516757965\n",
      "Epoch 16870: train loss: 0.47513899207115173\n",
      "Epoch 16871: train loss: 0.4751388728618622\n",
      "Epoch 16872: train loss: 0.4751388430595398\n",
      "Epoch 16873: train loss: 0.4751388430595398\n",
      "Epoch 16874: train loss: 0.47513872385025024\n",
      "Epoch 16875: train loss: 0.47513866424560547\n",
      "Epoch 16876: train loss: 0.4751386344432831\n",
      "Epoch 16877: train loss: 0.47513851523399353\n",
      "Epoch 16878: train loss: 0.47513845562934875\n",
      "Epoch 16879: train loss: 0.47513842582702637\n",
      "Epoch 16880: train loss: 0.47513842582702637\n",
      "Epoch 16881: train loss: 0.4751383066177368\n",
      "Epoch 16882: train loss: 0.47513824701309204\n",
      "Epoch 16883: train loss: 0.47513821721076965\n",
      "Epoch 16884: train loss: 0.4751380980014801\n",
      "Epoch 16885: train loss: 0.4751380980014801\n",
      "Epoch 16886: train loss: 0.4751380383968353\n",
      "Epoch 16887: train loss: 0.47513794898986816\n",
      "Epoch 16888: train loss: 0.4751378893852234\n",
      "Epoch 16889: train loss: 0.4751378297805786\n",
      "Epoch 16890: train loss: 0.47513774037361145\n",
      "Epoch 16891: train loss: 0.4751376807689667\n",
      "Epoch 16892: train loss: 0.4751376807689667\n",
      "Epoch 16893: train loss: 0.4751375913619995\n",
      "Epoch 16894: train loss: 0.47513753175735474\n",
      "Epoch 16895: train loss: 0.47513747215270996\n",
      "Epoch 16896: train loss: 0.4751373827457428\n",
      "Epoch 16897: train loss: 0.475137323141098\n",
      "Epoch 16898: train loss: 0.47513726353645325\n",
      "Epoch 16899: train loss: 0.47513720393180847\n",
      "Epoch 16900: train loss: 0.4751371741294861\n",
      "Epoch 16901: train loss: 0.4751371145248413\n",
      "Epoch 16902: train loss: 0.47513705492019653\n",
      "Epoch 16903: train loss: 0.47513699531555176\n",
      "Epoch 16904: train loss: 0.4751369059085846\n",
      "Epoch 16905: train loss: 0.4751368463039398\n",
      "Epoch 16906: train loss: 0.47513678669929504\n",
      "Epoch 16907: train loss: 0.47513675689697266\n",
      "Epoch 16908: train loss: 0.4751366972923279\n",
      "Epoch 16909: train loss: 0.4751366376876831\n",
      "Epoch 16910: train loss: 0.47513654828071594\n",
      "Epoch 16911: train loss: 0.47513654828071594\n",
      "Epoch 16912: train loss: 0.4751364290714264\n",
      "Epoch 16913: train loss: 0.4751363694667816\n",
      "Epoch 16914: train loss: 0.47513633966445923\n",
      "Epoch 16915: train loss: 0.47513628005981445\n",
      "Epoch 16916: train loss: 0.4751361608505249\n",
      "Epoch 16917: train loss: 0.4751361310482025\n",
      "Epoch 16918: train loss: 0.47513607144355774\n",
      "Epoch 16919: train loss: 0.47513601183891296\n",
      "Epoch 16920: train loss: 0.4751359522342682\n",
      "Epoch 16921: train loss: 0.4751359224319458\n",
      "Epoch 16922: train loss: 0.475135862827301\n",
      "Epoch 16923: train loss: 0.4751357436180115\n",
      "Epoch 16924: train loss: 0.4751356840133667\n",
      "Epoch 16925: train loss: 0.4751356542110443\n",
      "Epoch 16926: train loss: 0.47513559460639954\n",
      "Epoch 16927: train loss: 0.47513547539711\n",
      "Epoch 16928: train loss: 0.47513547539711\n",
      "Epoch 16929: train loss: 0.4751354455947876\n",
      "Epoch 16930: train loss: 0.4751353859901428\n",
      "Epoch 16931: train loss: 0.47513532638549805\n",
      "Epoch 16932: train loss: 0.4751351773738861\n",
      "Epoch 16933: train loss: 0.4751351773738861\n",
      "Epoch 16934: train loss: 0.4751351773738861\n",
      "Epoch 16935: train loss: 0.47513505816459656\n",
      "Epoch 16936: train loss: 0.4751349687576294\n",
      "Epoch 16937: train loss: 0.4751349687576294\n",
      "Epoch 16938: train loss: 0.47513484954833984\n",
      "Epoch 16939: train loss: 0.47513484954833984\n",
      "Epoch 16940: train loss: 0.4751347601413727\n",
      "Epoch 16941: train loss: 0.4751347601413727\n",
      "Epoch 16942: train loss: 0.47513461112976074\n",
      "Epoch 16943: train loss: 0.47513461112976074\n",
      "Epoch 16944: train loss: 0.4751344919204712\n",
      "Epoch 16945: train loss: 0.4751344323158264\n",
      "Epoch 16946: train loss: 0.47513440251350403\n",
      "Epoch 16947: train loss: 0.47513434290885925\n",
      "Epoch 16948: train loss: 0.4751342833042145\n",
      "Epoch 16949: train loss: 0.4751341938972473\n",
      "Epoch 16950: train loss: 0.47513413429260254\n",
      "Epoch 16951: train loss: 0.47513407468795776\n",
      "Epoch 16952: train loss: 0.475134015083313\n",
      "Epoch 16953: train loss: 0.475134015083313\n",
      "Epoch 16954: train loss: 0.4751339256763458\n",
      "Epoch 16955: train loss: 0.4751338064670563\n",
      "Epoch 16956: train loss: 0.4751337766647339\n",
      "Epoch 16957: train loss: 0.4751337170600891\n",
      "Epoch 16958: train loss: 0.47513365745544434\n",
      "Epoch 16959: train loss: 0.47513359785079956\n",
      "Epoch 16960: train loss: 0.4751335084438324\n",
      "Epoch 16961: train loss: 0.4751334488391876\n",
      "Epoch 16962: train loss: 0.4751334488391876\n",
      "Epoch 16963: train loss: 0.47513335943222046\n",
      "Epoch 16964: train loss: 0.4751332998275757\n",
      "Epoch 16965: train loss: 0.47513318061828613\n",
      "Epoch 16966: train loss: 0.47513318061828613\n",
      "Epoch 16967: train loss: 0.47513315081596375\n",
      "Epoch 16968: train loss: 0.47513309121131897\n",
      "Epoch 16969: train loss: 0.4751329720020294\n",
      "Epoch 16970: train loss: 0.47513294219970703\n",
      "Epoch 16971: train loss: 0.47513288259506226\n",
      "Epoch 16972: train loss: 0.4751327633857727\n",
      "Epoch 16973: train loss: 0.4751328229904175\n",
      "Epoch 16974: train loss: 0.4751327335834503\n",
      "Epoch 16975: train loss: 0.47513267397880554\n",
      "Epoch 16976: train loss: 0.47513261437416077\n",
      "Epoch 16977: train loss: 0.4751325249671936\n",
      "Epoch 16978: train loss: 0.47513246536254883\n",
      "Epoch 16979: train loss: 0.47513240575790405\n",
      "Epoch 16980: train loss: 0.4751323461532593\n",
      "Epoch 16981: train loss: 0.4751322567462921\n",
      "Epoch 16982: train loss: 0.47513219714164734\n",
      "Epoch 16983: train loss: 0.47513213753700256\n",
      "Epoch 16984: train loss: 0.4751321077346802\n",
      "Epoch 16985: train loss: 0.4751320481300354\n",
      "Epoch 16986: train loss: 0.47513192892074585\n",
      "Epoch 16987: train loss: 0.47513192892074585\n",
      "Epoch 16988: train loss: 0.4751318395137787\n",
      "Epoch 16989: train loss: 0.4751317799091339\n",
      "Epoch 16990: train loss: 0.47513172030448914\n",
      "Epoch 16991: train loss: 0.47513166069984436\n",
      "Epoch 16992: train loss: 0.475131630897522\n",
      "Epoch 16993: train loss: 0.4751315116882324\n",
      "Epoch 16994: train loss: 0.4751315116882324\n",
      "Epoch 16995: train loss: 0.47513142228126526\n",
      "Epoch 16996: train loss: 0.4751313626766205\n",
      "Epoch 16997: train loss: 0.4751313030719757\n",
      "Epoch 16998: train loss: 0.47513124346733093\n",
      "Epoch 16999: train loss: 0.47513121366500854\n",
      "Epoch 17000: train loss: 0.47513115406036377\n",
      "Epoch 17001: train loss: 0.475131094455719\n",
      "Epoch 17002: train loss: 0.4751310348510742\n",
      "Epoch 17003: train loss: 0.47513100504875183\n",
      "Epoch 17004: train loss: 0.4751308858394623\n",
      "Epoch 17005: train loss: 0.4751307964324951\n",
      "Epoch 17006: train loss: 0.4751307964324951\n",
      "Epoch 17007: train loss: 0.47513073682785034\n",
      "Epoch 17008: train loss: 0.4751306176185608\n",
      "Epoch 17009: train loss: 0.4751305878162384\n",
      "Epoch 17010: train loss: 0.47513052821159363\n",
      "Epoch 17011: train loss: 0.47513046860694885\n",
      "Epoch 17012: train loss: 0.4751304090023041\n",
      "Epoch 17013: train loss: 0.4751303195953369\n",
      "Epoch 17014: train loss: 0.47513025999069214\n",
      "Epoch 17015: train loss: 0.47513025999069214\n",
      "Epoch 17016: train loss: 0.47513020038604736\n",
      "Epoch 17017: train loss: 0.475130170583725\n",
      "Epoch 17018: train loss: 0.4751300513744354\n",
      "Epoch 17019: train loss: 0.47512999176979065\n",
      "Epoch 17020: train loss: 0.4751299023628235\n",
      "Epoch 17021: train loss: 0.4751298427581787\n",
      "Epoch 17022: train loss: 0.4751298427581787\n",
      "Epoch 17023: train loss: 0.47512975335121155\n",
      "Epoch 17024: train loss: 0.4751296937465668\n",
      "Epoch 17025: train loss: 0.475129634141922\n",
      "Epoch 17026: train loss: 0.47512954473495483\n",
      "Epoch 17027: train loss: 0.47512954473495483\n",
      "Epoch 17028: train loss: 0.4751294255256653\n",
      "Epoch 17029: train loss: 0.4751293659210205\n",
      "Epoch 17030: train loss: 0.4751293361186981\n",
      "Epoch 17031: train loss: 0.47512927651405334\n",
      "Epoch 17032: train loss: 0.47512921690940857\n",
      "Epoch 17033: train loss: 0.4751291573047638\n",
      "Epoch 17034: train loss: 0.4751291275024414\n",
      "Epoch 17035: train loss: 0.47512906789779663\n",
      "Epoch 17036: train loss: 0.4751289486885071\n",
      "Epoch 17037: train loss: 0.4751289486885071\n",
      "Epoch 17038: train loss: 0.4751288592815399\n",
      "Epoch 17039: train loss: 0.47512879967689514\n",
      "Epoch 17040: train loss: 0.47512874007225037\n",
      "Epoch 17041: train loss: 0.475128710269928\n",
      "Epoch 17042: train loss: 0.4751285910606384\n",
      "Epoch 17043: train loss: 0.47512850165367126\n",
      "Epoch 17044: train loss: 0.47512850165367126\n",
      "Epoch 17045: train loss: 0.4751284420490265\n",
      "Epoch 17046: train loss: 0.4751283824443817\n",
      "Epoch 17047: train loss: 0.47512829303741455\n",
      "Epoch 17048: train loss: 0.47512829303741455\n",
      "Epoch 17049: train loss: 0.475128173828125\n",
      "Epoch 17050: train loss: 0.4751281142234802\n",
      "Epoch 17051: train loss: 0.47512805461883545\n",
      "Epoch 17052: train loss: 0.47512802481651306\n",
      "Epoch 17053: train loss: 0.4751279652118683\n",
      "Epoch 17054: train loss: 0.47512784600257874\n",
      "Epoch 17055: train loss: 0.47512781620025635\n",
      "Epoch 17056: train loss: 0.4751277565956116\n",
      "Epoch 17057: train loss: 0.4751276969909668\n",
      "Epoch 17058: train loss: 0.475127637386322\n",
      "Epoch 17059: train loss: 0.47512760758399963\n",
      "Epoch 17060: train loss: 0.47512754797935486\n",
      "Epoch 17061: train loss: 0.4751274287700653\n",
      "Epoch 17062: train loss: 0.4751273989677429\n",
      "Epoch 17063: train loss: 0.4751273989677429\n",
      "Epoch 17064: train loss: 0.47512727975845337\n",
      "Epoch 17065: train loss: 0.4751272201538086\n",
      "Epoch 17066: train loss: 0.4751271903514862\n",
      "Epoch 17067: train loss: 0.47512713074684143\n",
      "Epoch 17068: train loss: 0.4751270115375519\n",
      "Epoch 17069: train loss: 0.4751270115375519\n",
      "Epoch 17070: train loss: 0.4751269221305847\n",
      "Epoch 17071: train loss: 0.47512686252593994\n",
      "Epoch 17072: train loss: 0.47512680292129517\n",
      "Epoch 17073: train loss: 0.475126713514328\n",
      "Epoch 17074: train loss: 0.475126713514328\n",
      "Epoch 17075: train loss: 0.4751266539096832\n",
      "Epoch 17076: train loss: 0.47512656450271606\n",
      "Epoch 17077: train loss: 0.4751265048980713\n",
      "Epoch 17078: train loss: 0.4751264452934265\n",
      "Epoch 17079: train loss: 0.47512638568878174\n",
      "Epoch 17080: train loss: 0.47512635588645935\n",
      "Epoch 17081: train loss: 0.4751262962818146\n",
      "Epoch 17082: train loss: 0.4751262366771698\n",
      "Epoch 17083: train loss: 0.47512614727020264\n",
      "Epoch 17084: train loss: 0.47512608766555786\n",
      "Epoch 17085: train loss: 0.4751260280609131\n",
      "Epoch 17086: train loss: 0.4751259684562683\n",
      "Epoch 17087: train loss: 0.47512587904930115\n",
      "Epoch 17088: train loss: 0.47512587904930115\n",
      "Epoch 17089: train loss: 0.4751257598400116\n",
      "Epoch 17090: train loss: 0.4751257300376892\n",
      "Epoch 17091: train loss: 0.47512567043304443\n",
      "Epoch 17092: train loss: 0.47512561082839966\n",
      "Epoch 17093: train loss: 0.4751255214214325\n",
      "Epoch 17094: train loss: 0.4751255214214325\n",
      "Epoch 17095: train loss: 0.47512540221214294\n",
      "Epoch 17096: train loss: 0.47512540221214294\n",
      "Epoch 17097: train loss: 0.4751253128051758\n",
      "Epoch 17098: train loss: 0.475125253200531\n",
      "Epoch 17099: train loss: 0.47512519359588623\n",
      "Epoch 17100: train loss: 0.47512513399124146\n",
      "Epoch 17101: train loss: 0.4751250445842743\n",
      "Epoch 17102: train loss: 0.4751249849796295\n",
      "Epoch 17103: train loss: 0.47512489557266235\n",
      "Epoch 17104: train loss: 0.47512489557266235\n",
      "Epoch 17105: train loss: 0.4751247763633728\n",
      "Epoch 17106: train loss: 0.4751247763633728\n",
      "Epoch 17107: train loss: 0.47512468695640564\n",
      "Epoch 17108: train loss: 0.47512462735176086\n",
      "Epoch 17109: train loss: 0.47512462735176086\n",
      "Epoch 17110: train loss: 0.4751245081424713\n",
      "Epoch 17111: train loss: 0.4751244783401489\n",
      "Epoch 17112: train loss: 0.47512441873550415\n",
      "Epoch 17113: train loss: 0.4751243591308594\n",
      "Epoch 17114: train loss: 0.4751242399215698\n",
      "Epoch 17115: train loss: 0.47512421011924744\n",
      "Epoch 17116: train loss: 0.47512415051460266\n",
      "Epoch 17117: train loss: 0.4751240909099579\n",
      "Epoch 17118: train loss: 0.4751240313053131\n",
      "Epoch 17119: train loss: 0.4751240015029907\n",
      "Epoch 17120: train loss: 0.47512394189834595\n",
      "Epoch 17121: train loss: 0.4751238226890564\n",
      "Epoch 17122: train loss: 0.475123792886734\n",
      "Epoch 17123: train loss: 0.47512373328208923\n",
      "Epoch 17124: train loss: 0.47512367367744446\n",
      "Epoch 17125: train loss: 0.4751235842704773\n",
      "Epoch 17126: train loss: 0.4751235842704773\n",
      "Epoch 17127: train loss: 0.4751235246658325\n",
      "Epoch 17128: train loss: 0.47512340545654297\n",
      "Epoch 17129: train loss: 0.4751233756542206\n",
      "Epoch 17130: train loss: 0.4751233160495758\n",
      "Epoch 17131: train loss: 0.4751233160495758\n",
      "Epoch 17132: train loss: 0.47512319684028625\n",
      "Epoch 17133: train loss: 0.47512316703796387\n",
      "Epoch 17134: train loss: 0.4751230478286743\n",
      "Epoch 17135: train loss: 0.47512298822402954\n",
      "Epoch 17136: train loss: 0.47512295842170715\n",
      "Epoch 17137: train loss: 0.4751228988170624\n",
      "Epoch 17138: train loss: 0.4751228392124176\n",
      "Epoch 17139: train loss: 0.4751227796077728\n",
      "Epoch 17140: train loss: 0.47512274980545044\n",
      "Epoch 17141: train loss: 0.47512269020080566\n",
      "Epoch 17142: train loss: 0.4751225709915161\n",
      "Epoch 17143: train loss: 0.4751225709915161\n",
      "Epoch 17144: train loss: 0.4751224219799042\n",
      "Epoch 17145: train loss: 0.4751223623752594\n",
      "Epoch 17146: train loss: 0.4751223623752594\n",
      "Epoch 17147: train loss: 0.475122332572937\n",
      "Epoch 17148: train loss: 0.47512227296829224\n",
      "Epoch 17149: train loss: 0.4751221537590027\n",
      "Epoch 17150: train loss: 0.4751221239566803\n",
      "Epoch 17151: train loss: 0.4751220643520355\n",
      "Epoch 17152: train loss: 0.47512200474739075\n",
      "Epoch 17153: train loss: 0.4751219153404236\n",
      "Epoch 17154: train loss: 0.4751218557357788\n",
      "Epoch 17155: train loss: 0.47512179613113403\n",
      "Epoch 17156: train loss: 0.47512173652648926\n",
      "Epoch 17157: train loss: 0.47512170672416687\n",
      "Epoch 17158: train loss: 0.47512170672416687\n",
      "Epoch 17159: train loss: 0.4751215875148773\n",
      "Epoch 17160: train loss: 0.47512152791023254\n",
      "Epoch 17161: train loss: 0.4751214385032654\n",
      "Epoch 17162: train loss: 0.4751214385032654\n",
      "Epoch 17163: train loss: 0.47512131929397583\n",
      "Epoch 17164: train loss: 0.47512131929397583\n",
      "Epoch 17165: train loss: 0.47512122988700867\n",
      "Epoch 17166: train loss: 0.4751211702823639\n",
      "Epoch 17167: train loss: 0.4751211106777191\n",
      "Epoch 17168: train loss: 0.47512108087539673\n",
      "Epoch 17169: train loss: 0.4751209616661072\n",
      "Epoch 17170: train loss: 0.4751209020614624\n",
      "Epoch 17171: train loss: 0.4751209020614624\n",
      "Epoch 17172: train loss: 0.47512081265449524\n",
      "Epoch 17173: train loss: 0.47512075304985046\n",
      "Epoch 17174: train loss: 0.4751206934452057\n",
      "Epoch 17175: train loss: 0.4751206040382385\n",
      "Epoch 17176: train loss: 0.47512054443359375\n",
      "Epoch 17177: train loss: 0.475120484828949\n",
      "Epoch 17178: train loss: 0.4751204252243042\n",
      "Epoch 17179: train loss: 0.4751204252243042\n",
      "Epoch 17180: train loss: 0.47512033581733704\n",
      "Epoch 17181: train loss: 0.47512027621269226\n",
      "Epoch 17182: train loss: 0.4751202166080475\n",
      "Epoch 17183: train loss: 0.4751201868057251\n",
      "Epoch 17184: train loss: 0.47512006759643555\n",
      "Epoch 17185: train loss: 0.47512000799179077\n",
      "Epoch 17186: train loss: 0.4751199781894684\n",
      "Epoch 17187: train loss: 0.47511985898017883\n",
      "Epoch 17188: train loss: 0.47511985898017883\n",
      "Epoch 17189: train loss: 0.47511976957321167\n",
      "Epoch 17190: train loss: 0.4751197099685669\n",
      "Epoch 17191: train loss: 0.4751196503639221\n",
      "Epoch 17192: train loss: 0.47511959075927734\n",
      "Epoch 17193: train loss: 0.47511956095695496\n",
      "Epoch 17194: train loss: 0.4751194417476654\n",
      "Epoch 17195: train loss: 0.47511938214302063\n",
      "Epoch 17196: train loss: 0.47511938214302063\n",
      "Epoch 17197: train loss: 0.47511929273605347\n",
      "Epoch 17198: train loss: 0.4751192331314087\n",
      "Epoch 17199: train loss: 0.4751191735267639\n",
      "Epoch 17200: train loss: 0.47511908411979675\n",
      "Epoch 17201: train loss: 0.47511908411979675\n",
      "Epoch 17202: train loss: 0.475119024515152\n",
      "Epoch 17203: train loss: 0.4751189649105072\n",
      "Epoch 17204: train loss: 0.47511887550354004\n",
      "Epoch 17205: train loss: 0.47511881589889526\n",
      "Epoch 17206: train loss: 0.4751187562942505\n",
      "Epoch 17207: train loss: 0.4751187264919281\n",
      "Epoch 17208: train loss: 0.47511860728263855\n",
      "Epoch 17209: train loss: 0.4751185476779938\n",
      "Epoch 17210: train loss: 0.4751185476779938\n",
      "Epoch 17211: train loss: 0.4751185178756714\n",
      "Epoch 17212: train loss: 0.47511839866638184\n",
      "Epoch 17213: train loss: 0.47511833906173706\n",
      "Epoch 17214: train loss: 0.4751182496547699\n",
      "Epoch 17215: train loss: 0.4751182496547699\n",
      "Epoch 17216: train loss: 0.47511813044548035\n",
      "Epoch 17217: train loss: 0.47511810064315796\n",
      "Epoch 17218: train loss: 0.4751180410385132\n",
      "Epoch 17219: train loss: 0.4751179814338684\n",
      "Epoch 17220: train loss: 0.47511792182922363\n",
      "Epoch 17221: train loss: 0.47511789202690125\n",
      "Epoch 17222: train loss: 0.47511783242225647\n",
      "Epoch 17223: train loss: 0.4751177728176117\n",
      "Epoch 17224: train loss: 0.47511768341064453\n",
      "Epoch 17225: train loss: 0.47511762380599976\n",
      "Epoch 17226: train loss: 0.4751175045967102\n",
      "Epoch 17227: train loss: 0.475117564201355\n",
      "Epoch 17228: train loss: 0.4751174747943878\n",
      "Epoch 17229: train loss: 0.47511741518974304\n",
      "Epoch 17230: train loss: 0.4751172959804535\n",
      "Epoch 17231: train loss: 0.4751172661781311\n",
      "Epoch 17232: train loss: 0.47511720657348633\n",
      "Epoch 17233: train loss: 0.47511720657348633\n",
      "Epoch 17234: train loss: 0.4751170873641968\n",
      "Epoch 17235: train loss: 0.4751170575618744\n",
      "Epoch 17236: train loss: 0.4751169979572296\n",
      "Epoch 17237: train loss: 0.47511693835258484\n",
      "Epoch 17238: train loss: 0.47511687874794006\n",
      "Epoch 17239: train loss: 0.4751167893409729\n",
      "Epoch 17240: train loss: 0.4751167297363281\n",
      "Epoch 17241: train loss: 0.47511667013168335\n",
      "Epoch 17242: train loss: 0.4751166105270386\n",
      "Epoch 17243: train loss: 0.4751165807247162\n",
      "Epoch 17244: train loss: 0.47511646151542664\n",
      "Epoch 17245: train loss: 0.47511640191078186\n",
      "Epoch 17246: train loss: 0.4751163721084595\n",
      "Epoch 17247: train loss: 0.4751163721084595\n",
      "Epoch 17248: train loss: 0.4751162528991699\n",
      "Epoch 17249: train loss: 0.47511616349220276\n",
      "Epoch 17250: train loss: 0.475116103887558\n",
      "Epoch 17251: train loss: 0.4751160442829132\n",
      "Epoch 17252: train loss: 0.47511598467826843\n",
      "Epoch 17253: train loss: 0.47511595487594604\n",
      "Epoch 17254: train loss: 0.47511589527130127\n",
      "Epoch 17255: train loss: 0.4751158356666565\n",
      "Epoch 17256: train loss: 0.4751157760620117\n",
      "Epoch 17257: train loss: 0.47511574625968933\n",
      "Epoch 17258: train loss: 0.4751156270503998\n",
      "Epoch 17259: train loss: 0.475115567445755\n",
      "Epoch 17260: train loss: 0.4751155376434326\n",
      "Epoch 17261: train loss: 0.47511547803878784\n",
      "Epoch 17262: train loss: 0.47511541843414307\n",
      "Epoch 17263: train loss: 0.4751153290271759\n",
      "Epoch 17264: train loss: 0.4751153290271759\n",
      "Epoch 17265: train loss: 0.47511526942253113\n",
      "Epoch 17266: train loss: 0.4751151502132416\n",
      "Epoch 17267: train loss: 0.4751151204109192\n",
      "Epoch 17268: train loss: 0.4751150608062744\n",
      "Epoch 17269: train loss: 0.47511500120162964\n",
      "Epoch 17270: train loss: 0.47511494159698486\n",
      "Epoch 17271: train loss: 0.4751148521900177\n",
      "Epoch 17272: train loss: 0.4751147925853729\n",
      "Epoch 17273: train loss: 0.47511473298072815\n",
      "Epoch 17274: train loss: 0.47511470317840576\n",
      "Epoch 17275: train loss: 0.475114643573761\n",
      "Epoch 17276: train loss: 0.4751145839691162\n",
      "Epoch 17277: train loss: 0.47511452436447144\n",
      "Epoch 17278: train loss: 0.4751144349575043\n",
      "Epoch 17279: train loss: 0.4751143753528595\n",
      "Epoch 17280: train loss: 0.4751143157482147\n",
      "Epoch 17281: train loss: 0.47511428594589233\n",
      "Epoch 17282: train loss: 0.4751141667366028\n",
      "Epoch 17283: train loss: 0.4751141667366028\n",
      "Epoch 17284: train loss: 0.4751140773296356\n",
      "Epoch 17285: train loss: 0.47511401772499084\n",
      "Epoch 17286: train loss: 0.47511395812034607\n",
      "Epoch 17287: train loss: 0.47511395812034607\n",
      "Epoch 17288: train loss: 0.4751138687133789\n",
      "Epoch 17289: train loss: 0.47511380910873413\n",
      "Epoch 17290: train loss: 0.47511374950408936\n",
      "Epoch 17291: train loss: 0.4751136898994446\n",
      "Epoch 17292: train loss: 0.4751136600971222\n",
      "Epoch 17293: train loss: 0.4751136004924774\n",
      "Epoch 17294: train loss: 0.47511348128318787\n",
      "Epoch 17295: train loss: 0.4751134514808655\n",
      "Epoch 17296: train loss: 0.4751133918762207\n",
      "Epoch 17297: train loss: 0.4751133322715759\n",
      "Epoch 17298: train loss: 0.47511324286460876\n",
      "Epoch 17299: train loss: 0.475113183259964\n",
      "Epoch 17300: train loss: 0.475113183259964\n",
      "Epoch 17301: train loss: 0.47511306405067444\n",
      "Epoch 17302: train loss: 0.47511303424835205\n",
      "Epoch 17303: train loss: 0.4751129150390625\n",
      "Epoch 17304: train loss: 0.4751128554344177\n",
      "Epoch 17305: train loss: 0.4751128554344177\n",
      "Epoch 17306: train loss: 0.47511279582977295\n",
      "Epoch 17307: train loss: 0.4751127064228058\n",
      "Epoch 17308: train loss: 0.4751127064228058\n",
      "Epoch 17309: train loss: 0.47511258721351624\n",
      "Epoch 17310: train loss: 0.47511255741119385\n",
      "Epoch 17311: train loss: 0.4751124978065491\n",
      "Epoch 17312: train loss: 0.4751124382019043\n",
      "Epoch 17313: train loss: 0.4751123785972595\n",
      "Epoch 17314: train loss: 0.47511228919029236\n",
      "Epoch 17315: train loss: 0.4751122295856476\n",
      "Epoch 17316: train loss: 0.4751121699810028\n",
      "Epoch 17317: train loss: 0.4751121401786804\n",
      "Epoch 17318: train loss: 0.47511202096939087\n",
      "Epoch 17319: train loss: 0.4751119613647461\n",
      "Epoch 17320: train loss: 0.4751119315624237\n",
      "Epoch 17321: train loss: 0.47511187195777893\n",
      "Epoch 17322: train loss: 0.47511181235313416\n",
      "Epoch 17323: train loss: 0.4751117527484894\n",
      "Epoch 17324: train loss: 0.475111722946167\n",
      "Epoch 17325: train loss: 0.4751116633415222\n",
      "Epoch 17326: train loss: 0.47511160373687744\n",
      "Epoch 17327: train loss: 0.4751115143299103\n",
      "Epoch 17328: train loss: 0.4751114547252655\n",
      "Epoch 17329: train loss: 0.4751113951206207\n",
      "Epoch 17330: train loss: 0.47511133551597595\n",
      "Epoch 17331: train loss: 0.47511130571365356\n",
      "Epoch 17332: train loss: 0.4751112461090088\n",
      "Epoch 17333: train loss: 0.47511112689971924\n",
      "Epoch 17334: train loss: 0.47511112689971924\n",
      "Epoch 17335: train loss: 0.4751110374927521\n",
      "Epoch 17336: train loss: 0.4751109778881073\n",
      "Epoch 17337: train loss: 0.4751109182834625\n",
      "Epoch 17338: train loss: 0.47511088848114014\n",
      "Epoch 17339: train loss: 0.47511082887649536\n",
      "Epoch 17340: train loss: 0.4751107096672058\n",
      "Epoch 17341: train loss: 0.4751107096672058\n",
      "Epoch 17342: train loss: 0.47511062026023865\n",
      "Epoch 17343: train loss: 0.47511056065559387\n",
      "Epoch 17344: train loss: 0.4751105010509491\n",
      "Epoch 17345: train loss: 0.4751104712486267\n",
      "Epoch 17346: train loss: 0.47511041164398193\n",
      "Epoch 17347: train loss: 0.47511035203933716\n",
      "Epoch 17348: train loss: 0.4751102924346924\n",
      "Epoch 17349: train loss: 0.47511026263237\n",
      "Epoch 17350: train loss: 0.47511014342308044\n",
      "Epoch 17351: train loss: 0.47511014342308044\n",
      "Epoch 17352: train loss: 0.4751100540161133\n",
      "Epoch 17353: train loss: 0.4751099944114685\n",
      "Epoch 17354: train loss: 0.47510987520217896\n",
      "Epoch 17355: train loss: 0.47510987520217896\n",
      "Epoch 17356: train loss: 0.4751097857952118\n",
      "Epoch 17357: train loss: 0.475109726190567\n",
      "Epoch 17358: train loss: 0.475109726190567\n",
      "Epoch 17359: train loss: 0.47510963678359985\n",
      "Epoch 17360: train loss: 0.4751095175743103\n",
      "Epoch 17361: train loss: 0.4751094579696655\n",
      "Epoch 17362: train loss: 0.47510942816734314\n",
      "Epoch 17363: train loss: 0.47510936856269836\n",
      "Epoch 17364: train loss: 0.47510936856269836\n",
      "Epoch 17365: train loss: 0.4751092493534088\n",
      "Epoch 17366: train loss: 0.4751092195510864\n",
      "Epoch 17367: train loss: 0.47510915994644165\n",
      "Epoch 17368: train loss: 0.4751090407371521\n",
      "Epoch 17369: train loss: 0.4751090407371521\n",
      "Epoch 17370: train loss: 0.47510895133018494\n",
      "Epoch 17371: train loss: 0.47510889172554016\n",
      "Epoch 17372: train loss: 0.4751088321208954\n",
      "Epoch 17373: train loss: 0.4751087427139282\n",
      "Epoch 17374: train loss: 0.4751087427139282\n",
      "Epoch 17375: train loss: 0.47510868310928345\n",
      "Epoch 17376: train loss: 0.47510862350463867\n",
      "Epoch 17377: train loss: 0.4751085340976715\n",
      "Epoch 17378: train loss: 0.4751085340976715\n",
      "Epoch 17379: train loss: 0.47510841488838196\n",
      "Epoch 17380: train loss: 0.4751083552837372\n",
      "Epoch 17381: train loss: 0.4751083254814148\n",
      "Epoch 17382: train loss: 0.47510826587677\n",
      "Epoch 17383: train loss: 0.47510820627212524\n",
      "Epoch 17384: train loss: 0.4751081168651581\n",
      "Epoch 17385: train loss: 0.4751081168651581\n",
      "Epoch 17386: train loss: 0.47510799765586853\n",
      "Epoch 17387: train loss: 0.47510793805122375\n",
      "Epoch 17388: train loss: 0.47510790824890137\n",
      "Epoch 17389: train loss: 0.4751077890396118\n",
      "Epoch 17390: train loss: 0.4751077890396118\n",
      "Epoch 17391: train loss: 0.47510769963264465\n",
      "Epoch 17392: train loss: 0.4751075804233551\n",
      "Epoch 17393: train loss: 0.4751075804233551\n",
      "Epoch 17394: train loss: 0.4751075208187103\n",
      "Epoch 17395: train loss: 0.47510743141174316\n",
      "Epoch 17396: train loss: 0.4751073718070984\n",
      "Epoch 17397: train loss: 0.4751073718070984\n",
      "Epoch 17398: train loss: 0.4751072824001312\n",
      "Epoch 17399: train loss: 0.47510722279548645\n",
      "Epoch 17400: train loss: 0.4751071631908417\n",
      "Epoch 17401: train loss: 0.4751071035861969\n",
      "Epoch 17402: train loss: 0.4751070737838745\n",
      "Epoch 17403: train loss: 0.47510701417922974\n",
      "Epoch 17404: train loss: 0.47510695457458496\n",
      "Epoch 17405: train loss: 0.4751068651676178\n",
      "Epoch 17406: train loss: 0.4751068651676178\n",
      "Epoch 17407: train loss: 0.47510674595832825\n",
      "Epoch 17408: train loss: 0.47510668635368347\n",
      "Epoch 17409: train loss: 0.4751066565513611\n",
      "Epoch 17410: train loss: 0.4751066565513611\n",
      "Epoch 17411: train loss: 0.47510653734207153\n",
      "Epoch 17412: train loss: 0.47510644793510437\n",
      "Epoch 17413: train loss: 0.4751063883304596\n",
      "Epoch 17414: train loss: 0.4751063287258148\n",
      "Epoch 17415: train loss: 0.47510626912117004\n",
      "Epoch 17416: train loss: 0.47510623931884766\n",
      "Epoch 17417: train loss: 0.4751061201095581\n",
      "Epoch 17418: train loss: 0.4751061201095581\n",
      "Epoch 17419: train loss: 0.47510606050491333\n",
      "Epoch 17420: train loss: 0.47510603070259094\n",
      "Epoch 17421: train loss: 0.4751059114933014\n",
      "Epoch 17422: train loss: 0.4751058518886566\n",
      "Epoch 17423: train loss: 0.4751058518886566\n",
      "Epoch 17424: train loss: 0.47510576248168945\n",
      "Epoch 17425: train loss: 0.4751057028770447\n",
      "Epoch 17426: train loss: 0.4751056134700775\n",
      "Epoch 17427: train loss: 0.47510549426078796\n",
      "Epoch 17428: train loss: 0.47510555386543274\n",
      "Epoch 17429: train loss: 0.4751054346561432\n",
      "Epoch 17430: train loss: 0.4751054048538208\n",
      "Epoch 17431: train loss: 0.475105345249176\n",
      "Epoch 17432: train loss: 0.47510528564453125\n",
      "Epoch 17433: train loss: 0.4751052260398865\n",
      "Epoch 17434: train loss: 0.4751051664352417\n",
      "Epoch 17435: train loss: 0.47510507702827454\n",
      "Epoch 17436: train loss: 0.47510501742362976\n",
      "Epoch 17437: train loss: 0.475104957818985\n",
      "Epoch 17438: train loss: 0.4751049280166626\n",
      "Epoch 17439: train loss: 0.4751048684120178\n",
      "Epoch 17440: train loss: 0.47510474920272827\n",
      "Epoch 17441: train loss: 0.4751047194004059\n",
      "Epoch 17442: train loss: 0.4751046597957611\n",
      "Epoch 17443: train loss: 0.47510460019111633\n",
      "Epoch 17444: train loss: 0.47510454058647156\n",
      "Epoch 17445: train loss: 0.47510451078414917\n",
      "Epoch 17446: train loss: 0.4751043915748596\n",
      "Epoch 17447: train loss: 0.4751043915748596\n",
      "Epoch 17448: train loss: 0.47510430216789246\n",
      "Epoch 17449: train loss: 0.4751042425632477\n",
      "Epoch 17450: train loss: 0.4751041829586029\n",
      "Epoch 17451: train loss: 0.47510412335395813\n",
      "Epoch 17452: train loss: 0.47510409355163574\n",
      "Epoch 17453: train loss: 0.47510403394699097\n",
      "Epoch 17454: train loss: 0.4751039743423462\n",
      "Epoch 17455: train loss: 0.47510388493537903\n",
      "Epoch 17456: train loss: 0.47510382533073425\n",
      "Epoch 17457: train loss: 0.4751037657260895\n",
      "Epoch 17458: train loss: 0.4751037061214447\n",
      "Epoch 17459: train loss: 0.47510361671447754\n",
      "Epoch 17460: train loss: 0.47510355710983276\n",
      "Epoch 17461: train loss: 0.475103497505188\n",
      "Epoch 17462: train loss: 0.475103497505188\n",
      "Epoch 17463: train loss: 0.4751034080982208\n",
      "Epoch 17464: train loss: 0.4751034080982208\n",
      "Epoch 17465: train loss: 0.4751032888889313\n",
      "Epoch 17466: train loss: 0.4751032590866089\n",
      "Epoch 17467: train loss: 0.4751031994819641\n",
      "Epoch 17468: train loss: 0.47510313987731934\n",
      "Epoch 17469: train loss: 0.4751030504703522\n",
      "Epoch 17470: train loss: 0.4751029908657074\n",
      "Epoch 17471: train loss: 0.4751029312610626\n",
      "Epoch 17472: train loss: 0.47510287165641785\n",
      "Epoch 17473: train loss: 0.47510284185409546\n",
      "Epoch 17474: train loss: 0.4751027822494507\n",
      "Epoch 17475: train loss: 0.47510266304016113\n",
      "Epoch 17476: train loss: 0.47510266304016113\n",
      "Epoch 17477: train loss: 0.47510257363319397\n",
      "Epoch 17478: train loss: 0.4751025140285492\n",
      "Epoch 17479: train loss: 0.4751024544239044\n",
      "Epoch 17480: train loss: 0.47510242462158203\n",
      "Epoch 17481: train loss: 0.47510236501693726\n",
      "Epoch 17482: train loss: 0.4751023054122925\n",
      "Epoch 17483: train loss: 0.4751022458076477\n",
      "Epoch 17484: train loss: 0.47510215640068054\n",
      "Epoch 17485: train loss: 0.47510209679603577\n",
      "Epoch 17486: train loss: 0.4751020073890686\n",
      "Epoch 17487: train loss: 0.47510194778442383\n",
      "Epoch 17488: train loss: 0.47510188817977905\n",
      "Epoch 17489: train loss: 0.47510188817977905\n",
      "Epoch 17490: train loss: 0.4751018285751343\n",
      "Epoch 17491: train loss: 0.4751017987728119\n",
      "Epoch 17492: train loss: 0.4751017391681671\n",
      "Epoch 17493: train loss: 0.47510161995887756\n",
      "Epoch 17494: train loss: 0.4751015901565552\n",
      "Epoch 17495: train loss: 0.4751015305519104\n",
      "Epoch 17496: train loss: 0.47510141134262085\n",
      "Epoch 17497: train loss: 0.4751013517379761\n",
      "Epoch 17498: train loss: 0.4751013219356537\n",
      "Epoch 17499: train loss: 0.4751012623310089\n",
      "Epoch 17500: train loss: 0.47510120272636414\n",
      "Epoch 17501: train loss: 0.47510114312171936\n",
      "Epoch 17502: train loss: 0.475101113319397\n",
      "Epoch 17503: train loss: 0.4751009941101074\n",
      "Epoch 17504: train loss: 0.4751009941101074\n",
      "Epoch 17505: train loss: 0.47510090470314026\n",
      "Epoch 17506: train loss: 0.4751008450984955\n",
      "Epoch 17507: train loss: 0.4751007854938507\n",
      "Epoch 17508: train loss: 0.47510072588920593\n",
      "Epoch 17509: train loss: 0.47510069608688354\n",
      "Epoch 17510: train loss: 0.47510063648223877\n",
      "Epoch 17511: train loss: 0.475100576877594\n",
      "Epoch 17512: train loss: 0.4751005172729492\n",
      "Epoch 17513: train loss: 0.47510042786598206\n",
      "Epoch 17514: train loss: 0.4751003682613373\n",
      "Epoch 17515: train loss: 0.4751003086566925\n",
      "Epoch 17516: train loss: 0.47510021924972534\n",
      "Epoch 17517: train loss: 0.47510015964508057\n",
      "Epoch 17518: train loss: 0.4751001000404358\n",
      "Epoch 17519: train loss: 0.4751000702381134\n",
      "Epoch 17520: train loss: 0.4751000702381134\n",
      "Epoch 17521: train loss: 0.47509995102882385\n",
      "Epoch 17522: train loss: 0.4750998616218567\n",
      "Epoch 17523: train loss: 0.4750998616218567\n",
      "Epoch 17524: train loss: 0.4750998020172119\n",
      "Epoch 17525: train loss: 0.47509974241256714\n",
      "Epoch 17526: train loss: 0.4750996530056\n",
      "Epoch 17527: train loss: 0.4750995934009552\n",
      "Epoch 17528: train loss: 0.4750995337963104\n",
      "Epoch 17529: train loss: 0.47509947419166565\n",
      "Epoch 17530: train loss: 0.47509944438934326\n",
      "Epoch 17531: train loss: 0.4750993847846985\n",
      "Epoch 17532: train loss: 0.4750993251800537\n",
      "Epoch 17533: train loss: 0.47509926557540894\n",
      "Epoch 17534: train loss: 0.4750991761684418\n",
      "Epoch 17535: train loss: 0.4750991761684418\n",
      "Epoch 17536: train loss: 0.475099116563797\n",
      "Epoch 17537: train loss: 0.47509896755218506\n",
      "Epoch 17538: train loss: 0.4750989079475403\n",
      "Epoch 17539: train loss: 0.4750989079475403\n",
      "Epoch 17540: train loss: 0.4750988483428955\n",
      "Epoch 17541: train loss: 0.47509875893592834\n",
      "Epoch 17542: train loss: 0.47509869933128357\n",
      "Epoch 17543: train loss: 0.4750986397266388\n",
      "Epoch 17544: train loss: 0.4750986099243164\n",
      "Epoch 17545: train loss: 0.47509855031967163\n",
      "Epoch 17546: train loss: 0.4750984311103821\n",
      "Epoch 17547: train loss: 0.4750984013080597\n",
      "Epoch 17548: train loss: 0.4750984013080597\n",
      "Epoch 17549: train loss: 0.47509828209877014\n",
      "Epoch 17550: train loss: 0.47509828209877014\n",
      "Epoch 17551: train loss: 0.475098192691803\n",
      "Epoch 17552: train loss: 0.4750981330871582\n",
      "Epoch 17553: train loss: 0.4750980734825134\n",
      "Epoch 17554: train loss: 0.47509801387786865\n",
      "Epoch 17555: train loss: 0.4750979244709015\n",
      "Epoch 17556: train loss: 0.4750978648662567\n",
      "Epoch 17557: train loss: 0.47509780526161194\n",
      "Epoch 17558: train loss: 0.47509777545928955\n",
      "Epoch 17559: train loss: 0.4750977158546448\n",
      "Epoch 17560: train loss: 0.4750975966453552\n",
      "Epoch 17561: train loss: 0.47509753704071045\n",
      "Epoch 17562: train loss: 0.47509750723838806\n",
      "Epoch 17563: train loss: 0.47509750723838806\n",
      "Epoch 17564: train loss: 0.4750973880290985\n",
      "Epoch 17565: train loss: 0.47509732842445374\n",
      "Epoch 17566: train loss: 0.4750972390174866\n",
      "Epoch 17567: train loss: 0.4750972390174866\n",
      "Epoch 17568: train loss: 0.4750971794128418\n",
      "Epoch 17569: train loss: 0.475097119808197\n",
      "Epoch 17570: train loss: 0.47509703040122986\n",
      "Epoch 17571: train loss: 0.47509703040122986\n",
      "Epoch 17572: train loss: 0.4750969111919403\n",
      "Epoch 17573: train loss: 0.4750969111919403\n",
      "Epoch 17574: train loss: 0.47509682178497314\n",
      "Epoch 17575: train loss: 0.47509676218032837\n",
      "Epoch 17576: train loss: 0.4750966727733612\n",
      "Epoch 17577: train loss: 0.47509661316871643\n",
      "Epoch 17578: train loss: 0.47509655356407166\n",
      "Epoch 17579: train loss: 0.4750964939594269\n",
      "Epoch 17580: train loss: 0.4750964045524597\n",
      "Epoch 17581: train loss: 0.4750964045524597\n",
      "Epoch 17582: train loss: 0.47509634494781494\n",
      "Epoch 17583: train loss: 0.4750962555408478\n",
      "Epoch 17584: train loss: 0.475096195936203\n",
      "Epoch 17585: train loss: 0.4750961363315582\n",
      "Epoch 17586: train loss: 0.4750961363315582\n",
      "Epoch 17587: train loss: 0.47509607672691345\n",
      "Epoch 17588: train loss: 0.4750959873199463\n",
      "Epoch 17589: train loss: 0.4750959277153015\n",
      "Epoch 17590: train loss: 0.47509586811065674\n",
      "Epoch 17591: train loss: 0.47509583830833435\n",
      "Epoch 17592: train loss: 0.4750957190990448\n",
      "Epoch 17593: train loss: 0.4750956594944\n",
      "Epoch 17594: train loss: 0.4750956594944\n",
      "Epoch 17595: train loss: 0.47509557008743286\n",
      "Epoch 17596: train loss: 0.4750955104827881\n",
      "Epoch 17597: train loss: 0.4750954508781433\n",
      "Epoch 17598: train loss: 0.4750954210758209\n",
      "Epoch 17599: train loss: 0.47509536147117615\n",
      "Epoch 17600: train loss: 0.4750952422618866\n",
      "Epoch 17601: train loss: 0.4750952422618866\n",
      "Epoch 17602: train loss: 0.47509515285491943\n",
      "Epoch 17603: train loss: 0.4750950336456299\n",
      "Epoch 17604: train loss: 0.4750950336456299\n",
      "Epoch 17605: train loss: 0.4750949442386627\n",
      "Epoch 17606: train loss: 0.4750949442386627\n",
      "Epoch 17607: train loss: 0.47509482502937317\n",
      "Epoch 17608: train loss: 0.47509482502937317\n",
      "Epoch 17609: train loss: 0.475094735622406\n",
      "Epoch 17610: train loss: 0.47509467601776123\n",
      "Epoch 17611: train loss: 0.47509461641311646\n",
      "Epoch 17612: train loss: 0.4750945270061493\n",
      "Epoch 17613: train loss: 0.4750944674015045\n",
      "Epoch 17614: train loss: 0.47509440779685974\n",
      "Epoch 17615: train loss: 0.47509440779685974\n",
      "Epoch 17616: train loss: 0.4750943183898926\n",
      "Epoch 17617: train loss: 0.4750942587852478\n",
      "Epoch 17618: train loss: 0.475094199180603\n",
      "Epoch 17619: train loss: 0.47509410977363586\n",
      "Epoch 17620: train loss: 0.4750940501689911\n",
      "Epoch 17621: train loss: 0.4750940501689911\n",
      "Epoch 17622: train loss: 0.4750939607620239\n",
      "Epoch 17623: train loss: 0.47509390115737915\n",
      "Epoch 17624: train loss: 0.4750938415527344\n",
      "Epoch 17625: train loss: 0.4750937819480896\n",
      "Epoch 17626: train loss: 0.47509369254112244\n",
      "Epoch 17627: train loss: 0.47509369254112244\n",
      "Epoch 17628: train loss: 0.47509363293647766\n",
      "Epoch 17629: train loss: 0.4750935733318329\n",
      "Epoch 17630: train loss: 0.4750934839248657\n",
      "Epoch 17631: train loss: 0.47509342432022095\n",
      "Epoch 17632: train loss: 0.47509336471557617\n",
      "Epoch 17633: train loss: 0.4750933051109314\n",
      "Epoch 17634: train loss: 0.47509321570396423\n",
      "Epoch 17635: train loss: 0.47509315609931946\n",
      "Epoch 17636: train loss: 0.47509315609931946\n",
      "Epoch 17637: train loss: 0.4750930666923523\n",
      "Epoch 17638: train loss: 0.4750930070877075\n",
      "Epoch 17639: train loss: 0.47509294748306274\n",
      "Epoch 17640: train loss: 0.4750928580760956\n",
      "Epoch 17641: train loss: 0.4750927984714508\n",
      "Epoch 17642: train loss: 0.4750927984714508\n",
      "Epoch 17643: train loss: 0.47509267926216125\n",
      "Epoch 17644: train loss: 0.47509267926216125\n",
      "Epoch 17645: train loss: 0.47509264945983887\n",
      "Epoch 17646: train loss: 0.4750925302505493\n",
      "Epoch 17647: train loss: 0.4750925302505493\n",
      "Epoch 17648: train loss: 0.47509247064590454\n",
      "Epoch 17649: train loss: 0.4750923812389374\n",
      "Epoch 17650: train loss: 0.4750923216342926\n",
      "Epoch 17651: train loss: 0.47509223222732544\n",
      "Epoch 17652: train loss: 0.47509223222732544\n",
      "Epoch 17653: train loss: 0.4750921130180359\n",
      "Epoch 17654: train loss: 0.4750920534133911\n",
      "Epoch 17655: train loss: 0.4750920236110687\n",
      "Epoch 17656: train loss: 0.47509196400642395\n",
      "Epoch 17657: train loss: 0.4750919044017792\n",
      "Epoch 17658: train loss: 0.4750918447971344\n",
      "Epoch 17659: train loss: 0.47509175539016724\n",
      "Epoch 17660: train loss: 0.47509169578552246\n",
      "Epoch 17661: train loss: 0.4750916361808777\n",
      "Epoch 17662: train loss: 0.4750916063785553\n",
      "Epoch 17663: train loss: 0.4750915467739105\n",
      "Epoch 17664: train loss: 0.47509148716926575\n",
      "Epoch 17665: train loss: 0.4750913977622986\n",
      "Epoch 17666: train loss: 0.4750913381576538\n",
      "Epoch 17667: train loss: 0.47509127855300903\n",
      "Epoch 17668: train loss: 0.47509121894836426\n",
      "Epoch 17669: train loss: 0.47509118914604187\n",
      "Epoch 17670: train loss: 0.4750911295413971\n",
      "Epoch 17671: train loss: 0.4750910699367523\n",
      "Epoch 17672: train loss: 0.47509098052978516\n",
      "Epoch 17673: train loss: 0.47509098052978516\n",
      "Epoch 17674: train loss: 0.4750908613204956\n",
      "Epoch 17675: train loss: 0.47509080171585083\n",
      "Epoch 17676: train loss: 0.47509071230888367\n",
      "Epoch 17677: train loss: 0.47509071230888367\n",
      "Epoch 17678: train loss: 0.4750906527042389\n",
      "Epoch 17679: train loss: 0.47509056329727173\n",
      "Epoch 17680: train loss: 0.47509056329727173\n",
      "Epoch 17681: train loss: 0.4750904440879822\n",
      "Epoch 17682: train loss: 0.4750904440879822\n",
      "Epoch 17683: train loss: 0.475090354681015\n",
      "Epoch 17684: train loss: 0.475090354681015\n",
      "Epoch 17685: train loss: 0.47509023547172546\n",
      "Epoch 17686: train loss: 0.4750901758670807\n",
      "Epoch 17687: train loss: 0.4750901460647583\n",
      "Epoch 17688: train loss: 0.47509002685546875\n",
      "Epoch 17689: train loss: 0.475089967250824\n",
      "Epoch 17690: train loss: 0.4750899076461792\n",
      "Epoch 17691: train loss: 0.4750898778438568\n",
      "Epoch 17692: train loss: 0.47508981823921204\n",
      "Epoch 17693: train loss: 0.4750896990299225\n",
      "Epoch 17694: train loss: 0.4750896692276001\n",
      "Epoch 17695: train loss: 0.4750896096229553\n",
      "Epoch 17696: train loss: 0.47508955001831055\n",
      "Epoch 17697: train loss: 0.47508949041366577\n",
      "Epoch 17698: train loss: 0.4750894606113434\n",
      "Epoch 17699: train loss: 0.47508934140205383\n",
      "Epoch 17700: train loss: 0.47508934140205383\n",
      "Epoch 17701: train loss: 0.47508928179740906\n",
      "Epoch 17702: train loss: 0.47508925199508667\n",
      "Epoch 17703: train loss: 0.4750891923904419\n",
      "Epoch 17704: train loss: 0.4750891327857971\n",
      "Epoch 17705: train loss: 0.47508904337882996\n",
      "Epoch 17706: train loss: 0.4750889837741852\n",
      "Epoch 17707: train loss: 0.4750889241695404\n",
      "Epoch 17708: train loss: 0.47508883476257324\n",
      "Epoch 17709: train loss: 0.47508883476257324\n",
      "Epoch 17710: train loss: 0.47508877515792847\n",
      "Epoch 17711: train loss: 0.4750886559486389\n",
      "Epoch 17712: train loss: 0.47508862614631653\n",
      "Epoch 17713: train loss: 0.47508856654167175\n",
      "Epoch 17714: train loss: 0.475088506937027\n",
      "Epoch 17715: train loss: 0.4750884473323822\n",
      "Epoch 17716: train loss: 0.47508835792541504\n",
      "Epoch 17717: train loss: 0.47508829832077026\n",
      "Epoch 17718: train loss: 0.47508829832077026\n",
      "Epoch 17719: train loss: 0.4750882387161255\n",
      "Epoch 17720: train loss: 0.4750881493091583\n",
      "Epoch 17721: train loss: 0.4750880300998688\n",
      "Epoch 17722: train loss: 0.4750880300998688\n",
      "Epoch 17723: train loss: 0.4750879406929016\n",
      "Epoch 17724: train loss: 0.47508788108825684\n",
      "Epoch 17725: train loss: 0.47508788108825684\n",
      "Epoch 17726: train loss: 0.4750877916812897\n",
      "Epoch 17727: train loss: 0.4750877320766449\n",
      "Epoch 17728: train loss: 0.47508761286735535\n",
      "Epoch 17729: train loss: 0.47508761286735535\n",
      "Epoch 17730: train loss: 0.4750875234603882\n",
      "Epoch 17731: train loss: 0.4750875234603882\n",
      "Epoch 17732: train loss: 0.47508740425109863\n",
      "Epoch 17733: train loss: 0.47508740425109863\n",
      "Epoch 17734: train loss: 0.47508731484413147\n",
      "Epoch 17735: train loss: 0.4750872552394867\n",
      "Epoch 17736: train loss: 0.4750871956348419\n",
      "Epoch 17737: train loss: 0.47508710622787476\n",
      "Epoch 17738: train loss: 0.47508710622787476\n",
      "Epoch 17739: train loss: 0.47508710622787476\n",
      "Epoch 17740: train loss: 0.4750869870185852\n",
      "Epoch 17741: train loss: 0.47508689761161804\n",
      "Epoch 17742: train loss: 0.47508689761161804\n",
      "Epoch 17743: train loss: 0.47508683800697327\n",
      "Epoch 17744: train loss: 0.4750867486000061\n",
      "Epoch 17745: train loss: 0.47508668899536133\n",
      "Epoch 17746: train loss: 0.47508662939071655\n",
      "Epoch 17747: train loss: 0.4750865697860718\n",
      "Epoch 17748: train loss: 0.4750864803791046\n",
      "Epoch 17749: train loss: 0.47508636116981506\n",
      "Epoch 17750: train loss: 0.47508636116981506\n",
      "Epoch 17751: train loss: 0.4750862717628479\n",
      "Epoch 17752: train loss: 0.4750862717628479\n",
      "Epoch 17753: train loss: 0.4750862717628479\n",
      "Epoch 17754: train loss: 0.47508615255355835\n",
      "Epoch 17755: train loss: 0.4750860929489136\n",
      "Epoch 17756: train loss: 0.4750860631465912\n",
      "Epoch 17757: train loss: 0.47508594393730164\n",
      "Epoch 17758: train loss: 0.47508588433265686\n",
      "Epoch 17759: train loss: 0.4750858545303345\n",
      "Epoch 17760: train loss: 0.4750857949256897\n",
      "Epoch 17761: train loss: 0.47508567571640015\n",
      "Epoch 17762: train loss: 0.47508567571640015\n",
      "Epoch 17763: train loss: 0.47508564591407776\n",
      "Epoch 17764: train loss: 0.4750855267047882\n",
      "Epoch 17765: train loss: 0.47508546710014343\n",
      "Epoch 17766: train loss: 0.47508543729782104\n",
      "Epoch 17767: train loss: 0.4750853180885315\n",
      "Epoch 17768: train loss: 0.4750852584838867\n",
      "Epoch 17769: train loss: 0.4750852584838867\n",
      "Epoch 17770: train loss: 0.47508522868156433\n",
      "Epoch 17771: train loss: 0.4750851094722748\n",
      "Epoch 17772: train loss: 0.47508504986763\n",
      "Epoch 17773: train loss: 0.4750850200653076\n",
      "Epoch 17774: train loss: 0.47508496046066284\n",
      "Epoch 17775: train loss: 0.47508490085601807\n",
      "Epoch 17776: train loss: 0.4750848412513733\n",
      "Epoch 17777: train loss: 0.4750848114490509\n",
      "Epoch 17778: train loss: 0.47508469223976135\n",
      "Epoch 17779: train loss: 0.47508469223976135\n",
      "Epoch 17780: train loss: 0.4750846028327942\n",
      "Epoch 17781: train loss: 0.4750845432281494\n",
      "Epoch 17782: train loss: 0.47508448362350464\n",
      "Epoch 17783: train loss: 0.47508442401885986\n",
      "Epoch 17784: train loss: 0.4750843346118927\n",
      "Epoch 17785: train loss: 0.4750843346118927\n",
      "Epoch 17786: train loss: 0.47508421540260315\n",
      "Epoch 17787: train loss: 0.47508418560028076\n",
      "Epoch 17788: train loss: 0.47508418560028076\n",
      "Epoch 17789: train loss: 0.4750840663909912\n",
      "Epoch 17790: train loss: 0.47508400678634644\n",
      "Epoch 17791: train loss: 0.47508397698402405\n",
      "Epoch 17792: train loss: 0.4750839173793793\n",
      "Epoch 17793: train loss: 0.4750838577747345\n",
      "Epoch 17794: train loss: 0.47508376836776733\n",
      "Epoch 17795: train loss: 0.47508370876312256\n",
      "Epoch 17796: train loss: 0.4750836491584778\n",
      "Epoch 17797: train loss: 0.475083589553833\n",
      "Epoch 17798: train loss: 0.4750835597515106\n",
      "Epoch 17799: train loss: 0.47508344054222107\n",
      "Epoch 17800: train loss: 0.47508344054222107\n",
      "Epoch 17801: train loss: 0.4750833809375763\n",
      "Epoch 17802: train loss: 0.47508329153060913\n",
      "Epoch 17803: train loss: 0.47508323192596436\n",
      "Epoch 17804: train loss: 0.4750831723213196\n",
      "Epoch 17805: train loss: 0.4750831425189972\n",
      "Epoch 17806: train loss: 0.4750830829143524\n",
      "Epoch 17807: train loss: 0.47508302330970764\n",
      "Epoch 17808: train loss: 0.4750829339027405\n",
      "Epoch 17809: train loss: 0.4750829339027405\n",
      "Epoch 17810: train loss: 0.4750828146934509\n",
      "Epoch 17811: train loss: 0.47508275508880615\n",
      "Epoch 17812: train loss: 0.47508272528648376\n",
      "Epoch 17813: train loss: 0.475082665681839\n",
      "Epoch 17814: train loss: 0.4750826060771942\n",
      "Epoch 17815: train loss: 0.47508254647254944\n",
      "Epoch 17816: train loss: 0.47508251667022705\n",
      "Epoch 17817: train loss: 0.4750823378562927\n",
      "Epoch 17818: train loss: 0.4750823974609375\n",
      "Epoch 17819: train loss: 0.4750823378562927\n",
      "Epoch 17820: train loss: 0.4750821888446808\n",
      "Epoch 17821: train loss: 0.4750821888446808\n",
      "Epoch 17822: train loss: 0.47508206963539124\n",
      "Epoch 17823: train loss: 0.47508203983306885\n",
      "Epoch 17824: train loss: 0.47508203983306885\n",
      "Epoch 17825: train loss: 0.4750819206237793\n",
      "Epoch 17826: train loss: 0.4750819206237793\n",
      "Epoch 17827: train loss: 0.47508183121681213\n",
      "Epoch 17828: train loss: 0.4750817120075226\n",
      "Epoch 17829: train loss: 0.4750817120075226\n",
      "Epoch 17830: train loss: 0.4750816226005554\n",
      "Epoch 17831: train loss: 0.47508156299591064\n",
      "Epoch 17832: train loss: 0.47508156299591064\n",
      "Epoch 17833: train loss: 0.4750814437866211\n",
      "Epoch 17834: train loss: 0.4750814139842987\n",
      "Epoch 17835: train loss: 0.47508135437965393\n",
      "Epoch 17836: train loss: 0.47508129477500916\n",
      "Epoch 17837: train loss: 0.475081205368042\n",
      "Epoch 17838: train loss: 0.4750811457633972\n",
      "Epoch 17839: train loss: 0.47508108615875244\n",
      "Epoch 17840: train loss: 0.47508102655410767\n",
      "Epoch 17841: train loss: 0.47508102655410767\n",
      "Epoch 17842: train loss: 0.4750809371471405\n",
      "Epoch 17843: train loss: 0.4750808775424957\n",
      "Epoch 17844: train loss: 0.47508081793785095\n",
      "Epoch 17845: train loss: 0.47508078813552856\n",
      "Epoch 17846: train loss: 0.4750807285308838\n",
      "Epoch 17847: train loss: 0.47508060932159424\n",
      "Epoch 17848: train loss: 0.47508057951927185\n",
      "Epoch 17849: train loss: 0.4750805199146271\n",
      "Epoch 17850: train loss: 0.4750805199146271\n",
      "Epoch 17851: train loss: 0.4750804007053375\n",
      "Epoch 17852: train loss: 0.47508031129837036\n",
      "Epoch 17853: train loss: 0.47508031129837036\n",
      "Epoch 17854: train loss: 0.4750801920890808\n",
      "Epoch 17855: train loss: 0.4750801920890808\n",
      "Epoch 17856: train loss: 0.47508010268211365\n",
      "Epoch 17857: train loss: 0.47508004307746887\n",
      "Epoch 17858: train loss: 0.4750799834728241\n",
      "Epoch 17859: train loss: 0.4750799834728241\n",
      "Epoch 17860: train loss: 0.47507989406585693\n",
      "Epoch 17861: train loss: 0.47507983446121216\n",
      "Epoch 17862: train loss: 0.475079745054245\n",
      "Epoch 17863: train loss: 0.4750796854496002\n",
      "Epoch 17864: train loss: 0.47507956624031067\n",
      "Epoch 17865: train loss: 0.47507956624031067\n",
      "Epoch 17866: train loss: 0.4750795364379883\n",
      "Epoch 17867: train loss: 0.4750794768333435\n",
      "Epoch 17868: train loss: 0.47507935762405396\n",
      "Epoch 17869: train loss: 0.47507935762405396\n",
      "Epoch 17870: train loss: 0.4750792682170868\n",
      "Epoch 17871: train loss: 0.4750792682170868\n",
      "Epoch 17872: train loss: 0.47507914900779724\n",
      "Epoch 17873: train loss: 0.47507911920547485\n",
      "Epoch 17874: train loss: 0.4750790596008301\n",
      "Epoch 17875: train loss: 0.4750789999961853\n",
      "Epoch 17876: train loss: 0.4750789403915405\n",
      "Epoch 17877: train loss: 0.47507885098457336\n",
      "Epoch 17878: train loss: 0.4750787913799286\n",
      "Epoch 17879: train loss: 0.4750787019729614\n",
      "Epoch 17880: train loss: 0.4750787019729614\n",
      "Epoch 17881: train loss: 0.47507864236831665\n",
      "Epoch 17882: train loss: 0.4750785827636719\n",
      "Epoch 17883: train loss: 0.4750785231590271\n",
      "Epoch 17884: train loss: 0.4750784635543823\n",
      "Epoch 17885: train loss: 0.47507837414741516\n",
      "Epoch 17886: train loss: 0.4750783145427704\n",
      "Epoch 17887: train loss: 0.4750782549381256\n",
      "Epoch 17888: train loss: 0.4750782251358032\n",
      "Epoch 17889: train loss: 0.47507810592651367\n",
      "Epoch 17890: train loss: 0.47507810592651367\n",
      "Epoch 17891: train loss: 0.4750780165195465\n",
      "Epoch 17892: train loss: 0.47507795691490173\n",
      "Epoch 17893: train loss: 0.47507789731025696\n",
      "Epoch 17894: train loss: 0.4750778377056122\n",
      "Epoch 17895: train loss: 0.4750778079032898\n",
      "Epoch 17896: train loss: 0.475077748298645\n",
      "Epoch 17897: train loss: 0.47507768869400024\n",
      "Epoch 17898: train loss: 0.47507762908935547\n",
      "Epoch 17899: train loss: 0.4750775396823883\n",
      "Epoch 17900: train loss: 0.4750775396823883\n",
      "Epoch 17901: train loss: 0.47507748007774353\n",
      "Epoch 17902: train loss: 0.47507739067077637\n",
      "Epoch 17903: train loss: 0.4750773310661316\n",
      "Epoch 17904: train loss: 0.4750772714614868\n",
      "Epoch 17905: train loss: 0.47507721185684204\n",
      "Epoch 17906: train loss: 0.47507718205451965\n",
      "Epoch 17907: train loss: 0.4750771224498749\n",
      "Epoch 17908: train loss: 0.4750770032405853\n",
      "Epoch 17909: train loss: 0.4750770032405853\n",
      "Epoch 17910: train loss: 0.47507691383361816\n",
      "Epoch 17911: train loss: 0.4750768542289734\n",
      "Epoch 17912: train loss: 0.4750767946243286\n",
      "Epoch 17913: train loss: 0.4750767648220062\n",
      "Epoch 17914: train loss: 0.47507670521736145\n",
      "Epoch 17915: train loss: 0.4750766456127167\n",
      "Epoch 17916: train loss: 0.4750765562057495\n",
      "Epoch 17917: train loss: 0.4750765562057495\n",
      "Epoch 17918: train loss: 0.47507643699645996\n",
      "Epoch 17919: train loss: 0.4750763773918152\n",
      "Epoch 17920: train loss: 0.4750763475894928\n",
      "Epoch 17921: train loss: 0.475076287984848\n",
      "Epoch 17922: train loss: 0.47507622838020325\n",
      "Epoch 17923: train loss: 0.47507616877555847\n",
      "Epoch 17924: train loss: 0.4750761389732361\n",
      "Epoch 17925: train loss: 0.47507601976394653\n",
      "Epoch 17926: train loss: 0.47507596015930176\n",
      "Epoch 17927: train loss: 0.47507593035697937\n",
      "Epoch 17928: train loss: 0.4750758707523346\n",
      "Epoch 17929: train loss: 0.4750758111476898\n",
      "Epoch 17930: train loss: 0.47507572174072266\n",
      "Epoch 17931: train loss: 0.47507572174072266\n",
      "Epoch 17932: train loss: 0.4750756025314331\n",
      "Epoch 17933: train loss: 0.47507554292678833\n",
      "Epoch 17934: train loss: 0.47507551312446594\n",
      "Epoch 17935: train loss: 0.47507545351982117\n",
      "Epoch 17936: train loss: 0.4750753343105316\n",
      "Epoch 17937: train loss: 0.47507530450820923\n",
      "Epoch 17938: train loss: 0.47507524490356445\n",
      "Epoch 17939: train loss: 0.4750751852989197\n",
      "Epoch 17940: train loss: 0.4750751256942749\n",
      "Epoch 17941: train loss: 0.47507503628730774\n",
      "Epoch 17942: train loss: 0.47507503628730774\n",
      "Epoch 17943: train loss: 0.47507497668266296\n",
      "Epoch 17944: train loss: 0.4750749170780182\n",
      "Epoch 17945: train loss: 0.4750748872756958\n",
      "Epoch 17946: train loss: 0.47507476806640625\n",
      "Epoch 17947: train loss: 0.47507476806640625\n",
      "Epoch 17948: train loss: 0.4750746488571167\n",
      "Epoch 17949: train loss: 0.47507455945014954\n",
      "Epoch 17950: train loss: 0.47507455945014954\n",
      "Epoch 17951: train loss: 0.47507444024086\n",
      "Epoch 17952: train loss: 0.47507444024086\n",
      "Epoch 17953: train loss: 0.4750743508338928\n",
      "Epoch 17954: train loss: 0.47507423162460327\n",
      "Epoch 17955: train loss: 0.47507423162460327\n",
      "Epoch 17956: train loss: 0.4750742018222809\n",
      "Epoch 17957: train loss: 0.4750741422176361\n",
      "Epoch 17958: train loss: 0.47507408261299133\n",
      "Epoch 17959: train loss: 0.47507402300834656\n",
      "Epoch 17960: train loss: 0.4750739336013794\n",
      "Epoch 17961: train loss: 0.4750739336013794\n",
      "Epoch 17962: train loss: 0.47507381439208984\n",
      "Epoch 17963: train loss: 0.47507381439208984\n",
      "Epoch 17964: train loss: 0.4750737249851227\n",
      "Epoch 17965: train loss: 0.4750736653804779\n",
      "Epoch 17966: train loss: 0.47507360577583313\n",
      "Epoch 17967: train loss: 0.47507351636886597\n",
      "Epoch 17968: train loss: 0.47507351636886597\n",
      "Epoch 17969: train loss: 0.4750734567642212\n",
      "Epoch 17970: train loss: 0.4750733971595764\n",
      "Epoch 17971: train loss: 0.47507336735725403\n",
      "Epoch 17972: train loss: 0.4750731885433197\n",
      "Epoch 17973: train loss: 0.4750731885433197\n",
      "Epoch 17974: train loss: 0.4750731587409973\n",
      "Epoch 17975: train loss: 0.47507309913635254\n",
      "Epoch 17976: train loss: 0.475072979927063\n",
      "Epoch 17977: train loss: 0.475072979927063\n",
      "Epoch 17978: train loss: 0.4750728905200958\n",
      "Epoch 17979: train loss: 0.4750728905200958\n",
      "Epoch 17980: train loss: 0.4750727713108063\n",
      "Epoch 17981: train loss: 0.4750727415084839\n",
      "Epoch 17982: train loss: 0.4750726819038391\n",
      "Epoch 17983: train loss: 0.47507262229919434\n",
      "Epoch 17984: train loss: 0.47507256269454956\n",
      "Epoch 17985: train loss: 0.4750725328922272\n",
      "Epoch 17986: train loss: 0.4750724732875824\n",
      "Epoch 17987: train loss: 0.47507235407829285\n",
      "Epoch 17988: train loss: 0.47507232427597046\n",
      "Epoch 17989: train loss: 0.4750722646713257\n",
      "Epoch 17990: train loss: 0.4750722646713257\n",
      "Epoch 17991: train loss: 0.47507214546203613\n",
      "Epoch 17992: train loss: 0.47507211565971375\n",
      "Epoch 17993: train loss: 0.47507205605506897\n",
      "Epoch 17994: train loss: 0.4750719964504242\n",
      "Epoch 17995: train loss: 0.47507190704345703\n",
      "Epoch 17996: train loss: 0.47507184743881226\n",
      "Epoch 17997: train loss: 0.4750717878341675\n",
      "Epoch 17998: train loss: 0.4750717282295227\n",
      "Epoch 17999: train loss: 0.47507163882255554\n",
      "Epoch 18000: train loss: 0.47507163882255554\n",
      "Epoch 18001: train loss: 0.47507157921791077\n",
      "Epoch 18002: train loss: 0.4750714898109436\n",
      "Epoch 18003: train loss: 0.47507143020629883\n",
      "Epoch 18004: train loss: 0.4750713109970093\n",
      "Epoch 18005: train loss: 0.4750713109970093\n",
      "Epoch 18006: train loss: 0.4750712811946869\n",
      "Epoch 18007: train loss: 0.4750712215900421\n",
      "Epoch 18008: train loss: 0.47507116198539734\n",
      "Epoch 18009: train loss: 0.4750710725784302\n",
      "Epoch 18010: train loss: 0.4750710129737854\n",
      "Epoch 18011: train loss: 0.4750709533691406\n",
      "Epoch 18012: train loss: 0.47507089376449585\n",
      "Epoch 18013: train loss: 0.4750708043575287\n",
      "Epoch 18014: train loss: 0.4750708043575287\n",
      "Epoch 18015: train loss: 0.47507068514823914\n",
      "Epoch 18016: train loss: 0.47507068514823914\n",
      "Epoch 18017: train loss: 0.475070595741272\n",
      "Epoch 18018: train loss: 0.475070595741272\n",
      "Epoch 18019: train loss: 0.4750704765319824\n",
      "Epoch 18020: train loss: 0.47507041692733765\n",
      "Epoch 18021: train loss: 0.47507038712501526\n",
      "Epoch 18022: train loss: 0.4750702679157257\n",
      "Epoch 18023: train loss: 0.47507020831108093\n",
      "Epoch 18024: train loss: 0.47507017850875854\n",
      "Epoch 18025: train loss: 0.47507011890411377\n",
      "Epoch 18026: train loss: 0.475070059299469\n",
      "Epoch 18027: train loss: 0.475070059299469\n",
      "Epoch 18028: train loss: 0.47506996989250183\n",
      "Epoch 18029: train loss: 0.47506991028785706\n",
      "Epoch 18030: train loss: 0.4750698506832123\n",
      "Epoch 18031: train loss: 0.4750697612762451\n",
      "Epoch 18032: train loss: 0.47506970167160034\n",
      "Epoch 18033: train loss: 0.47506970167160034\n",
      "Epoch 18034: train loss: 0.4750695824623108\n",
      "Epoch 18035: train loss: 0.4750695526599884\n",
      "Epoch 18036: train loss: 0.47506949305534363\n",
      "Epoch 18037: train loss: 0.4750693738460541\n",
      "Epoch 18038: train loss: 0.4750693440437317\n",
      "Epoch 18039: train loss: 0.4750692844390869\n",
      "Epoch 18040: train loss: 0.47506922483444214\n",
      "Epoch 18041: train loss: 0.47506916522979736\n",
      "Epoch 18042: train loss: 0.475069135427475\n",
      "Epoch 18043: train loss: 0.4750690758228302\n",
      "Epoch 18044: train loss: 0.4750690162181854\n",
      "Epoch 18045: train loss: 0.47506895661354065\n",
      "Epoch 18046: train loss: 0.47506892681121826\n",
      "Epoch 18047: train loss: 0.4750688076019287\n",
      "Epoch 18048: train loss: 0.4750688076019287\n",
      "Epoch 18049: train loss: 0.47506871819496155\n",
      "Epoch 18050: train loss: 0.4750686585903168\n",
      "Epoch 18051: train loss: 0.475068598985672\n",
      "Epoch 18052: train loss: 0.4750685393810272\n",
      "Epoch 18053: train loss: 0.47506850957870483\n",
      "Epoch 18054: train loss: 0.47506844997406006\n",
      "Epoch 18055: train loss: 0.4750683903694153\n",
      "Epoch 18056: train loss: 0.4750683307647705\n",
      "Epoch 18057: train loss: 0.47506824135780334\n",
      "Epoch 18058: train loss: 0.47506818175315857\n",
      "Epoch 18059: train loss: 0.4750681221485138\n",
      "Epoch 18060: train loss: 0.4750680923461914\n",
      "Epoch 18061: train loss: 0.47506803274154663\n",
      "Epoch 18062: train loss: 0.47506797313690186\n",
      "Epoch 18063: train loss: 0.4750678837299347\n",
      "Epoch 18064: train loss: 0.4750678241252899\n",
      "Epoch 18065: train loss: 0.47506776452064514\n",
      "Epoch 18066: train loss: 0.475067675113678\n",
      "Epoch 18067: train loss: 0.4750676155090332\n",
      "Epoch 18068: train loss: 0.4750675559043884\n",
      "Epoch 18069: train loss: 0.4750675559043884\n",
      "Epoch 18070: train loss: 0.47506749629974365\n",
      "Epoch 18071: train loss: 0.47506746649742126\n",
      "Epoch 18072: train loss: 0.4750673472881317\n",
      "Epoch 18073: train loss: 0.47506728768348694\n",
      "Epoch 18074: train loss: 0.47506725788116455\n",
      "Epoch 18075: train loss: 0.4750671982765198\n",
      "Epoch 18076: train loss: 0.4750670790672302\n",
      "Epoch 18077: train loss: 0.4750670790672302\n",
      "Epoch 18078: train loss: 0.47506701946258545\n",
      "Epoch 18079: train loss: 0.47506698966026306\n",
      "Epoch 18080: train loss: 0.4750668704509735\n",
      "Epoch 18081: train loss: 0.47506681084632874\n",
      "Epoch 18082: train loss: 0.47506678104400635\n",
      "Epoch 18083: train loss: 0.4750667214393616\n",
      "Epoch 18084: train loss: 0.4750666618347168\n",
      "Epoch 18085: train loss: 0.475066602230072\n",
      "Epoch 18086: train loss: 0.475066602230072\n",
      "Epoch 18087: train loss: 0.47506651282310486\n",
      "Epoch 18088: train loss: 0.4750663936138153\n",
      "Epoch 18089: train loss: 0.4750663638114929\n",
      "Epoch 18090: train loss: 0.47506630420684814\n",
      "Epoch 18091: train loss: 0.4750661849975586\n",
      "Epoch 18092: train loss: 0.4750661849975586\n",
      "Epoch 18093: train loss: 0.4750661551952362\n",
      "Epoch 18094: train loss: 0.47506609559059143\n",
      "Epoch 18095: train loss: 0.47506603598594666\n",
      "Epoch 18096: train loss: 0.4750659465789795\n",
      "Epoch 18097: train loss: 0.4750658869743347\n",
      "Epoch 18098: train loss: 0.47506582736968994\n",
      "Epoch 18099: train loss: 0.4750657379627228\n",
      "Epoch 18100: train loss: 0.4750657379627228\n",
      "Epoch 18101: train loss: 0.4750656187534332\n",
      "Epoch 18102: train loss: 0.4750656187534332\n",
      "Epoch 18103: train loss: 0.47506555914878845\n",
      "Epoch 18104: train loss: 0.4750654697418213\n",
      "Epoch 18105: train loss: 0.4750654101371765\n",
      "Epoch 18106: train loss: 0.47506535053253174\n",
      "Epoch 18107: train loss: 0.47506532073020935\n",
      "Epoch 18108: train loss: 0.4750652611255646\n",
      "Epoch 18109: train loss: 0.4750652015209198\n",
      "Epoch 18110: train loss: 0.475065141916275\n",
      "Epoch 18111: train loss: 0.47506505250930786\n",
      "Epoch 18112: train loss: 0.4750649929046631\n",
      "Epoch 18113: train loss: 0.4750649333000183\n",
      "Epoch 18114: train loss: 0.4750649034976959\n",
      "Epoch 18115: train loss: 0.47506484389305115\n",
      "Epoch 18116: train loss: 0.47506478428840637\n",
      "Epoch 18117: train loss: 0.4750647246837616\n",
      "Epoch 18118: train loss: 0.47506463527679443\n",
      "Epoch 18119: train loss: 0.47506457567214966\n",
      "Epoch 18120: train loss: 0.47506457567214966\n",
      "Epoch 18121: train loss: 0.4750644862651825\n",
      "Epoch 18122: train loss: 0.4750644266605377\n",
      "Epoch 18123: train loss: 0.47506436705589294\n",
      "Epoch 18124: train loss: 0.4750642776489258\n",
      "Epoch 18125: train loss: 0.4750642776489258\n",
      "Epoch 18126: train loss: 0.47506415843963623\n",
      "Epoch 18127: train loss: 0.47506409883499146\n",
      "Epoch 18128: train loss: 0.47506409883499146\n",
      "Epoch 18129: train loss: 0.4750640094280243\n",
      "Epoch 18130: train loss: 0.4750639498233795\n",
      "Epoch 18131: train loss: 0.47506389021873474\n",
      "Epoch 18132: train loss: 0.47506386041641235\n",
      "Epoch 18133: train loss: 0.4750638008117676\n",
      "Epoch 18134: train loss: 0.475063681602478\n",
      "Epoch 18135: train loss: 0.475063681602478\n",
      "Epoch 18136: train loss: 0.47506365180015564\n",
      "Epoch 18137: train loss: 0.47506359219551086\n",
      "Epoch 18138: train loss: 0.4750634729862213\n",
      "Epoch 18139: train loss: 0.4750634729862213\n",
      "Epoch 18140: train loss: 0.47506338357925415\n",
      "Epoch 18141: train loss: 0.4750633239746094\n",
      "Epoch 18142: train loss: 0.4750632643699646\n",
      "Epoch 18143: train loss: 0.47506317496299744\n",
      "Epoch 18144: train loss: 0.4750630557537079\n",
      "Epoch 18145: train loss: 0.4750630557537079\n",
      "Epoch 18146: train loss: 0.4750629961490631\n",
      "Epoch 18147: train loss: 0.4750629663467407\n",
      "Epoch 18148: train loss: 0.47506284713745117\n",
      "Epoch 18149: train loss: 0.47506284713745117\n",
      "Epoch 18150: train loss: 0.4750627875328064\n",
      "Epoch 18151: train loss: 0.47506269812583923\n",
      "Epoch 18152: train loss: 0.47506269812583923\n",
      "Epoch 18153: train loss: 0.4750625789165497\n",
      "Epoch 18154: train loss: 0.4750625491142273\n",
      "Epoch 18155: train loss: 0.4750624895095825\n",
      "Epoch 18156: train loss: 0.47506242990493774\n",
      "Epoch 18157: train loss: 0.47506237030029297\n",
      "Epoch 18158: train loss: 0.4750623404979706\n",
      "Epoch 18159: train loss: 0.47506222128868103\n",
      "Epoch 18160: train loss: 0.47506213188171387\n",
      "Epoch 18161: train loss: 0.47506213188171387\n",
      "Epoch 18162: train loss: 0.4750620722770691\n",
      "Epoch 18163: train loss: 0.4750620126724243\n",
      "Epoch 18164: train loss: 0.47506195306777954\n",
      "Epoch 18165: train loss: 0.47506192326545715\n",
      "Epoch 18166: train loss: 0.4750618636608124\n",
      "Epoch 18167: train loss: 0.4750617444515228\n",
      "Epoch 18168: train loss: 0.4750617444515228\n",
      "Epoch 18169: train loss: 0.47506165504455566\n",
      "Epoch 18170: train loss: 0.4750615954399109\n",
      "Epoch 18171: train loss: 0.4750615954399109\n",
      "Epoch 18172: train loss: 0.4750615060329437\n",
      "Epoch 18173: train loss: 0.4750613868236542\n",
      "Epoch 18174: train loss: 0.4750613272190094\n",
      "Epoch 18175: train loss: 0.4750613272190094\n",
      "Epoch 18176: train loss: 0.47506117820739746\n",
      "Epoch 18177: train loss: 0.47506117820739746\n",
      "Epoch 18178: train loss: 0.47506117820739746\n",
      "Epoch 18179: train loss: 0.4750610291957855\n",
      "Epoch 18180: train loss: 0.4750610291957855\n",
      "Epoch 18181: train loss: 0.47506096959114075\n",
      "Epoch 18182: train loss: 0.4750608801841736\n",
      "Epoch 18183: train loss: 0.4750608801841736\n",
      "Epoch 18184: train loss: 0.4750608205795288\n",
      "Epoch 18185: train loss: 0.47506070137023926\n",
      "Epoch 18186: train loss: 0.47506067156791687\n",
      "Epoch 18187: train loss: 0.4750606119632721\n",
      "Epoch 18188: train loss: 0.4750605523586273\n",
      "Epoch 18189: train loss: 0.47506049275398254\n",
      "Epoch 18190: train loss: 0.47506046295166016\n",
      "Epoch 18191: train loss: 0.4750604033470154\n",
      "Epoch 18192: train loss: 0.4750603437423706\n",
      "Epoch 18193: train loss: 0.47506025433540344\n",
      "Epoch 18194: train loss: 0.47506019473075867\n",
      "Epoch 18195: train loss: 0.47506019473075867\n",
      "Epoch 18196: train loss: 0.47506004571914673\n",
      "Epoch 18197: train loss: 0.47506004571914673\n",
      "Epoch 18198: train loss: 0.47505998611450195\n",
      "Epoch 18199: train loss: 0.4750599265098572\n",
      "Epoch 18200: train loss: 0.4750598669052124\n",
      "Epoch 18201: train loss: 0.47505977749824524\n",
      "Epoch 18202: train loss: 0.47505971789360046\n",
      "Epoch 18203: train loss: 0.47505971789360046\n",
      "Epoch 18204: train loss: 0.4750595688819885\n",
      "Epoch 18205: train loss: 0.4750595688819885\n",
      "Epoch 18206: train loss: 0.47505950927734375\n",
      "Epoch 18207: train loss: 0.4750593900680542\n",
      "Epoch 18208: train loss: 0.4750593900680542\n",
      "Epoch 18209: train loss: 0.47505930066108704\n",
      "Epoch 18210: train loss: 0.47505924105644226\n",
      "Epoch 18211: train loss: 0.4750591814517975\n",
      "Epoch 18212: train loss: 0.4750591516494751\n",
      "Epoch 18213: train loss: 0.4750590920448303\n",
      "Epoch 18214: train loss: 0.47505903244018555\n",
      "Epoch 18215: train loss: 0.47505897283554077\n",
      "Epoch 18216: train loss: 0.4750588834285736\n",
      "Epoch 18217: train loss: 0.47505882382392883\n",
      "Epoch 18218: train loss: 0.47505876421928406\n",
      "Epoch 18219: train loss: 0.4750586748123169\n",
      "Epoch 18220: train loss: 0.4750586748123169\n",
      "Epoch 18221: train loss: 0.4750586152076721\n",
      "Epoch 18222: train loss: 0.47505855560302734\n",
      "Epoch 18223: train loss: 0.47505852580070496\n",
      "Epoch 18224: train loss: 0.4750584065914154\n",
      "Epoch 18225: train loss: 0.47505834698677063\n",
      "Epoch 18226: train loss: 0.47505831718444824\n",
      "Epoch 18227: train loss: 0.47505825757980347\n",
      "Epoch 18228: train loss: 0.4750581979751587\n",
      "Epoch 18229: train loss: 0.4750581383705139\n",
      "Epoch 18230: train loss: 0.47505810856819153\n",
      "Epoch 18231: train loss: 0.47505804896354675\n",
      "Epoch 18232: train loss: 0.475057989358902\n",
      "Epoch 18233: train loss: 0.4750579297542572\n",
      "Epoch 18234: train loss: 0.4750578999519348\n",
      "Epoch 18235: train loss: 0.47505778074264526\n",
      "Epoch 18236: train loss: 0.4750577211380005\n",
      "Epoch 18237: train loss: 0.4750576913356781\n",
      "Epoch 18238: train loss: 0.4750576317310333\n",
      "Epoch 18239: train loss: 0.4750575125217438\n",
      "Epoch 18240: train loss: 0.4750575125217438\n",
      "Epoch 18241: train loss: 0.4750574827194214\n",
      "Epoch 18242: train loss: 0.47505736351013184\n",
      "Epoch 18243: train loss: 0.47505730390548706\n",
      "Epoch 18244: train loss: 0.4750572741031647\n",
      "Epoch 18245: train loss: 0.4750572144985199\n",
      "Epoch 18246: train loss: 0.47505709528923035\n",
      "Epoch 18247: train loss: 0.47505709528923035\n",
      "Epoch 18248: train loss: 0.4750570058822632\n",
      "Epoch 18249: train loss: 0.4750569462776184\n",
      "Epoch 18250: train loss: 0.47505688667297363\n",
      "Epoch 18251: train loss: 0.47505688667297363\n",
      "Epoch 18252: train loss: 0.47505679726600647\n",
      "Epoch 18253: train loss: 0.4750566780567169\n",
      "Epoch 18254: train loss: 0.4750566780567169\n",
      "Epoch 18255: train loss: 0.47505664825439453\n",
      "Epoch 18256: train loss: 0.47505658864974976\n",
      "Epoch 18257: train loss: 0.4750564694404602\n",
      "Epoch 18258: train loss: 0.4750564694404602\n",
      "Epoch 18259: train loss: 0.47505638003349304\n",
      "Epoch 18260: train loss: 0.47505638003349304\n",
      "Epoch 18261: train loss: 0.4750562608242035\n",
      "Epoch 18262: train loss: 0.4750562608242035\n",
      "Epoch 18263: train loss: 0.47505611181259155\n",
      "Epoch 18264: train loss: 0.4750560522079468\n",
      "Epoch 18265: train loss: 0.4750560522079468\n",
      "Epoch 18266: train loss: 0.4750559628009796\n",
      "Epoch 18267: train loss: 0.47505590319633484\n",
      "Epoch 18268: train loss: 0.47505584359169006\n",
      "Epoch 18269: train loss: 0.4750558137893677\n",
      "Epoch 18270: train loss: 0.4750558137893677\n",
      "Epoch 18271: train loss: 0.4750556945800781\n",
      "Epoch 18272: train loss: 0.4750555753707886\n",
      "Epoch 18273: train loss: 0.4750555455684662\n",
      "Epoch 18274: train loss: 0.4750554859638214\n",
      "Epoch 18275: train loss: 0.47505542635917664\n",
      "Epoch 18276: train loss: 0.4750553369522095\n",
      "Epoch 18277: train loss: 0.4750553369522095\n",
      "Epoch 18278: train loss: 0.4750552773475647\n",
      "Epoch 18279: train loss: 0.4750552177429199\n",
      "Epoch 18280: train loss: 0.47505512833595276\n",
      "Epoch 18281: train loss: 0.475055068731308\n",
      "Epoch 18282: train loss: 0.4750550091266632\n",
      "Epoch 18283: train loss: 0.47505494952201843\n",
      "Epoch 18284: train loss: 0.47505486011505127\n",
      "Epoch 18285: train loss: 0.47505486011505127\n",
      "Epoch 18286: train loss: 0.4750548005104065\n",
      "Epoch 18287: train loss: 0.4750547409057617\n",
      "Epoch 18288: train loss: 0.47505471110343933\n",
      "Epoch 18289: train loss: 0.4750545918941498\n",
      "Epoch 18290: train loss: 0.4750545918941498\n",
      "Epoch 18291: train loss: 0.4750545024871826\n",
      "Epoch 18292: train loss: 0.47505444288253784\n",
      "Epoch 18293: train loss: 0.47505438327789307\n",
      "Epoch 18294: train loss: 0.47505438327789307\n",
      "Epoch 18295: train loss: 0.4750542938709259\n",
      "Epoch 18296: train loss: 0.47505423426628113\n",
      "Epoch 18297: train loss: 0.47505417466163635\n",
      "Epoch 18298: train loss: 0.4750540852546692\n",
      "Epoch 18299: train loss: 0.4750540256500244\n",
      "Epoch 18300: train loss: 0.47505396604537964\n",
      "Epoch 18301: train loss: 0.47505390644073486\n",
      "Epoch 18302: train loss: 0.4750538766384125\n",
      "Epoch 18303: train loss: 0.4750538170337677\n",
      "Epoch 18304: train loss: 0.4750537574291229\n",
      "Epoch 18305: train loss: 0.47505366802215576\n",
      "Epoch 18306: train loss: 0.475053608417511\n",
      "Epoch 18307: train loss: 0.475053608417511\n",
      "Epoch 18308: train loss: 0.47505348920822144\n",
      "Epoch 18309: train loss: 0.47505345940589905\n",
      "Epoch 18310: train loss: 0.4750533401966095\n",
      "Epoch 18311: train loss: 0.4750533401966095\n",
      "Epoch 18312: train loss: 0.47505325078964233\n",
      "Epoch 18313: train loss: 0.47505325078964233\n",
      "Epoch 18314: train loss: 0.4750531315803528\n",
      "Epoch 18315: train loss: 0.475053071975708\n",
      "Epoch 18316: train loss: 0.4750530421733856\n",
      "Epoch 18317: train loss: 0.47505298256874084\n",
      "Epoch 18318: train loss: 0.47505292296409607\n",
      "Epoch 18319: train loss: 0.4750528633594513\n",
      "Epoch 18320: train loss: 0.4750528335571289\n",
      "Epoch 18321: train loss: 0.47505277395248413\n",
      "Epoch 18322: train loss: 0.4750526547431946\n",
      "Epoch 18323: train loss: 0.4750526249408722\n",
      "Epoch 18324: train loss: 0.4750525653362274\n",
      "Epoch 18325: train loss: 0.47505250573158264\n",
      "Epoch 18326: train loss: 0.47505244612693787\n",
      "Epoch 18327: train loss: 0.4750524163246155\n",
      "Epoch 18328: train loss: 0.4750523567199707\n",
      "Epoch 18329: train loss: 0.47505223751068115\n",
      "Epoch 18330: train loss: 0.47505220770835876\n",
      "Epoch 18331: train loss: 0.47505220770835876\n",
      "Epoch 18332: train loss: 0.4750520884990692\n",
      "Epoch 18333: train loss: 0.47505199909210205\n",
      "Epoch 18334: train loss: 0.47505199909210205\n",
      "Epoch 18335: train loss: 0.47505199909210205\n",
      "Epoch 18336: train loss: 0.4750518798828125\n",
      "Epoch 18337: train loss: 0.4750518202781677\n",
      "Epoch 18338: train loss: 0.47505173087120056\n",
      "Epoch 18339: train loss: 0.4750516712665558\n",
      "Epoch 18340: train loss: 0.475051611661911\n",
      "Epoch 18341: train loss: 0.475051611661911\n",
      "Epoch 18342: train loss: 0.47505155205726624\n",
      "Epoch 18343: train loss: 0.4750514030456543\n",
      "Epoch 18344: train loss: 0.4750514030456543\n",
      "Epoch 18345: train loss: 0.4750513434410095\n",
      "Epoch 18346: train loss: 0.47505125403404236\n",
      "Epoch 18347: train loss: 0.47505125403404236\n",
      "Epoch 18348: train loss: 0.4750511348247528\n",
      "Epoch 18349: train loss: 0.4750511050224304\n",
      "Epoch 18350: train loss: 0.47505104541778564\n",
      "Epoch 18351: train loss: 0.47505098581314087\n",
      "Epoch 18352: train loss: 0.4750509262084961\n",
      "Epoch 18353: train loss: 0.4750508964061737\n",
      "Epoch 18354: train loss: 0.47505083680152893\n",
      "Epoch 18355: train loss: 0.4750507175922394\n",
      "Epoch 18356: train loss: 0.475050687789917\n",
      "Epoch 18357: train loss: 0.4750506281852722\n",
      "Epoch 18358: train loss: 0.47505050897598267\n",
      "Epoch 18359: train loss: 0.47505050897598267\n",
      "Epoch 18360: train loss: 0.4750504791736603\n",
      "Epoch 18361: train loss: 0.4750504195690155\n",
      "Epoch 18362: train loss: 0.4750503599643707\n",
      "Epoch 18363: train loss: 0.47505027055740356\n",
      "Epoch 18364: train loss: 0.4750502109527588\n",
      "Epoch 18365: train loss: 0.47505009174346924\n",
      "Epoch 18366: train loss: 0.47505009174346924\n",
      "Epoch 18367: train loss: 0.47505006194114685\n",
      "Epoch 18368: train loss: 0.4750500023365021\n",
      "Epoch 18369: train loss: 0.4750499427318573\n",
      "Epoch 18370: train loss: 0.47504985332489014\n",
      "Epoch 18371: train loss: 0.47504985332489014\n",
      "Epoch 18372: train loss: 0.4750497341156006\n",
      "Epoch 18373: train loss: 0.4750496745109558\n",
      "Epoch 18374: train loss: 0.4750496447086334\n",
      "Epoch 18375: train loss: 0.47504958510398865\n",
      "Epoch 18376: train loss: 0.4750494658946991\n",
      "Epoch 18377: train loss: 0.4750494658946991\n",
      "Epoch 18378: train loss: 0.47504937648773193\n",
      "Epoch 18379: train loss: 0.47504931688308716\n",
      "Epoch 18380: train loss: 0.4750492572784424\n",
      "Epoch 18381: train loss: 0.47504922747612\n",
      "Epoch 18382: train loss: 0.4750491678714752\n",
      "Epoch 18383: train loss: 0.47504910826683044\n",
      "Epoch 18384: train loss: 0.47504904866218567\n",
      "Epoch 18385: train loss: 0.4750489592552185\n",
      "Epoch 18386: train loss: 0.4750489592552185\n",
      "Epoch 18387: train loss: 0.47504889965057373\n",
      "Epoch 18388: train loss: 0.47504881024360657\n",
      "Epoch 18389: train loss: 0.4750487506389618\n",
      "Epoch 18390: train loss: 0.475048691034317\n",
      "Epoch 18391: train loss: 0.47504863142967224\n",
      "Epoch 18392: train loss: 0.47504860162734985\n",
      "Epoch 18393: train loss: 0.4750485420227051\n",
      "Epoch 18394: train loss: 0.4750484228134155\n",
      "Epoch 18395: train loss: 0.4750484228134155\n",
      "Epoch 18396: train loss: 0.47504833340644836\n",
      "Epoch 18397: train loss: 0.4750482738018036\n",
      "Epoch 18398: train loss: 0.4750482141971588\n",
      "Epoch 18399: train loss: 0.4750481843948364\n",
      "Epoch 18400: train loss: 0.47504812479019165\n",
      "Epoch 18401: train loss: 0.4750480651855469\n",
      "Epoch 18402: train loss: 0.4750479459762573\n",
      "Epoch 18403: train loss: 0.4750479459762573\n",
      "Epoch 18404: train loss: 0.47504785656929016\n",
      "Epoch 18405: train loss: 0.47504785656929016\n",
      "Epoch 18406: train loss: 0.4750477969646454\n",
      "Epoch 18407: train loss: 0.4750477075576782\n",
      "Epoch 18408: train loss: 0.47504764795303345\n",
      "Epoch 18409: train loss: 0.4750475287437439\n",
      "Epoch 18410: train loss: 0.4750475287437439\n",
      "Epoch 18411: train loss: 0.4750474989414215\n",
      "Epoch 18412: train loss: 0.47504737973213196\n",
      "Epoch 18413: train loss: 0.47504737973213196\n",
      "Epoch 18414: train loss: 0.4750472903251648\n",
      "Epoch 18415: train loss: 0.4750472903251648\n",
      "Epoch 18416: train loss: 0.47504717111587524\n",
      "Epoch 18417: train loss: 0.47504711151123047\n",
      "Epoch 18418: train loss: 0.4750470221042633\n",
      "Epoch 18419: train loss: 0.4750470221042633\n",
      "Epoch 18420: train loss: 0.47504696249961853\n",
      "Epoch 18421: train loss: 0.47504687309265137\n",
      "Epoch 18422: train loss: 0.47504687309265137\n",
      "Epoch 18423: train loss: 0.4750467538833618\n",
      "Epoch 18424: train loss: 0.4750467538833618\n",
      "Epoch 18425: train loss: 0.47504666447639465\n",
      "Epoch 18426: train loss: 0.4750465452671051\n",
      "Epoch 18427: train loss: 0.4750464856624603\n",
      "Epoch 18428: train loss: 0.47504645586013794\n",
      "Epoch 18429: train loss: 0.47504639625549316\n",
      "Epoch 18430: train loss: 0.47504639625549316\n",
      "Epoch 18431: train loss: 0.4750462770462036\n",
      "Epoch 18432: train loss: 0.4750462472438812\n",
      "Epoch 18433: train loss: 0.4750462472438812\n",
      "Epoch 18434: train loss: 0.4750461280345917\n",
      "Epoch 18435: train loss: 0.4750461280345917\n",
      "Epoch 18436: train loss: 0.4750460386276245\n",
      "Epoch 18437: train loss: 0.47504591941833496\n",
      "Epoch 18438: train loss: 0.47504591941833496\n",
      "Epoch 18439: train loss: 0.4750458300113678\n",
      "Epoch 18440: train loss: 0.4750458300113678\n",
      "Epoch 18441: train loss: 0.47504571080207825\n",
      "Epoch 18442: train loss: 0.47504565119743347\n",
      "Epoch 18443: train loss: 0.4750456213951111\n",
      "Epoch 18444: train loss: 0.4750455617904663\n",
      "Epoch 18445: train loss: 0.47504550218582153\n",
      "Epoch 18446: train loss: 0.47504541277885437\n",
      "Epoch 18447: train loss: 0.4750453531742096\n",
      "Epoch 18448: train loss: 0.4750452935695648\n",
      "Epoch 18449: train loss: 0.47504523396492004\n",
      "Epoch 18450: train loss: 0.47504520416259766\n",
      "Epoch 18451: train loss: 0.4750451445579529\n",
      "Epoch 18452: train loss: 0.4750450849533081\n",
      "Epoch 18453: train loss: 0.47504499554634094\n",
      "Epoch 18454: train loss: 0.47504499554634094\n",
      "Epoch 18455: train loss: 0.4750448763370514\n",
      "Epoch 18456: train loss: 0.4750448167324066\n",
      "Epoch 18457: train loss: 0.47504478693008423\n",
      "Epoch 18458: train loss: 0.47504472732543945\n",
      "Epoch 18459: train loss: 0.4750446677207947\n",
      "Epoch 18460: train loss: 0.4750446677207947\n",
      "Epoch 18461: train loss: 0.4750445783138275\n",
      "Epoch 18462: train loss: 0.47504445910453796\n",
      "Epoch 18463: train loss: 0.47504445910453796\n",
      "Epoch 18464: train loss: 0.4750443994998932\n",
      "Epoch 18465: train loss: 0.475044310092926\n",
      "Epoch 18466: train loss: 0.47504425048828125\n",
      "Epoch 18467: train loss: 0.4750441312789917\n",
      "Epoch 18468: train loss: 0.4750441312789917\n",
      "Epoch 18469: train loss: 0.4750441014766693\n",
      "Epoch 18470: train loss: 0.47504404187202454\n",
      "Epoch 18471: train loss: 0.475043922662735\n",
      "Epoch 18472: train loss: 0.4750438928604126\n",
      "Epoch 18473: train loss: 0.4750438332557678\n",
      "Epoch 18474: train loss: 0.47504377365112305\n",
      "Epoch 18475: train loss: 0.47504371404647827\n",
      "Epoch 18476: train loss: 0.4750436842441559\n",
      "Epoch 18477: train loss: 0.4750436246395111\n",
      "Epoch 18478: train loss: 0.47504356503486633\n",
      "Epoch 18479: train loss: 0.47504350543022156\n",
      "Epoch 18480: train loss: 0.4750434160232544\n",
      "Epoch 18481: train loss: 0.4750433564186096\n",
      "Epoch 18482: train loss: 0.47504329681396484\n",
      "Epoch 18483: train loss: 0.4750432074069977\n",
      "Epoch 18484: train loss: 0.4750432074069977\n",
      "Epoch 18485: train loss: 0.47504308819770813\n",
      "Epoch 18486: train loss: 0.47504308819770813\n",
      "Epoch 18487: train loss: 0.47504299879074097\n",
      "Epoch 18488: train loss: 0.4750429391860962\n",
      "Epoch 18489: train loss: 0.4750428795814514\n",
      "Epoch 18490: train loss: 0.47504284977912903\n",
      "Epoch 18491: train loss: 0.47504279017448425\n",
      "Epoch 18492: train loss: 0.4750426709651947\n",
      "Epoch 18493: train loss: 0.4750426709651947\n",
      "Epoch 18494: train loss: 0.4750426411628723\n",
      "Epoch 18495: train loss: 0.47504258155822754\n",
      "Epoch 18496: train loss: 0.475042462348938\n",
      "Epoch 18497: train loss: 0.4750424325466156\n",
      "Epoch 18498: train loss: 0.4750424325466156\n",
      "Epoch 18499: train loss: 0.47504231333732605\n",
      "Epoch 18500: train loss: 0.4750422537326813\n",
      "Epoch 18501: train loss: 0.4750421643257141\n",
      "Epoch 18502: train loss: 0.4750421643257141\n",
      "Epoch 18503: train loss: 0.47504204511642456\n",
      "Epoch 18504: train loss: 0.4750420153141022\n",
      "Epoch 18505: train loss: 0.4750419557094574\n",
      "Epoch 18506: train loss: 0.4750418961048126\n",
      "Epoch 18507: train loss: 0.47504183650016785\n",
      "Epoch 18508: train loss: 0.4750417470932007\n",
      "Epoch 18509: train loss: 0.4750417470932007\n",
      "Epoch 18510: train loss: 0.4750416874885559\n",
      "Epoch 18511: train loss: 0.47504159808158875\n",
      "Epoch 18512: train loss: 0.47504153847694397\n",
      "Epoch 18513: train loss: 0.4750414788722992\n",
      "Epoch 18514: train loss: 0.4750414192676544\n",
      "Epoch 18515: train loss: 0.47504138946533203\n",
      "Epoch 18516: train loss: 0.47504132986068726\n",
      "Epoch 18517: train loss: 0.4750412106513977\n",
      "Epoch 18518: train loss: 0.4750412106513977\n",
      "Epoch 18519: train loss: 0.4750411808490753\n",
      "Epoch 18520: train loss: 0.47504112124443054\n",
      "Epoch 18521: train loss: 0.47504106163978577\n",
      "Epoch 18522: train loss: 0.4750409722328186\n",
      "Epoch 18523: train loss: 0.47504091262817383\n",
      "Epoch 18524: train loss: 0.47504085302352905\n",
      "Epoch 18525: train loss: 0.4750407934188843\n",
      "Epoch 18526: train loss: 0.4750407636165619\n",
      "Epoch 18527: train loss: 0.4750407040119171\n",
      "Epoch 18528: train loss: 0.47504064440727234\n",
      "Epoch 18529: train loss: 0.47504058480262756\n",
      "Epoch 18530: train loss: 0.4750404953956604\n",
      "Epoch 18531: train loss: 0.4750404953956604\n",
      "Epoch 18532: train loss: 0.47504037618637085\n",
      "Epoch 18533: train loss: 0.4750403165817261\n",
      "Epoch 18534: train loss: 0.4750402271747589\n",
      "Epoch 18535: train loss: 0.4750402271747589\n",
      "Epoch 18536: train loss: 0.47504016757011414\n",
      "Epoch 18537: train loss: 0.475040078163147\n",
      "Epoch 18538: train loss: 0.4750400185585022\n",
      "Epoch 18539: train loss: 0.4750399589538574\n",
      "Epoch 18540: train loss: 0.47503989934921265\n",
      "Epoch 18541: train loss: 0.4750398099422455\n",
      "Epoch 18542: train loss: 0.4750397503376007\n",
      "Epoch 18543: train loss: 0.4750397503376007\n",
      "Epoch 18544: train loss: 0.47503966093063354\n",
      "Epoch 18545: train loss: 0.47503966093063354\n",
      "Epoch 18546: train loss: 0.475039541721344\n",
      "Epoch 18547: train loss: 0.4750394821166992\n",
      "Epoch 18548: train loss: 0.47503945231437683\n",
      "Epoch 18549: train loss: 0.47503939270973206\n",
      "Epoch 18550: train loss: 0.4750393331050873\n",
      "Epoch 18551: train loss: 0.4750392735004425\n",
      "Epoch 18552: train loss: 0.4750392436981201\n",
      "Epoch 18553: train loss: 0.47503918409347534\n",
      "Epoch 18554: train loss: 0.47503912448883057\n",
      "Epoch 18555: train loss: 0.4750390350818634\n",
      "Epoch 18556: train loss: 0.47503897547721863\n",
      "Epoch 18557: train loss: 0.47503891587257385\n",
      "Epoch 18558: train loss: 0.4750388562679291\n",
      "Epoch 18559: train loss: 0.4750388264656067\n",
      "Epoch 18560: train loss: 0.4750387668609619\n",
      "Epoch 18561: train loss: 0.47503864765167236\n",
      "Epoch 18562: train loss: 0.47503864765167236\n",
      "Epoch 18563: train loss: 0.4750385582447052\n",
      "Epoch 18564: train loss: 0.4750384986400604\n",
      "Epoch 18565: train loss: 0.47503843903541565\n",
      "Epoch 18566: train loss: 0.47503840923309326\n",
      "Epoch 18567: train loss: 0.4750383496284485\n",
      "Epoch 18568: train loss: 0.4750382900238037\n",
      "Epoch 18569: train loss: 0.47503820061683655\n",
      "Epoch 18570: train loss: 0.4750381410121918\n",
      "Epoch 18571: train loss: 0.4750381410121918\n",
      "Epoch 18572: train loss: 0.4750380218029022\n",
      "Epoch 18573: train loss: 0.47503799200057983\n",
      "Epoch 18574: train loss: 0.47503799200057983\n",
      "Epoch 18575: train loss: 0.4750378727912903\n",
      "Epoch 18576: train loss: 0.4750378131866455\n",
      "Epoch 18577: train loss: 0.47503772377967834\n",
      "Epoch 18578: train loss: 0.47503766417503357\n",
      "Epoch 18579: train loss: 0.4750376045703888\n",
      "Epoch 18580: train loss: 0.4750375747680664\n",
      "Epoch 18581: train loss: 0.47503751516342163\n",
      "Epoch 18582: train loss: 0.47503745555877686\n",
      "Epoch 18583: train loss: 0.4750373661518097\n",
      "Epoch 18584: train loss: 0.4750373661518097\n",
      "Epoch 18585: train loss: 0.47503724694252014\n",
      "Epoch 18586: train loss: 0.47503718733787537\n",
      "Epoch 18587: train loss: 0.475037157535553\n",
      "Epoch 18588: train loss: 0.475037157535553\n",
      "Epoch 18589: train loss: 0.4750370979309082\n",
      "Epoch 18590: train loss: 0.47503697872161865\n",
      "Epoch 18591: train loss: 0.47503694891929626\n",
      "Epoch 18592: train loss: 0.4750368893146515\n",
      "Epoch 18593: train loss: 0.4750368297100067\n",
      "Epoch 18594: train loss: 0.47503677010536194\n",
      "Epoch 18595: train loss: 0.47503674030303955\n",
      "Epoch 18596: train loss: 0.47503662109375\n",
      "Epoch 18597: train loss: 0.4750365614891052\n",
      "Epoch 18598: train loss: 0.47503650188446045\n",
      "Epoch 18599: train loss: 0.47503650188446045\n",
      "Epoch 18600: train loss: 0.4750364124774933\n",
      "Epoch 18601: train loss: 0.47503629326820374\n",
      "Epoch 18602: train loss: 0.47503629326820374\n",
      "Epoch 18603: train loss: 0.47503626346588135\n",
      "Epoch 18604: train loss: 0.4750362038612366\n",
      "Epoch 18605: train loss: 0.4750361442565918\n",
      "Epoch 18606: train loss: 0.47503605484962463\n",
      "Epoch 18607: train loss: 0.47503599524497986\n",
      "Epoch 18608: train loss: 0.4750359356403351\n",
      "Epoch 18609: train loss: 0.4750358760356903\n",
      "Epoch 18610: train loss: 0.4750358462333679\n",
      "Epoch 18611: train loss: 0.47503578662872314\n",
      "Epoch 18612: train loss: 0.4750356674194336\n",
      "Epoch 18613: train loss: 0.4750356674194336\n",
      "Epoch 18614: train loss: 0.47503551840782166\n",
      "Epoch 18615: train loss: 0.47503551840782166\n",
      "Epoch 18616: train loss: 0.4750354588031769\n",
      "Epoch 18617: train loss: 0.4750354290008545\n",
      "Epoch 18618: train loss: 0.4750353693962097\n",
      "Epoch 18619: train loss: 0.47503530979156494\n",
      "Epoch 18620: train loss: 0.47503525018692017\n",
      "Epoch 18621: train loss: 0.475035160779953\n",
      "Epoch 18622: train loss: 0.475035160779953\n",
      "Epoch 18623: train loss: 0.47503504157066345\n",
      "Epoch 18624: train loss: 0.47503504157066345\n",
      "Epoch 18625: train loss: 0.4750348925590515\n",
      "Epoch 18626: train loss: 0.4750348925590515\n",
      "Epoch 18627: train loss: 0.47503483295440674\n",
      "Epoch 18628: train loss: 0.47503480315208435\n",
      "Epoch 18629: train loss: 0.4750347435474396\n",
      "Epoch 18630: train loss: 0.47503462433815\n",
      "Epoch 18631: train loss: 0.47503462433815\n",
      "Epoch 18632: train loss: 0.47503453493118286\n",
      "Epoch 18633: train loss: 0.4750344753265381\n",
      "Epoch 18634: train loss: 0.4750344157218933\n",
      "Epoch 18635: train loss: 0.4750343859195709\n",
      "Epoch 18636: train loss: 0.47503432631492615\n",
      "Epoch 18637: train loss: 0.4750342071056366\n",
      "Epoch 18638: train loss: 0.4750342071056366\n",
      "Epoch 18639: train loss: 0.4750341773033142\n",
      "Epoch 18640: train loss: 0.4750339984893799\n",
      "Epoch 18641: train loss: 0.4750339984893799\n",
      "Epoch 18642: train loss: 0.4750339686870575\n",
      "Epoch 18643: train loss: 0.4750339090824127\n",
      "Epoch 18644: train loss: 0.47503378987312317\n",
      "Epoch 18645: train loss: 0.47503378987312317\n",
      "Epoch 18646: train loss: 0.475033700466156\n",
      "Epoch 18647: train loss: 0.475033700466156\n",
      "Epoch 18648: train loss: 0.47503364086151123\n",
      "Epoch 18649: train loss: 0.47503355145454407\n",
      "Epoch 18650: train loss: 0.4750334918498993\n",
      "Epoch 18651: train loss: 0.4750334918498993\n",
      "Epoch 18652: train loss: 0.47503337264060974\n",
      "Epoch 18653: train loss: 0.4750332832336426\n",
      "Epoch 18654: train loss: 0.4750332832336426\n",
      "Epoch 18655: train loss: 0.4750332236289978\n",
      "Epoch 18656: train loss: 0.47503313422203064\n",
      "Epoch 18657: train loss: 0.47503307461738586\n",
      "Epoch 18658: train loss: 0.47503307461738586\n",
      "Epoch 18659: train loss: 0.4750329554080963\n",
      "Epoch 18660: train loss: 0.4750329256057739\n",
      "Epoch 18661: train loss: 0.47503286600112915\n",
      "Epoch 18662: train loss: 0.4750328063964844\n",
      "Epoch 18663: train loss: 0.4750326871871948\n",
      "Epoch 18664: train loss: 0.47503265738487244\n",
      "Epoch 18665: train loss: 0.47503265738487244\n",
      "Epoch 18666: train loss: 0.47503259778022766\n",
      "Epoch 18667: train loss: 0.4750325381755829\n",
      "Epoch 18668: train loss: 0.4750324487686157\n",
      "Epoch 18669: train loss: 0.47503238916397095\n",
      "Epoch 18670: train loss: 0.47503232955932617\n",
      "Epoch 18671: train loss: 0.4750322699546814\n",
      "Epoch 18672: train loss: 0.47503218054771423\n",
      "Epoch 18673: train loss: 0.47503212094306946\n",
      "Epoch 18674: train loss: 0.4750320613384247\n",
      "Epoch 18675: train loss: 0.4750320315361023\n",
      "Epoch 18676: train loss: 0.4750319719314575\n",
      "Epoch 18677: train loss: 0.47503185272216797\n",
      "Epoch 18678: train loss: 0.47503185272216797\n",
      "Epoch 18679: train loss: 0.4750318229198456\n",
      "Epoch 18680: train loss: 0.4750317633152008\n",
      "Epoch 18681: train loss: 0.47503170371055603\n",
      "Epoch 18682: train loss: 0.47503161430358887\n",
      "Epoch 18683: train loss: 0.4750315546989441\n",
      "Epoch 18684: train loss: 0.4750314950942993\n",
      "Epoch 18685: train loss: 0.47503143548965454\n",
      "Epoch 18686: train loss: 0.4750313460826874\n",
      "Epoch 18687: train loss: 0.4750312864780426\n",
      "Epoch 18688: train loss: 0.4750312268733978\n",
      "Epoch 18689: train loss: 0.47503119707107544\n",
      "Epoch 18690: train loss: 0.47503113746643066\n",
      "Epoch 18691: train loss: 0.4750310778617859\n",
      "Epoch 18692: train loss: 0.4750310182571411\n",
      "Epoch 18693: train loss: 0.4750309884548187\n",
      "Epoch 18694: train loss: 0.47503092885017395\n",
      "Epoch 18695: train loss: 0.4750308692455292\n",
      "Epoch 18696: train loss: 0.4750308096408844\n",
      "Epoch 18697: train loss: 0.475030779838562\n",
      "Epoch 18698: train loss: 0.47503066062927246\n",
      "Epoch 18699: train loss: 0.4750306010246277\n",
      "Epoch 18700: train loss: 0.4750305712223053\n",
      "Epoch 18701: train loss: 0.4750305712223053\n",
      "Epoch 18702: train loss: 0.47503045201301575\n",
      "Epoch 18703: train loss: 0.47503039240837097\n",
      "Epoch 18704: train loss: 0.4750303030014038\n",
      "Epoch 18705: train loss: 0.47503024339675903\n",
      "Epoch 18706: train loss: 0.47503024339675903\n",
      "Epoch 18707: train loss: 0.47503018379211426\n",
      "Epoch 18708: train loss: 0.4750300943851471\n",
      "Epoch 18709: train loss: 0.4750300347805023\n",
      "Epoch 18710: train loss: 0.47502994537353516\n",
      "Epoch 18711: train loss: 0.47502994537353516\n",
      "Epoch 18712: train loss: 0.4750298857688904\n",
      "Epoch 18713: train loss: 0.4750298261642456\n",
      "Epoch 18714: train loss: 0.47502976655960083\n",
      "Epoch 18715: train loss: 0.47502973675727844\n",
      "Epoch 18716: train loss: 0.47502967715263367\n",
      "Epoch 18717: train loss: 0.4750295579433441\n",
      "Epoch 18718: train loss: 0.47502952814102173\n",
      "Epoch 18719: train loss: 0.47502946853637695\n",
      "Epoch 18720: train loss: 0.4750294089317322\n",
      "Epoch 18721: train loss: 0.4750293493270874\n",
      "Epoch 18722: train loss: 0.475029319524765\n",
      "Epoch 18723: train loss: 0.47502920031547546\n",
      "Epoch 18724: train loss: 0.4750291407108307\n",
      "Epoch 18725: train loss: 0.4750291109085083\n",
      "Epoch 18726: train loss: 0.4750290513038635\n",
      "Epoch 18727: train loss: 0.47502899169921875\n",
      "Epoch 18728: train loss: 0.4750288724899292\n",
      "Epoch 18729: train loss: 0.4750288724899292\n",
      "Epoch 18730: train loss: 0.4750288426876068\n",
      "Epoch 18731: train loss: 0.47502872347831726\n",
      "Epoch 18732: train loss: 0.4750286638736725\n",
      "Epoch 18733: train loss: 0.4750286340713501\n",
      "Epoch 18734: train loss: 0.4750286340713501\n",
      "Epoch 18735: train loss: 0.47502851486206055\n",
      "Epoch 18736: train loss: 0.47502845525741577\n",
      "Epoch 18737: train loss: 0.4750284254550934\n",
      "Epoch 18738: train loss: 0.4750283658504486\n",
      "Epoch 18739: train loss: 0.47502830624580383\n",
      "Epoch 18740: train loss: 0.47502821683883667\n",
      "Epoch 18741: train loss: 0.4750281572341919\n",
      "Epoch 18742: train loss: 0.4750280976295471\n",
      "Epoch 18743: train loss: 0.47502803802490234\n",
      "Epoch 18744: train loss: 0.47502800822257996\n",
      "Epoch 18745: train loss: 0.4750279486179352\n",
      "Epoch 18746: train loss: 0.4750278890132904\n",
      "Epoch 18747: train loss: 0.47502779960632324\n",
      "Epoch 18748: train loss: 0.47502774000167847\n",
      "Epoch 18749: train loss: 0.47502774000167847\n",
      "Epoch 18750: train loss: 0.4750276207923889\n",
      "Epoch 18751: train loss: 0.47502753138542175\n",
      "Epoch 18752: train loss: 0.47502753138542175\n",
      "Epoch 18753: train loss: 0.47502753138542175\n",
      "Epoch 18754: train loss: 0.4750273823738098\n",
      "Epoch 18755: train loss: 0.4750273823738098\n",
      "Epoch 18756: train loss: 0.47502732276916504\n",
      "Epoch 18757: train loss: 0.4750272035598755\n",
      "Epoch 18758: train loss: 0.4750271737575531\n",
      "Epoch 18759: train loss: 0.4750271141529083\n",
      "Epoch 18760: train loss: 0.47502705454826355\n",
      "Epoch 18761: train loss: 0.4750269651412964\n",
      "Epoch 18762: train loss: 0.4750269651412964\n",
      "Epoch 18763: train loss: 0.4750269055366516\n",
      "Epoch 18764: train loss: 0.47502684593200684\n",
      "Epoch 18765: train loss: 0.47502678632736206\n",
      "Epoch 18766: train loss: 0.4750266969203949\n",
      "Epoch 18767: train loss: 0.4750266373157501\n",
      "Epoch 18768: train loss: 0.4750266373157501\n",
      "Epoch 18769: train loss: 0.47502657771110535\n",
      "Epoch 18770: train loss: 0.4750264883041382\n",
      "Epoch 18771: train loss: 0.47502636909484863\n",
      "Epoch 18772: train loss: 0.47502636909484863\n",
      "Epoch 18773: train loss: 0.47502633929252625\n",
      "Epoch 18774: train loss: 0.4750262200832367\n",
      "Epoch 18775: train loss: 0.4750261604785919\n",
      "Epoch 18776: train loss: 0.4750261604785919\n",
      "Epoch 18777: train loss: 0.47502607107162476\n",
      "Epoch 18778: train loss: 0.47502601146698\n",
      "Epoch 18779: train loss: 0.4750259518623352\n",
      "Epoch 18780: train loss: 0.4750259220600128\n",
      "Epoch 18781: train loss: 0.47502586245536804\n",
      "Epoch 18782: train loss: 0.47502580285072327\n",
      "Epoch 18783: train loss: 0.4750257432460785\n",
      "Epoch 18784: train loss: 0.4750257134437561\n",
      "Epoch 18785: train loss: 0.47502565383911133\n",
      "Epoch 18786: train loss: 0.47502559423446655\n",
      "Epoch 18787: train loss: 0.4750255048274994\n",
      "Epoch 18788: train loss: 0.4750254452228546\n",
      "Epoch 18789: train loss: 0.4750254452228546\n",
      "Epoch 18790: train loss: 0.47502532601356506\n",
      "Epoch 18791: train loss: 0.4750252366065979\n",
      "Epoch 18792: train loss: 0.4750252366065979\n",
      "Epoch 18793: train loss: 0.4750251770019531\n",
      "Epoch 18794: train loss: 0.4750250577926636\n",
      "Epoch 18795: train loss: 0.4750250279903412\n",
      "Epoch 18796: train loss: 0.4750249683856964\n",
      "Epoch 18797: train loss: 0.47502490878105164\n",
      "Epoch 18798: train loss: 0.47502484917640686\n",
      "Epoch 18799: train loss: 0.4750248193740845\n",
      "Epoch 18800: train loss: 0.4750247597694397\n",
      "Epoch 18801: train loss: 0.4750247001647949\n",
      "Epoch 18802: train loss: 0.47502461075782776\n",
      "Epoch 18803: train loss: 0.475024551153183\n",
      "Epoch 18804: train loss: 0.4750244915485382\n",
      "Epoch 18805: train loss: 0.47502443194389343\n",
      "Epoch 18806: train loss: 0.47502440214157104\n",
      "Epoch 18807: train loss: 0.4750242829322815\n",
      "Epoch 18808: train loss: 0.4750242233276367\n",
      "Epoch 18809: train loss: 0.47502419352531433\n",
      "Epoch 18810: train loss: 0.47502419352531433\n",
      "Epoch 18811: train loss: 0.4750240743160248\n",
      "Epoch 18812: train loss: 0.47502401471138\n",
      "Epoch 18813: train loss: 0.4750239849090576\n",
      "Epoch 18814: train loss: 0.47502392530441284\n",
      "Epoch 18815: train loss: 0.47502386569976807\n",
      "Epoch 18816: train loss: 0.4750238060951233\n",
      "Epoch 18817: train loss: 0.4750237762928009\n",
      "Epoch 18818: train loss: 0.47502371668815613\n",
      "Epoch 18819: train loss: 0.47502365708351135\n",
      "Epoch 18820: train loss: 0.4750235676765442\n",
      "Epoch 18821: train loss: 0.47502344846725464\n",
      "Epoch 18822: train loss: 0.47502344846725464\n",
      "Epoch 18823: train loss: 0.47502338886260986\n",
      "Epoch 18824: train loss: 0.4750233590602875\n",
      "Epoch 18825: train loss: 0.4750232398509979\n",
      "Epoch 18826: train loss: 0.47502318024635315\n",
      "Epoch 18827: train loss: 0.47502318024635315\n",
      "Epoch 18828: train loss: 0.47502315044403076\n",
      "Epoch 18829: train loss: 0.4750230312347412\n",
      "Epoch 18830: train loss: 0.47502297163009644\n",
      "Epoch 18831: train loss: 0.47502294182777405\n",
      "Epoch 18832: train loss: 0.4750228822231293\n",
      "Epoch 18833: train loss: 0.4750228226184845\n",
      "Epoch 18834: train loss: 0.47502273321151733\n",
      "Epoch 18835: train loss: 0.47502273321151733\n",
      "Epoch 18836: train loss: 0.47502267360687256\n",
      "Epoch 18837: train loss: 0.475022554397583\n",
      "Epoch 18838: train loss: 0.4750225245952606\n",
      "Epoch 18839: train loss: 0.47502246499061584\n",
      "Epoch 18840: train loss: 0.47502246499061584\n",
      "Epoch 18841: train loss: 0.4750223159790039\n",
      "Epoch 18842: train loss: 0.4750223159790039\n",
      "Epoch 18843: train loss: 0.47502225637435913\n",
      "Epoch 18844: train loss: 0.47502219676971436\n",
      "Epoch 18845: train loss: 0.4750221073627472\n",
      "Epoch 18846: train loss: 0.4750220477581024\n",
      "Epoch 18847: train loss: 0.47502198815345764\n",
      "Epoch 18848: train loss: 0.47502192854881287\n",
      "Epoch 18849: train loss: 0.4750218987464905\n",
      "Epoch 18850: train loss: 0.4750218391418457\n",
      "Epoch 18851: train loss: 0.4750218391418457\n",
      "Epoch 18852: train loss: 0.47502171993255615\n",
      "Epoch 18853: train loss: 0.47502169013023376\n",
      "Epoch 18854: train loss: 0.475021630525589\n",
      "Epoch 18855: train loss: 0.4750215709209442\n",
      "Epoch 18856: train loss: 0.47502148151397705\n",
      "Epoch 18857: train loss: 0.4750214219093323\n",
      "Epoch 18858: train loss: 0.4750214219093323\n",
      "Epoch 18859: train loss: 0.4750213027000427\n",
      "Epoch 18860: train loss: 0.47502124309539795\n",
      "Epoch 18861: train loss: 0.47502121329307556\n",
      "Epoch 18862: train loss: 0.475021094083786\n",
      "Epoch 18863: train loss: 0.475021094083786\n",
      "Epoch 18864: train loss: 0.47502100467681885\n",
      "Epoch 18865: train loss: 0.4750209450721741\n",
      "Epoch 18866: train loss: 0.4750208854675293\n",
      "Epoch 18867: train loss: 0.4750208258628845\n",
      "Epoch 18868: train loss: 0.47502079606056213\n",
      "Epoch 18869: train loss: 0.47502073645591736\n",
      "Epoch 18870: train loss: 0.4750206768512726\n",
      "Epoch 18871: train loss: 0.4750205874443054\n",
      "Epoch 18872: train loss: 0.47502052783966064\n",
      "Epoch 18873: train loss: 0.47502046823501587\n",
      "Epoch 18874: train loss: 0.4750203788280487\n",
      "Epoch 18875: train loss: 0.4750204086303711\n",
      "Epoch 18876: train loss: 0.47502031922340393\n",
      "Epoch 18877: train loss: 0.47502031922340393\n",
      "Epoch 18878: train loss: 0.4750202000141144\n",
      "Epoch 18879: train loss: 0.475020170211792\n",
      "Epoch 18880: train loss: 0.47502005100250244\n",
      "Epoch 18881: train loss: 0.47502005100250244\n",
      "Epoch 18882: train loss: 0.4750199615955353\n",
      "Epoch 18883: train loss: 0.4750199615955353\n",
      "Epoch 18884: train loss: 0.4750198423862457\n",
      "Epoch 18885: train loss: 0.47501978278160095\n",
      "Epoch 18886: train loss: 0.47501975297927856\n",
      "Epoch 18887: train loss: 0.4750196933746338\n",
      "Epoch 18888: train loss: 0.475019633769989\n",
      "Epoch 18889: train loss: 0.47501954436302185\n",
      "Epoch 18890: train loss: 0.4750194847583771\n",
      "Epoch 18891: train loss: 0.4750194251537323\n",
      "Epoch 18892: train loss: 0.4750193655490875\n",
      "Epoch 18893: train loss: 0.47501933574676514\n",
      "Epoch 18894: train loss: 0.47501927614212036\n",
      "Epoch 18895: train loss: 0.4750192165374756\n",
      "Epoch 18896: train loss: 0.4750191569328308\n",
      "Epoch 18897: train loss: 0.47501906752586365\n",
      "Epoch 18898: train loss: 0.47501906752586365\n",
      "Epoch 18899: train loss: 0.4750189483165741\n",
      "Epoch 18900: train loss: 0.4750189483165741\n",
      "Epoch 18901: train loss: 0.47501885890960693\n",
      "Epoch 18902: train loss: 0.47501879930496216\n",
      "Epoch 18903: train loss: 0.4750187397003174\n",
      "Epoch 18904: train loss: 0.475018709897995\n",
      "Epoch 18905: train loss: 0.4750186502933502\n",
      "Epoch 18906: train loss: 0.47501853108406067\n",
      "Epoch 18907: train loss: 0.47501853108406067\n",
      "Epoch 18908: train loss: 0.4750184416770935\n",
      "Epoch 18909: train loss: 0.47501838207244873\n",
      "Epoch 18910: train loss: 0.47501832246780396\n",
      "Epoch 18911: train loss: 0.47501829266548157\n",
      "Epoch 18912: train loss: 0.4750182330608368\n",
      "Epoch 18913: train loss: 0.475018173456192\n",
      "Epoch 18914: train loss: 0.47501811385154724\n",
      "Epoch 18915: train loss: 0.47501808404922485\n",
      "Epoch 18916: train loss: 0.4750179648399353\n",
      "Epoch 18917: train loss: 0.4750179052352905\n",
      "Epoch 18918: train loss: 0.47501787543296814\n",
      "Epoch 18919: train loss: 0.47501781582832336\n",
      "Epoch 18920: train loss: 0.4750177562236786\n",
      "Epoch 18921: train loss: 0.4750176966190338\n",
      "Epoch 18922: train loss: 0.4750176668167114\n",
      "Epoch 18923: train loss: 0.47501760721206665\n",
      "Epoch 18924: train loss: 0.4750175476074219\n",
      "Epoch 18925: train loss: 0.4750174880027771\n",
      "Epoch 18926: train loss: 0.47501739859580994\n",
      "Epoch 18927: train loss: 0.47501733899116516\n",
      "Epoch 18928: train loss: 0.4750172793865204\n",
      "Epoch 18929: train loss: 0.4750172197818756\n",
      "Epoch 18930: train loss: 0.4750171899795532\n",
      "Epoch 18931: train loss: 0.47501713037490845\n",
      "Epoch 18932: train loss: 0.47501707077026367\n",
      "Epoch 18933: train loss: 0.4750170111656189\n",
      "Epoch 18934: train loss: 0.47501692175865173\n",
      "Epoch 18935: train loss: 0.47501686215400696\n",
      "Epoch 18936: train loss: 0.47501686215400696\n",
      "Epoch 18937: train loss: 0.4750167727470398\n",
      "Epoch 18938: train loss: 0.4750167727470398\n",
      "Epoch 18939: train loss: 0.47501665353775024\n",
      "Epoch 18940: train loss: 0.47501659393310547\n",
      "Epoch 18941: train loss: 0.4750165045261383\n",
      "Epoch 18942: train loss: 0.4750165045261383\n",
      "Epoch 18943: train loss: 0.47501644492149353\n",
      "Epoch 18944: train loss: 0.47501635551452637\n",
      "Epoch 18945: train loss: 0.47501635551452637\n",
      "Epoch 18946: train loss: 0.4750162363052368\n",
      "Epoch 18947: train loss: 0.47501617670059204\n",
      "Epoch 18948: train loss: 0.47501614689826965\n",
      "Epoch 18949: train loss: 0.4750160872936249\n",
      "Epoch 18950: train loss: 0.4750160276889801\n",
      "Epoch 18951: train loss: 0.4750159680843353\n",
      "Epoch 18952: train loss: 0.47501593828201294\n",
      "Epoch 18953: train loss: 0.47501587867736816\n",
      "Epoch 18954: train loss: 0.4750157594680786\n",
      "Epoch 18955: train loss: 0.4750157296657562\n",
      "Epoch 18956: train loss: 0.47501567006111145\n",
      "Epoch 18957: train loss: 0.4750156104564667\n",
      "Epoch 18958: train loss: 0.4750156104564667\n",
      "Epoch 18959: train loss: 0.47501546144485474\n",
      "Epoch 18960: train loss: 0.47501546144485474\n",
      "Epoch 18961: train loss: 0.4750153422355652\n",
      "Epoch 18962: train loss: 0.4750153422355652\n",
      "Epoch 18963: train loss: 0.475015252828598\n",
      "Epoch 18964: train loss: 0.475015252828598\n",
      "Epoch 18965: train loss: 0.47501513361930847\n",
      "Epoch 18966: train loss: 0.4750151038169861\n",
      "Epoch 18967: train loss: 0.4750150442123413\n",
      "Epoch 18968: train loss: 0.47501498460769653\n",
      "Epoch 18969: train loss: 0.47501489520072937\n",
      "Epoch 18970: train loss: 0.4750148355960846\n",
      "Epoch 18971: train loss: 0.4750148355960846\n",
      "Epoch 18972: train loss: 0.47501471638679504\n",
      "Epoch 18973: train loss: 0.47501468658447266\n",
      "Epoch 18974: train loss: 0.4750146269798279\n",
      "Epoch 18975: train loss: 0.4750145673751831\n",
      "Epoch 18976: train loss: 0.47501447796821594\n",
      "Epoch 18977: train loss: 0.47501447796821594\n",
      "Epoch 18978: train loss: 0.47501441836357117\n",
      "Epoch 18979: train loss: 0.4750142991542816\n",
      "Epoch 18980: train loss: 0.47501426935195923\n",
      "Epoch 18981: train loss: 0.47501426935195923\n",
      "Epoch 18982: train loss: 0.4750141501426697\n",
      "Epoch 18983: train loss: 0.4750140905380249\n",
      "Epoch 18984: train loss: 0.4750140607357025\n",
      "Epoch 18985: train loss: 0.47501400113105774\n",
      "Epoch 18986: train loss: 0.47501394152641296\n",
      "Epoch 18987: train loss: 0.4750138521194458\n",
      "Epoch 18988: train loss: 0.475013792514801\n",
      "Epoch 18989: train loss: 0.475013792514801\n",
      "Epoch 18990: train loss: 0.47501373291015625\n",
      "Epoch 18991: train loss: 0.4750136137008667\n",
      "Epoch 18992: train loss: 0.4750136137008667\n",
      "Epoch 18993: train loss: 0.47501352429389954\n",
      "Epoch 18994: train loss: 0.47501346468925476\n",
      "Epoch 18995: train loss: 0.47501340508461\n",
      "Epoch 18996: train loss: 0.4750133752822876\n",
      "Epoch 18997: train loss: 0.4750133156776428\n",
      "Epoch 18998: train loss: 0.47501325607299805\n",
      "Epoch 18999: train loss: 0.4750131666660309\n",
      "Epoch 19000: train loss: 0.4750131666660309\n",
      "Epoch 19001: train loss: 0.4750131070613861\n",
      "Epoch 19002: train loss: 0.47501298785209656\n",
      "Epoch 19003: train loss: 0.4750128984451294\n",
      "Epoch 19004: train loss: 0.4750128984451294\n",
      "Epoch 19005: train loss: 0.4750128388404846\n",
      "Epoch 19006: train loss: 0.47501277923583984\n",
      "Epoch 19007: train loss: 0.4750126898288727\n",
      "Epoch 19008: train loss: 0.4750126302242279\n",
      "Epoch 19009: train loss: 0.47501257061958313\n",
      "Epoch 19010: train loss: 0.47501254081726074\n",
      "Epoch 19011: train loss: 0.47501248121261597\n",
      "Epoch 19012: train loss: 0.4750124216079712\n",
      "Epoch 19013: train loss: 0.47501233220100403\n",
      "Epoch 19014: train loss: 0.47501233220100403\n",
      "Epoch 19015: train loss: 0.47501227259635925\n",
      "Epoch 19016: train loss: 0.4750121533870697\n",
      "Epoch 19017: train loss: 0.4750121235847473\n",
      "Epoch 19018: train loss: 0.47501206398010254\n",
      "Epoch 19019: train loss: 0.47501200437545776\n",
      "Epoch 19020: train loss: 0.4750119149684906\n",
      "Epoch 19021: train loss: 0.4750118553638458\n",
      "Epoch 19022: train loss: 0.47501179575920105\n",
      "Epoch 19023: train loss: 0.4750117361545563\n",
      "Epoch 19024: train loss: 0.4750117361545563\n",
      "Epoch 19025: train loss: 0.4750116467475891\n",
      "Epoch 19026: train loss: 0.47501158714294434\n",
      "Epoch 19027: train loss: 0.47501152753829956\n",
      "Epoch 19028: train loss: 0.4750114381313324\n",
      "Epoch 19029: train loss: 0.4750114381313324\n",
      "Epoch 19030: train loss: 0.47501131892204285\n",
      "Epoch 19031: train loss: 0.47501131892204285\n",
      "Epoch 19032: train loss: 0.4750112295150757\n",
      "Epoch 19033: train loss: 0.4750112295150757\n",
      "Epoch 19034: train loss: 0.47501111030578613\n",
      "Epoch 19035: train loss: 0.47501108050346375\n",
      "Epoch 19036: train loss: 0.47501108050346375\n",
      "Epoch 19037: train loss: 0.4750109612941742\n",
      "Epoch 19038: train loss: 0.4750109016895294\n",
      "Epoch 19039: train loss: 0.47501087188720703\n",
      "Epoch 19040: train loss: 0.47501081228256226\n",
      "Epoch 19041: train loss: 0.4750106930732727\n",
      "Epoch 19042: train loss: 0.4750106930732727\n",
      "Epoch 19043: train loss: 0.47501060366630554\n",
      "Epoch 19044: train loss: 0.47501060366630554\n",
      "Epoch 19045: train loss: 0.475010484457016\n",
      "Epoch 19046: train loss: 0.4750104546546936\n",
      "Epoch 19047: train loss: 0.47501039505004883\n",
      "Epoch 19048: train loss: 0.47501039505004883\n",
      "Epoch 19049: train loss: 0.4750102758407593\n",
      "Epoch 19050: train loss: 0.4750102460384369\n",
      "Epoch 19051: train loss: 0.4750101864337921\n",
      "Epoch 19052: train loss: 0.47501006722450256\n",
      "Epoch 19053: train loss: 0.47501006722450256\n",
      "Epoch 19054: train loss: 0.4750099778175354\n",
      "Epoch 19055: train loss: 0.4750099182128906\n",
      "Epoch 19056: train loss: 0.47500985860824585\n",
      "Epoch 19057: train loss: 0.4750097990036011\n",
      "Epoch 19058: train loss: 0.4750097692012787\n",
      "Epoch 19059: train loss: 0.4750097095966339\n",
      "Epoch 19060: train loss: 0.47500964999198914\n",
      "Epoch 19061: train loss: 0.47500959038734436\n",
      "Epoch 19062: train loss: 0.475009560585022\n",
      "Epoch 19063: train loss: 0.4750094413757324\n",
      "Epoch 19064: train loss: 0.47500938177108765\n",
      "Epoch 19065: train loss: 0.47500935196876526\n",
      "Epoch 19066: train loss: 0.4750092923641205\n",
      "Epoch 19067: train loss: 0.4750092327594757\n",
      "Epoch 19068: train loss: 0.47500914335250854\n",
      "Epoch 19069: train loss: 0.47500914335250854\n",
      "Epoch 19070: train loss: 0.475009024143219\n",
      "Epoch 19071: train loss: 0.4750089645385742\n",
      "Epoch 19072: train loss: 0.47500893473625183\n",
      "Epoch 19073: train loss: 0.47500887513160706\n",
      "Epoch 19074: train loss: 0.4750088155269623\n",
      "Epoch 19075: train loss: 0.4750087559223175\n",
      "Epoch 19076: train loss: 0.47500866651535034\n",
      "Epoch 19077: train loss: 0.47500866651535034\n",
      "Epoch 19078: train loss: 0.47500860691070557\n",
      "Epoch 19079: train loss: 0.4750085473060608\n",
      "Epoch 19080: train loss: 0.4750085175037384\n",
      "Epoch 19081: train loss: 0.47500839829444885\n",
      "Epoch 19082: train loss: 0.4750083386898041\n",
      "Epoch 19083: train loss: 0.4750083088874817\n",
      "Epoch 19084: train loss: 0.4750083088874817\n",
      "Epoch 19085: train loss: 0.47500818967819214\n",
      "Epoch 19086: train loss: 0.47500818967819214\n",
      "Epoch 19087: train loss: 0.4750080406665802\n",
      "Epoch 19088: train loss: 0.4750080406665802\n",
      "Epoch 19089: train loss: 0.4750079810619354\n",
      "Epoch 19090: train loss: 0.47500792145729065\n",
      "Epoch 19091: train loss: 0.47500789165496826\n",
      "Epoch 19092: train loss: 0.4750077724456787\n",
      "Epoch 19093: train loss: 0.4750077724456787\n",
      "Epoch 19094: train loss: 0.47500768303871155\n",
      "Epoch 19095: train loss: 0.4750076234340668\n",
      "Epoch 19096: train loss: 0.475007563829422\n",
      "Epoch 19097: train loss: 0.4750075042247772\n",
      "Epoch 19098: train loss: 0.47500747442245483\n",
      "Epoch 19099: train loss: 0.47500741481781006\n",
      "Epoch 19100: train loss: 0.4750073552131653\n",
      "Epoch 19101: train loss: 0.4750072658061981\n",
      "Epoch 19102: train loss: 0.4750072658061981\n",
      "Epoch 19103: train loss: 0.47500714659690857\n",
      "Epoch 19104: train loss: 0.4750070869922638\n",
      "Epoch 19105: train loss: 0.4750070571899414\n",
      "Epoch 19106: train loss: 0.47500699758529663\n",
      "Epoch 19107: train loss: 0.47500693798065186\n",
      "Epoch 19108: train loss: 0.4750068783760071\n",
      "Epoch 19109: train loss: 0.4750067889690399\n",
      "Epoch 19110: train loss: 0.47500672936439514\n",
      "Epoch 19111: train loss: 0.47500666975975037\n",
      "Epoch 19112: train loss: 0.475006639957428\n",
      "Epoch 19113: train loss: 0.4750065207481384\n",
      "Epoch 19114: train loss: 0.4750065207481384\n",
      "Epoch 19115: train loss: 0.47500643134117126\n",
      "Epoch 19116: train loss: 0.47500643134117126\n",
      "Epoch 19117: train loss: 0.4750063121318817\n",
      "Epoch 19118: train loss: 0.47500625252723694\n",
      "Epoch 19119: train loss: 0.47500625252723694\n",
      "Epoch 19120: train loss: 0.4750061631202698\n",
      "Epoch 19121: train loss: 0.475006103515625\n",
      "Epoch 19122: train loss: 0.4750060439109802\n",
      "Epoch 19123: train loss: 0.47500598430633545\n",
      "Epoch 19124: train loss: 0.47500595450401306\n",
      "Epoch 19125: train loss: 0.4750058948993683\n",
      "Epoch 19126: train loss: 0.47500577569007874\n",
      "Epoch 19127: train loss: 0.47500574588775635\n",
      "Epoch 19128: train loss: 0.4750056862831116\n",
      "Epoch 19129: train loss: 0.475005567073822\n",
      "Epoch 19130: train loss: 0.475005567073822\n",
      "Epoch 19131: train loss: 0.47500547766685486\n",
      "Epoch 19132: train loss: 0.47500547766685486\n",
      "Epoch 19133: train loss: 0.4750053584575653\n",
      "Epoch 19134: train loss: 0.4750053286552429\n",
      "Epoch 19135: train loss: 0.47500526905059814\n",
      "Epoch 19136: train loss: 0.47500526905059814\n",
      "Epoch 19137: train loss: 0.4750051498413086\n",
      "Epoch 19138: train loss: 0.4750051200389862\n",
      "Epoch 19139: train loss: 0.47500506043434143\n",
      "Epoch 19140: train loss: 0.47500500082969666\n",
      "Epoch 19141: train loss: 0.4750049114227295\n",
      "Epoch 19142: train loss: 0.4750049114227295\n",
      "Epoch 19143: train loss: 0.4750048518180847\n",
      "Epoch 19144: train loss: 0.47500473260879517\n",
      "Epoch 19145: train loss: 0.47500473260879517\n",
      "Epoch 19146: train loss: 0.475004643201828\n",
      "Epoch 19147: train loss: 0.4750045835971832\n",
      "Epoch 19148: train loss: 0.47500452399253845\n",
      "Epoch 19149: train loss: 0.47500449419021606\n",
      "Epoch 19150: train loss: 0.4750044345855713\n",
      "Epoch 19151: train loss: 0.4750043749809265\n",
      "Epoch 19152: train loss: 0.47500431537628174\n",
      "Epoch 19153: train loss: 0.4750042259693146\n",
      "Epoch 19154: train loss: 0.4750041663646698\n",
      "Epoch 19155: train loss: 0.475004106760025\n",
      "Epoch 19156: train loss: 0.475004106760025\n",
      "Epoch 19157: train loss: 0.47500401735305786\n",
      "Epoch 19158: train loss: 0.4750039577484131\n",
      "Epoch 19159: train loss: 0.4750038981437683\n",
      "Epoch 19160: train loss: 0.47500380873680115\n",
      "Epoch 19161: train loss: 0.47500380873680115\n",
      "Epoch 19162: train loss: 0.47500374913215637\n",
      "Epoch 19163: train loss: 0.4750036597251892\n",
      "Epoch 19164: train loss: 0.4750036597251892\n",
      "Epoch 19165: train loss: 0.47500354051589966\n",
      "Epoch 19166: train loss: 0.4750034809112549\n",
      "Epoch 19167: train loss: 0.4750034511089325\n",
      "Epoch 19168: train loss: 0.4750033915042877\n",
      "Epoch 19169: train loss: 0.4750033915042877\n",
      "Epoch 19170: train loss: 0.47500327229499817\n",
      "Epoch 19171: train loss: 0.47500327229499817\n",
      "Epoch 19172: train loss: 0.475003182888031\n",
      "Epoch 19173: train loss: 0.47500306367874146\n",
      "Epoch 19174: train loss: 0.4750029742717743\n",
      "Epoch 19175: train loss: 0.4750029742717743\n",
      "Epoch 19176: train loss: 0.4750029742717743\n",
      "Epoch 19177: train loss: 0.4750029146671295\n",
      "Epoch 19178: train loss: 0.47500282526016235\n",
      "Epoch 19179: train loss: 0.47500282526016235\n",
      "Epoch 19180: train loss: 0.4750027060508728\n",
      "Epoch 19181: train loss: 0.475002646446228\n",
      "Epoch 19182: train loss: 0.47500255703926086\n",
      "Epoch 19183: train loss: 0.47500255703926086\n",
      "Epoch 19184: train loss: 0.4750024378299713\n",
      "Epoch 19185: train loss: 0.4750024080276489\n",
      "Epoch 19186: train loss: 0.47500234842300415\n",
      "Epoch 19187: train loss: 0.4750022888183594\n",
      "Epoch 19188: train loss: 0.4750022292137146\n",
      "Epoch 19189: train loss: 0.4750021696090698\n",
      "Epoch 19190: train loss: 0.47500213980674744\n",
      "Epoch 19191: train loss: 0.47500208020210266\n",
      "Epoch 19192: train loss: 0.4750019609928131\n",
      "Epoch 19193: train loss: 0.4750019311904907\n",
      "Epoch 19194: train loss: 0.47500187158584595\n",
      "Epoch 19195: train loss: 0.47500181198120117\n",
      "Epoch 19196: train loss: 0.47500181198120117\n",
      "Epoch 19197: train loss: 0.4750017523765564\n",
      "Epoch 19198: train loss: 0.47500166296958923\n",
      "Epoch 19199: train loss: 0.47500160336494446\n",
      "Epoch 19200: train loss: 0.4750015437602997\n",
      "Epoch 19201: train loss: 0.4750015139579773\n",
      "Epoch 19202: train loss: 0.4750014543533325\n",
      "Epoch 19203: train loss: 0.47500133514404297\n",
      "Epoch 19204: train loss: 0.4750013053417206\n",
      "Epoch 19205: train loss: 0.4750013053417206\n",
      "Epoch 19206: train loss: 0.47500118613243103\n",
      "Epoch 19207: train loss: 0.47500112652778625\n",
      "Epoch 19208: train loss: 0.47500109672546387\n",
      "Epoch 19209: train loss: 0.4750010371208191\n",
      "Epoch 19210: train loss: 0.47500091791152954\n",
      "Epoch 19211: train loss: 0.47500091791152954\n",
      "Epoch 19212: train loss: 0.4750008285045624\n",
      "Epoch 19213: train loss: 0.4750007688999176\n",
      "Epoch 19214: train loss: 0.4750007092952728\n",
      "Epoch 19215: train loss: 0.47500067949295044\n",
      "Epoch 19216: train loss: 0.47500061988830566\n",
      "Epoch 19217: train loss: 0.4750005602836609\n",
      "Epoch 19218: train loss: 0.4750005006790161\n",
      "Epoch 19219: train loss: 0.4750004708766937\n",
      "Epoch 19220: train loss: 0.4750003516674042\n",
      "Epoch 19221: train loss: 0.4750003516674042\n",
      "Epoch 19222: train loss: 0.4750002920627594\n",
      "Epoch 19223: train loss: 0.47500020265579224\n",
      "Epoch 19224: train loss: 0.47500014305114746\n",
      "Epoch 19225: train loss: 0.47500014305114746\n",
      "Epoch 19226: train loss: 0.4750000536441803\n",
      "Epoch 19227: train loss: 0.4750000536441803\n",
      "Epoch 19228: train loss: 0.47499993443489075\n",
      "Epoch 19229: train loss: 0.4749998450279236\n",
      "Epoch 19230: train loss: 0.4749998450279236\n",
      "Epoch 19231: train loss: 0.4749997854232788\n",
      "Epoch 19232: train loss: 0.47499966621398926\n",
      "Epoch 19233: train loss: 0.47499963641166687\n",
      "Epoch 19234: train loss: 0.47499963641166687\n",
      "Epoch 19235: train loss: 0.4749995172023773\n",
      "Epoch 19236: train loss: 0.47499945759773254\n",
      "Epoch 19237: train loss: 0.47499942779541016\n",
      "Epoch 19238: train loss: 0.4749993681907654\n",
      "Epoch 19239: train loss: 0.4749993085861206\n",
      "Epoch 19240: train loss: 0.47499924898147583\n",
      "Epoch 19241: train loss: 0.47499921917915344\n",
      "Epoch 19242: train loss: 0.47499915957450867\n",
      "Epoch 19243: train loss: 0.4749990403652191\n",
      "Epoch 19244: train loss: 0.4749990403652191\n",
      "Epoch 19245: train loss: 0.47499901056289673\n",
      "Epoch 19246: train loss: 0.4749988913536072\n",
      "Epoch 19247: train loss: 0.4749988317489624\n",
      "Epoch 19248: train loss: 0.47499874234199524\n",
      "Epoch 19249: train loss: 0.47499874234199524\n",
      "Epoch 19250: train loss: 0.47499868273735046\n",
      "Epoch 19251: train loss: 0.4749985933303833\n",
      "Epoch 19252: train loss: 0.4749985337257385\n",
      "Epoch 19253: train loss: 0.4749985337257385\n",
      "Epoch 19254: train loss: 0.47499847412109375\n",
      "Epoch 19255: train loss: 0.474998414516449\n",
      "Epoch 19256: train loss: 0.4749983549118042\n",
      "Epoch 19257: train loss: 0.47499826550483704\n",
      "Epoch 19258: train loss: 0.47499826550483704\n",
      "Epoch 19259: train loss: 0.47499820590019226\n",
      "Epoch 19260: train loss: 0.4749981164932251\n",
      "Epoch 19261: train loss: 0.47499799728393555\n",
      "Epoch 19262: train loss: 0.47499793767929077\n",
      "Epoch 19263: train loss: 0.4749979078769684\n",
      "Epoch 19264: train loss: 0.4749978482723236\n",
      "Epoch 19265: train loss: 0.4749978482723236\n",
      "Epoch 19266: train loss: 0.47499772906303406\n",
      "Epoch 19267: train loss: 0.47499772906303406\n",
      "Epoch 19268: train loss: 0.4749976396560669\n",
      "Epoch 19269: train loss: 0.4749975800514221\n",
      "Epoch 19270: train loss: 0.47499752044677734\n",
      "Epoch 19271: train loss: 0.4749974310398102\n",
      "Epoch 19272: train loss: 0.4749973714351654\n",
      "Epoch 19273: train loss: 0.47499731183052063\n",
      "Epoch 19274: train loss: 0.47499731183052063\n",
      "Epoch 19275: train loss: 0.47499722242355347\n",
      "Epoch 19276: train loss: 0.4749971628189087\n",
      "Epoch 19277: train loss: 0.4749971032142639\n",
      "Epoch 19278: train loss: 0.47499707341194153\n",
      "Epoch 19279: train loss: 0.47499701380729675\n",
      "Epoch 19280: train loss: 0.474996954202652\n",
      "Epoch 19281: train loss: 0.4749968647956848\n",
      "Epoch 19282: train loss: 0.47499680519104004\n",
      "Epoch 19283: train loss: 0.47499680519104004\n",
      "Epoch 19284: train loss: 0.4749966859817505\n",
      "Epoch 19285: train loss: 0.4749966561794281\n",
      "Epoch 19286: train loss: 0.4749965965747833\n",
      "Epoch 19287: train loss: 0.47499653697013855\n",
      "Epoch 19288: train loss: 0.4749964773654938\n",
      "Epoch 19289: train loss: 0.4749964475631714\n",
      "Epoch 19290: train loss: 0.4749963879585266\n",
      "Epoch 19291: train loss: 0.47499626874923706\n",
      "Epoch 19292: train loss: 0.4749962389469147\n",
      "Epoch 19293: train loss: 0.4749961793422699\n",
      "Epoch 19294: train loss: 0.4749961197376251\n",
      "Epoch 19295: train loss: 0.4749961197376251\n",
      "Epoch 19296: train loss: 0.47499606013298035\n",
      "Epoch 19297: train loss: 0.4749959707260132\n",
      "Epoch 19298: train loss: 0.4749959111213684\n",
      "Epoch 19299: train loss: 0.47499585151672363\n",
      "Epoch 19300: train loss: 0.47499576210975647\n",
      "Epoch 19301: train loss: 0.4749957025051117\n",
      "Epoch 19302: train loss: 0.4749957025051117\n",
      "Epoch 19303: train loss: 0.4749956429004669\n",
      "Epoch 19304: train loss: 0.47499561309814453\n",
      "Epoch 19305: train loss: 0.4749954342842102\n",
      "Epoch 19306: train loss: 0.4749954342842102\n",
      "Epoch 19307: train loss: 0.4749954044818878\n",
      "Epoch 19308: train loss: 0.47499534487724304\n",
      "Epoch 19309: train loss: 0.4749952256679535\n",
      "Epoch 19310: train loss: 0.4749952256679535\n",
      "Epoch 19311: train loss: 0.4749951958656311\n",
      "Epoch 19312: train loss: 0.47499513626098633\n",
      "Epoch 19313: train loss: 0.4749950170516968\n",
      "Epoch 19314: train loss: 0.4749950170516968\n",
      "Epoch 19315: train loss: 0.4749949276447296\n",
      "Epoch 19316: train loss: 0.47499486804008484\n",
      "Epoch 19317: train loss: 0.47499480843544006\n",
      "Epoch 19318: train loss: 0.47499480843544006\n",
      "Epoch 19319: train loss: 0.4749946594238281\n",
      "Epoch 19320: train loss: 0.47499459981918335\n",
      "Epoch 19321: train loss: 0.47499459981918335\n",
      "Epoch 19322: train loss: 0.4749945402145386\n",
      "Epoch 19323: train loss: 0.4749945104122162\n",
      "Epoch 19324: train loss: 0.47499439120292664\n",
      "Epoch 19325: train loss: 0.47499439120292664\n",
      "Epoch 19326: train loss: 0.4749943017959595\n",
      "Epoch 19327: train loss: 0.4749942421913147\n",
      "Epoch 19328: train loss: 0.4749941825866699\n",
      "Epoch 19329: train loss: 0.47499412298202515\n",
      "Epoch 19330: train loss: 0.47499409317970276\n",
      "Epoch 19331: train loss: 0.4749939739704132\n",
      "Epoch 19332: train loss: 0.47499391436576843\n",
      "Epoch 19333: train loss: 0.47499391436576843\n",
      "Epoch 19334: train loss: 0.47499382495880127\n",
      "Epoch 19335: train loss: 0.4749937653541565\n",
      "Epoch 19336: train loss: 0.4749937057495117\n",
      "Epoch 19337: train loss: 0.47499367594718933\n",
      "Epoch 19338: train loss: 0.4749935567378998\n",
      "Epoch 19339: train loss: 0.4749935567378998\n",
      "Epoch 19340: train loss: 0.4749934673309326\n",
      "Epoch 19341: train loss: 0.4749934673309326\n",
      "Epoch 19342: train loss: 0.47499334812164307\n",
      "Epoch 19343: train loss: 0.4749932885169983\n",
      "Epoch 19344: train loss: 0.4749932885169983\n",
      "Epoch 19345: train loss: 0.4749932587146759\n",
      "Epoch 19346: train loss: 0.47499313950538635\n",
      "Epoch 19347: train loss: 0.4749930799007416\n",
      "Epoch 19348: train loss: 0.4749930500984192\n",
      "Epoch 19349: train loss: 0.4749929904937744\n",
      "Epoch 19350: train loss: 0.47499293088912964\n",
      "Epoch 19351: train loss: 0.47499287128448486\n",
      "Epoch 19352: train loss: 0.4749928414821625\n",
      "Epoch 19353: train loss: 0.4749927818775177\n",
      "Epoch 19354: train loss: 0.47499266266822815\n",
      "Epoch 19355: train loss: 0.47499266266822815\n",
      "Epoch 19356: train loss: 0.474992573261261\n",
      "Epoch 19357: train loss: 0.4749925136566162\n",
      "Epoch 19358: train loss: 0.4749925136566162\n",
      "Epoch 19359: train loss: 0.47499242424964905\n",
      "Epoch 19360: train loss: 0.4749923646450043\n",
      "Epoch 19361: train loss: 0.4749922454357147\n",
      "Epoch 19362: train loss: 0.4749922454357147\n",
      "Epoch 19363: train loss: 0.47499215602874756\n",
      "Epoch 19364: train loss: 0.4749920964241028\n",
      "Epoch 19365: train loss: 0.4749920964241028\n",
      "Epoch 19366: train loss: 0.4749920070171356\n",
      "Epoch 19367: train loss: 0.47499194741249084\n",
      "Epoch 19368: train loss: 0.47499188780784607\n",
      "Epoch 19369: train loss: 0.47499188780784607\n",
      "Epoch 19370: train loss: 0.4749917984008789\n",
      "Epoch 19371: train loss: 0.47499167919158936\n",
      "Epoch 19372: train loss: 0.47499167919158936\n",
      "Epoch 19373: train loss: 0.4749915897846222\n",
      "Epoch 19374: train loss: 0.4749915897846222\n",
      "Epoch 19375: train loss: 0.47499147057533264\n",
      "Epoch 19376: train loss: 0.47499141097068787\n",
      "Epoch 19377: train loss: 0.4749913215637207\n",
      "Epoch 19378: train loss: 0.4749913215637207\n",
      "Epoch 19379: train loss: 0.47499120235443115\n",
      "Epoch 19380: train loss: 0.4749912619590759\n",
      "Epoch 19381: train loss: 0.47499117255210876\n",
      "Epoch 19382: train loss: 0.474991112947464\n",
      "Epoch 19383: train loss: 0.4749910533428192\n",
      "Epoch 19384: train loss: 0.47499096393585205\n",
      "Epoch 19385: train loss: 0.47499096393585205\n",
      "Epoch 19386: train loss: 0.4749908447265625\n",
      "Epoch 19387: train loss: 0.4749907851219177\n",
      "Epoch 19388: train loss: 0.47499072551727295\n",
      "Epoch 19389: train loss: 0.47499069571495056\n",
      "Epoch 19390: train loss: 0.4749906361103058\n",
      "Epoch 19391: train loss: 0.47499051690101624\n",
      "Epoch 19392: train loss: 0.47499051690101624\n",
      "Epoch 19393: train loss: 0.47499048709869385\n",
      "Epoch 19394: train loss: 0.4749904274940491\n",
      "Epoch 19395: train loss: 0.4749903678894043\n",
      "Epoch 19396: train loss: 0.4749903082847595\n",
      "Epoch 19397: train loss: 0.47499021887779236\n",
      "Epoch 19398: train loss: 0.4749900996685028\n",
      "Epoch 19399: train loss: 0.4749900996685028\n",
      "Epoch 19400: train loss: 0.47499001026153564\n",
      "Epoch 19401: train loss: 0.47498995065689087\n",
      "Epoch 19402: train loss: 0.47498995065689087\n",
      "Epoch 19403: train loss: 0.4749898612499237\n",
      "Epoch 19404: train loss: 0.4749898612499237\n",
      "Epoch 19405: train loss: 0.47498980164527893\n",
      "Epoch 19406: train loss: 0.4749896824359894\n",
      "Epoch 19407: train loss: 0.4749896824359894\n",
      "Epoch 19408: train loss: 0.474989652633667\n",
      "Epoch 19409: train loss: 0.47498953342437744\n",
      "Epoch 19410: train loss: 0.47498947381973267\n",
      "Epoch 19411: train loss: 0.4749894440174103\n",
      "Epoch 19412: train loss: 0.4749893844127655\n",
      "Epoch 19413: train loss: 0.4749893248081207\n",
      "Epoch 19414: train loss: 0.47498926520347595\n",
      "Epoch 19415: train loss: 0.4749891757965088\n",
      "Epoch 19416: train loss: 0.4749891757965088\n",
      "Epoch 19417: train loss: 0.47498905658721924\n",
      "Epoch 19418: train loss: 0.47498902678489685\n",
      "Epoch 19419: train loss: 0.4749889671802521\n",
      "Epoch 19420: train loss: 0.4749889075756073\n",
      "Epoch 19421: train loss: 0.4749888479709625\n",
      "Epoch 19422: train loss: 0.47498881816864014\n",
      "Epoch 19423: train loss: 0.47498875856399536\n",
      "Epoch 19424: train loss: 0.4749886989593506\n",
      "Epoch 19425: train loss: 0.4749886393547058\n",
      "Epoch 19426: train loss: 0.4749886095523834\n",
      "Epoch 19427: train loss: 0.47498849034309387\n",
      "Epoch 19428: train loss: 0.47498849034309387\n",
      "Epoch 19429: train loss: 0.4749884307384491\n",
      "Epoch 19430: train loss: 0.47498834133148193\n",
      "Epoch 19431: train loss: 0.47498828172683716\n",
      "Epoch 19432: train loss: 0.4749882221221924\n",
      "Epoch 19433: train loss: 0.47498819231987\n",
      "Epoch 19434: train loss: 0.4749881327152252\n",
      "Epoch 19435: train loss: 0.47498807311058044\n",
      "Epoch 19436: train loss: 0.47498801350593567\n",
      "Epoch 19437: train loss: 0.4749879837036133\n",
      "Epoch 19438: train loss: 0.4749879240989685\n",
      "Epoch 19439: train loss: 0.47498780488967896\n",
      "Epoch 19440: train loss: 0.47498780488967896\n",
      "Epoch 19441: train loss: 0.4749877154827118\n",
      "Epoch 19442: train loss: 0.474987655878067\n",
      "Epoch 19443: train loss: 0.47498759627342224\n",
      "Epoch 19444: train loss: 0.47498756647109985\n",
      "Epoch 19445: train loss: 0.4749875068664551\n",
      "Epoch 19446: train loss: 0.4749874472618103\n",
      "Epoch 19447: train loss: 0.47498735785484314\n",
      "Epoch 19448: train loss: 0.47498735785484314\n",
      "Epoch 19449: train loss: 0.47498729825019836\n",
      "Epoch 19450: train loss: 0.4749871790409088\n",
      "Epoch 19451: train loss: 0.4749871790409088\n",
      "Epoch 19452: train loss: 0.4749871492385864\n",
      "Epoch 19453: train loss: 0.47498708963394165\n",
      "Epoch 19454: train loss: 0.4749869704246521\n",
      "Epoch 19455: train loss: 0.47498688101768494\n",
      "Epoch 19456: train loss: 0.47498682141304016\n",
      "Epoch 19457: train loss: 0.4749867618083954\n",
      "Epoch 19458: train loss: 0.4749867618083954\n",
      "Epoch 19459: train loss: 0.4749867022037506\n",
      "Epoch 19460: train loss: 0.4749866724014282\n",
      "Epoch 19461: train loss: 0.47498655319213867\n",
      "Epoch 19462: train loss: 0.47498655319213867\n",
      "Epoch 19463: train loss: 0.4749864935874939\n",
      "Epoch 19464: train loss: 0.4749864637851715\n",
      "Epoch 19465: train loss: 0.47498634457588196\n",
      "Epoch 19466: train loss: 0.47498634457588196\n",
      "Epoch 19467: train loss: 0.47498619556427\n",
      "Epoch 19468: train loss: 0.47498613595962524\n",
      "Epoch 19469: train loss: 0.47498613595962524\n",
      "Epoch 19470: train loss: 0.47498607635498047\n",
      "Epoch 19471: train loss: 0.4749859869480133\n",
      "Epoch 19472: train loss: 0.4749859869480133\n",
      "Epoch 19473: train loss: 0.47498592734336853\n",
      "Epoch 19474: train loss: 0.47498583793640137\n",
      "Epoch 19475: train loss: 0.4749857187271118\n",
      "Epoch 19476: train loss: 0.4749857187271118\n",
      "Epoch 19477: train loss: 0.47498565912246704\n",
      "Epoch 19478: train loss: 0.47498562932014465\n",
      "Epoch 19479: train loss: 0.4749855101108551\n",
      "Epoch 19480: train loss: 0.4749855101108551\n",
      "Epoch 19481: train loss: 0.4749854505062103\n",
      "Epoch 19482: train loss: 0.47498542070388794\n",
      "Epoch 19483: train loss: 0.47498536109924316\n",
      "Epoch 19484: train loss: 0.4749853014945984\n",
      "Epoch 19485: train loss: 0.4749852418899536\n",
      "Epoch 19486: train loss: 0.47498515248298645\n",
      "Epoch 19487: train loss: 0.4749850928783417\n",
      "Epoch 19488: train loss: 0.4749850332736969\n",
      "Epoch 19489: train loss: 0.4749850034713745\n",
      "Epoch 19490: train loss: 0.47498494386672974\n",
      "Epoch 19491: train loss: 0.47498488426208496\n",
      "Epoch 19492: train loss: 0.4749847948551178\n",
      "Epoch 19493: train loss: 0.4749847948551178\n",
      "Epoch 19494: train loss: 0.474984735250473\n",
      "Epoch 19495: train loss: 0.47498467564582825\n",
      "Epoch 19496: train loss: 0.4749845862388611\n",
      "Epoch 19497: train loss: 0.4749845266342163\n",
      "Epoch 19498: train loss: 0.47498446702957153\n",
      "Epoch 19499: train loss: 0.47498440742492676\n",
      "Epoch 19500: train loss: 0.47498437762260437\n",
      "Epoch 19501: train loss: 0.4749843180179596\n",
      "Epoch 19502: train loss: 0.47498419880867004\n",
      "Epoch 19503: train loss: 0.47498419880867004\n",
      "Epoch 19504: train loss: 0.47498416900634766\n",
      "Epoch 19505: train loss: 0.4749841094017029\n",
      "Epoch 19506: train loss: 0.47498399019241333\n",
      "Epoch 19507: train loss: 0.47498396039009094\n",
      "Epoch 19508: train loss: 0.47498396039009094\n",
      "Epoch 19509: train loss: 0.4749838411808014\n",
      "Epoch 19510: train loss: 0.4749837815761566\n",
      "Epoch 19511: train loss: 0.47498375177383423\n",
      "Epoch 19512: train loss: 0.47498369216918945\n",
      "Epoch 19513: train loss: 0.4749835729598999\n",
      "Epoch 19514: train loss: 0.4749835431575775\n",
      "Epoch 19515: train loss: 0.4749835431575775\n",
      "Epoch 19516: train loss: 0.47498348355293274\n",
      "Epoch 19517: train loss: 0.4749833345413208\n",
      "Epoch 19518: train loss: 0.4749833643436432\n",
      "Epoch 19519: train loss: 0.474983274936676\n",
      "Epoch 19520: train loss: 0.47498321533203125\n",
      "Epoch 19521: train loss: 0.4749831557273865\n",
      "Epoch 19522: train loss: 0.4749830663204193\n",
      "Epoch 19523: train loss: 0.4749830663204193\n",
      "Epoch 19524: train loss: 0.47498300671577454\n",
      "Epoch 19525: train loss: 0.474982887506485\n",
      "Epoch 19526: train loss: 0.4749828577041626\n",
      "Epoch 19527: train loss: 0.4749828577041626\n",
      "Epoch 19528: train loss: 0.47498273849487305\n",
      "Epoch 19529: train loss: 0.47498267889022827\n",
      "Epoch 19530: train loss: 0.4749826490879059\n",
      "Epoch 19531: train loss: 0.47498252987861633\n",
      "Epoch 19532: train loss: 0.47498252987861633\n",
      "Epoch 19533: train loss: 0.47498247027397156\n",
      "Epoch 19534: train loss: 0.47498244047164917\n",
      "Epoch 19535: train loss: 0.4749823808670044\n",
      "Epoch 19536: train loss: 0.47498226165771484\n",
      "Epoch 19537: train loss: 0.47498226165771484\n",
      "Epoch 19538: train loss: 0.47498223185539246\n",
      "Epoch 19539: train loss: 0.4749821126461029\n",
      "Epoch 19540: train loss: 0.47498205304145813\n",
      "Epoch 19541: train loss: 0.47498202323913574\n",
      "Epoch 19542: train loss: 0.47498196363449097\n",
      "Epoch 19543: train loss: 0.4749819040298462\n",
      "Epoch 19544: train loss: 0.4749818444252014\n",
      "Epoch 19545: train loss: 0.47498175501823425\n",
      "Epoch 19546: train loss: 0.47498175501823425\n",
      "Epoch 19547: train loss: 0.4749816954135895\n",
      "Epoch 19548: train loss: 0.4749816060066223\n",
      "Epoch 19549: train loss: 0.47498154640197754\n",
      "Epoch 19550: train loss: 0.47498148679733276\n",
      "Epoch 19551: train loss: 0.474981427192688\n",
      "Epoch 19552: train loss: 0.4749813973903656\n",
      "Epoch 19553: train loss: 0.4749813377857208\n",
      "Epoch 19554: train loss: 0.47498127818107605\n",
      "Epoch 19555: train loss: 0.4749811887741089\n",
      "Epoch 19556: train loss: 0.4749811291694641\n",
      "Epoch 19557: train loss: 0.4749811291694641\n",
      "Epoch 19558: train loss: 0.47498100996017456\n",
      "Epoch 19559: train loss: 0.47498100996017456\n",
      "Epoch 19560: train loss: 0.4749809801578522\n",
      "Epoch 19561: train loss: 0.4749809205532074\n",
      "Epoch 19562: train loss: 0.47498080134391785\n",
      "Epoch 19563: train loss: 0.47498080134391785\n",
      "Epoch 19564: train loss: 0.4749807119369507\n",
      "Epoch 19565: train loss: 0.47498059272766113\n",
      "Epoch 19566: train loss: 0.47498056292533875\n",
      "Epoch 19567: train loss: 0.47498050332069397\n",
      "Epoch 19568: train loss: 0.47498050332069397\n",
      "Epoch 19569: train loss: 0.4749804437160492\n",
      "Epoch 19570: train loss: 0.47498035430908203\n",
      "Epoch 19571: train loss: 0.47498029470443726\n",
      "Epoch 19572: train loss: 0.47498029470443726\n",
      "Epoch 19573: train loss: 0.4749801754951477\n",
      "Epoch 19574: train loss: 0.4749801754951477\n",
      "Epoch 19575: train loss: 0.47498008608818054\n",
      "Epoch 19576: train loss: 0.47498002648353577\n",
      "Epoch 19577: train loss: 0.474979966878891\n",
      "Epoch 19578: train loss: 0.4749799370765686\n",
      "Epoch 19579: train loss: 0.47497981786727905\n",
      "Epoch 19580: train loss: 0.47497981786727905\n",
      "Epoch 19581: train loss: 0.4749797284603119\n",
      "Epoch 19582: train loss: 0.4749797284603119\n",
      "Epoch 19583: train loss: 0.4749796688556671\n",
      "Epoch 19584: train loss: 0.47497960925102234\n",
      "Epoch 19585: train loss: 0.4749795198440552\n",
      "Epoch 19586: train loss: 0.4749794602394104\n",
      "Epoch 19587: train loss: 0.4749794006347656\n",
      "Epoch 19588: train loss: 0.47497934103012085\n",
      "Epoch 19589: train loss: 0.4749792516231537\n",
      "Epoch 19590: train loss: 0.4749792516231537\n",
      "Epoch 19591: train loss: 0.4749791920185089\n",
      "Epoch 19592: train loss: 0.47497913241386414\n",
      "Epoch 19593: train loss: 0.474979043006897\n",
      "Epoch 19594: train loss: 0.4749789834022522\n",
      "Epoch 19595: train loss: 0.474979043006897\n",
      "Epoch 19596: train loss: 0.47497886419296265\n",
      "Epoch 19597: train loss: 0.47497883439064026\n",
      "Epoch 19598: train loss: 0.4749787747859955\n",
      "Epoch 19599: train loss: 0.4749787151813507\n",
      "Epoch 19600: train loss: 0.47497862577438354\n",
      "Epoch 19601: train loss: 0.47497862577438354\n",
      "Epoch 19602: train loss: 0.47497862577438354\n",
      "Epoch 19603: train loss: 0.474978506565094\n",
      "Epoch 19604: train loss: 0.4749784469604492\n",
      "Epoch 19605: train loss: 0.47497841715812683\n",
      "Epoch 19606: train loss: 0.4749782979488373\n",
      "Epoch 19607: train loss: 0.4749782383441925\n",
      "Epoch 19608: train loss: 0.4749782085418701\n",
      "Epoch 19609: train loss: 0.4749782085418701\n",
      "Epoch 19610: train loss: 0.47497814893722534\n",
      "Epoch 19611: train loss: 0.4749780297279358\n",
      "Epoch 19612: train loss: 0.4749779999256134\n",
      "Epoch 19613: train loss: 0.47497794032096863\n",
      "Epoch 19614: train loss: 0.47497788071632385\n",
      "Epoch 19615: train loss: 0.4749777913093567\n",
      "Epoch 19616: train loss: 0.4749777913093567\n",
      "Epoch 19617: train loss: 0.4749777317047119\n",
      "Epoch 19618: train loss: 0.47497767210006714\n",
      "Epoch 19619: train loss: 0.4749775826931\n",
      "Epoch 19620: train loss: 0.4749775826931\n",
      "Epoch 19621: train loss: 0.4749774634838104\n",
      "Epoch 19622: train loss: 0.4749774634838104\n",
      "Epoch 19623: train loss: 0.47497737407684326\n",
      "Epoch 19624: train loss: 0.4749773144721985\n",
      "Epoch 19625: train loss: 0.47497719526290894\n",
      "Epoch 19626: train loss: 0.47497719526290894\n",
      "Epoch 19627: train loss: 0.47497716546058655\n",
      "Epoch 19628: train loss: 0.4749771058559418\n",
      "Epoch 19629: train loss: 0.474977046251297\n",
      "Epoch 19630: train loss: 0.47497695684432983\n",
      "Epoch 19631: train loss: 0.47497689723968506\n",
      "Epoch 19632: train loss: 0.4749768376350403\n",
      "Epoch 19633: train loss: 0.4749767482280731\n",
      "Epoch 19634: train loss: 0.4749767482280731\n",
      "Epoch 19635: train loss: 0.47497668862342834\n",
      "Epoch 19636: train loss: 0.47497662901878357\n",
      "Epoch 19637: train loss: 0.4749765396118164\n",
      "Epoch 19638: train loss: 0.47497648000717163\n",
      "Epoch 19639: train loss: 0.47497642040252686\n",
      "Epoch 19640: train loss: 0.47497642040252686\n",
      "Epoch 19641: train loss: 0.4749763309955597\n",
      "Epoch 19642: train loss: 0.4749762713909149\n",
      "Epoch 19643: train loss: 0.47497621178627014\n",
      "Epoch 19644: train loss: 0.47497615218162537\n",
      "Epoch 19645: train loss: 0.474976122379303\n",
      "Epoch 19646: train loss: 0.4749760627746582\n",
      "Epoch 19647: train loss: 0.4749760031700134\n",
      "Epoch 19648: train loss: 0.47497594356536865\n",
      "Epoch 19649: train loss: 0.47497591376304626\n",
      "Epoch 19650: train loss: 0.4749758541584015\n",
      "Epoch 19651: train loss: 0.4749757945537567\n",
      "Epoch 19652: train loss: 0.47497573494911194\n",
      "Epoch 19653: train loss: 0.4749756455421448\n",
      "Epoch 19654: train loss: 0.4749755859375\n",
      "Epoch 19655: train loss: 0.4749755263328552\n",
      "Epoch 19656: train loss: 0.47497546672821045\n",
      "Epoch 19657: train loss: 0.47497543692588806\n",
      "Epoch 19658: train loss: 0.4749753773212433\n",
      "Epoch 19659: train loss: 0.4749753177165985\n",
      "Epoch 19660: train loss: 0.47497525811195374\n",
      "Epoch 19661: train loss: 0.47497522830963135\n",
      "Epoch 19662: train loss: 0.4749751687049866\n",
      "Epoch 19663: train loss: 0.474975049495697\n",
      "Epoch 19664: train loss: 0.47497501969337463\n",
      "Epoch 19665: train loss: 0.47497496008872986\n",
      "Epoch 19666: train loss: 0.47497496008872986\n",
      "Epoch 19667: train loss: 0.4749748408794403\n",
      "Epoch 19668: train loss: 0.47497475147247314\n",
      "Epoch 19669: train loss: 0.47497475147247314\n",
      "Epoch 19670: train loss: 0.47497469186782837\n",
      "Epoch 19671: train loss: 0.4749746322631836\n",
      "Epoch 19672: train loss: 0.4749746024608612\n",
      "Epoch 19673: train loss: 0.47497448325157166\n",
      "Epoch 19674: train loss: 0.47497448325157166\n",
      "Epoch 19675: train loss: 0.4749743938446045\n",
      "Epoch 19676: train loss: 0.4749743342399597\n",
      "Epoch 19677: train loss: 0.47497427463531494\n",
      "Epoch 19678: train loss: 0.47497427463531494\n",
      "Epoch 19679: train loss: 0.4749741852283478\n",
      "Epoch 19680: train loss: 0.4749740660190582\n",
      "Epoch 19681: train loss: 0.4749740660190582\n",
      "Epoch 19682: train loss: 0.47497400641441345\n",
      "Epoch 19683: train loss: 0.47497397661209106\n",
      "Epoch 19684: train loss: 0.4749739170074463\n",
      "Epoch 19685: train loss: 0.4749738574028015\n",
      "Epoch 19686: train loss: 0.47497376799583435\n",
      "Epoch 19687: train loss: 0.47497376799583435\n",
      "Epoch 19688: train loss: 0.4749737083911896\n",
      "Epoch 19689: train loss: 0.4749735891819\n",
      "Epoch 19690: train loss: 0.47497355937957764\n",
      "Epoch 19691: train loss: 0.47497349977493286\n",
      "Epoch 19692: train loss: 0.4749734401702881\n",
      "Epoch 19693: train loss: 0.4749733805656433\n",
      "Epoch 19694: train loss: 0.4749733507633209\n",
      "Epoch 19695: train loss: 0.47497329115867615\n",
      "Epoch 19696: train loss: 0.4749731719493866\n",
      "Epoch 19697: train loss: 0.4749731719493866\n",
      "Epoch 19698: train loss: 0.4749731421470642\n",
      "Epoch 19699: train loss: 0.47497302293777466\n",
      "Epoch 19700: train loss: 0.4749729633331299\n",
      "Epoch 19701: train loss: 0.4749729335308075\n",
      "Epoch 19702: train loss: 0.4749728739261627\n",
      "Epoch 19703: train loss: 0.47497281432151794\n",
      "Epoch 19704: train loss: 0.47497275471687317\n",
      "Epoch 19705: train loss: 0.4749727249145508\n",
      "Epoch 19706: train loss: 0.474972665309906\n",
      "Epoch 19707: train loss: 0.47497260570526123\n",
      "Epoch 19708: train loss: 0.47497254610061646\n",
      "Epoch 19709: train loss: 0.4749724566936493\n",
      "Epoch 19710: train loss: 0.4749724566936493\n",
      "Epoch 19711: train loss: 0.47497233748435974\n",
      "Epoch 19712: train loss: 0.47497230768203735\n",
      "Epoch 19713: train loss: 0.47497230768203735\n",
      "Epoch 19714: train loss: 0.4749721884727478\n",
      "Epoch 19715: train loss: 0.474972128868103\n",
      "Epoch 19716: train loss: 0.47497209906578064\n",
      "Epoch 19717: train loss: 0.47497203946113586\n",
      "Epoch 19718: train loss: 0.4749719798564911\n",
      "Epoch 19719: train loss: 0.4749719202518463\n",
      "Epoch 19720: train loss: 0.4749718904495239\n",
      "Epoch 19721: train loss: 0.47497183084487915\n",
      "Epoch 19722: train loss: 0.4749717712402344\n",
      "Epoch 19723: train loss: 0.4749716520309448\n",
      "Epoch 19724: train loss: 0.4749716520309448\n",
      "Epoch 19725: train loss: 0.47497162222862244\n",
      "Epoch 19726: train loss: 0.4749715030193329\n",
      "Epoch 19727: train loss: 0.4749714434146881\n",
      "Epoch 19728: train loss: 0.4749714136123657\n",
      "Epoch 19729: train loss: 0.4749714136123657\n",
      "Epoch 19730: train loss: 0.47497129440307617\n",
      "Epoch 19731: train loss: 0.474971204996109\n",
      "Epoch 19732: train loss: 0.47497114539146423\n",
      "Epoch 19733: train loss: 0.47497114539146423\n",
      "Epoch 19734: train loss: 0.47497108578681946\n",
      "Epoch 19735: train loss: 0.4749710261821747\n",
      "Epoch 19736: train loss: 0.4749709963798523\n",
      "Epoch 19737: train loss: 0.4749709367752075\n",
      "Epoch 19738: train loss: 0.47497081756591797\n",
      "Epoch 19739: train loss: 0.47497081756591797\n",
      "Epoch 19740: train loss: 0.4749707281589508\n",
      "Epoch 19741: train loss: 0.4749707281589508\n",
      "Epoch 19742: train loss: 0.47497060894966125\n",
      "Epoch 19743: train loss: 0.47497057914733887\n",
      "Epoch 19744: train loss: 0.4749705195426941\n",
      "Epoch 19745: train loss: 0.4749704599380493\n",
      "Epoch 19746: train loss: 0.47497037053108215\n",
      "Epoch 19747: train loss: 0.4749703109264374\n",
      "Epoch 19748: train loss: 0.4749702513217926\n",
      "Epoch 19749: train loss: 0.4749702513217926\n",
      "Epoch 19750: train loss: 0.47497016191482544\n",
      "Epoch 19751: train loss: 0.47497010231018066\n",
      "Epoch 19752: train loss: 0.4749700427055359\n",
      "Epoch 19753: train loss: 0.4749699831008911\n",
      "Epoch 19754: train loss: 0.4749699532985687\n",
      "Epoch 19755: train loss: 0.47496989369392395\n",
      "Epoch 19756: train loss: 0.4749698340892792\n",
      "Epoch 19757: train loss: 0.474969744682312\n",
      "Epoch 19758: train loss: 0.47496968507766724\n",
      "Epoch 19759: train loss: 0.47496962547302246\n",
      "Epoch 19760: train loss: 0.47496962547302246\n",
      "Epoch 19761: train loss: 0.4749695360660553\n",
      "Epoch 19762: train loss: 0.4749694764614105\n",
      "Epoch 19763: train loss: 0.47496941685676575\n",
      "Epoch 19764: train loss: 0.47496935725212097\n",
      "Epoch 19765: train loss: 0.4749693274497986\n",
      "Epoch 19766: train loss: 0.4749692678451538\n",
      "Epoch 19767: train loss: 0.47496920824050903\n",
      "Epoch 19768: train loss: 0.47496911883354187\n",
      "Epoch 19769: train loss: 0.4749690592288971\n",
      "Epoch 19770: train loss: 0.4749689996242523\n",
      "Epoch 19771: train loss: 0.47496894001960754\n",
      "Epoch 19772: train loss: 0.47496891021728516\n",
      "Epoch 19773: train loss: 0.4749688506126404\n",
      "Epoch 19774: train loss: 0.4749687910079956\n",
      "Epoch 19775: train loss: 0.47496873140335083\n",
      "Epoch 19776: train loss: 0.47496870160102844\n",
      "Epoch 19777: train loss: 0.47496864199638367\n",
      "Epoch 19778: train loss: 0.4749685823917389\n",
      "Epoch 19779: train loss: 0.4749685227870941\n",
      "Epoch 19780: train loss: 0.47496849298477173\n",
      "Epoch 19781: train loss: 0.47496843338012695\n",
      "Epoch 19782: train loss: 0.4749683737754822\n",
      "Epoch 19783: train loss: 0.4749683141708374\n",
      "Epoch 19784: train loss: 0.47496816515922546\n",
      "Epoch 19785: train loss: 0.47496816515922546\n",
      "Epoch 19786: train loss: 0.47496816515922546\n",
      "Epoch 19787: train loss: 0.4749681055545807\n",
      "Epoch 19788: train loss: 0.4749680161476135\n",
      "Epoch 19789: train loss: 0.47496795654296875\n",
      "Epoch 19790: train loss: 0.474967896938324\n",
      "Epoch 19791: train loss: 0.4749678373336792\n",
      "Epoch 19792: train loss: 0.4749678075313568\n",
      "Epoch 19793: train loss: 0.47496768832206726\n",
      "Epoch 19794: train loss: 0.47496768832206726\n",
      "Epoch 19795: train loss: 0.4749676287174225\n",
      "Epoch 19796: train loss: 0.4749675393104553\n",
      "Epoch 19797: train loss: 0.4749675393104553\n",
      "Epoch 19798: train loss: 0.4749675393104553\n",
      "Epoch 19799: train loss: 0.47496742010116577\n",
      "Epoch 19800: train loss: 0.47496727108955383\n",
      "Epoch 19801: train loss: 0.47496727108955383\n",
      "Epoch 19802: train loss: 0.47496721148490906\n",
      "Epoch 19803: train loss: 0.47496721148490906\n",
      "Epoch 19804: train loss: 0.4749671220779419\n",
      "Epoch 19805: train loss: 0.47496700286865234\n",
      "Epoch 19806: train loss: 0.47496700286865234\n",
      "Epoch 19807: train loss: 0.4749669134616852\n",
      "Epoch 19808: train loss: 0.4749668538570404\n",
      "Epoch 19809: train loss: 0.4749668538570404\n",
      "Epoch 19810: train loss: 0.47496676445007324\n",
      "Epoch 19811: train loss: 0.47496676445007324\n",
      "Epoch 19812: train loss: 0.4749666452407837\n",
      "Epoch 19813: train loss: 0.4749665856361389\n",
      "Epoch 19814: train loss: 0.47496655583381653\n",
      "Epoch 19815: train loss: 0.47496649622917175\n",
      "Epoch 19816: train loss: 0.474966436624527\n",
      "Epoch 19817: train loss: 0.4749663770198822\n",
      "Epoch 19818: train loss: 0.4749663472175598\n",
      "Epoch 19819: train loss: 0.47496628761291504\n",
      "Epoch 19820: train loss: 0.47496622800827026\n",
      "Epoch 19821: train loss: 0.4749661684036255\n",
      "Epoch 19822: train loss: 0.4749661386013031\n",
      "Epoch 19823: train loss: 0.4749660789966583\n",
      "Epoch 19824: train loss: 0.4749659597873688\n",
      "Epoch 19825: train loss: 0.4749659597873688\n",
      "Epoch 19826: train loss: 0.4749658703804016\n",
      "Epoch 19827: train loss: 0.47496581077575684\n",
      "Epoch 19828: train loss: 0.47496575117111206\n",
      "Epoch 19829: train loss: 0.4749656617641449\n",
      "Epoch 19830: train loss: 0.4749656617641449\n",
      "Epoch 19831: train loss: 0.47496554255485535\n",
      "Epoch 19832: train loss: 0.47496554255485535\n",
      "Epoch 19833: train loss: 0.47496551275253296\n",
      "Epoch 19834: train loss: 0.4749654531478882\n",
      "Epoch 19835: train loss: 0.47496533393859863\n",
      "Epoch 19836: train loss: 0.47496530413627625\n",
      "Epoch 19837: train loss: 0.47496524453163147\n",
      "Epoch 19838: train loss: 0.4749651849269867\n",
      "Epoch 19839: train loss: 0.4749651253223419\n",
      "Epoch 19840: train loss: 0.47496503591537476\n",
      "Epoch 19841: train loss: 0.47496503591537476\n",
      "Epoch 19842: train loss: 0.47496497631073\n",
      "Epoch 19843: train loss: 0.4749648869037628\n",
      "Epoch 19844: train loss: 0.4749648869037628\n",
      "Epoch 19845: train loss: 0.47496476769447327\n",
      "Epoch 19846: train loss: 0.4749647080898285\n",
      "Epoch 19847: train loss: 0.47496461868286133\n",
      "Epoch 19848: train loss: 0.47496461868286133\n",
      "Epoch 19849: train loss: 0.47496455907821655\n",
      "Epoch 19850: train loss: 0.47496455907821655\n",
      "Epoch 19851: train loss: 0.4749644696712494\n",
      "Epoch 19852: train loss: 0.4749644100666046\n",
      "Epoch 19853: train loss: 0.47496429085731506\n",
      "Epoch 19854: train loss: 0.4749642610549927\n",
      "Epoch 19855: train loss: 0.4749642610549927\n",
      "Epoch 19856: train loss: 0.4749642014503479\n",
      "Epoch 19857: train loss: 0.4749641418457031\n",
      "Epoch 19858: train loss: 0.47496408224105835\n",
      "Epoch 19859: train loss: 0.4749640226364136\n",
      "Epoch 19860: train loss: 0.4749639928340912\n",
      "Epoch 19861: train loss: 0.47496387362480164\n",
      "Epoch 19862: train loss: 0.47496381402015686\n",
      "Epoch 19863: train loss: 0.4749637246131897\n",
      "Epoch 19864: train loss: 0.4749637246131897\n",
      "Epoch 19865: train loss: 0.4749636650085449\n",
      "Epoch 19866: train loss: 0.47496360540390015\n",
      "Epoch 19867: train loss: 0.474963515996933\n",
      "Epoch 19868: train loss: 0.4749634563922882\n",
      "Epoch 19869: train loss: 0.4749634563922882\n",
      "Epoch 19870: train loss: 0.47496336698532104\n",
      "Epoch 19871: train loss: 0.47496336698532104\n",
      "Epoch 19872: train loss: 0.47496330738067627\n",
      "Epoch 19873: train loss: 0.4749631881713867\n",
      "Epoch 19874: train loss: 0.47496315836906433\n",
      "Epoch 19875: train loss: 0.47496309876441956\n",
      "Epoch 19876: train loss: 0.4749630391597748\n",
      "Epoch 19877: train loss: 0.47496297955513\n",
      "Epoch 19878: train loss: 0.47496289014816284\n",
      "Epoch 19879: train loss: 0.47496283054351807\n",
      "Epoch 19880: train loss: 0.47496283054351807\n",
      "Epoch 19881: train loss: 0.4749627411365509\n",
      "Epoch 19882: train loss: 0.47496268153190613\n",
      "Epoch 19883: train loss: 0.47496268153190613\n",
      "Epoch 19884: train loss: 0.4749625623226166\n",
      "Epoch 19885: train loss: 0.4749625325202942\n",
      "Epoch 19886: train loss: 0.4749624729156494\n",
      "Epoch 19887: train loss: 0.47496241331100464\n",
      "Epoch 19888: train loss: 0.47496235370635986\n",
      "Epoch 19889: train loss: 0.4749623239040375\n",
      "Epoch 19890: train loss: 0.4749622046947479\n",
      "Epoch 19891: train loss: 0.4749622046947479\n",
      "Epoch 19892: train loss: 0.47496211528778076\n",
      "Epoch 19893: train loss: 0.47496211528778076\n",
      "Epoch 19894: train loss: 0.4749619960784912\n",
      "Epoch 19895: train loss: 0.47496193647384644\n",
      "Epoch 19896: train loss: 0.47496190667152405\n",
      "Epoch 19897: train loss: 0.47496190667152405\n",
      "Epoch 19898: train loss: 0.4749617874622345\n",
      "Epoch 19899: train loss: 0.4749617278575897\n",
      "Epoch 19900: train loss: 0.47496169805526733\n",
      "Epoch 19901: train loss: 0.47496163845062256\n",
      "Epoch 19902: train loss: 0.47496163845062256\n",
      "Epoch 19903: train loss: 0.474961519241333\n",
      "Epoch 19904: train loss: 0.4749614894390106\n",
      "Epoch 19905: train loss: 0.47496142983436584\n",
      "Epoch 19906: train loss: 0.4749613106250763\n",
      "Epoch 19907: train loss: 0.4749612808227539\n",
      "Epoch 19908: train loss: 0.47496122121810913\n",
      "Epoch 19909: train loss: 0.47496116161346436\n",
      "Epoch 19910: train loss: 0.4749611020088196\n",
      "Epoch 19911: train loss: 0.4749610722064972\n",
      "Epoch 19912: train loss: 0.4749610722064972\n",
      "Epoch 19913: train loss: 0.4749610126018524\n",
      "Epoch 19914: train loss: 0.4749608635902405\n",
      "Epoch 19915: train loss: 0.4749608039855957\n",
      "Epoch 19916: train loss: 0.4749608039855957\n",
      "Epoch 19917: train loss: 0.4749607443809509\n",
      "Epoch 19918: train loss: 0.47496065497398376\n",
      "Epoch 19919: train loss: 0.474960595369339\n",
      "Epoch 19920: train loss: 0.474960595369339\n",
      "Epoch 19921: train loss: 0.47496047616004944\n",
      "Epoch 19922: train loss: 0.47496047616004944\n",
      "Epoch 19923: train loss: 0.4749603271484375\n",
      "Epoch 19924: train loss: 0.4749603271484375\n",
      "Epoch 19925: train loss: 0.4749602675437927\n",
      "Epoch 19926: train loss: 0.47496020793914795\n",
      "Epoch 19927: train loss: 0.47496017813682556\n",
      "Epoch 19928: train loss: 0.4749601185321808\n",
      "Epoch 19929: train loss: 0.474960058927536\n",
      "Epoch 19930: train loss: 0.47495999932289124\n",
      "Epoch 19931: train loss: 0.4749599099159241\n",
      "Epoch 19932: train loss: 0.4749599099159241\n",
      "Epoch 19933: train loss: 0.4749597907066345\n",
      "Epoch 19934: train loss: 0.47495976090431213\n",
      "Epoch 19935: train loss: 0.47495976090431213\n",
      "Epoch 19936: train loss: 0.4749595820903778\n",
      "Epoch 19937: train loss: 0.4749595820903778\n",
      "Epoch 19938: train loss: 0.4749595522880554\n",
      "Epoch 19939: train loss: 0.47495949268341064\n",
      "Epoch 19940: train loss: 0.47495943307876587\n",
      "Epoch 19941: train loss: 0.4749593436717987\n",
      "Epoch 19942: train loss: 0.4749593436717987\n",
      "Epoch 19943: train loss: 0.47495928406715393\n",
      "Epoch 19944: train loss: 0.4749591648578644\n",
      "Epoch 19945: train loss: 0.4749591648578644\n",
      "Epoch 19946: train loss: 0.4749590754508972\n",
      "Epoch 19947: train loss: 0.47495901584625244\n",
      "Epoch 19948: train loss: 0.47495895624160767\n",
      "Epoch 19949: train loss: 0.4749589264392853\n",
      "Epoch 19950: train loss: 0.4749588668346405\n",
      "Epoch 19951: train loss: 0.4749588072299957\n",
      "Epoch 19952: train loss: 0.47495874762535095\n",
      "Epoch 19953: train loss: 0.47495871782302856\n",
      "Epoch 19954: train loss: 0.474958598613739\n",
      "Epoch 19955: train loss: 0.47495853900909424\n",
      "Epoch 19956: train loss: 0.47495853900909424\n",
      "Epoch 19957: train loss: 0.47495850920677185\n",
      "Epoch 19958: train loss: 0.4749584496021271\n",
      "Epoch 19959: train loss: 0.4749583899974823\n",
      "Epoch 19960: train loss: 0.4749583303928375\n",
      "Epoch 19961: train loss: 0.47495824098587036\n",
      "Epoch 19962: train loss: 0.4749581813812256\n",
      "Epoch 19963: train loss: 0.4749581217765808\n",
      "Epoch 19964: train loss: 0.4749580919742584\n",
      "Epoch 19965: train loss: 0.4749580919742584\n",
      "Epoch 19966: train loss: 0.47495797276496887\n",
      "Epoch 19967: train loss: 0.4749579131603241\n",
      "Epoch 19968: train loss: 0.4749578833580017\n",
      "Epoch 19969: train loss: 0.47495776414871216\n",
      "Epoch 19970: train loss: 0.4749577045440674\n",
      "Epoch 19971: train loss: 0.474957674741745\n",
      "Epoch 19972: train loss: 0.4749576151371002\n",
      "Epoch 19973: train loss: 0.4749576151371002\n",
      "Epoch 19974: train loss: 0.47495749592781067\n",
      "Epoch 19975: train loss: 0.4749574661254883\n",
      "Epoch 19976: train loss: 0.4749574065208435\n",
      "Epoch 19977: train loss: 0.4749574065208435\n",
      "Epoch 19978: train loss: 0.47495728731155396\n",
      "Epoch 19979: train loss: 0.47495725750923157\n",
      "Epoch 19980: train loss: 0.4749571979045868\n",
      "Epoch 19981: train loss: 0.474957138299942\n",
      "Epoch 19982: train loss: 0.47495707869529724\n",
      "Epoch 19983: train loss: 0.47495704889297485\n",
      "Epoch 19984: train loss: 0.4749569892883301\n",
      "Epoch 19985: train loss: 0.4749569296836853\n",
      "Epoch 19986: train loss: 0.47495684027671814\n",
      "Epoch 19987: train loss: 0.47495678067207336\n",
      "Epoch 19988: train loss: 0.47495678067207336\n",
      "Epoch 19989: train loss: 0.4749566614627838\n",
      "Epoch 19990: train loss: 0.47495657205581665\n",
      "Epoch 19991: train loss: 0.47495657205581665\n",
      "Epoch 19992: train loss: 0.4749564528465271\n",
      "Epoch 19993: train loss: 0.4749564528465271\n",
      "Epoch 19994: train loss: 0.47495636343955994\n",
      "Epoch 19995: train loss: 0.47495636343955994\n",
      "Epoch 19996: train loss: 0.47495630383491516\n",
      "Epoch 19997: train loss: 0.4749561846256256\n",
      "Epoch 19998: train loss: 0.4749561548233032\n",
      "Epoch 19999: train loss: 0.47495609521865845\n",
      "Epoch 20000: train loss: 0.47495603561401367\n",
      "Epoch 20001: train loss: 0.47495603561401367\n",
      "Epoch 20002: train loss: 0.4749559462070465\n",
      "Epoch 20003: train loss: 0.47495588660240173\n",
      "Epoch 20004: train loss: 0.47495582699775696\n",
      "Epoch 20005: train loss: 0.4749557673931122\n",
      "Epoch 20006: train loss: 0.4749557375907898\n",
      "Epoch 20007: train loss: 0.474955677986145\n",
      "Epoch 20008: train loss: 0.47495561838150024\n",
      "Epoch 20009: train loss: 0.4749555289745331\n",
      "Epoch 20010: train loss: 0.4749555289745331\n",
      "Epoch 20011: train loss: 0.4749554693698883\n",
      "Epoch 20012: train loss: 0.47495535016059875\n",
      "Epoch 20013: train loss: 0.47495532035827637\n",
      "Epoch 20014: train loss: 0.47495532035827637\n",
      "Epoch 20015: train loss: 0.4749552011489868\n",
      "Epoch 20016: train loss: 0.47495514154434204\n",
      "Epoch 20017: train loss: 0.47495511174201965\n",
      "Epoch 20018: train loss: 0.4749550521373749\n",
      "Epoch 20019: train loss: 0.4749549925327301\n",
      "Epoch 20020: train loss: 0.4749549329280853\n",
      "Epoch 20021: train loss: 0.47495490312576294\n",
      "Epoch 20022: train loss: 0.4749547839164734\n",
      "Epoch 20023: train loss: 0.4749547243118286\n",
      "Epoch 20024: train loss: 0.4749546945095062\n",
      "Epoch 20025: train loss: 0.4749545753002167\n",
      "Epoch 20026: train loss: 0.47495463490486145\n",
      "Epoch 20027: train loss: 0.4749544858932495\n",
      "Epoch 20028: train loss: 0.4749544858932495\n",
      "Epoch 20029: train loss: 0.47495442628860474\n",
      "Epoch 20030: train loss: 0.4749543070793152\n",
      "Epoch 20031: train loss: 0.4749543070793152\n",
      "Epoch 20032: train loss: 0.4749542772769928\n",
      "Epoch 20033: train loss: 0.474954217672348\n",
      "Epoch 20034: train loss: 0.47495409846305847\n",
      "Epoch 20035: train loss: 0.4749540686607361\n",
      "Epoch 20036: train loss: 0.4749540090560913\n",
      "Epoch 20037: train loss: 0.47495394945144653\n",
      "Epoch 20038: train loss: 0.47495388984680176\n",
      "Epoch 20039: train loss: 0.47495386004447937\n",
      "Epoch 20040: train loss: 0.4749538004398346\n",
      "Epoch 20041: train loss: 0.4749538004398346\n",
      "Epoch 20042: train loss: 0.47495365142822266\n",
      "Epoch 20043: train loss: 0.47495365142822266\n",
      "Epoch 20044: train loss: 0.4749535918235779\n",
      "Epoch 20045: train loss: 0.4749535918235779\n",
      "Epoch 20046: train loss: 0.47495347261428833\n",
      "Epoch 20047: train loss: 0.47495344281196594\n",
      "Epoch 20048: train loss: 0.47495338320732117\n",
      "Epoch 20049: train loss: 0.4749533236026764\n",
      "Epoch 20050: train loss: 0.47495323419570923\n",
      "Epoch 20051: train loss: 0.47495317459106445\n",
      "Epoch 20052: train loss: 0.4749531149864197\n",
      "Epoch 20053: train loss: 0.4749530553817749\n",
      "Epoch 20054: train loss: 0.4749530255794525\n",
      "Epoch 20055: train loss: 0.47495296597480774\n",
      "Epoch 20056: train loss: 0.47495290637016296\n",
      "Epoch 20057: train loss: 0.4749528169631958\n",
      "Epoch 20058: train loss: 0.4749528169631958\n",
      "Epoch 20059: train loss: 0.474952757358551\n",
      "Epoch 20060: train loss: 0.47495269775390625\n",
      "Epoch 20061: train loss: 0.4749526381492615\n",
      "Epoch 20062: train loss: 0.4749525785446167\n",
      "Epoch 20063: train loss: 0.47495248913764954\n",
      "Epoch 20064: train loss: 0.47495242953300476\n",
      "Epoch 20065: train loss: 0.47495236992836\n",
      "Epoch 20066: train loss: 0.47495236992836\n",
      "Epoch 20067: train loss: 0.4749522805213928\n",
      "Epoch 20068: train loss: 0.4749522805213928\n",
      "Epoch 20069: train loss: 0.47495216131210327\n",
      "Epoch 20070: train loss: 0.4749520719051361\n",
      "Epoch 20071: train loss: 0.4749520719051361\n",
      "Epoch 20072: train loss: 0.47495195269584656\n",
      "Epoch 20073: train loss: 0.47495195269584656\n",
      "Epoch 20074: train loss: 0.47495192289352417\n",
      "Epoch 20075: train loss: 0.4749518632888794\n",
      "Epoch 20076: train loss: 0.4749518036842346\n",
      "Epoch 20077: train loss: 0.47495174407958984\n",
      "Epoch 20078: train loss: 0.4749516546726227\n",
      "Epoch 20079: train loss: 0.4749515950679779\n",
      "Epoch 20080: train loss: 0.47495153546333313\n",
      "Epoch 20081: train loss: 0.47495150566101074\n",
      "Epoch 20082: train loss: 0.47495144605636597\n",
      "Epoch 20083: train loss: 0.4749513864517212\n",
      "Epoch 20084: train loss: 0.4749513268470764\n",
      "Epoch 20085: train loss: 0.47495129704475403\n",
      "Epoch 20086: train loss: 0.4749511778354645\n",
      "Epoch 20087: train loss: 0.4749511182308197\n",
      "Epoch 20088: train loss: 0.4749511182308197\n",
      "Epoch 20089: train loss: 0.47495102882385254\n",
      "Epoch 20090: train loss: 0.47495096921920776\n",
      "Epoch 20091: train loss: 0.47495096921920776\n",
      "Epoch 20092: train loss: 0.474950909614563\n",
      "Epoch 20093: train loss: 0.4749508202075958\n",
      "Epoch 20094: train loss: 0.47495076060295105\n",
      "Epoch 20095: train loss: 0.4749507009983063\n",
      "Epoch 20096: train loss: 0.4749506711959839\n",
      "Epoch 20097: train loss: 0.4749506115913391\n",
      "Epoch 20098: train loss: 0.47495055198669434\n",
      "Epoch 20099: train loss: 0.4749504625797272\n",
      "Epoch 20100: train loss: 0.4749504625797272\n",
      "Epoch 20101: train loss: 0.4749504029750824\n",
      "Epoch 20102: train loss: 0.4749503433704376\n",
      "Epoch 20103: train loss: 0.47495025396347046\n",
      "Epoch 20104: train loss: 0.4749501943588257\n",
      "Epoch 20105: train loss: 0.4749501943588257\n",
      "Epoch 20106: train loss: 0.47495007514953613\n",
      "Epoch 20107: train loss: 0.47495004534721375\n",
      "Epoch 20108: train loss: 0.4749499261379242\n",
      "Epoch 20109: train loss: 0.4749498665332794\n",
      "Epoch 20110: train loss: 0.4749498665332794\n",
      "Epoch 20111: train loss: 0.4749498665332794\n",
      "Epoch 20112: train loss: 0.47494977712631226\n",
      "Epoch 20113: train loss: 0.4749497175216675\n",
      "Epoch 20114: train loss: 0.4749496281147003\n",
      "Epoch 20115: train loss: 0.47494956851005554\n",
      "Epoch 20116: train loss: 0.47494950890541077\n",
      "Epoch 20117: train loss: 0.474949449300766\n",
      "Epoch 20118: train loss: 0.4749494194984436\n",
      "Epoch 20119: train loss: 0.47494935989379883\n",
      "Epoch 20120: train loss: 0.47494930028915405\n",
      "Epoch 20121: train loss: 0.4749492406845093\n",
      "Epoch 20122: train loss: 0.4749492108821869\n",
      "Epoch 20123: train loss: 0.47494909167289734\n",
      "Epoch 20124: train loss: 0.47494909167289734\n",
      "Epoch 20125: train loss: 0.47494909167289734\n",
      "Epoch 20126: train loss: 0.4749490022659302\n",
      "Epoch 20127: train loss: 0.4749489426612854\n",
      "Epoch 20128: train loss: 0.4749488830566406\n",
      "Epoch 20129: train loss: 0.4749487638473511\n",
      "Epoch 20130: train loss: 0.4749487340450287\n",
      "Epoch 20131: train loss: 0.4749486744403839\n",
      "Epoch 20132: train loss: 0.47494861483573914\n",
      "Epoch 20133: train loss: 0.47494861483573914\n",
      "Epoch 20134: train loss: 0.474948525428772\n",
      "Epoch 20135: train loss: 0.4749484658241272\n",
      "Epoch 20136: train loss: 0.4749484062194824\n",
      "Epoch 20137: train loss: 0.4749484062194824\n",
      "Epoch 20138: train loss: 0.47494831681251526\n",
      "Epoch 20139: train loss: 0.4749482572078705\n",
      "Epoch 20140: train loss: 0.4749481976032257\n",
      "Epoch 20141: train loss: 0.47494813799858093\n",
      "Epoch 20142: train loss: 0.47494810819625854\n",
      "Epoch 20143: train loss: 0.474947988986969\n",
      "Epoch 20144: train loss: 0.474947988986969\n",
      "Epoch 20145: train loss: 0.47494789958000183\n",
      "Epoch 20146: train loss: 0.47494783997535706\n",
      "Epoch 20147: train loss: 0.4749477803707123\n",
      "Epoch 20148: train loss: 0.4749477207660675\n",
      "Epoch 20149: train loss: 0.4749476909637451\n",
      "Epoch 20150: train loss: 0.47494763135910034\n",
      "Epoch 20151: train loss: 0.47494757175445557\n",
      "Epoch 20152: train loss: 0.4749474823474884\n",
      "Epoch 20153: train loss: 0.4749474823474884\n",
      "Epoch 20154: train loss: 0.47494742274284363\n",
      "Epoch 20155: train loss: 0.4749473035335541\n",
      "Epoch 20156: train loss: 0.4749472737312317\n",
      "Epoch 20157: train loss: 0.4749472141265869\n",
      "Epoch 20158: train loss: 0.4749472141265869\n",
      "Epoch 20159: train loss: 0.47494715452194214\n",
      "Epoch 20160: train loss: 0.47494709491729736\n",
      "Epoch 20161: train loss: 0.4749470055103302\n",
      "Epoch 20162: train loss: 0.4749469459056854\n",
      "Epoch 20163: train loss: 0.47494685649871826\n",
      "Epoch 20164: train loss: 0.47494685649871826\n",
      "Epoch 20165: train loss: 0.4749467968940735\n",
      "Epoch 20166: train loss: 0.47494667768478394\n",
      "Epoch 20167: train loss: 0.47494664788246155\n",
      "Epoch 20168: train loss: 0.47494664788246155\n",
      "Epoch 20169: train loss: 0.474946528673172\n",
      "Epoch 20170: train loss: 0.474946528673172\n",
      "Epoch 20171: train loss: 0.47494643926620483\n",
      "Epoch 20172: train loss: 0.47494643926620483\n",
      "Epoch 20173: train loss: 0.47494637966156006\n",
      "Epoch 20174: train loss: 0.4749462604522705\n",
      "Epoch 20175: train loss: 0.4749462306499481\n",
      "Epoch 20176: train loss: 0.47494611144065857\n",
      "Epoch 20177: train loss: 0.47494611144065857\n",
      "Epoch 20178: train loss: 0.4749460518360138\n",
      "Epoch 20179: train loss: 0.4749460518360138\n",
      "Epoch 20180: train loss: 0.47494590282440186\n",
      "Epoch 20181: train loss: 0.47494590282440186\n",
      "Epoch 20182: train loss: 0.4749458432197571\n",
      "Epoch 20183: train loss: 0.4749457538127899\n",
      "Epoch 20184: train loss: 0.4749457538127899\n",
      "Epoch 20185: train loss: 0.47494569420814514\n",
      "Epoch 20186: train loss: 0.474945604801178\n",
      "Epoch 20187: train loss: 0.4749455451965332\n",
      "Epoch 20188: train loss: 0.4749454855918884\n",
      "Epoch 20189: train loss: 0.4749454855918884\n",
      "Epoch 20190: train loss: 0.47494542598724365\n",
      "Epoch 20191: train loss: 0.4749453365802765\n",
      "Epoch 20192: train loss: 0.47494521737098694\n",
      "Epoch 20193: train loss: 0.47494521737098694\n",
      "Epoch 20194: train loss: 0.47494518756866455\n",
      "Epoch 20195: train loss: 0.474945068359375\n",
      "Epoch 20196: train loss: 0.474945068359375\n",
      "Epoch 20197: train loss: 0.47494494915008545\n",
      "Epoch 20198: train loss: 0.47494494915008545\n",
      "Epoch 20199: train loss: 0.47494491934776306\n",
      "Epoch 20200: train loss: 0.4749448001384735\n",
      "Epoch 20201: train loss: 0.47494474053382874\n",
      "Epoch 20202: train loss: 0.47494474053382874\n",
      "Epoch 20203: train loss: 0.4749446511268616\n",
      "Epoch 20204: train loss: 0.474944531917572\n",
      "Epoch 20205: train loss: 0.474944531917572\n",
      "Epoch 20206: train loss: 0.47494450211524963\n",
      "Epoch 20207: train loss: 0.47494444251060486\n",
      "Epoch 20208: train loss: 0.4749443829059601\n",
      "Epoch 20209: train loss: 0.4749443233013153\n",
      "Epoch 20210: train loss: 0.47494423389434814\n",
      "Epoch 20211: train loss: 0.47494423389434814\n",
      "Epoch 20212: train loss: 0.47494417428970337\n",
      "Epoch 20213: train loss: 0.4749441146850586\n",
      "Epoch 20214: train loss: 0.4749440848827362\n",
      "Epoch 20215: train loss: 0.47494396567344666\n",
      "Epoch 20216: train loss: 0.4749439060688019\n",
      "Epoch 20217: train loss: 0.4749438762664795\n",
      "Epoch 20218: train loss: 0.4749438166618347\n",
      "Epoch 20219: train loss: 0.47494369745254517\n",
      "Epoch 20220: train loss: 0.47494369745254517\n",
      "Epoch 20221: train loss: 0.4749436676502228\n",
      "Epoch 20222: train loss: 0.4749435484409332\n",
      "Epoch 20223: train loss: 0.4749435484409332\n",
      "Epoch 20224: train loss: 0.47494348883628845\n",
      "Epoch 20225: train loss: 0.4749433994293213\n",
      "Epoch 20226: train loss: 0.4749433994293213\n",
      "Epoch 20227: train loss: 0.4749433398246765\n",
      "Epoch 20228: train loss: 0.47494325041770935\n",
      "Epoch 20229: train loss: 0.4749431908130646\n",
      "Epoch 20230: train loss: 0.4749431908130646\n",
      "Epoch 20231: train loss: 0.474943071603775\n",
      "Epoch 20232: train loss: 0.474943071603775\n",
      "Epoch 20233: train loss: 0.4749429225921631\n",
      "Epoch 20234: train loss: 0.4749429225921631\n",
      "Epoch 20235: train loss: 0.4749428629875183\n",
      "Epoch 20236: train loss: 0.4749428331851959\n",
      "Epoch 20237: train loss: 0.47494277358055115\n",
      "Epoch 20238: train loss: 0.4749426543712616\n",
      "Epoch 20239: train loss: 0.4749426245689392\n",
      "Epoch 20240: train loss: 0.47494256496429443\n",
      "Epoch 20241: train loss: 0.47494250535964966\n",
      "Epoch 20242: train loss: 0.47494250535964966\n",
      "Epoch 20243: train loss: 0.4749424457550049\n",
      "Epoch 20244: train loss: 0.4749423563480377\n",
      "Epoch 20245: train loss: 0.4749423563480377\n",
      "Epoch 20246: train loss: 0.47494223713874817\n",
      "Epoch 20247: train loss: 0.47494223713874817\n",
      "Epoch 20248: train loss: 0.474942147731781\n",
      "Epoch 20249: train loss: 0.47494208812713623\n",
      "Epoch 20250: train loss: 0.47494202852249146\n",
      "Epoch 20251: train loss: 0.47494199872016907\n",
      "Epoch 20252: train loss: 0.4749419391155243\n",
      "Epoch 20253: train loss: 0.47494181990623474\n",
      "Epoch 20254: train loss: 0.4749418795108795\n",
      "Epoch 20255: train loss: 0.47494179010391235\n",
      "Epoch 20256: train loss: 0.4749416708946228\n",
      "Epoch 20257: train loss: 0.474941611289978\n",
      "Epoch 20258: train loss: 0.47494158148765564\n",
      "Epoch 20259: train loss: 0.4749414622783661\n",
      "Epoch 20260: train loss: 0.4749414622783661\n",
      "Epoch 20261: train loss: 0.4749414026737213\n",
      "Epoch 20262: train loss: 0.4749414026737213\n",
      "Epoch 20263: train loss: 0.47494131326675415\n",
      "Epoch 20264: train loss: 0.4749411940574646\n",
      "Epoch 20265: train loss: 0.4749411940574646\n",
      "Epoch 20266: train loss: 0.4749411344528198\n",
      "Epoch 20267: train loss: 0.47494104504585266\n",
      "Epoch 20268: train loss: 0.4749409854412079\n",
      "Epoch 20269: train loss: 0.4749409854412079\n",
      "Epoch 20270: train loss: 0.4749409258365631\n",
      "Epoch 20271: train loss: 0.4749408960342407\n",
      "Epoch 20272: train loss: 0.47494077682495117\n",
      "Epoch 20273: train loss: 0.4749407172203064\n",
      "Epoch 20274: train loss: 0.474940687417984\n",
      "Epoch 20275: train loss: 0.47494062781333923\n",
      "Epoch 20276: train loss: 0.47494056820869446\n",
      "Epoch 20277: train loss: 0.4749404788017273\n",
      "Epoch 20278: train loss: 0.4749404788017273\n",
      "Epoch 20279: train loss: 0.4749404191970825\n",
      "Epoch 20280: train loss: 0.47494035959243774\n",
      "Epoch 20281: train loss: 0.47494035959243774\n",
      "Epoch 20282: train loss: 0.4749402701854706\n",
      "Epoch 20283: train loss: 0.47494015097618103\n",
      "Epoch 20284: train loss: 0.47494009137153625\n",
      "Epoch 20285: train loss: 0.47494009137153625\n",
      "Epoch 20286: train loss: 0.47494006156921387\n",
      "Epoch 20287: train loss: 0.4749399423599243\n",
      "Epoch 20288: train loss: 0.47493988275527954\n",
      "Epoch 20289: train loss: 0.47493985295295715\n",
      "Epoch 20290: train loss: 0.4749397337436676\n",
      "Epoch 20291: train loss: 0.4749397337436676\n",
      "Epoch 20292: train loss: 0.47493964433670044\n",
      "Epoch 20293: train loss: 0.47493964433670044\n",
      "Epoch 20294: train loss: 0.47493958473205566\n",
      "Epoch 20295: train loss: 0.4749394655227661\n",
      "Epoch 20296: train loss: 0.4749394357204437\n",
      "Epoch 20297: train loss: 0.47493937611579895\n",
      "Epoch 20298: train loss: 0.4749393165111542\n",
      "Epoch 20299: train loss: 0.474939227104187\n",
      "Epoch 20300: train loss: 0.474939227104187\n",
      "Epoch 20301: train loss: 0.47493916749954224\n",
      "Epoch 20302: train loss: 0.47493910789489746\n",
      "Epoch 20303: train loss: 0.4749390482902527\n",
      "Epoch 20304: train loss: 0.4749390184879303\n",
      "Epoch 20305: train loss: 0.47493889927864075\n",
      "Epoch 20306: train loss: 0.47493889927864075\n",
      "Epoch 20307: train loss: 0.47493883967399597\n",
      "Epoch 20308: train loss: 0.4749387502670288\n",
      "Epoch 20309: train loss: 0.47493869066238403\n",
      "Epoch 20310: train loss: 0.47493869066238403\n",
      "Epoch 20311: train loss: 0.47493863105773926\n",
      "Epoch 20312: train loss: 0.47493860125541687\n",
      "Epoch 20313: train loss: 0.4749384820461273\n",
      "Epoch 20314: train loss: 0.47493842244148254\n",
      "Epoch 20315: train loss: 0.47493839263916016\n",
      "Epoch 20316: train loss: 0.4749383330345154\n",
      "Epoch 20317: train loss: 0.4749383330345154\n",
      "Epoch 20318: train loss: 0.47493818402290344\n",
      "Epoch 20319: train loss: 0.47493818402290344\n",
      "Epoch 20320: train loss: 0.47493812441825867\n",
      "Epoch 20321: train loss: 0.4749380052089691\n",
      "Epoch 20322: train loss: 0.47493797540664673\n",
      "Epoch 20323: train loss: 0.47493797540664673\n",
      "Epoch 20324: train loss: 0.4749378561973572\n",
      "Epoch 20325: train loss: 0.4749377965927124\n",
      "Epoch 20326: train loss: 0.4749377965927124\n",
      "Epoch 20327: train loss: 0.47493770718574524\n",
      "Epoch 20328: train loss: 0.47493764758110046\n",
      "Epoch 20329: train loss: 0.4749375879764557\n",
      "Epoch 20330: train loss: 0.4749375581741333\n",
      "Epoch 20331: train loss: 0.4749375581741333\n",
      "Epoch 20332: train loss: 0.4749374985694885\n",
      "Epoch 20333: train loss: 0.474937379360199\n",
      "Epoch 20334: train loss: 0.4749373197555542\n",
      "Epoch 20335: train loss: 0.4749372899532318\n",
      "Epoch 20336: train loss: 0.47493717074394226\n",
      "Epoch 20337: train loss: 0.4749371111392975\n",
      "Epoch 20338: train loss: 0.4749371111392975\n",
      "Epoch 20339: train loss: 0.4749370217323303\n",
      "Epoch 20340: train loss: 0.47493696212768555\n",
      "Epoch 20341: train loss: 0.47493690252304077\n",
      "Epoch 20342: train loss: 0.4749368727207184\n",
      "Epoch 20343: train loss: 0.4749368131160736\n",
      "Epoch 20344: train loss: 0.4749368131160736\n",
      "Epoch 20345: train loss: 0.47493669390678406\n",
      "Epoch 20346: train loss: 0.47493669390678406\n",
      "Epoch 20347: train loss: 0.4749366044998169\n",
      "Epoch 20348: train loss: 0.4749365448951721\n",
      "Epoch 20349: train loss: 0.47493645548820496\n",
      "Epoch 20350: train loss: 0.4749363958835602\n",
      "Epoch 20351: train loss: 0.4749363958835602\n",
      "Epoch 20352: train loss: 0.4749363362789154\n",
      "Epoch 20353: train loss: 0.47493624687194824\n",
      "Epoch 20354: train loss: 0.47493618726730347\n",
      "Epoch 20355: train loss: 0.4749361276626587\n",
      "Epoch 20356: train loss: 0.4749360680580139\n",
      "Epoch 20357: train loss: 0.4749360680580139\n",
      "Epoch 20358: train loss: 0.47493597865104675\n",
      "Epoch 20359: train loss: 0.4749358594417572\n",
      "Epoch 20360: train loss: 0.4749358594417572\n",
      "Epoch 20361: train loss: 0.4749358296394348\n",
      "Epoch 20362: train loss: 0.47493577003479004\n",
      "Epoch 20363: train loss: 0.47493571043014526\n",
      "Epoch 20364: train loss: 0.4749356508255005\n",
      "Epoch 20365: train loss: 0.4749356210231781\n",
      "Epoch 20366: train loss: 0.4749355614185333\n",
      "Epoch 20367: train loss: 0.4749354422092438\n",
      "Epoch 20368: train loss: 0.4749354422092438\n",
      "Epoch 20369: train loss: 0.4749354124069214\n",
      "Epoch 20370: train loss: 0.47493529319763184\n",
      "Epoch 20371: train loss: 0.47493523359298706\n",
      "Epoch 20372: train loss: 0.4749352037906647\n",
      "Epoch 20373: train loss: 0.4749351441860199\n",
      "Epoch 20374: train loss: 0.4749350845813751\n",
      "Epoch 20375: train loss: 0.47493502497673035\n",
      "Epoch 20376: train loss: 0.47493499517440796\n",
      "Epoch 20377: train loss: 0.4749349355697632\n",
      "Epoch 20378: train loss: 0.4749348759651184\n",
      "Epoch 20379: train loss: 0.47493478655815125\n",
      "Epoch 20380: train loss: 0.47493472695350647\n",
      "Epoch 20381: train loss: 0.47493472695350647\n",
      "Epoch 20382: train loss: 0.4749346077442169\n",
      "Epoch 20383: train loss: 0.47493457794189453\n",
      "Epoch 20384: train loss: 0.47493451833724976\n",
      "Epoch 20385: train loss: 0.474934458732605\n",
      "Epoch 20386: train loss: 0.4749343991279602\n",
      "Epoch 20387: train loss: 0.4749343693256378\n",
      "Epoch 20388: train loss: 0.47493430972099304\n",
      "Epoch 20389: train loss: 0.4749341905117035\n",
      "Epoch 20390: train loss: 0.4749341905117035\n",
      "Epoch 20391: train loss: 0.4749341905117035\n",
      "Epoch 20392: train loss: 0.47493410110473633\n",
      "Epoch 20393: train loss: 0.4749339818954468\n",
      "Epoch 20394: train loss: 0.4749339520931244\n",
      "Epoch 20395: train loss: 0.4749338924884796\n",
      "Epoch 20396: train loss: 0.4749338924884796\n",
      "Epoch 20397: train loss: 0.47493377327919006\n",
      "Epoch 20398: train loss: 0.47493377327919006\n",
      "Epoch 20399: train loss: 0.4749336838722229\n",
      "Epoch 20400: train loss: 0.4749336242675781\n",
      "Epoch 20401: train loss: 0.4749336242675781\n",
      "Epoch 20402: train loss: 0.4749335050582886\n",
      "Epoch 20403: train loss: 0.4749334752559662\n",
      "Epoch 20404: train loss: 0.47493335604667664\n",
      "Epoch 20405: train loss: 0.47493335604667664\n",
      "Epoch 20406: train loss: 0.47493329644203186\n",
      "Epoch 20407: train loss: 0.4749332070350647\n",
      "Epoch 20408: train loss: 0.4749331474304199\n",
      "Epoch 20409: train loss: 0.47493308782577515\n",
      "Epoch 20410: train loss: 0.47493305802345276\n",
      "Epoch 20411: train loss: 0.474932998418808\n",
      "Epoch 20412: train loss: 0.4749329388141632\n",
      "Epoch 20413: train loss: 0.47493287920951843\n",
      "Epoch 20414: train loss: 0.47493284940719604\n",
      "Epoch 20415: train loss: 0.47493278980255127\n",
      "Epoch 20416: train loss: 0.4749327301979065\n",
      "Epoch 20417: train loss: 0.4749326705932617\n",
      "Epoch 20418: train loss: 0.47493264079093933\n",
      "Epoch 20419: train loss: 0.47493264079093933\n",
      "Epoch 20420: train loss: 0.4749325215816498\n",
      "Epoch 20421: train loss: 0.474932461977005\n",
      "Epoch 20422: train loss: 0.4749324321746826\n",
      "Epoch 20423: train loss: 0.47493231296539307\n",
      "Epoch 20424: train loss: 0.47493231296539307\n",
      "Epoch 20425: train loss: 0.4749322533607483\n",
      "Epoch 20426: train loss: 0.4749322235584259\n",
      "Epoch 20427: train loss: 0.47493210434913635\n",
      "Epoch 20428: train loss: 0.4749320447444916\n",
      "Epoch 20429: train loss: 0.4749320149421692\n",
      "Epoch 20430: train loss: 0.4749320149421692\n",
      "Epoch 20431: train loss: 0.47493183612823486\n",
      "Epoch 20432: train loss: 0.47493183612823486\n",
      "Epoch 20433: train loss: 0.4749318063259125\n",
      "Epoch 20434: train loss: 0.4749318063259125\n",
      "Epoch 20435: train loss: 0.4749316871166229\n",
      "Epoch 20436: train loss: 0.47493162751197815\n",
      "Epoch 20437: train loss: 0.47493159770965576\n",
      "Epoch 20438: train loss: 0.474931538105011\n",
      "Epoch 20439: train loss: 0.4749314785003662\n",
      "Epoch 20440: train loss: 0.47493138909339905\n",
      "Epoch 20441: train loss: 0.4749313294887543\n",
      "Epoch 20442: train loss: 0.4749312698841095\n",
      "Epoch 20443: train loss: 0.4749312102794647\n",
      "Epoch 20444: train loss: 0.47493118047714233\n",
      "Epoch 20445: train loss: 0.47493112087249756\n",
      "Epoch 20446: train loss: 0.474931001663208\n",
      "Epoch 20447: train loss: 0.4749309718608856\n",
      "Epoch 20448: train loss: 0.4749309718608856\n",
      "Epoch 20449: train loss: 0.47493091225624084\n",
      "Epoch 20450: train loss: 0.47493085265159607\n",
      "Epoch 20451: train loss: 0.4749307930469513\n",
      "Epoch 20452: train loss: 0.47493070363998413\n",
      "Epoch 20453: train loss: 0.47493064403533936\n",
      "Epoch 20454: train loss: 0.47493064403533936\n",
      "Epoch 20455: train loss: 0.4749305546283722\n",
      "Epoch 20456: train loss: 0.4749305546283722\n",
      "Epoch 20457: train loss: 0.47493043541908264\n",
      "Epoch 20458: train loss: 0.47493037581443787\n",
      "Epoch 20459: train loss: 0.4749303460121155\n",
      "Epoch 20460: train loss: 0.4749302864074707\n",
      "Epoch 20461: train loss: 0.4749302268028259\n",
      "Epoch 20462: train loss: 0.47493016719818115\n",
      "Epoch 20463: train loss: 0.47493013739585876\n",
      "Epoch 20464: train loss: 0.4749300181865692\n",
      "Epoch 20465: train loss: 0.4749300181865692\n",
      "Epoch 20466: train loss: 0.4749300181865692\n",
      "Epoch 20467: train loss: 0.47492992877960205\n",
      "Epoch 20468: train loss: 0.4749298095703125\n",
      "Epoch 20469: train loss: 0.4749298095703125\n",
      "Epoch 20470: train loss: 0.4749297499656677\n",
      "Epoch 20471: train loss: 0.47492969036102295\n",
      "Epoch 20472: train loss: 0.4749296009540558\n",
      "Epoch 20473: train loss: 0.474929541349411\n",
      "Epoch 20474: train loss: 0.474929541349411\n",
      "Epoch 20475: train loss: 0.4749293923377991\n",
      "Epoch 20476: train loss: 0.47492945194244385\n",
      "Epoch 20477: train loss: 0.4749293327331543\n",
      "Epoch 20478: train loss: 0.4749293327331543\n",
      "Epoch 20479: train loss: 0.47492918372154236\n",
      "Epoch 20480: train loss: 0.47492918372154236\n",
      "Epoch 20481: train loss: 0.4749291241168976\n",
      "Epoch 20482: train loss: 0.4749290645122528\n",
      "Epoch 20483: train loss: 0.47492897510528564\n",
      "Epoch 20484: train loss: 0.47492891550064087\n",
      "Epoch 20485: train loss: 0.47492891550064087\n",
      "Epoch 20486: train loss: 0.4749288260936737\n",
      "Epoch 20487: train loss: 0.4749288260936737\n",
      "Epoch 20488: train loss: 0.47492870688438416\n",
      "Epoch 20489: train loss: 0.4749286472797394\n",
      "Epoch 20490: train loss: 0.474928617477417\n",
      "Epoch 20491: train loss: 0.4749285578727722\n",
      "Epoch 20492: train loss: 0.47492849826812744\n",
      "Epoch 20493: train loss: 0.4749284088611603\n",
      "Epoch 20494: train loss: 0.4749284088611603\n",
      "Epoch 20495: train loss: 0.4749283492565155\n",
      "Epoch 20496: train loss: 0.4749282896518707\n",
      "Epoch 20497: train loss: 0.47492820024490356\n",
      "Epoch 20498: train loss: 0.47492820024490356\n",
      "Epoch 20499: train loss: 0.4749281406402588\n",
      "Epoch 20500: train loss: 0.47492802143096924\n",
      "Epoch 20501: train loss: 0.47492802143096924\n",
      "Epoch 20502: train loss: 0.4749279320240021\n",
      "Epoch 20503: train loss: 0.4749278724193573\n",
      "Epoch 20504: train loss: 0.4749278128147125\n",
      "Epoch 20505: train loss: 0.47492772340774536\n",
      "Epoch 20506: train loss: 0.47492772340774536\n",
      "Epoch 20507: train loss: 0.4749276638031006\n",
      "Epoch 20508: train loss: 0.4749276041984558\n",
      "Epoch 20509: train loss: 0.4749275743961334\n",
      "Epoch 20510: train loss: 0.47492751479148865\n",
      "Epoch 20511: train loss: 0.47492745518684387\n",
      "Epoch 20512: train loss: 0.4749273955821991\n",
      "Epoch 20513: train loss: 0.47492730617523193\n",
      "Epoch 20514: train loss: 0.47492724657058716\n",
      "Epoch 20515: train loss: 0.47492724657058716\n",
      "Epoch 20516: train loss: 0.47492715716362\n",
      "Epoch 20517: train loss: 0.4749270975589752\n",
      "Epoch 20518: train loss: 0.47492703795433044\n",
      "Epoch 20519: train loss: 0.47492697834968567\n",
      "Epoch 20520: train loss: 0.4749269485473633\n",
      "Epoch 20521: train loss: 0.4749268889427185\n",
      "Epoch 20522: train loss: 0.47492682933807373\n",
      "Epoch 20523: train loss: 0.47492676973342896\n",
      "Epoch 20524: train loss: 0.47492673993110657\n",
      "Epoch 20525: train loss: 0.4749266803264618\n",
      "Epoch 20526: train loss: 0.474926620721817\n",
      "Epoch 20527: train loss: 0.47492653131484985\n",
      "Epoch 20528: train loss: 0.4749264717102051\n",
      "Epoch 20529: train loss: 0.4749264121055603\n",
      "Epoch 20530: train loss: 0.4749263525009155\n",
      "Epoch 20531: train loss: 0.47492632269859314\n",
      "Epoch 20532: train loss: 0.47492626309394836\n",
      "Epoch 20533: train loss: 0.47492626309394836\n",
      "Epoch 20534: train loss: 0.4749261438846588\n",
      "Epoch 20535: train loss: 0.4749261140823364\n",
      "Epoch 20536: train loss: 0.47492605447769165\n",
      "Epoch 20537: train loss: 0.4749259948730469\n",
      "Epoch 20538: train loss: 0.4749259352684021\n",
      "Epoch 20539: train loss: 0.47492584586143494\n",
      "Epoch 20540: train loss: 0.4749258756637573\n",
      "Epoch 20541: train loss: 0.4749257266521454\n",
      "Epoch 20542: train loss: 0.4749257266521454\n",
      "Epoch 20543: train loss: 0.4749256670475006\n",
      "Epoch 20544: train loss: 0.47492557764053345\n",
      "Epoch 20545: train loss: 0.47492551803588867\n",
      "Epoch 20546: train loss: 0.4749254584312439\n",
      "Epoch 20547: train loss: 0.4749254286289215\n",
      "Epoch 20548: train loss: 0.47492536902427673\n",
      "Epoch 20549: train loss: 0.47492530941963196\n",
      "Epoch 20550: train loss: 0.47492530941963196\n",
      "Epoch 20551: train loss: 0.4749252200126648\n",
      "Epoch 20552: train loss: 0.47492516040802\n",
      "Epoch 20553: train loss: 0.47492510080337524\n",
      "Epoch 20554: train loss: 0.47492510080337524\n",
      "Epoch 20555: train loss: 0.4749250113964081\n",
      "Epoch 20556: train loss: 0.4749249517917633\n",
      "Epoch 20557: train loss: 0.47492489218711853\n",
      "Epoch 20558: train loss: 0.47492483258247375\n",
      "Epoch 20559: train loss: 0.4749247431755066\n",
      "Epoch 20560: train loss: 0.4749246835708618\n",
      "Epoch 20561: train loss: 0.4749246835708618\n",
      "Epoch 20562: train loss: 0.47492459416389465\n",
      "Epoch 20563: train loss: 0.47492459416389465\n",
      "Epoch 20564: train loss: 0.4749244749546051\n",
      "Epoch 20565: train loss: 0.4749244749546051\n",
      "Epoch 20566: train loss: 0.47492438554763794\n",
      "Epoch 20567: train loss: 0.47492432594299316\n",
      "Epoch 20568: train loss: 0.4749242663383484\n",
      "Epoch 20569: train loss: 0.4749241769313812\n",
      "Epoch 20570: train loss: 0.4749241769313812\n",
      "Epoch 20571: train loss: 0.47492411732673645\n",
      "Epoch 20572: train loss: 0.4749239981174469\n",
      "Epoch 20573: train loss: 0.4749239683151245\n",
      "Epoch 20574: train loss: 0.4749239683151245\n",
      "Epoch 20575: train loss: 0.47492384910583496\n",
      "Epoch 20576: train loss: 0.4749237895011902\n",
      "Epoch 20577: train loss: 0.4749237596988678\n",
      "Epoch 20578: train loss: 0.474923700094223\n",
      "Epoch 20579: train loss: 0.47492364048957825\n",
      "Epoch 20580: train loss: 0.47492364048957825\n",
      "Epoch 20581: train loss: 0.4749235510826111\n",
      "Epoch 20582: train loss: 0.4749235510826111\n",
      "Epoch 20583: train loss: 0.47492343187332153\n",
      "Epoch 20584: train loss: 0.47492334246635437\n",
      "Epoch 20585: train loss: 0.47492334246635437\n",
      "Epoch 20586: train loss: 0.4749232828617096\n",
      "Epoch 20587: train loss: 0.4749232232570648\n",
      "Epoch 20588: train loss: 0.47492316365242004\n",
      "Epoch 20589: train loss: 0.47492313385009766\n",
      "Epoch 20590: train loss: 0.4749230146408081\n",
      "Epoch 20591: train loss: 0.4749230146408081\n",
      "Epoch 20592: train loss: 0.47492295503616333\n",
      "Epoch 20593: train loss: 0.47492292523384094\n",
      "Epoch 20594: train loss: 0.4749228060245514\n",
      "Epoch 20595: train loss: 0.4749227464199066\n",
      "Epoch 20596: train loss: 0.47492271661758423\n",
      "Epoch 20597: train loss: 0.4749225974082947\n",
      "Epoch 20598: train loss: 0.4749225974082947\n",
      "Epoch 20599: train loss: 0.4749225378036499\n",
      "Epoch 20600: train loss: 0.4749225080013275\n",
      "Epoch 20601: train loss: 0.47492244839668274\n",
      "Epoch 20602: train loss: 0.47492238879203796\n",
      "Epoch 20603: train loss: 0.4749222993850708\n",
      "Epoch 20604: train loss: 0.4749222993850708\n",
      "Epoch 20605: train loss: 0.47492218017578125\n",
      "Epoch 20606: train loss: 0.47492218017578125\n",
      "Epoch 20607: train loss: 0.4749221205711365\n",
      "Epoch 20608: train loss: 0.4749220609664917\n",
      "Epoch 20609: train loss: 0.47492197155952454\n",
      "Epoch 20610: train loss: 0.47492197155952454\n",
      "Epoch 20611: train loss: 0.4749218225479126\n",
      "Epoch 20612: train loss: 0.474921852350235\n",
      "Epoch 20613: train loss: 0.4749217629432678\n",
      "Epoch 20614: train loss: 0.47492170333862305\n",
      "Epoch 20615: train loss: 0.47492164373397827\n",
      "Epoch 20616: train loss: 0.4749216139316559\n",
      "Epoch 20617: train loss: 0.4749215543270111\n",
      "Epoch 20618: train loss: 0.47492149472236633\n",
      "Epoch 20619: train loss: 0.47492149472236633\n",
      "Epoch 20620: train loss: 0.47492140531539917\n",
      "Epoch 20621: train loss: 0.4749213457107544\n",
      "Epoch 20622: train loss: 0.47492122650146484\n",
      "Epoch 20623: train loss: 0.47492122650146484\n",
      "Epoch 20624: train loss: 0.4749211370944977\n",
      "Epoch 20625: train loss: 0.4749210774898529\n",
      "Epoch 20626: train loss: 0.4749210774898529\n",
      "Epoch 20627: train loss: 0.47492092847824097\n",
      "Epoch 20628: train loss: 0.47492092847824097\n",
      "Epoch 20629: train loss: 0.4749208092689514\n",
      "Epoch 20630: train loss: 0.4749208092689514\n",
      "Epoch 20631: train loss: 0.47492077946662903\n",
      "Epoch 20632: train loss: 0.47492071986198425\n",
      "Epoch 20633: train loss: 0.4749206602573395\n",
      "Epoch 20634: train loss: 0.4749206006526947\n",
      "Epoch 20635: train loss: 0.47492051124572754\n",
      "Epoch 20636: train loss: 0.47492051124572754\n",
      "Epoch 20637: train loss: 0.47492045164108276\n",
      "Epoch 20638: train loss: 0.474920392036438\n",
      "Epoch 20639: train loss: 0.4749203026294708\n",
      "Epoch 20640: train loss: 0.4749203026294708\n",
      "Epoch 20641: train loss: 0.4749201834201813\n",
      "Epoch 20642: train loss: 0.4749201536178589\n",
      "Epoch 20643: train loss: 0.4749200940132141\n",
      "Epoch 20644: train loss: 0.47492003440856934\n",
      "Epoch 20645: train loss: 0.47491997480392456\n",
      "Epoch 20646: train loss: 0.4749199450016022\n",
      "Epoch 20647: train loss: 0.4749198853969574\n",
      "Epoch 20648: train loss: 0.4749198257923126\n",
      "Epoch 20649: train loss: 0.4749198257923126\n",
      "Epoch 20650: train loss: 0.4749196767807007\n",
      "Epoch 20651: train loss: 0.4749196767807007\n",
      "Epoch 20652: train loss: 0.47491955757141113\n",
      "Epoch 20653: train loss: 0.47491955757141113\n",
      "Epoch 20654: train loss: 0.47491946816444397\n",
      "Epoch 20655: train loss: 0.47491946816444397\n",
      "Epoch 20656: train loss: 0.4749194085597992\n",
      "Epoch 20657: train loss: 0.4749193489551544\n",
      "Epoch 20658: train loss: 0.47491925954818726\n",
      "Epoch 20659: train loss: 0.4749191999435425\n",
      "Epoch 20660: train loss: 0.4749191403388977\n",
      "Epoch 20661: train loss: 0.4749191105365753\n",
      "Epoch 20662: train loss: 0.47491899132728577\n",
      "Epoch 20663: train loss: 0.474918931722641\n",
      "Epoch 20664: train loss: 0.474918931722641\n",
      "Epoch 20665: train loss: 0.4749189019203186\n",
      "Epoch 20666: train loss: 0.47491884231567383\n",
      "Epoch 20667: train loss: 0.47491878271102905\n",
      "Epoch 20668: train loss: 0.4749187231063843\n",
      "Epoch 20669: train loss: 0.4749186933040619\n",
      "Epoch 20670: train loss: 0.4749186336994171\n",
      "Epoch 20671: train loss: 0.47491857409477234\n",
      "Epoch 20672: train loss: 0.47491851449012756\n",
      "Epoch 20673: train loss: 0.4749184846878052\n",
      "Epoch 20674: train loss: 0.4749184250831604\n",
      "Epoch 20675: train loss: 0.47491830587387085\n",
      "Epoch 20676: train loss: 0.4749182462692261\n",
      "Epoch 20677: train loss: 0.4749182164669037\n",
      "Epoch 20678: train loss: 0.4749181568622589\n",
      "Epoch 20679: train loss: 0.4749181568622589\n",
      "Epoch 20680: train loss: 0.47491803765296936\n",
      "Epoch 20681: train loss: 0.474918007850647\n",
      "Epoch 20682: train loss: 0.4749178886413574\n",
      "Epoch 20683: train loss: 0.4749178886413574\n",
      "Epoch 20684: train loss: 0.47491782903671265\n",
      "Epoch 20685: train loss: 0.47491779923439026\n",
      "Epoch 20686: train loss: 0.4749177396297455\n",
      "Epoch 20687: train loss: 0.47491762042045593\n",
      "Epoch 20688: train loss: 0.47491762042045593\n",
      "Epoch 20689: train loss: 0.47491753101348877\n",
      "Epoch 20690: train loss: 0.47491753101348877\n",
      "Epoch 20691: train loss: 0.4749174118041992\n",
      "Epoch 20692: train loss: 0.47491738200187683\n",
      "Epoch 20693: train loss: 0.4749172627925873\n",
      "Epoch 20694: train loss: 0.4749172627925873\n",
      "Epoch 20695: train loss: 0.4749172031879425\n",
      "Epoch 20696: train loss: 0.47491711378097534\n",
      "Epoch 20697: train loss: 0.47491705417633057\n",
      "Epoch 20698: train loss: 0.47491705417633057\n",
      "Epoch 20699: train loss: 0.4749169945716858\n",
      "Epoch 20700: train loss: 0.4749169647693634\n",
      "Epoch 20701: train loss: 0.4749169647693634\n",
      "Epoch 20702: train loss: 0.47491684556007385\n",
      "Epoch 20703: train loss: 0.4749167561531067\n",
      "Epoch 20704: train loss: 0.4749166965484619\n",
      "Epoch 20705: train loss: 0.47491663694381714\n",
      "Epoch 20706: train loss: 0.47491657733917236\n",
      "Epoch 20707: train loss: 0.47491654753685\n",
      "Epoch 20708: train loss: 0.47491654753685\n",
      "Epoch 20709: train loss: 0.4749164283275604\n",
      "Epoch 20710: train loss: 0.47491636872291565\n",
      "Epoch 20711: train loss: 0.47491633892059326\n",
      "Epoch 20712: train loss: 0.4749162793159485\n",
      "Epoch 20713: train loss: 0.4749162197113037\n",
      "Epoch 20714: train loss: 0.47491613030433655\n",
      "Epoch 20715: train loss: 0.47491613030433655\n",
      "Epoch 20716: train loss: 0.4749160706996918\n",
      "Epoch 20717: train loss: 0.474916011095047\n",
      "Epoch 20718: train loss: 0.4749159514904022\n",
      "Epoch 20719: train loss: 0.47491586208343506\n",
      "Epoch 20720: train loss: 0.4749158024787903\n",
      "Epoch 20721: train loss: 0.4749158024787903\n",
      "Epoch 20722: train loss: 0.4749157130718231\n",
      "Epoch 20723: train loss: 0.47491565346717834\n",
      "Epoch 20724: train loss: 0.47491565346717834\n",
      "Epoch 20725: train loss: 0.4749155342578888\n",
      "Epoch 20726: train loss: 0.4749155342578888\n",
      "Epoch 20727: train loss: 0.47491544485092163\n",
      "Epoch 20728: train loss: 0.47491538524627686\n",
      "Epoch 20729: train loss: 0.4749153256416321\n",
      "Epoch 20730: train loss: 0.4749152362346649\n",
      "Epoch 20731: train loss: 0.4749152362346649\n",
      "Epoch 20732: train loss: 0.4749152362346649\n",
      "Epoch 20733: train loss: 0.47491511702537537\n",
      "Epoch 20734: train loss: 0.474915087223053\n",
      "Epoch 20735: train loss: 0.4749149680137634\n",
      "Epoch 20736: train loss: 0.47491490840911865\n",
      "Epoch 20737: train loss: 0.47491487860679626\n",
      "Epoch 20738: train loss: 0.47491487860679626\n",
      "Epoch 20739: train loss: 0.4749148190021515\n",
      "Epoch 20740: train loss: 0.47491469979286194\n",
      "Epoch 20741: train loss: 0.47491466999053955\n",
      "Epoch 20742: train loss: 0.4749146103858948\n",
      "Epoch 20743: train loss: 0.47491455078125\n",
      "Epoch 20744: train loss: 0.47491455078125\n",
      "Epoch 20745: train loss: 0.47491443157196045\n",
      "Epoch 20746: train loss: 0.47491440176963806\n",
      "Epoch 20747: train loss: 0.4749142825603485\n",
      "Epoch 20748: train loss: 0.4749142825603485\n",
      "Epoch 20749: train loss: 0.47491422295570374\n",
      "Epoch 20750: train loss: 0.4749141335487366\n",
      "Epoch 20751: train loss: 0.4749140739440918\n",
      "Epoch 20752: train loss: 0.474914014339447\n",
      "Epoch 20753: train loss: 0.474914014339447\n",
      "Epoch 20754: train loss: 0.47491392493247986\n",
      "Epoch 20755: train loss: 0.47491392493247986\n",
      "Epoch 20756: train loss: 0.4749138653278351\n",
      "Epoch 20757: train loss: 0.4749137759208679\n",
      "Epoch 20758: train loss: 0.47491371631622314\n",
      "Epoch 20759: train loss: 0.47491365671157837\n",
      "Epoch 20760: train loss: 0.4749135971069336\n",
      "Epoch 20761: train loss: 0.4749135971069336\n",
      "Epoch 20762: train loss: 0.47491350769996643\n",
      "Epoch 20763: train loss: 0.47491344809532166\n",
      "Epoch 20764: train loss: 0.4749133884906769\n",
      "Epoch 20765: train loss: 0.4749133586883545\n",
      "Epoch 20766: train loss: 0.4749132990837097\n",
      "Epoch 20767: train loss: 0.47491317987442017\n",
      "Epoch 20768: train loss: 0.47491317987442017\n",
      "Epoch 20769: train loss: 0.474913090467453\n",
      "Epoch 20770: train loss: 0.474913090467453\n",
      "Epoch 20771: train loss: 0.47491297125816345\n",
      "Epoch 20772: train loss: 0.47491297125816345\n",
      "Epoch 20773: train loss: 0.47491294145584106\n",
      "Epoch 20774: train loss: 0.4749128222465515\n",
      "Epoch 20775: train loss: 0.47491276264190674\n",
      "Epoch 20776: train loss: 0.47491273283958435\n",
      "Epoch 20777: train loss: 0.4749126732349396\n",
      "Epoch 20778: train loss: 0.4749126136302948\n",
      "Epoch 20779: train loss: 0.47491255402565\n",
      "Epoch 20780: train loss: 0.47491252422332764\n",
      "Epoch 20781: train loss: 0.47491246461868286\n",
      "Epoch 20782: train loss: 0.4749124050140381\n",
      "Epoch 20783: train loss: 0.4749123454093933\n",
      "Epoch 20784: train loss: 0.47491225600242615\n",
      "Epoch 20785: train loss: 0.47491219639778137\n",
      "Epoch 20786: train loss: 0.4749121367931366\n",
      "Epoch 20787: train loss: 0.4749121069908142\n",
      "Epoch 20788: train loss: 0.4749121069908142\n",
      "Epoch 20789: train loss: 0.47491198778152466\n",
      "Epoch 20790: train loss: 0.4749119281768799\n",
      "Epoch 20791: train loss: 0.4749118983745575\n",
      "Epoch 20792: train loss: 0.4749118387699127\n",
      "Epoch 20793: train loss: 0.47491177916526794\n",
      "Epoch 20794: train loss: 0.47491171956062317\n",
      "Epoch 20795: train loss: 0.474911630153656\n",
      "Epoch 20796: train loss: 0.474911630153656\n",
      "Epoch 20797: train loss: 0.47491157054901123\n",
      "Epoch 20798: train loss: 0.47491151094436646\n",
      "Epoch 20799: train loss: 0.4749114215373993\n",
      "Epoch 20800: train loss: 0.4749114215373993\n",
      "Epoch 20801: train loss: 0.47491130232810974\n",
      "Epoch 20802: train loss: 0.47491127252578735\n",
      "Epoch 20803: train loss: 0.4749112129211426\n",
      "Epoch 20804: train loss: 0.4749111533164978\n",
      "Epoch 20805: train loss: 0.474911093711853\n",
      "Epoch 20806: train loss: 0.47491106390953064\n",
      "Epoch 20807: train loss: 0.47491100430488586\n",
      "Epoch 20808: train loss: 0.47491100430488586\n",
      "Epoch 20809: train loss: 0.4749108850955963\n",
      "Epoch 20810: train loss: 0.4749108552932739\n",
      "Epoch 20811: train loss: 0.4749108552932739\n",
      "Epoch 20812: train loss: 0.4749107360839844\n",
      "Epoch 20813: train loss: 0.4749106764793396\n",
      "Epoch 20814: train loss: 0.4749106168746948\n",
      "Epoch 20815: train loss: 0.47491058707237244\n",
      "Epoch 20816: train loss: 0.4749104678630829\n",
      "Epoch 20817: train loss: 0.4749104082584381\n",
      "Epoch 20818: train loss: 0.4749104082584381\n",
      "Epoch 20819: train loss: 0.4749103784561157\n",
      "Epoch 20820: train loss: 0.47491025924682617\n",
      "Epoch 20821: train loss: 0.4749101996421814\n",
      "Epoch 20822: train loss: 0.474910169839859\n",
      "Epoch 20823: train loss: 0.47491011023521423\n",
      "Epoch 20824: train loss: 0.47491005063056946\n",
      "Epoch 20825: train loss: 0.4749099910259247\n",
      "Epoch 20826: train loss: 0.4749099612236023\n",
      "Epoch 20827: train loss: 0.4749099612236023\n",
      "Epoch 20828: train loss: 0.47490984201431274\n",
      "Epoch 20829: train loss: 0.4749097526073456\n",
      "Epoch 20830: train loss: 0.4749097526073456\n",
      "Epoch 20831: train loss: 0.47490963339805603\n",
      "Epoch 20832: train loss: 0.47490963339805603\n",
      "Epoch 20833: train loss: 0.47490957379341125\n",
      "Epoch 20834: train loss: 0.47490954399108887\n",
      "Epoch 20835: train loss: 0.4749094843864441\n",
      "Epoch 20836: train loss: 0.47490936517715454\n",
      "Epoch 20837: train loss: 0.47490936517715454\n",
      "Epoch 20838: train loss: 0.4749092757701874\n",
      "Epoch 20839: train loss: 0.4749092757701874\n",
      "Epoch 20840: train loss: 0.4749092161655426\n",
      "Epoch 20841: train loss: 0.47490912675857544\n",
      "Epoch 20842: train loss: 0.47490906715393066\n",
      "Epoch 20843: train loss: 0.47490906715393066\n",
      "Epoch 20844: train loss: 0.4749089479446411\n",
      "Epoch 20845: train loss: 0.4749089181423187\n",
      "Epoch 20846: train loss: 0.4749087989330292\n",
      "Epoch 20847: train loss: 0.4749087989330292\n",
      "Epoch 20848: train loss: 0.4749087393283844\n",
      "Epoch 20849: train loss: 0.474908709526062\n",
      "Epoch 20850: train loss: 0.47490864992141724\n",
      "Epoch 20851: train loss: 0.4749085009098053\n",
      "Epoch 20852: train loss: 0.4749085009098053\n",
      "Epoch 20853: train loss: 0.4749085009098053\n",
      "Epoch 20854: train loss: 0.47490838170051575\n",
      "Epoch 20855: train loss: 0.47490832209587097\n",
      "Epoch 20856: train loss: 0.47490832209587097\n",
      "Epoch 20857: train loss: 0.4749082326889038\n",
      "Epoch 20858: train loss: 0.47490817308425903\n",
      "Epoch 20859: train loss: 0.47490811347961426\n",
      "Epoch 20860: train loss: 0.47490808367729187\n",
      "Epoch 20861: train loss: 0.4749080240726471\n",
      "Epoch 20862: train loss: 0.4749079644680023\n",
      "Epoch 20863: train loss: 0.47490790486335754\n",
      "Epoch 20864: train loss: 0.47490787506103516\n",
      "Epoch 20865: train loss: 0.47490787506103516\n",
      "Epoch 20866: train loss: 0.4749078154563904\n",
      "Epoch 20867: train loss: 0.47490769624710083\n",
      "Epoch 20868: train loss: 0.47490760684013367\n",
      "Epoch 20869: train loss: 0.47490760684013367\n",
      "Epoch 20870: train loss: 0.4749075472354889\n",
      "Epoch 20871: train loss: 0.47490745782852173\n",
      "Epoch 20872: train loss: 0.47490745782852173\n",
      "Epoch 20873: train loss: 0.4749073386192322\n",
      "Epoch 20874: train loss: 0.4749073386192322\n",
      "Epoch 20875: train loss: 0.47490718960762024\n",
      "Epoch 20876: train loss: 0.47490718960762024\n",
      "Epoch 20877: train loss: 0.47490713000297546\n",
      "Epoch 20878: train loss: 0.4749070703983307\n",
      "Epoch 20879: train loss: 0.4749070405960083\n",
      "Epoch 20880: train loss: 0.4749069809913635\n",
      "Epoch 20881: train loss: 0.47490692138671875\n",
      "Epoch 20882: train loss: 0.474906861782074\n",
      "Epoch 20883: train loss: 0.4749068021774292\n",
      "Epoch 20884: train loss: 0.47490671277046204\n",
      "Epoch 20885: train loss: 0.47490665316581726\n",
      "Epoch 20886: train loss: 0.4749065935611725\n",
      "Epoch 20887: train loss: 0.4749065637588501\n",
      "Epoch 20888: train loss: 0.4749065637588501\n",
      "Epoch 20889: train loss: 0.47490644454956055\n",
      "Epoch 20890: train loss: 0.47490638494491577\n",
      "Epoch 20891: train loss: 0.4749063551425934\n",
      "Epoch 20892: train loss: 0.4749062955379486\n",
      "Epoch 20893: train loss: 0.47490623593330383\n",
      "Epoch 20894: train loss: 0.47490617632865906\n",
      "Epoch 20895: train loss: 0.4749060869216919\n",
      "Epoch 20896: train loss: 0.4749060869216919\n",
      "Epoch 20897: train loss: 0.47490596771240234\n",
      "Epoch 20898: train loss: 0.47490593791007996\n",
      "Epoch 20899: train loss: 0.47490593791007996\n",
      "Epoch 20900: train loss: 0.4749058783054352\n",
      "Epoch 20901: train loss: 0.4749058187007904\n",
      "Epoch 20902: train loss: 0.47490575909614563\n",
      "Epoch 20903: train loss: 0.47490566968917847\n",
      "Epoch 20904: train loss: 0.4749056100845337\n",
      "Epoch 20905: train loss: 0.4749055504798889\n",
      "Epoch 20906: train loss: 0.4749055504798889\n",
      "Epoch 20907: train loss: 0.47490546107292175\n",
      "Epoch 20908: train loss: 0.474905401468277\n",
      "Epoch 20909: train loss: 0.4749053418636322\n",
      "Epoch 20910: train loss: 0.4749053120613098\n",
      "Epoch 20911: train loss: 0.47490525245666504\n",
      "Epoch 20912: train loss: 0.47490519285202026\n",
      "Epoch 20913: train loss: 0.4749051332473755\n",
      "Epoch 20914: train loss: 0.4749051034450531\n",
      "Epoch 20915: train loss: 0.4749050438404083\n",
      "Epoch 20916: train loss: 0.4749049246311188\n",
      "Epoch 20917: train loss: 0.4749049246311188\n",
      "Epoch 20918: train loss: 0.4749048948287964\n",
      "Epoch 20919: train loss: 0.4749048352241516\n",
      "Epoch 20920: train loss: 0.47490471601486206\n",
      "Epoch 20921: train loss: 0.4749046862125397\n",
      "Epoch 20922: train loss: 0.4749046266078949\n",
      "Epoch 20923: train loss: 0.4749045670032501\n",
      "Epoch 20924: train loss: 0.47490450739860535\n",
      "Epoch 20925: train loss: 0.47490447759628296\n",
      "Epoch 20926: train loss: 0.47490447759628296\n",
      "Epoch 20927: train loss: 0.4749043583869934\n",
      "Epoch 20928: train loss: 0.47490429878234863\n",
      "Epoch 20929: train loss: 0.47490426898002625\n",
      "Epoch 20930: train loss: 0.47490420937538147\n",
      "Epoch 20931: train loss: 0.47490420937538147\n",
      "Epoch 20932: train loss: 0.4749040901660919\n",
      "Epoch 20933: train loss: 0.47490406036376953\n",
      "Epoch 20934: train loss: 0.47490394115448\n",
      "Epoch 20935: train loss: 0.47490394115448\n",
      "Epoch 20936: train loss: 0.4749038815498352\n",
      "Epoch 20937: train loss: 0.47490379214286804\n",
      "Epoch 20938: train loss: 0.47490379214286804\n",
      "Epoch 20939: train loss: 0.4749036729335785\n",
      "Epoch 20940: train loss: 0.4749036431312561\n",
      "Epoch 20941: train loss: 0.47490358352661133\n",
      "Epoch 20942: train loss: 0.47490352392196655\n",
      "Epoch 20943: train loss: 0.4749034643173218\n",
      "Epoch 20944: train loss: 0.4749034345149994\n",
      "Epoch 20945: train loss: 0.4749033749103546\n",
      "Epoch 20946: train loss: 0.4749033749103546\n",
      "Epoch 20947: train loss: 0.47490325570106506\n",
      "Epoch 20948: train loss: 0.4749032258987427\n",
      "Epoch 20949: train loss: 0.4749031066894531\n",
      "Epoch 20950: train loss: 0.4749031066894531\n",
      "Epoch 20951: train loss: 0.47490304708480835\n",
      "Epoch 20952: train loss: 0.4749029874801636\n",
      "Epoch 20953: train loss: 0.4749028980731964\n",
      "Epoch 20954: train loss: 0.47490283846855164\n",
      "Epoch 20955: train loss: 0.47490277886390686\n",
      "Epoch 20956: train loss: 0.47490277886390686\n",
      "Epoch 20957: train loss: 0.4749026894569397\n",
      "Epoch 20958: train loss: 0.4749026298522949\n",
      "Epoch 20959: train loss: 0.47490257024765015\n",
      "Epoch 20960: train loss: 0.47490254044532776\n",
      "Epoch 20961: train loss: 0.474902480840683\n",
      "Epoch 20962: train loss: 0.4749024212360382\n",
      "Epoch 20963: train loss: 0.47490236163139343\n",
      "Epoch 20964: train loss: 0.47490227222442627\n",
      "Epoch 20965: train loss: 0.47490227222442627\n",
      "Epoch 20966: train loss: 0.4749021530151367\n",
      "Epoch 20967: train loss: 0.4749021530151367\n",
      "Epoch 20968: train loss: 0.47490212321281433\n",
      "Epoch 20969: train loss: 0.47490206360816956\n",
      "Epoch 20970: train loss: 0.47490194439888\n",
      "Epoch 20971: train loss: 0.47490194439888\n",
      "Epoch 20972: train loss: 0.4749019145965576\n",
      "Epoch 20973: train loss: 0.47490179538726807\n",
      "Epoch 20974: train loss: 0.47490179538726807\n",
      "Epoch 20975: train loss: 0.4749017357826233\n",
      "Epoch 20976: train loss: 0.47490164637565613\n",
      "Epoch 20977: train loss: 0.47490158677101135\n",
      "Epoch 20978: train loss: 0.4749015271663666\n",
      "Epoch 20979: train loss: 0.4749014973640442\n",
      "Epoch 20980: train loss: 0.47490137815475464\n",
      "Epoch 20981: train loss: 0.47490137815475464\n",
      "Epoch 20982: train loss: 0.47490131855010986\n",
      "Epoch 20983: train loss: 0.4749012291431427\n",
      "Epoch 20984: train loss: 0.4749012291431427\n",
      "Epoch 20985: train loss: 0.4749011695384979\n",
      "Epoch 20986: train loss: 0.47490110993385315\n",
      "Epoch 20987: train loss: 0.474901020526886\n",
      "Epoch 20988: train loss: 0.4749009609222412\n",
      "Epoch 20989: train loss: 0.4749009609222412\n",
      "Epoch 20990: train loss: 0.47490090131759644\n",
      "Epoch 20991: train loss: 0.47490087151527405\n",
      "Epoch 20992: train loss: 0.4749007523059845\n",
      "Epoch 20993: train loss: 0.4749006927013397\n",
      "Epoch 20994: train loss: 0.47490066289901733\n",
      "Epoch 20995: train loss: 0.47490066289901733\n",
      "Epoch 20996: train loss: 0.4749005436897278\n",
      "Epoch 20997: train loss: 0.474900484085083\n",
      "Epoch 20998: train loss: 0.4749004542827606\n",
      "Epoch 20999: train loss: 0.47490039467811584\n",
      "Epoch 21000: train loss: 0.47490033507347107\n",
      "Epoch 21001: train loss: 0.4749002754688263\n",
      "Epoch 21002: train loss: 0.4749002456665039\n",
      "Epoch 21003: train loss: 0.47490018606185913\n",
      "Epoch 21004: train loss: 0.47490012645721436\n",
      "Epoch 21005: train loss: 0.4749000370502472\n",
      "Epoch 21006: train loss: 0.4749000370502472\n",
      "Epoch 21007: train loss: 0.47489991784095764\n",
      "Epoch 21008: train loss: 0.47489985823631287\n",
      "Epoch 21009: train loss: 0.47489985823631287\n",
      "Epoch 21010: train loss: 0.4748998284339905\n",
      "Epoch 21011: train loss: 0.4748997092247009\n",
      "Epoch 21012: train loss: 0.4748997092247009\n",
      "Epoch 21013: train loss: 0.47489961981773376\n",
      "Epoch 21014: train loss: 0.4748995006084442\n",
      "Epoch 21015: train loss: 0.4748995006084442\n",
      "Epoch 21016: train loss: 0.4748995006084442\n",
      "Epoch 21017: train loss: 0.47489941120147705\n",
      "Epoch 21018: train loss: 0.4748992919921875\n",
      "Epoch 21019: train loss: 0.4748992919921875\n",
      "Epoch 21020: train loss: 0.47489917278289795\n",
      "Epoch 21021: train loss: 0.47489917278289795\n",
      "Epoch 21022: train loss: 0.47489914298057556\n",
      "Epoch 21023: train loss: 0.4748990833759308\n",
      "Epoch 21024: train loss: 0.47489896416664124\n",
      "Epoch 21025: train loss: 0.47489893436431885\n",
      "Epoch 21026: train loss: 0.4748988747596741\n",
      "Epoch 21027: train loss: 0.4748987555503845\n",
      "Epoch 21028: train loss: 0.47489872574806213\n",
      "Epoch 21029: train loss: 0.47489872574806213\n",
      "Epoch 21030: train loss: 0.47489866614341736\n",
      "Epoch 21031: train loss: 0.4748986065387726\n",
      "Epoch 21032: train loss: 0.4748986065387726\n",
      "Epoch 21033: train loss: 0.4748985171318054\n",
      "Epoch 21034: train loss: 0.47489845752716064\n",
      "Epoch 21035: train loss: 0.47489839792251587\n",
      "Epoch 21036: train loss: 0.4748983383178711\n",
      "Epoch 21037: train loss: 0.4748983085155487\n",
      "Epoch 21038: train loss: 0.47489818930625916\n",
      "Epoch 21039: train loss: 0.47489818930625916\n",
      "Epoch 21040: train loss: 0.4748981297016144\n",
      "Epoch 21041: train loss: 0.4748980402946472\n",
      "Epoch 21042: train loss: 0.4748980402946472\n",
      "Epoch 21043: train loss: 0.47489798069000244\n",
      "Epoch 21044: train loss: 0.4748978912830353\n",
      "Epoch 21045: train loss: 0.4748978316783905\n",
      "Epoch 21046: train loss: 0.4748977720737457\n",
      "Epoch 21047: train loss: 0.47489771246910095\n",
      "Epoch 21048: train loss: 0.47489768266677856\n",
      "Epoch 21049: train loss: 0.4748976230621338\n",
      "Epoch 21050: train loss: 0.47489750385284424\n",
      "Epoch 21051: train loss: 0.47489750385284424\n",
      "Epoch 21052: train loss: 0.4748974144458771\n",
      "Epoch 21053: train loss: 0.4748974144458771\n",
      "Epoch 21054: train loss: 0.4748973548412323\n",
      "Epoch 21055: train loss: 0.47489726543426514\n",
      "Epoch 21056: train loss: 0.47489726543426514\n",
      "Epoch 21057: train loss: 0.47489720582962036\n",
      "Epoch 21058: train loss: 0.4748970866203308\n",
      "Epoch 21059: train loss: 0.4748970568180084\n",
      "Epoch 21060: train loss: 0.47489699721336365\n",
      "Epoch 21061: train loss: 0.47489693760871887\n",
      "Epoch 21062: train loss: 0.4748968780040741\n",
      "Epoch 21063: train loss: 0.4748968482017517\n",
      "Epoch 21064: train loss: 0.4748968482017517\n",
      "Epoch 21065: train loss: 0.47489678859710693\n",
      "Epoch 21066: train loss: 0.4748966693878174\n",
      "Epoch 21067: train loss: 0.474896639585495\n",
      "Epoch 21068: train loss: 0.4748965799808502\n",
      "Epoch 21069: train loss: 0.47489652037620544\n",
      "Epoch 21070: train loss: 0.47489646077156067\n",
      "Epoch 21071: train loss: 0.4748964309692383\n",
      "Epoch 21072: train loss: 0.4748963713645935\n",
      "Epoch 21073: train loss: 0.47489625215530396\n",
      "Epoch 21074: train loss: 0.47489625215530396\n",
      "Epoch 21075: train loss: 0.4748961627483368\n",
      "Epoch 21076: train loss: 0.4748961627483368\n",
      "Epoch 21077: train loss: 0.47489604353904724\n",
      "Epoch 21078: train loss: 0.47489604353904724\n",
      "Epoch 21079: train loss: 0.47489601373672485\n",
      "Epoch 21080: train loss: 0.4748958945274353\n",
      "Epoch 21081: train loss: 0.4748958349227905\n",
      "Epoch 21082: train loss: 0.47489580512046814\n",
      "Epoch 21083: train loss: 0.47489574551582336\n",
      "Epoch 21084: train loss: 0.47489574551582336\n",
      "Epoch 21085: train loss: 0.4748956263065338\n",
      "Epoch 21086: train loss: 0.4748955965042114\n",
      "Epoch 21087: train loss: 0.47489553689956665\n",
      "Epoch 21088: train loss: 0.4748954176902771\n",
      "Epoch 21089: train loss: 0.4748954176902771\n",
      "Epoch 21090: train loss: 0.4748953580856323\n",
      "Epoch 21091: train loss: 0.47489532828330994\n",
      "Epoch 21092: train loss: 0.47489526867866516\n",
      "Epoch 21093: train loss: 0.4748951494693756\n",
      "Epoch 21094: train loss: 0.4748951196670532\n",
      "Epoch 21095: train loss: 0.4748951196670532\n",
      "Epoch 21096: train loss: 0.47489500045776367\n",
      "Epoch 21097: train loss: 0.4748949408531189\n",
      "Epoch 21098: train loss: 0.4748949408531189\n",
      "Epoch 21099: train loss: 0.4748949110507965\n",
      "Epoch 21100: train loss: 0.47489479184150696\n",
      "Epoch 21101: train loss: 0.4748947322368622\n",
      "Epoch 21102: train loss: 0.474894642829895\n",
      "Epoch 21103: train loss: 0.474894642829895\n",
      "Epoch 21104: train loss: 0.47489452362060547\n",
      "Epoch 21105: train loss: 0.47489452362060547\n",
      "Epoch 21106: train loss: 0.4748944938182831\n",
      "Epoch 21107: train loss: 0.4748944342136383\n",
      "Epoch 21108: train loss: 0.47489431500434875\n",
      "Epoch 21109: train loss: 0.47489431500434875\n",
      "Epoch 21110: train loss: 0.47489428520202637\n",
      "Epoch 21111: train loss: 0.4748942255973816\n",
      "Epoch 21112: train loss: 0.47489410638809204\n",
      "Epoch 21113: train loss: 0.47489410638809204\n",
      "Epoch 21114: train loss: 0.47489407658576965\n",
      "Epoch 21115: train loss: 0.4748940169811249\n",
      "Epoch 21116: train loss: 0.4748938977718353\n",
      "Epoch 21117: train loss: 0.47489386796951294\n",
      "Epoch 21118: train loss: 0.47489380836486816\n",
      "Epoch 21119: train loss: 0.4748937487602234\n",
      "Epoch 21120: train loss: 0.4748936593532562\n",
      "Epoch 21121: train loss: 0.4748936593532562\n",
      "Epoch 21122: train loss: 0.47489359974861145\n",
      "Epoch 21123: train loss: 0.47489359974861145\n",
      "Epoch 21124: train loss: 0.4748934805393219\n",
      "Epoch 21125: train loss: 0.4748934507369995\n",
      "Epoch 21126: train loss: 0.47489339113235474\n",
      "Epoch 21127: train loss: 0.47489333152770996\n",
      "Epoch 21128: train loss: 0.4748932421207428\n",
      "Epoch 21129: train loss: 0.474893182516098\n",
      "Epoch 21130: train loss: 0.474893182516098\n",
      "Epoch 21131: train loss: 0.47489306330680847\n",
      "Epoch 21132: train loss: 0.4748930335044861\n",
      "Epoch 21133: train loss: 0.4748930335044861\n",
      "Epoch 21134: train loss: 0.47489291429519653\n",
      "Epoch 21135: train loss: 0.47489285469055176\n",
      "Epoch 21136: train loss: 0.47489282488822937\n",
      "Epoch 21137: train loss: 0.4748927652835846\n",
      "Epoch 21138: train loss: 0.4748927652835846\n",
      "Epoch 21139: train loss: 0.47489264607429504\n",
      "Epoch 21140: train loss: 0.47489261627197266\n",
      "Epoch 21141: train loss: 0.47489261627197266\n",
      "Epoch 21142: train loss: 0.4748924970626831\n",
      "Epoch 21143: train loss: 0.47489240765571594\n",
      "Epoch 21144: train loss: 0.47489240765571594\n",
      "Epoch 21145: train loss: 0.47489234805107117\n",
      "Epoch 21146: train loss: 0.4748922288417816\n",
      "Epoch 21147: train loss: 0.4748922288417816\n",
      "Epoch 21148: train loss: 0.47489213943481445\n",
      "Epoch 21149: train loss: 0.4748920798301697\n",
      "Epoch 21150: train loss: 0.4748920798301697\n",
      "Epoch 21151: train loss: 0.4748920202255249\n",
      "Epoch 21152: train loss: 0.47489193081855774\n",
      "Epoch 21153: train loss: 0.47489193081855774\n",
      "Epoch 21154: train loss: 0.4748918116092682\n",
      "Epoch 21155: train loss: 0.4748917818069458\n",
      "Epoch 21156: train loss: 0.47489166259765625\n",
      "Epoch 21157: train loss: 0.47489166259765625\n",
      "Epoch 21158: train loss: 0.47489166259765625\n",
      "Epoch 21159: train loss: 0.4748916029930115\n",
      "Epoch 21160: train loss: 0.4748915135860443\n",
      "Epoch 21161: train loss: 0.47489145398139954\n",
      "Epoch 21162: train loss: 0.47489139437675476\n",
      "Epoch 21163: train loss: 0.47489133477211\n",
      "Epoch 21164: train loss: 0.4748912453651428\n",
      "Epoch 21165: train loss: 0.4748912453651428\n",
      "Epoch 21166: train loss: 0.47489112615585327\n",
      "Epoch 21167: train loss: 0.4748910963535309\n",
      "Epoch 21168: train loss: 0.4748910963535309\n",
      "Epoch 21169: train loss: 0.47489097714424133\n",
      "Epoch 21170: train loss: 0.47489091753959656\n",
      "Epoch 21171: train loss: 0.47489088773727417\n",
      "Epoch 21172: train loss: 0.4748908281326294\n",
      "Epoch 21173: train loss: 0.4748907685279846\n",
      "Epoch 21174: train loss: 0.47489070892333984\n",
      "Epoch 21175: train loss: 0.47489070892333984\n",
      "Epoch 21176: train loss: 0.47489067912101746\n",
      "Epoch 21177: train loss: 0.4748905599117279\n",
      "Epoch 21178: train loss: 0.47489050030708313\n",
      "Epoch 21179: train loss: 0.47489041090011597\n",
      "Epoch 21180: train loss: 0.47489041090011597\n",
      "Epoch 21181: train loss: 0.4748903512954712\n",
      "Epoch 21182: train loss: 0.4748902916908264\n",
      "Epoch 21183: train loss: 0.47489026188850403\n",
      "Epoch 21184: train loss: 0.4748901426792145\n",
      "Epoch 21185: train loss: 0.4748901426792145\n",
      "Epoch 21186: train loss: 0.4748900532722473\n",
      "Epoch 21187: train loss: 0.47488999366760254\n",
      "Epoch 21188: train loss: 0.47488993406295776\n",
      "Epoch 21189: train loss: 0.474889874458313\n",
      "Epoch 21190: train loss: 0.474889874458313\n",
      "Epoch 21191: train loss: 0.4748897850513458\n",
      "Epoch 21192: train loss: 0.47488972544670105\n",
      "Epoch 21193: train loss: 0.4748896360397339\n",
      "Epoch 21194: train loss: 0.4748895764350891\n",
      "Epoch 21195: train loss: 0.4748895764350891\n",
      "Epoch 21196: train loss: 0.47488951683044434\n",
      "Epoch 21197: train loss: 0.47488945722579956\n",
      "Epoch 21198: train loss: 0.4748893678188324\n",
      "Epoch 21199: train loss: 0.4748893678188324\n",
      "Epoch 21200: train loss: 0.4748893082141876\n",
      "Epoch 21201: train loss: 0.47488924860954285\n",
      "Epoch 21202: train loss: 0.47488921880722046\n",
      "Epoch 21203: train loss: 0.4748891592025757\n",
      "Epoch 21204: train loss: 0.4748890995979309\n",
      "Epoch 21205: train loss: 0.47488901019096375\n",
      "Epoch 21206: train loss: 0.47488895058631897\n",
      "Epoch 21207: train loss: 0.4748888909816742\n",
      "Epoch 21208: train loss: 0.4748888909816742\n",
      "Epoch 21209: train loss: 0.47488880157470703\n",
      "Epoch 21210: train loss: 0.47488874197006226\n",
      "Epoch 21211: train loss: 0.4748886823654175\n",
      "Epoch 21212: train loss: 0.4748886227607727\n",
      "Epoch 21213: train loss: 0.4748885929584503\n",
      "Epoch 21214: train loss: 0.47488847374916077\n",
      "Epoch 21215: train loss: 0.474888414144516\n",
      "Epoch 21216: train loss: 0.474888414144516\n",
      "Epoch 21217: train loss: 0.4748883843421936\n",
      "Epoch 21218: train loss: 0.47488832473754883\n",
      "Epoch 21219: train loss: 0.4748882055282593\n",
      "Epoch 21220: train loss: 0.4748882055282593\n",
      "Epoch 21221: train loss: 0.4748881757259369\n",
      "Epoch 21222: train loss: 0.4748881161212921\n",
      "Epoch 21223: train loss: 0.47488799691200256\n",
      "Epoch 21224: train loss: 0.47488799691200256\n",
      "Epoch 21225: train loss: 0.4748879671096802\n",
      "Epoch 21226: train loss: 0.4748879075050354\n",
      "Epoch 21227: train loss: 0.4748878479003906\n",
      "Epoch 21228: train loss: 0.4748877286911011\n",
      "Epoch 21229: train loss: 0.4748876988887787\n",
      "Epoch 21230: train loss: 0.4748876392841339\n",
      "Epoch 21231: train loss: 0.47488757967948914\n",
      "Epoch 21232: train loss: 0.47488752007484436\n",
      "Epoch 21233: train loss: 0.474887490272522\n",
      "Epoch 21234: train loss: 0.4748874306678772\n",
      "Epoch 21235: train loss: 0.4748873710632324\n",
      "Epoch 21236: train loss: 0.47488728165626526\n",
      "Epoch 21237: train loss: 0.47488728165626526\n",
      "Epoch 21238: train loss: 0.4748872220516205\n",
      "Epoch 21239: train loss: 0.4748871624469757\n",
      "Epoch 21240: train loss: 0.47488710284233093\n",
      "Epoch 21241: train loss: 0.47488701343536377\n",
      "Epoch 21242: train loss: 0.474886953830719\n",
      "Epoch 21243: train loss: 0.4748868942260742\n",
      "Epoch 21244: train loss: 0.47488686442375183\n",
      "Epoch 21245: train loss: 0.47488680481910706\n",
      "Epoch 21246: train loss: 0.4748867452144623\n",
      "Epoch 21247: train loss: 0.4748866856098175\n",
      "Epoch 21248: train loss: 0.4748866856098175\n",
      "Epoch 21249: train loss: 0.47488659620285034\n",
      "Epoch 21250: train loss: 0.47488653659820557\n",
      "Epoch 21251: train loss: 0.4748864769935608\n",
      "Epoch 21252: train loss: 0.4748864471912384\n",
      "Epoch 21253: train loss: 0.47488638758659363\n",
      "Epoch 21254: train loss: 0.47488638758659363\n",
      "Epoch 21255: train loss: 0.4748862385749817\n",
      "Epoch 21256: train loss: 0.4748862385749817\n",
      "Epoch 21257: train loss: 0.4748861789703369\n",
      "Epoch 21258: train loss: 0.47488611936569214\n",
      "Epoch 21259: train loss: 0.474886029958725\n",
      "Epoch 21260: train loss: 0.4748859703540802\n",
      "Epoch 21261: train loss: 0.4748859107494354\n",
      "Epoch 21262: train loss: 0.47488585114479065\n",
      "Epoch 21263: train loss: 0.47488582134246826\n",
      "Epoch 21264: train loss: 0.4748857617378235\n",
      "Epoch 21265: train loss: 0.4748857617378235\n",
      "Epoch 21266: train loss: 0.47488564252853394\n",
      "Epoch 21267: train loss: 0.47488561272621155\n",
      "Epoch 21268: train loss: 0.4748855531215668\n",
      "Epoch 21269: train loss: 0.474885493516922\n",
      "Epoch 21270: train loss: 0.4748854339122772\n",
      "Epoch 21271: train loss: 0.47488540410995483\n",
      "Epoch 21272: train loss: 0.47488534450531006\n",
      "Epoch 21273: train loss: 0.4748852252960205\n",
      "Epoch 21274: train loss: 0.4748851954936981\n",
      "Epoch 21275: train loss: 0.4748851954936981\n",
      "Epoch 21276: train loss: 0.47488513588905334\n",
      "Epoch 21277: train loss: 0.4748850166797638\n",
      "Epoch 21278: train loss: 0.4748849868774414\n",
      "Epoch 21279: train loss: 0.4748849868774414\n",
      "Epoch 21280: train loss: 0.47488486766815186\n",
      "Epoch 21281: train loss: 0.4748848080635071\n",
      "Epoch 21282: train loss: 0.4748847782611847\n",
      "Epoch 21283: train loss: 0.4748847782611847\n",
      "Epoch 21284: train loss: 0.4748847186565399\n",
      "Epoch 21285: train loss: 0.47488465905189514\n",
      "Epoch 21286: train loss: 0.474884569644928\n",
      "Epoch 21287: train loss: 0.4748845100402832\n",
      "Epoch 21288: train loss: 0.4748844504356384\n",
      "Epoch 21289: train loss: 0.47488436102867126\n",
      "Epoch 21290: train loss: 0.47488436102867126\n",
      "Epoch 21291: train loss: 0.4748843014240265\n",
      "Epoch 21292: train loss: 0.4748842418193817\n",
      "Epoch 21293: train loss: 0.47488418221473694\n",
      "Epoch 21294: train loss: 0.47488415241241455\n",
      "Epoch 21295: train loss: 0.4748840928077698\n",
      "Epoch 21296: train loss: 0.474884033203125\n",
      "Epoch 21297: train loss: 0.4748839735984802\n",
      "Epoch 21298: train loss: 0.47488391399383545\n",
      "Epoch 21299: train loss: 0.4748838245868683\n",
      "Epoch 21300: train loss: 0.4748838245868683\n",
      "Epoch 21301: train loss: 0.47488370537757874\n",
      "Epoch 21302: train loss: 0.47488367557525635\n",
      "Epoch 21303: train loss: 0.4748836159706116\n",
      "Epoch 21304: train loss: 0.4748835563659668\n",
      "Epoch 21305: train loss: 0.474883496761322\n",
      "Epoch 21306: train loss: 0.474883496761322\n",
      "Epoch 21307: train loss: 0.47488346695899963\n",
      "Epoch 21308: train loss: 0.4748833477497101\n",
      "Epoch 21309: train loss: 0.4748832881450653\n",
      "Epoch 21310: train loss: 0.4748832583427429\n",
      "Epoch 21311: train loss: 0.4748832583427429\n",
      "Epoch 21312: train loss: 0.47488313913345337\n",
      "Epoch 21313: train loss: 0.4748830497264862\n",
      "Epoch 21314: train loss: 0.4748830497264862\n",
      "Epoch 21315: train loss: 0.47488293051719666\n",
      "Epoch 21316: train loss: 0.47488293051719666\n",
      "Epoch 21317: train loss: 0.4748828411102295\n",
      "Epoch 21318: train loss: 0.4748827815055847\n",
      "Epoch 21319: train loss: 0.47488272190093994\n",
      "Epoch 21320: train loss: 0.47488266229629517\n",
      "Epoch 21321: train loss: 0.4748826324939728\n",
      "Epoch 21322: train loss: 0.474882572889328\n",
      "Epoch 21323: train loss: 0.474882572889328\n",
      "Epoch 21324: train loss: 0.47488245368003845\n",
      "Epoch 21325: train loss: 0.47488242387771606\n",
      "Epoch 21326: train loss: 0.4748823642730713\n",
      "Epoch 21327: train loss: 0.4748823046684265\n",
      "Epoch 21328: train loss: 0.47488224506378174\n",
      "Epoch 21329: train loss: 0.47488221526145935\n",
      "Epoch 21330: train loss: 0.47488221526145935\n",
      "Epoch 21331: train loss: 0.4748820960521698\n",
      "Epoch 21332: train loss: 0.474882036447525\n",
      "Epoch 21333: train loss: 0.47488200664520264\n",
      "Epoch 21334: train loss: 0.47488194704055786\n",
      "Epoch 21335: train loss: 0.4748818874359131\n",
      "Epoch 21336: train loss: 0.4748818278312683\n",
      "Epoch 21337: train loss: 0.47488173842430115\n",
      "Epoch 21338: train loss: 0.47488167881965637\n",
      "Epoch 21339: train loss: 0.4748816192150116\n",
      "Epoch 21340: train loss: 0.4748816192150116\n",
      "Epoch 21341: train loss: 0.4748815894126892\n",
      "Epoch 21342: train loss: 0.47488147020339966\n",
      "Epoch 21343: train loss: 0.4748813807964325\n",
      "Epoch 21344: train loss: 0.4748813807964325\n",
      "Epoch 21345: train loss: 0.4748813211917877\n",
      "Epoch 21346: train loss: 0.47488126158714294\n",
      "Epoch 21347: train loss: 0.47488120198249817\n",
      "Epoch 21348: train loss: 0.4748811721801758\n",
      "Epoch 21349: train loss: 0.474881112575531\n",
      "Epoch 21350: train loss: 0.47488105297088623\n",
      "Epoch 21351: train loss: 0.47488096356391907\n",
      "Epoch 21352: train loss: 0.47488096356391907\n",
      "Epoch 21353: train loss: 0.4748808443546295\n",
      "Epoch 21354: train loss: 0.47488078474998474\n",
      "Epoch 21355: train loss: 0.47488078474998474\n",
      "Epoch 21356: train loss: 0.47488075494766235\n",
      "Epoch 21357: train loss: 0.4748806357383728\n",
      "Epoch 21358: train loss: 0.4748806357383728\n",
      "Epoch 21359: train loss: 0.474880576133728\n",
      "Epoch 21360: train loss: 0.47488054633140564\n",
      "Epoch 21361: train loss: 0.47488048672676086\n",
      "Epoch 21362: train loss: 0.4748803675174713\n",
      "Epoch 21363: train loss: 0.4748803377151489\n",
      "Epoch 21364: train loss: 0.4748803377151489\n",
      "Epoch 21365: train loss: 0.4748802185058594\n",
      "Epoch 21366: train loss: 0.4748801589012146\n",
      "Epoch 21367: train loss: 0.4748800992965698\n",
      "Epoch 21368: train loss: 0.47488006949424744\n",
      "Epoch 21369: train loss: 0.4748799502849579\n",
      "Epoch 21370: train loss: 0.4748799502849579\n",
      "Epoch 21371: train loss: 0.4748798906803131\n",
      "Epoch 21372: train loss: 0.4748798608779907\n",
      "Epoch 21373: train loss: 0.47487980127334595\n",
      "Epoch 21374: train loss: 0.4748796820640564\n",
      "Epoch 21375: train loss: 0.474879652261734\n",
      "Epoch 21376: train loss: 0.47487959265708923\n",
      "Epoch 21377: train loss: 0.47487953305244446\n",
      "Epoch 21378: train loss: 0.47487953305244446\n",
      "Epoch 21379: train loss: 0.4748794436454773\n",
      "Epoch 21380: train loss: 0.4748794436454773\n",
      "Epoch 21381: train loss: 0.47487932443618774\n",
      "Epoch 21382: train loss: 0.47487926483154297\n",
      "Epoch 21383: train loss: 0.47487926483154297\n",
      "Epoch 21384: train loss: 0.4748791754245758\n",
      "Epoch 21385: train loss: 0.47487905621528625\n",
      "Epoch 21386: train loss: 0.47487905621528625\n",
      "Epoch 21387: train loss: 0.47487902641296387\n",
      "Epoch 21388: train loss: 0.4748789668083191\n",
      "Epoch 21389: train loss: 0.4748789072036743\n",
      "Epoch 21390: train loss: 0.47487884759902954\n",
      "Epoch 21391: train loss: 0.4748787581920624\n",
      "Epoch 21392: train loss: 0.4748787581920624\n",
      "Epoch 21393: train loss: 0.4748786985874176\n",
      "Epoch 21394: train loss: 0.47487860918045044\n",
      "Epoch 21395: train loss: 0.47487854957580566\n",
      "Epoch 21396: train loss: 0.4748784899711609\n",
      "Epoch 21397: train loss: 0.4748784899711609\n",
      "Epoch 21398: train loss: 0.4748784303665161\n",
      "Epoch 21399: train loss: 0.4748784005641937\n",
      "Epoch 21400: train loss: 0.4748782813549042\n",
      "Epoch 21401: train loss: 0.4748782813549042\n",
      "Epoch 21402: train loss: 0.474878191947937\n",
      "Epoch 21403: train loss: 0.47487813234329224\n",
      "Epoch 21404: train loss: 0.47487807273864746\n",
      "Epoch 21405: train loss: 0.47487807273864746\n",
      "Epoch 21406: train loss: 0.4748779833316803\n",
      "Epoch 21407: train loss: 0.4748779237270355\n",
      "Epoch 21408: train loss: 0.47487786412239075\n",
      "Epoch 21409: train loss: 0.47487780451774597\n",
      "Epoch 21410: train loss: 0.4748777747154236\n",
      "Epoch 21411: train loss: 0.4748777151107788\n",
      "Epoch 21412: train loss: 0.47487759590148926\n",
      "Epoch 21413: train loss: 0.47487756609916687\n",
      "Epoch 21414: train loss: 0.47487756609916687\n",
      "Epoch 21415: train loss: 0.4748774468898773\n",
      "Epoch 21416: train loss: 0.4748774468898773\n",
      "Epoch 21417: train loss: 0.47487738728523254\n",
      "Epoch 21418: train loss: 0.4748772978782654\n",
      "Epoch 21419: train loss: 0.4748772382736206\n",
      "Epoch 21420: train loss: 0.47487717866897583\n",
      "Epoch 21421: train loss: 0.47487714886665344\n",
      "Epoch 21422: train loss: 0.47487714886665344\n",
      "Epoch 21423: train loss: 0.4748770296573639\n",
      "Epoch 21424: train loss: 0.4748769700527191\n",
      "Epoch 21425: train loss: 0.47487694025039673\n",
      "Epoch 21426: train loss: 0.47487688064575195\n",
      "Epoch 21427: train loss: 0.4748768210411072\n",
      "Epoch 21428: train loss: 0.4748767614364624\n",
      "Epoch 21429: train loss: 0.47487673163414\n",
      "Epoch 21430: train loss: 0.47487667202949524\n",
      "Epoch 21431: train loss: 0.47487661242485046\n",
      "Epoch 21432: train loss: 0.4748765528202057\n",
      "Epoch 21433: train loss: 0.4748764634132385\n",
      "Epoch 21434: train loss: 0.4748764634132385\n",
      "Epoch 21435: train loss: 0.474876344203949\n",
      "Epoch 21436: train loss: 0.4748762845993042\n",
      "Epoch 21437: train loss: 0.4748762845993042\n",
      "Epoch 21438: train loss: 0.4748762547969818\n",
      "Epoch 21439: train loss: 0.47487613558769226\n",
      "Epoch 21440: train loss: 0.4748760759830475\n",
      "Epoch 21441: train loss: 0.4748760759830475\n",
      "Epoch 21442: train loss: 0.4748759865760803\n",
      "Epoch 21443: train loss: 0.47487592697143555\n",
      "Epoch 21444: train loss: 0.47487592697143555\n",
      "Epoch 21445: train loss: 0.4748757779598236\n",
      "Epoch 21446: train loss: 0.4748757779598236\n",
      "Epoch 21447: train loss: 0.47487571835517883\n",
      "Epoch 21448: train loss: 0.47487571835517883\n",
      "Epoch 21449: train loss: 0.47487562894821167\n",
      "Epoch 21450: train loss: 0.47487562894821167\n",
      "Epoch 21451: train loss: 0.4748755097389221\n",
      "Epoch 21452: train loss: 0.47487545013427734\n",
      "Epoch 21453: train loss: 0.4748753607273102\n",
      "Epoch 21454: train loss: 0.4748753607273102\n",
      "Epoch 21455: train loss: 0.4748753011226654\n",
      "Epoch 21456: train loss: 0.47487524151802063\n",
      "Epoch 21457: train loss: 0.47487515211105347\n",
      "Epoch 21458: train loss: 0.4748750329017639\n",
      "Epoch 21459: train loss: 0.4748750329017639\n",
      "Epoch 21460: train loss: 0.4748750329017639\n",
      "Epoch 21461: train loss: 0.47487494349479675\n",
      "Epoch 21462: train loss: 0.474874883890152\n",
      "Epoch 21463: train loss: 0.4748748242855072\n",
      "Epoch 21464: train loss: 0.4748748242855072\n",
      "Epoch 21465: train loss: 0.47487473487854004\n",
      "Epoch 21466: train loss: 0.47487467527389526\n",
      "Epoch 21467: train loss: 0.4748746156692505\n",
      "Epoch 21468: train loss: 0.4748745858669281\n",
      "Epoch 21469: train loss: 0.4748745262622833\n",
      "Epoch 21470: train loss: 0.47487446665763855\n",
      "Epoch 21471: train loss: 0.4748744070529938\n",
      "Epoch 21472: train loss: 0.4748743772506714\n",
      "Epoch 21473: train loss: 0.47487425804138184\n",
      "Epoch 21474: train loss: 0.47487425804138184\n",
      "Epoch 21475: train loss: 0.4748741686344147\n",
      "Epoch 21476: train loss: 0.4748741090297699\n",
      "Epoch 21477: train loss: 0.4748741090297699\n",
      "Epoch 21478: train loss: 0.47487398982048035\n",
      "Epoch 21479: train loss: 0.47487398982048035\n",
      "Epoch 21480: train loss: 0.4748739004135132\n",
      "Epoch 21481: train loss: 0.4748739004135132\n",
      "Epoch 21482: train loss: 0.47487378120422363\n",
      "Epoch 21483: train loss: 0.47487375140190125\n",
      "Epoch 21484: train loss: 0.47487369179725647\n",
      "Epoch 21485: train loss: 0.47487369179725647\n",
      "Epoch 21486: train loss: 0.4748736321926117\n",
      "Epoch 21487: train loss: 0.47487354278564453\n",
      "Epoch 21488: train loss: 0.47487348318099976\n",
      "Epoch 21489: train loss: 0.4748733639717102\n",
      "Epoch 21490: train loss: 0.4748733639717102\n",
      "Epoch 21491: train loss: 0.47487327456474304\n",
      "Epoch 21492: train loss: 0.47487327456474304\n",
      "Epoch 21493: train loss: 0.47487327456474304\n",
      "Epoch 21494: train loss: 0.4748731553554535\n",
      "Epoch 21495: train loss: 0.4748731255531311\n",
      "Epoch 21496: train loss: 0.47487300634384155\n",
      "Epoch 21497: train loss: 0.4748729467391968\n",
      "Epoch 21498: train loss: 0.4748728573322296\n",
      "Epoch 21499: train loss: 0.4748729169368744\n",
      "Epoch 21500: train loss: 0.47487279772758484\n",
      "Epoch 21501: train loss: 0.47487273812294006\n",
      "Epoch 21502: train loss: 0.47487273812294006\n",
      "Epoch 21503: train loss: 0.4748726487159729\n",
      "Epoch 21504: train loss: 0.4748725891113281\n",
      "Epoch 21505: train loss: 0.47487252950668335\n",
      "Epoch 21506: train loss: 0.4748724699020386\n",
      "Epoch 21507: train loss: 0.4748724400997162\n",
      "Epoch 21508: train loss: 0.4748724400997162\n",
      "Epoch 21509: train loss: 0.4748723804950714\n",
      "Epoch 21510: train loss: 0.4748722314834595\n",
      "Epoch 21511: train loss: 0.4748722314834595\n",
      "Epoch 21512: train loss: 0.4748721718788147\n",
      "Epoch 21513: train loss: 0.47487205266952515\n",
      "Epoch 21514: train loss: 0.4748721122741699\n",
      "Epoch 21515: train loss: 0.474871963262558\n",
      "Epoch 21516: train loss: 0.4748719036579132\n",
      "Epoch 21517: train loss: 0.4748719036579132\n",
      "Epoch 21518: train loss: 0.47487181425094604\n",
      "Epoch 21519: train loss: 0.47487181425094604\n",
      "Epoch 21520: train loss: 0.47487175464630127\n",
      "Epoch 21521: train loss: 0.4748716354370117\n",
      "Epoch 21522: train loss: 0.47487160563468933\n",
      "Epoch 21523: train loss: 0.47487160563468933\n",
      "Epoch 21524: train loss: 0.474871426820755\n",
      "Epoch 21525: train loss: 0.474871426820755\n",
      "Epoch 21526: train loss: 0.4748713970184326\n",
      "Epoch 21527: train loss: 0.47487133741378784\n",
      "Epoch 21528: train loss: 0.47487127780914307\n",
      "Epoch 21529: train loss: 0.4748712182044983\n",
      "Epoch 21530: train loss: 0.4748711884021759\n",
      "Epoch 21531: train loss: 0.47487112879753113\n",
      "Epoch 21532: train loss: 0.4748710095882416\n",
      "Epoch 21533: train loss: 0.47487106919288635\n",
      "Epoch 21534: train loss: 0.4748709201812744\n",
      "Epoch 21535: train loss: 0.4748709201812744\n",
      "Epoch 21536: train loss: 0.47487086057662964\n",
      "Epoch 21537: train loss: 0.47487080097198486\n",
      "Epoch 21538: train loss: 0.4748707115650177\n",
      "Epoch 21539: train loss: 0.4748706519603729\n",
      "Epoch 21540: train loss: 0.4748706519603729\n",
      "Epoch 21541: train loss: 0.47487059235572815\n",
      "Epoch 21542: train loss: 0.47487056255340576\n",
      "Epoch 21543: train loss: 0.4748704433441162\n",
      "Epoch 21544: train loss: 0.47487038373947144\n",
      "Epoch 21545: train loss: 0.47487035393714905\n",
      "Epoch 21546: train loss: 0.4748702943325043\n",
      "Epoch 21547: train loss: 0.4748702347278595\n",
      "Epoch 21548: train loss: 0.4748701751232147\n",
      "Epoch 21549: train loss: 0.4748701751232147\n",
      "Epoch 21550: train loss: 0.47487008571624756\n",
      "Epoch 21551: train loss: 0.4748700261116028\n",
      "Epoch 21552: train loss: 0.4748699367046356\n",
      "Epoch 21553: train loss: 0.4748699367046356\n",
      "Epoch 21554: train loss: 0.47486981749534607\n",
      "Epoch 21555: train loss: 0.47486981749534607\n",
      "Epoch 21556: train loss: 0.4748697280883789\n",
      "Epoch 21557: train loss: 0.4748697280883789\n",
      "Epoch 21558: train loss: 0.47486966848373413\n",
      "Epoch 21559: train loss: 0.47486960887908936\n",
      "Epoch 21560: train loss: 0.4748695492744446\n",
      "Epoch 21561: train loss: 0.4748694598674774\n",
      "Epoch 21562: train loss: 0.4748694598674774\n",
      "Epoch 21563: train loss: 0.47486934065818787\n",
      "Epoch 21564: train loss: 0.47486934065818787\n",
      "Epoch 21565: train loss: 0.4748692512512207\n",
      "Epoch 21566: train loss: 0.4748691916465759\n",
      "Epoch 21567: train loss: 0.47486913204193115\n",
      "Epoch 21568: train loss: 0.47486910223960876\n",
      "Epoch 21569: train loss: 0.474869042634964\n",
      "Epoch 21570: train loss: 0.4748689830303192\n",
      "Epoch 21571: train loss: 0.47486892342567444\n",
      "Epoch 21572: train loss: 0.47486889362335205\n",
      "Epoch 21573: train loss: 0.4748688340187073\n",
      "Epoch 21574: train loss: 0.4748687148094177\n",
      "Epoch 21575: train loss: 0.4748687148094177\n",
      "Epoch 21576: train loss: 0.47486865520477295\n",
      "Epoch 21577: train loss: 0.47486862540245056\n",
      "Epoch 21578: train loss: 0.4748685657978058\n",
      "Epoch 21579: train loss: 0.47486844658851624\n",
      "Epoch 21580: train loss: 0.47486841678619385\n",
      "Epoch 21581: train loss: 0.47486841678619385\n",
      "Epoch 21582: train loss: 0.4748683571815491\n",
      "Epoch 21583: train loss: 0.4748682379722595\n",
      "Epoch 21584: train loss: 0.4748682379722595\n",
      "Epoch 21585: train loss: 0.47486814856529236\n",
      "Epoch 21586: train loss: 0.4748680889606476\n",
      "Epoch 21587: train loss: 0.4748680293560028\n",
      "Epoch 21588: train loss: 0.4748679995536804\n",
      "Epoch 21589: train loss: 0.47486793994903564\n",
      "Epoch 21590: train loss: 0.47486788034439087\n",
      "Epoch 21591: train loss: 0.47486788034439087\n",
      "Epoch 21592: train loss: 0.4748678207397461\n",
      "Epoch 21593: train loss: 0.47486773133277893\n",
      "Epoch 21594: train loss: 0.47486767172813416\n",
      "Epoch 21595: train loss: 0.47486767172813416\n",
      "Epoch 21596: train loss: 0.4748675227165222\n",
      "Epoch 21597: train loss: 0.4748675227165222\n",
      "Epoch 21598: train loss: 0.47486746311187744\n",
      "Epoch 21599: train loss: 0.47486740350723267\n",
      "Epoch 21600: train loss: 0.4748673141002655\n",
      "Epoch 21601: train loss: 0.4748673141002655\n",
      "Epoch 21602: train loss: 0.4748672544956207\n",
      "Epoch 21603: train loss: 0.47486716508865356\n",
      "Epoch 21604: train loss: 0.47486716508865356\n",
      "Epoch 21605: train loss: 0.474867045879364\n",
      "Epoch 21606: train loss: 0.47486698627471924\n",
      "Epoch 21607: train loss: 0.47486695647239685\n",
      "Epoch 21608: train loss: 0.47486695647239685\n",
      "Epoch 21609: train loss: 0.4748668372631073\n",
      "Epoch 21610: train loss: 0.4748667776584625\n",
      "Epoch 21611: train loss: 0.4748667776584625\n",
      "Epoch 21612: train loss: 0.47486668825149536\n",
      "Epoch 21613: train loss: 0.47486668825149536\n",
      "Epoch 21614: train loss: 0.4748665690422058\n",
      "Epoch 21615: train loss: 0.4748665392398834\n",
      "Epoch 21616: train loss: 0.47486647963523865\n",
      "Epoch 21617: train loss: 0.4748663604259491\n",
      "Epoch 21618: train loss: 0.4748663604259491\n",
      "Epoch 21619: train loss: 0.4748663306236267\n",
      "Epoch 21620: train loss: 0.47486627101898193\n",
      "Epoch 21621: train loss: 0.4748661518096924\n",
      "Epoch 21622: train loss: 0.4748661518096924\n",
      "Epoch 21623: train loss: 0.47486612200737\n",
      "Epoch 21624: train loss: 0.47486600279808044\n",
      "Epoch 21625: train loss: 0.47486600279808044\n",
      "Epoch 21626: train loss: 0.4748659133911133\n",
      "Epoch 21627: train loss: 0.4748658537864685\n",
      "Epoch 21628: train loss: 0.4748658537864685\n",
      "Epoch 21629: train loss: 0.47486573457717896\n",
      "Epoch 21630: train loss: 0.47486573457717896\n",
      "Epoch 21631: train loss: 0.4748656451702118\n",
      "Epoch 21632: train loss: 0.47486552596092224\n",
      "Epoch 21633: train loss: 0.47486552596092224\n",
      "Epoch 21634: train loss: 0.4748654365539551\n",
      "Epoch 21635: train loss: 0.47486549615859985\n",
      "Epoch 21636: train loss: 0.4748653173446655\n",
      "Epoch 21637: train loss: 0.4748653173446655\n",
      "Epoch 21638: train loss: 0.47486528754234314\n",
      "Epoch 21639: train loss: 0.47486522793769836\n",
      "Epoch 21640: train loss: 0.4748651087284088\n",
      "Epoch 21641: train loss: 0.4748651087284088\n",
      "Epoch 21642: train loss: 0.47486501932144165\n",
      "Epoch 21643: train loss: 0.4748649597167969\n",
      "Epoch 21644: train loss: 0.4748649597167969\n",
      "Epoch 21645: train loss: 0.4748648405075073\n",
      "Epoch 21646: train loss: 0.4748648405075073\n",
      "Epoch 21647: train loss: 0.47486475110054016\n",
      "Epoch 21648: train loss: 0.47486475110054016\n",
      "Epoch 21649: train loss: 0.4748646318912506\n",
      "Epoch 21650: train loss: 0.4748646020889282\n",
      "Epoch 21651: train loss: 0.47486454248428345\n",
      "Epoch 21652: train loss: 0.47486448287963867\n",
      "Epoch 21653: train loss: 0.4748644232749939\n",
      "Epoch 21654: train loss: 0.4748643934726715\n",
      "Epoch 21655: train loss: 0.4748643934726715\n",
      "Epoch 21656: train loss: 0.47486427426338196\n",
      "Epoch 21657: train loss: 0.4748641848564148\n",
      "Epoch 21658: train loss: 0.4748641848564148\n",
      "Epoch 21659: train loss: 0.47486412525177\n",
      "Epoch 21660: train loss: 0.47486406564712524\n",
      "Epoch 21661: train loss: 0.4748639762401581\n",
      "Epoch 21662: train loss: 0.4748639762401581\n",
      "Epoch 21663: train loss: 0.4748639166355133\n",
      "Epoch 21664: train loss: 0.47486385703086853\n",
      "Epoch 21665: train loss: 0.47486379742622375\n",
      "Epoch 21666: train loss: 0.47486376762390137\n",
      "Epoch 21667: train loss: 0.4748636484146118\n",
      "Epoch 21668: train loss: 0.47486358880996704\n",
      "Epoch 21669: train loss: 0.47486355900764465\n",
      "Epoch 21670: train loss: 0.4748634994029999\n",
      "Epoch 21671: train loss: 0.4748634994029999\n",
      "Epoch 21672: train loss: 0.4748633801937103\n",
      "Epoch 21673: train loss: 0.47486335039138794\n",
      "Epoch 21674: train loss: 0.47486329078674316\n",
      "Epoch 21675: train loss: 0.4748632311820984\n",
      "Epoch 21676: train loss: 0.4748631417751312\n",
      "Epoch 21677: train loss: 0.4748631417751312\n",
      "Epoch 21678: train loss: 0.47486308217048645\n",
      "Epoch 21679: train loss: 0.47486308217048645\n",
      "Epoch 21680: train loss: 0.4748629331588745\n",
      "Epoch 21681: train loss: 0.4748629331588745\n",
      "Epoch 21682: train loss: 0.47486281394958496\n",
      "Epoch 21683: train loss: 0.4748627543449402\n",
      "Epoch 21684: train loss: 0.4748627543449402\n",
      "Epoch 21685: train loss: 0.474862664937973\n",
      "Epoch 21686: train loss: 0.47486260533332825\n",
      "Epoch 21687: train loss: 0.47486254572868347\n",
      "Epoch 21688: train loss: 0.4748625159263611\n",
      "Epoch 21689: train loss: 0.4748625159263611\n",
      "Epoch 21690: train loss: 0.47486239671707153\n",
      "Epoch 21691: train loss: 0.47486239671707153\n",
      "Epoch 21692: train loss: 0.47486230731010437\n",
      "Epoch 21693: train loss: 0.47486230731010437\n",
      "Epoch 21694: train loss: 0.4748622477054596\n",
      "Epoch 21695: train loss: 0.47486209869384766\n",
      "Epoch 21696: train loss: 0.47486209869384766\n",
      "Epoch 21697: train loss: 0.4748620390892029\n",
      "Epoch 21698: train loss: 0.4748619794845581\n",
      "Epoch 21699: train loss: 0.47486191987991333\n",
      "Epoch 21700: train loss: 0.47486189007759094\n",
      "Epoch 21701: train loss: 0.47486183047294617\n",
      "Epoch 21702: train loss: 0.4748617708683014\n",
      "Epoch 21703: train loss: 0.47486168146133423\n",
      "Epoch 21704: train loss: 0.47486168146133423\n",
      "Epoch 21705: train loss: 0.4748615622520447\n",
      "Epoch 21706: train loss: 0.4748615026473999\n",
      "Epoch 21707: train loss: 0.4748615026473999\n",
      "Epoch 21708: train loss: 0.4748614728450775\n",
      "Epoch 21709: train loss: 0.47486135363578796\n",
      "Epoch 21710: train loss: 0.47486135363578796\n",
      "Epoch 21711: train loss: 0.4748612940311432\n",
      "Epoch 21712: train loss: 0.474861204624176\n",
      "Epoch 21713: train loss: 0.47486114501953125\n",
      "Epoch 21714: train loss: 0.4748610854148865\n",
      "Epoch 21715: train loss: 0.4748610258102417\n",
      "Epoch 21716: train loss: 0.4748610258102417\n",
      "Epoch 21717: train loss: 0.47486093640327454\n",
      "Epoch 21718: train loss: 0.47486087679862976\n",
      "Epoch 21719: train loss: 0.474860817193985\n",
      "Epoch 21720: train loss: 0.4748607277870178\n",
      "Epoch 21721: train loss: 0.4748607277870178\n",
      "Epoch 21722: train loss: 0.47486066818237305\n",
      "Epoch 21723: train loss: 0.47486066818237305\n",
      "Epoch 21724: train loss: 0.4748605787754059\n",
      "Epoch 21725: train loss: 0.47486045956611633\n",
      "Epoch 21726: train loss: 0.47486039996147156\n",
      "Epoch 21727: train loss: 0.47486039996147156\n",
      "Epoch 21728: train loss: 0.4748603105545044\n",
      "Epoch 21729: train loss: 0.4748603105545044\n",
      "Epoch 21730: train loss: 0.47486019134521484\n",
      "Epoch 21731: train loss: 0.47486016154289246\n",
      "Epoch 21732: train loss: 0.47486016154289246\n",
      "Epoch 21733: train loss: 0.4748601019382477\n",
      "Epoch 21734: train loss: 0.47485998272895813\n",
      "Epoch 21735: train loss: 0.47485995292663574\n",
      "Epoch 21736: train loss: 0.47485989332199097\n",
      "Epoch 21737: train loss: 0.47485989332199097\n",
      "Epoch 21738: train loss: 0.4748598337173462\n",
      "Epoch 21739: train loss: 0.47485974431037903\n",
      "Epoch 21740: train loss: 0.47485968470573425\n",
      "Epoch 21741: train loss: 0.4748596251010895\n",
      "Epoch 21742: train loss: 0.4748595654964447\n",
      "Epoch 21743: train loss: 0.4748595356941223\n",
      "Epoch 21744: train loss: 0.47485947608947754\n",
      "Epoch 21745: train loss: 0.47485941648483276\n",
      "Epoch 21746: train loss: 0.474859356880188\n",
      "Epoch 21747: train loss: 0.4748593270778656\n",
      "Epoch 21748: train loss: 0.47485920786857605\n",
      "Epoch 21749: train loss: 0.4748591482639313\n",
      "Epoch 21750: train loss: 0.4748591184616089\n",
      "Epoch 21751: train loss: 0.4748591184616089\n",
      "Epoch 21752: train loss: 0.4748590588569641\n",
      "Epoch 21753: train loss: 0.47485893964767456\n",
      "Epoch 21754: train loss: 0.47485893964767456\n",
      "Epoch 21755: train loss: 0.4748589098453522\n",
      "Epoch 21756: train loss: 0.4748587906360626\n",
      "Epoch 21757: train loss: 0.47485873103141785\n",
      "Epoch 21758: train loss: 0.47485870122909546\n",
      "Epoch 21759: train loss: 0.4748586416244507\n",
      "Epoch 21760: train loss: 0.4748585820198059\n",
      "Epoch 21761: train loss: 0.4748585820198059\n",
      "Epoch 21762: train loss: 0.47485852241516113\n",
      "Epoch 21763: train loss: 0.47485849261283875\n",
      "Epoch 21764: train loss: 0.4748583734035492\n",
      "Epoch 21765: train loss: 0.4748583137989044\n",
      "Epoch 21766: train loss: 0.47485828399658203\n",
      "Epoch 21767: train loss: 0.47485822439193726\n",
      "Epoch 21768: train loss: 0.4748581647872925\n",
      "Epoch 21769: train loss: 0.4748581051826477\n",
      "Epoch 21770: train loss: 0.4748580753803253\n",
      "Epoch 21771: train loss: 0.47485801577568054\n",
      "Epoch 21772: train loss: 0.47485795617103577\n",
      "Epoch 21773: train loss: 0.4748578667640686\n",
      "Epoch 21774: train loss: 0.4748578667640686\n",
      "Epoch 21775: train loss: 0.47485780715942383\n",
      "Epoch 21776: train loss: 0.47485774755477905\n",
      "Epoch 21777: train loss: 0.4748576581478119\n",
      "Epoch 21778: train loss: 0.4748575985431671\n",
      "Epoch 21779: train loss: 0.4748575985431671\n",
      "Epoch 21780: train loss: 0.47485753893852234\n",
      "Epoch 21781: train loss: 0.4748574495315552\n",
      "Epoch 21782: train loss: 0.4748573899269104\n",
      "Epoch 21783: train loss: 0.4748573303222656\n",
      "Epoch 21784: train loss: 0.47485727071762085\n",
      "Epoch 21785: train loss: 0.47485727071762085\n",
      "Epoch 21786: train loss: 0.4748571813106537\n",
      "Epoch 21787: train loss: 0.4748571217060089\n",
      "Epoch 21788: train loss: 0.47485706210136414\n",
      "Epoch 21789: train loss: 0.47485700249671936\n",
      "Epoch 21790: train loss: 0.474856972694397\n",
      "Epoch 21791: train loss: 0.474856972694397\n",
      "Epoch 21792: train loss: 0.47485679388046265\n",
      "Epoch 21793: train loss: 0.47485679388046265\n",
      "Epoch 21794: train loss: 0.47485676407814026\n",
      "Epoch 21795: train loss: 0.4748566448688507\n",
      "Epoch 21796: train loss: 0.47485658526420593\n",
      "Epoch 21797: train loss: 0.47485655546188354\n",
      "Epoch 21798: train loss: 0.47485649585723877\n",
      "Epoch 21799: train loss: 0.47485649585723877\n",
      "Epoch 21800: train loss: 0.474856436252594\n",
      "Epoch 21801: train loss: 0.47485634684562683\n",
      "Epoch 21802: train loss: 0.47485628724098206\n",
      "Epoch 21803: train loss: 0.47485628724098206\n",
      "Epoch 21804: train loss: 0.4748562276363373\n",
      "Epoch 21805: train loss: 0.4748561382293701\n",
      "Epoch 21806: train loss: 0.47485607862472534\n",
      "Epoch 21807: train loss: 0.47485601902008057\n",
      "Epoch 21808: train loss: 0.4748559594154358\n",
      "Epoch 21809: train loss: 0.4748559296131134\n",
      "Epoch 21810: train loss: 0.47485587000846863\n",
      "Epoch 21811: train loss: 0.47485581040382385\n",
      "Epoch 21812: train loss: 0.4748557209968567\n",
      "Epoch 21813: train loss: 0.4748557209968567\n",
      "Epoch 21814: train loss: 0.4748556613922119\n",
      "Epoch 21815: train loss: 0.47485560178756714\n",
      "Epoch 21816: train loss: 0.4748555123806\n",
      "Epoch 21817: train loss: 0.4748554527759552\n",
      "Epoch 21818: train loss: 0.4748553931713104\n",
      "Epoch 21819: train loss: 0.4748553931713104\n",
      "Epoch 21820: train loss: 0.47485533356666565\n",
      "Epoch 21821: train loss: 0.47485530376434326\n",
      "Epoch 21822: train loss: 0.4748552441596985\n",
      "Epoch 21823: train loss: 0.47485512495040894\n",
      "Epoch 21824: train loss: 0.47485512495040894\n",
      "Epoch 21825: train loss: 0.4748550355434418\n",
      "Epoch 21826: train loss: 0.4748550355434418\n",
      "Epoch 21827: train loss: 0.4748549163341522\n",
      "Epoch 21828: train loss: 0.4748549163341522\n",
      "Epoch 21829: train loss: 0.47485482692718506\n",
      "Epoch 21830: train loss: 0.4748547673225403\n",
      "Epoch 21831: train loss: 0.4748547077178955\n",
      "Epoch 21832: train loss: 0.4748546779155731\n",
      "Epoch 21833: train loss: 0.47485461831092834\n",
      "Epoch 21834: train loss: 0.47485455870628357\n",
      "Epoch 21835: train loss: 0.4748544692993164\n",
      "Epoch 21836: train loss: 0.4748544991016388\n",
      "Epoch 21837: train loss: 0.47485435009002686\n",
      "Epoch 21838: train loss: 0.47485435009002686\n",
      "Epoch 21839: train loss: 0.4748542606830597\n",
      "Epoch 21840: train loss: 0.4748542904853821\n",
      "Epoch 21841: train loss: 0.4748542010784149\n",
      "Epoch 21842: train loss: 0.47485414147377014\n",
      "Epoch 21843: train loss: 0.474854052066803\n",
      "Epoch 21844: train loss: 0.474854052066803\n",
      "Epoch 21845: train loss: 0.4748539924621582\n",
      "Epoch 21846: train loss: 0.47485387325286865\n",
      "Epoch 21847: train loss: 0.47485384345054626\n",
      "Epoch 21848: train loss: 0.4748537838459015\n",
      "Epoch 21849: train loss: 0.4748537242412567\n",
      "Epoch 21850: train loss: 0.4748537242412567\n",
      "Epoch 21851: train loss: 0.47485363483428955\n",
      "Epoch 21852: train loss: 0.4748535752296448\n",
      "Epoch 21853: train loss: 0.474853515625\n",
      "Epoch 21854: train loss: 0.4748534560203552\n",
      "Epoch 21855: train loss: 0.47485339641571045\n",
      "Epoch 21856: train loss: 0.47485336661338806\n",
      "Epoch 21857: train loss: 0.4748533070087433\n",
      "Epoch 21858: train loss: 0.4748532474040985\n",
      "Epoch 21859: train loss: 0.47485318779945374\n",
      "Epoch 21860: train loss: 0.4748530983924866\n",
      "Epoch 21861: train loss: 0.4748530983924866\n",
      "Epoch 21862: train loss: 0.4748530387878418\n",
      "Epoch 21863: train loss: 0.47485294938087463\n",
      "Epoch 21864: train loss: 0.47485294938087463\n",
      "Epoch 21865: train loss: 0.4748528301715851\n",
      "Epoch 21866: train loss: 0.4748528301715851\n",
      "Epoch 21867: train loss: 0.4748527705669403\n",
      "Epoch 21868: train loss: 0.47485268115997314\n",
      "Epoch 21869: train loss: 0.47485262155532837\n",
      "Epoch 21870: train loss: 0.4748525619506836\n",
      "Epoch 21871: train loss: 0.4748525321483612\n",
      "Epoch 21872: train loss: 0.47485247254371643\n",
      "Epoch 21873: train loss: 0.47485241293907166\n",
      "Epoch 21874: train loss: 0.4748523533344269\n",
      "Epoch 21875: train loss: 0.4748523533344269\n",
      "Epoch 21876: train loss: 0.4748522639274597\n",
      "Epoch 21877: train loss: 0.47485220432281494\n",
      "Epoch 21878: train loss: 0.47485214471817017\n",
      "Epoch 21879: train loss: 0.4748521149158478\n",
      "Epoch 21880: train loss: 0.474852055311203\n",
      "Epoch 21881: train loss: 0.47485193610191345\n",
      "Epoch 21882: train loss: 0.4748519957065582\n",
      "Epoch 21883: train loss: 0.4748518466949463\n",
      "Epoch 21884: train loss: 0.4748518466949463\n",
      "Epoch 21885: train loss: 0.47485172748565674\n",
      "Epoch 21886: train loss: 0.47485172748565674\n",
      "Epoch 21887: train loss: 0.4748516380786896\n",
      "Epoch 21888: train loss: 0.4748516380786896\n",
      "Epoch 21889: train loss: 0.4748515188694\n",
      "Epoch 21890: train loss: 0.47485148906707764\n",
      "Epoch 21891: train loss: 0.47485148906707764\n",
      "Epoch 21892: train loss: 0.47485142946243286\n",
      "Epoch 21893: train loss: 0.4748513102531433\n",
      "Epoch 21894: train loss: 0.4748513102531433\n",
      "Epoch 21895: train loss: 0.47485122084617615\n",
      "Epoch 21896: train loss: 0.47485122084617615\n",
      "Epoch 21897: train loss: 0.47485116124153137\n",
      "Epoch 21898: train loss: 0.4748510718345642\n",
      "Epoch 21899: train loss: 0.47485101222991943\n",
      "Epoch 21900: train loss: 0.47485095262527466\n",
      "Epoch 21901: train loss: 0.4748508930206299\n",
      "Epoch 21902: train loss: 0.4748508930206299\n",
      "Epoch 21903: train loss: 0.4748508036136627\n",
      "Epoch 21904: train loss: 0.47485068440437317\n",
      "Epoch 21905: train loss: 0.4748506546020508\n",
      "Epoch 21906: train loss: 0.4748506546020508\n",
      "Epoch 21907: train loss: 0.474850594997406\n",
      "Epoch 21908: train loss: 0.47485047578811646\n",
      "Epoch 21909: train loss: 0.47485047578811646\n",
      "Epoch 21910: train loss: 0.4748503863811493\n",
      "Epoch 21911: train loss: 0.4748503863811493\n",
      "Epoch 21912: train loss: 0.4748503267765045\n",
      "Epoch 21913: train loss: 0.47485026717185974\n",
      "Epoch 21914: train loss: 0.4748501777648926\n",
      "Epoch 21915: train loss: 0.4748501777648926\n",
      "Epoch 21916: train loss: 0.4748501181602478\n",
      "Epoch 21917: train loss: 0.47485002875328064\n",
      "Epoch 21918: train loss: 0.47485002875328064\n",
      "Epoch 21919: train loss: 0.47484996914863586\n",
      "Epoch 21920: train loss: 0.4748499095439911\n",
      "Epoch 21921: train loss: 0.4748498201370239\n",
      "Epoch 21922: train loss: 0.47484976053237915\n",
      "Epoch 21923: train loss: 0.4748497009277344\n",
      "Epoch 21924: train loss: 0.4748497009277344\n",
      "Epoch 21925: train loss: 0.4748495817184448\n",
      "Epoch 21926: train loss: 0.4748495817184448\n",
      "Epoch 21927: train loss: 0.47484949231147766\n",
      "Epoch 21928: train loss: 0.4748494327068329\n",
      "Epoch 21929: train loss: 0.4748493731021881\n",
      "Epoch 21930: train loss: 0.4748493432998657\n",
      "Epoch 21931: train loss: 0.47484928369522095\n",
      "Epoch 21932: train loss: 0.4748491644859314\n",
      "Epoch 21933: train loss: 0.474849134683609\n",
      "Epoch 21934: train loss: 0.474849134683609\n",
      "Epoch 21935: train loss: 0.47484907507896423\n",
      "Epoch 21936: train loss: 0.47484901547431946\n",
      "Epoch 21937: train loss: 0.4748489260673523\n",
      "Epoch 21938: train loss: 0.4748488664627075\n",
      "Epoch 21939: train loss: 0.47484880685806274\n",
      "Epoch 21940: train loss: 0.47484880685806274\n",
      "Epoch 21941: train loss: 0.4748487174510956\n",
      "Epoch 21942: train loss: 0.4748487174510956\n",
      "Epoch 21943: train loss: 0.47484859824180603\n",
      "Epoch 21944: train loss: 0.47484853863716125\n",
      "Epoch 21945: train loss: 0.47484850883483887\n",
      "Epoch 21946: train loss: 0.47484850883483887\n",
      "Epoch 21947: train loss: 0.4748483896255493\n",
      "Epoch 21948: train loss: 0.4748483896255493\n",
      "Epoch 21949: train loss: 0.47484830021858215\n",
      "Epoch 21950: train loss: 0.4748481810092926\n",
      "Epoch 21951: train loss: 0.4748481810092926\n",
      "Epoch 21952: train loss: 0.4748481214046478\n",
      "Epoch 21953: train loss: 0.47484809160232544\n",
      "Epoch 21954: train loss: 0.47484809160232544\n",
      "Epoch 21955: train loss: 0.4748479723930359\n",
      "Epoch 21956: train loss: 0.4748479127883911\n",
      "Epoch 21957: train loss: 0.47484782338142395\n",
      "Epoch 21958: train loss: 0.4748477637767792\n",
      "Epoch 21959: train loss: 0.4748477041721344\n",
      "Epoch 21960: train loss: 0.474847674369812\n",
      "Epoch 21961: train loss: 0.474847674369812\n",
      "Epoch 21962: train loss: 0.47484761476516724\n",
      "Epoch 21963: train loss: 0.4748474955558777\n",
      "Epoch 21964: train loss: 0.4748474657535553\n",
      "Epoch 21965: train loss: 0.47484734654426575\n",
      "Epoch 21966: train loss: 0.4748474061489105\n",
      "Epoch 21967: train loss: 0.47484728693962097\n",
      "Epoch 21968: train loss: 0.4748472571372986\n",
      "Epoch 21969: train loss: 0.4748472571372986\n",
      "Epoch 21970: train loss: 0.47484713792800903\n",
      "Epoch 21971: train loss: 0.47484713792800903\n",
      "Epoch 21972: train loss: 0.47484704852104187\n",
      "Epoch 21973: train loss: 0.4748469889163971\n",
      "Epoch 21974: train loss: 0.47484686970710754\n",
      "Epoch 21975: train loss: 0.47484686970710754\n",
      "Epoch 21976: train loss: 0.47484683990478516\n",
      "Epoch 21977: train loss: 0.4748467803001404\n",
      "Epoch 21978: train loss: 0.4748467206954956\n",
      "Epoch 21979: train loss: 0.47484663128852844\n",
      "Epoch 21980: train loss: 0.47484663128852844\n",
      "Epoch 21981: train loss: 0.4748465120792389\n",
      "Epoch 21982: train loss: 0.4748465120792389\n",
      "Epoch 21983: train loss: 0.4748464524745941\n",
      "Epoch 21984: train loss: 0.47484642267227173\n",
      "Epoch 21985: train loss: 0.47484636306762695\n",
      "Epoch 21986: train loss: 0.4748463034629822\n",
      "Epoch 21987: train loss: 0.4748462438583374\n",
      "Epoch 21988: train loss: 0.47484615445137024\n",
      "Epoch 21989: train loss: 0.47484609484672546\n",
      "Epoch 21990: train loss: 0.47484609484672546\n",
      "Epoch 21991: train loss: 0.4748460352420807\n",
      "Epoch 21992: train loss: 0.4748459458351135\n",
      "Epoch 21993: train loss: 0.47484588623046875\n",
      "Epoch 21994: train loss: 0.474845826625824\n",
      "Epoch 21995: train loss: 0.4748457670211792\n",
      "Epoch 21996: train loss: 0.4748457372188568\n",
      "Epoch 21997: train loss: 0.4748457372188568\n",
      "Epoch 21998: train loss: 0.47484561800956726\n",
      "Epoch 21999: train loss: 0.4748455584049225\n",
      "Epoch 22000: train loss: 0.4748455286026001\n",
      "Epoch 22001: train loss: 0.4748454689979553\n",
      "Epoch 22002: train loss: 0.47484540939331055\n",
      "Epoch 22003: train loss: 0.47484534978866577\n",
      "Epoch 22004: train loss: 0.4748452603816986\n",
      "Epoch 22005: train loss: 0.4748452603816986\n",
      "Epoch 22006: train loss: 0.47484520077705383\n",
      "Epoch 22007: train loss: 0.47484514117240906\n",
      "Epoch 22008: train loss: 0.4748450517654419\n",
      "Epoch 22009: train loss: 0.4748450517654419\n",
      "Epoch 22010: train loss: 0.4748449921607971\n",
      "Epoch 22011: train loss: 0.47484490275382996\n",
      "Epoch 22012: train loss: 0.4748448431491852\n",
      "Epoch 22013: train loss: 0.4748447835445404\n",
      "Epoch 22014: train loss: 0.47484472393989563\n",
      "Epoch 22015: train loss: 0.47484472393989563\n",
      "Epoch 22016: train loss: 0.47484463453292847\n",
      "Epoch 22017: train loss: 0.4748445749282837\n",
      "Epoch 22018: train loss: 0.4748445153236389\n",
      "Epoch 22019: train loss: 0.47484448552131653\n",
      "Epoch 22020: train loss: 0.47484442591667175\n",
      "Epoch 22021: train loss: 0.4748443067073822\n",
      "Epoch 22022: train loss: 0.4748443067073822\n",
      "Epoch 22023: train loss: 0.4748442769050598\n",
      "Epoch 22024: train loss: 0.47484421730041504\n",
      "Epoch 22025: train loss: 0.47484415769577026\n",
      "Epoch 22026: train loss: 0.4748440980911255\n",
      "Epoch 22027: train loss: 0.4748440682888031\n",
      "Epoch 22028: train loss: 0.4748440086841583\n",
      "Epoch 22029: train loss: 0.4748438894748688\n",
      "Epoch 22030: train loss: 0.4748438894748688\n",
      "Epoch 22031: train loss: 0.4748438596725464\n",
      "Epoch 22032: train loss: 0.4748438000679016\n",
      "Epoch 22033: train loss: 0.47484368085861206\n",
      "Epoch 22034: train loss: 0.4748436510562897\n",
      "Epoch 22035: train loss: 0.4748436510562897\n",
      "Epoch 22036: train loss: 0.4748435914516449\n",
      "Epoch 22037: train loss: 0.47484347224235535\n",
      "Epoch 22038: train loss: 0.47484344244003296\n",
      "Epoch 22039: train loss: 0.4748433828353882\n",
      "Epoch 22040: train loss: 0.4748433232307434\n",
      "Epoch 22041: train loss: 0.47484326362609863\n",
      "Epoch 22042: train loss: 0.47484323382377625\n",
      "Epoch 22043: train loss: 0.47484317421913147\n",
      "Epoch 22044: train loss: 0.4748431146144867\n",
      "Epoch 22045: train loss: 0.4748430550098419\n",
      "Epoch 22046: train loss: 0.4748430550098419\n",
      "Epoch 22047: train loss: 0.47484296560287476\n",
      "Epoch 22048: train loss: 0.47484290599823\n",
      "Epoch 22049: train loss: 0.4748428463935852\n",
      "Epoch 22050: train loss: 0.4748428165912628\n",
      "Epoch 22051: train loss: 0.47484275698661804\n",
      "Epoch 22052: train loss: 0.47484269738197327\n",
      "Epoch 22053: train loss: 0.4748426377773285\n",
      "Epoch 22054: train loss: 0.4748426079750061\n",
      "Epoch 22055: train loss: 0.47484254837036133\n",
      "Epoch 22056: train loss: 0.47484248876571655\n",
      "Epoch 22057: train loss: 0.4748424291610718\n",
      "Epoch 22058: train loss: 0.4748423397541046\n",
      "Epoch 22059: train loss: 0.4748423397541046\n",
      "Epoch 22060: train loss: 0.47484228014945984\n",
      "Epoch 22061: train loss: 0.47484222054481506\n",
      "Epoch 22062: train loss: 0.4748421311378479\n",
      "Epoch 22063: train loss: 0.4748420715332031\n",
      "Epoch 22064: train loss: 0.47484201192855835\n",
      "Epoch 22065: train loss: 0.4748419523239136\n",
      "Epoch 22066: train loss: 0.4748419523239136\n",
      "Epoch 22067: train loss: 0.4748419225215912\n",
      "Epoch 22068: train loss: 0.47484180331230164\n",
      "Epoch 22069: train loss: 0.4748417139053345\n",
      "Epoch 22070: train loss: 0.4748417139053345\n",
      "Epoch 22071: train loss: 0.4748417139053345\n",
      "Epoch 22072: train loss: 0.4748415946960449\n",
      "Epoch 22073: train loss: 0.47484153509140015\n",
      "Epoch 22074: train loss: 0.47484150528907776\n",
      "Epoch 22075: train loss: 0.474841445684433\n",
      "Epoch 22076: train loss: 0.4748413860797882\n",
      "Epoch 22077: train loss: 0.47484132647514343\n",
      "Epoch 22078: train loss: 0.47484129667282104\n",
      "Epoch 22079: train loss: 0.4748411774635315\n",
      "Epoch 22080: train loss: 0.4748411774635315\n",
      "Epoch 22081: train loss: 0.47484108805656433\n",
      "Epoch 22082: train loss: 0.47484108805656433\n",
      "Epoch 22083: train loss: 0.47484102845191956\n",
      "Epoch 22084: train loss: 0.4748409688472748\n",
      "Epoch 22085: train loss: 0.4748408794403076\n",
      "Epoch 22086: train loss: 0.4748408794403076\n",
      "Epoch 22087: train loss: 0.47484081983566284\n",
      "Epoch 22088: train loss: 0.4748407006263733\n",
      "Epoch 22089: train loss: 0.4748406708240509\n",
      "Epoch 22090: train loss: 0.47484061121940613\n",
      "Epoch 22091: train loss: 0.47484061121940613\n",
      "Epoch 22092: train loss: 0.4748404920101166\n",
      "Epoch 22093: train loss: 0.4748404026031494\n",
      "Epoch 22094: train loss: 0.4748404026031494\n",
      "Epoch 22095: train loss: 0.47484034299850464\n",
      "Epoch 22096: train loss: 0.47484034299850464\n",
      "Epoch 22097: train loss: 0.4748402535915375\n",
      "Epoch 22098: train loss: 0.4748401343822479\n",
      "Epoch 22099: train loss: 0.4748401343822479\n",
      "Epoch 22100: train loss: 0.47484007477760315\n",
      "Epoch 22101: train loss: 0.47484004497528076\n",
      "Epoch 22102: train loss: 0.4748399257659912\n",
      "Epoch 22103: train loss: 0.4748399257659912\n",
      "Epoch 22104: train loss: 0.47483986616134644\n",
      "Epoch 22105: train loss: 0.47483986616134644\n",
      "Epoch 22106: train loss: 0.4748397767543793\n",
      "Epoch 22107: train loss: 0.4748396575450897\n",
      "Epoch 22108: train loss: 0.47483962774276733\n",
      "Epoch 22109: train loss: 0.47483956813812256\n",
      "Epoch 22110: train loss: 0.4748395085334778\n",
      "Epoch 22111: train loss: 0.4748395085334778\n",
      "Epoch 22112: train loss: 0.474839448928833\n",
      "Epoch 22113: train loss: 0.47483935952186584\n",
      "Epoch 22114: train loss: 0.47483929991722107\n",
      "Epoch 22115: train loss: 0.47483929991722107\n",
      "Epoch 22116: train loss: 0.4748392105102539\n",
      "Epoch 22117: train loss: 0.47483915090560913\n",
      "Epoch 22118: train loss: 0.47483909130096436\n",
      "Epoch 22119: train loss: 0.4748390316963196\n",
      "Epoch 22120: train loss: 0.4748390018939972\n",
      "Epoch 22121: train loss: 0.47483888268470764\n",
      "Epoch 22122: train loss: 0.47483888268470764\n",
      "Epoch 22123: train loss: 0.4748387932777405\n",
      "Epoch 22124: train loss: 0.4748387932777405\n",
      "Epoch 22125: train loss: 0.4748387336730957\n",
      "Epoch 22126: train loss: 0.4748386740684509\n",
      "Epoch 22127: train loss: 0.47483861446380615\n",
      "Epoch 22128: train loss: 0.47483858466148376\n",
      "Epoch 22129: train loss: 0.474838525056839\n",
      "Epoch 22130: train loss: 0.47483840584754944\n",
      "Epoch 22131: train loss: 0.47483840584754944\n",
      "Epoch 22132: train loss: 0.47483837604522705\n",
      "Epoch 22133: train loss: 0.4748383164405823\n",
      "Epoch 22134: train loss: 0.4748382568359375\n",
      "Epoch 22135: train loss: 0.4748381972312927\n",
      "Epoch 22136: train loss: 0.47483813762664795\n",
      "Epoch 22137: train loss: 0.47483810782432556\n",
      "Epoch 22138: train loss: 0.474837988615036\n",
      "Epoch 22139: train loss: 0.47483792901039124\n",
      "Epoch 22140: train loss: 0.47483789920806885\n",
      "Epoch 22141: train loss: 0.4748378396034241\n",
      "Epoch 22142: train loss: 0.4748377799987793\n",
      "Epoch 22143: train loss: 0.4748377203941345\n",
      "Epoch 22144: train loss: 0.47483769059181213\n",
      "Epoch 22145: train loss: 0.47483763098716736\n",
      "Epoch 22146: train loss: 0.4748375713825226\n",
      "Epoch 22147: train loss: 0.4748375117778778\n",
      "Epoch 22148: train loss: 0.4748374819755554\n",
      "Epoch 22149: train loss: 0.4748374819755554\n",
      "Epoch 22150: train loss: 0.47483736276626587\n",
      "Epoch 22151: train loss: 0.4748373031616211\n",
      "Epoch 22152: train loss: 0.47483721375465393\n",
      "Epoch 22153: train loss: 0.47483721375465393\n",
      "Epoch 22154: train loss: 0.4748370945453644\n",
      "Epoch 22155: train loss: 0.474837064743042\n",
      "Epoch 22156: train loss: 0.474837064743042\n",
      "Epoch 22157: train loss: 0.4748370051383972\n",
      "Epoch 22158: train loss: 0.47483694553375244\n",
      "Epoch 22159: train loss: 0.47483688592910767\n",
      "Epoch 22160: train loss: 0.4748368561267853\n",
      "Epoch 22161: train loss: 0.4748367965221405\n",
      "Epoch 22162: train loss: 0.47483667731285095\n",
      "Epoch 22163: train loss: 0.47483667731285095\n",
      "Epoch 22164: train loss: 0.4748365879058838\n",
      "Epoch 22165: train loss: 0.4748365879058838\n",
      "Epoch 22166: train loss: 0.47483646869659424\n",
      "Epoch 22167: train loss: 0.47483646869659424\n",
      "Epoch 22168: train loss: 0.47483643889427185\n",
      "Epoch 22169: train loss: 0.4748363792896271\n",
      "Epoch 22170: train loss: 0.4748362600803375\n",
      "Epoch 22171: train loss: 0.4748362600803375\n",
      "Epoch 22172: train loss: 0.47483617067337036\n",
      "Epoch 22173: train loss: 0.47483617067337036\n",
      "Epoch 22174: train loss: 0.4748360514640808\n",
      "Epoch 22175: train loss: 0.47483596205711365\n",
      "Epoch 22176: train loss: 0.47483596205711365\n",
      "Epoch 22177: train loss: 0.4748358428478241\n",
      "Epoch 22178: train loss: 0.4748358428478241\n",
      "Epoch 22179: train loss: 0.4748358130455017\n",
      "Epoch 22180: train loss: 0.47483575344085693\n",
      "Epoch 22181: train loss: 0.4748356342315674\n",
      "Epoch 22182: train loss: 0.4748356342315674\n",
      "Epoch 22183: train loss: 0.4748355448246002\n",
      "Epoch 22184: train loss: 0.47483548521995544\n",
      "Epoch 22185: train loss: 0.47483542561531067\n",
      "Epoch 22186: train loss: 0.47483542561531067\n",
      "Epoch 22187: train loss: 0.4748353958129883\n",
      "Epoch 22188: train loss: 0.4748353362083435\n",
      "Epoch 22189: train loss: 0.47483527660369873\n",
      "Epoch 22190: train loss: 0.47483521699905396\n",
      "Epoch 22191: train loss: 0.474835067987442\n",
      "Epoch 22192: train loss: 0.474835067987442\n",
      "Epoch 22193: train loss: 0.47483500838279724\n",
      "Epoch 22194: train loss: 0.47483497858047485\n",
      "Epoch 22195: train loss: 0.47483497858047485\n",
      "Epoch 22196: train loss: 0.4748348593711853\n",
      "Epoch 22197: train loss: 0.47483476996421814\n",
      "Epoch 22198: train loss: 0.47483471035957336\n",
      "Epoch 22199: train loss: 0.47483471035957336\n",
      "Epoch 22200: train loss: 0.4748346507549286\n",
      "Epoch 22201: train loss: 0.4748345911502838\n",
      "Epoch 22202: train loss: 0.4748345613479614\n",
      "Epoch 22203: train loss: 0.47483450174331665\n",
      "Epoch 22204: train loss: 0.4748344421386719\n",
      "Epoch 22205: train loss: 0.4748343825340271\n",
      "Epoch 22206: train loss: 0.4748343229293823\n",
      "Epoch 22207: train loss: 0.47483429312705994\n",
      "Epoch 22208: train loss: 0.4748341739177704\n",
      "Epoch 22209: train loss: 0.4748341739177704\n",
      "Epoch 22210: train loss: 0.4748341143131256\n",
      "Epoch 22211: train loss: 0.4748340845108032\n",
      "Epoch 22212: train loss: 0.47483402490615845\n",
      "Epoch 22213: train loss: 0.47483396530151367\n",
      "Epoch 22214: train loss: 0.4748338758945465\n",
      "Epoch 22215: train loss: 0.47483381628990173\n",
      "Epoch 22216: train loss: 0.4748338758945465\n",
      "Epoch 22217: train loss: 0.4748336970806122\n",
      "Epoch 22218: train loss: 0.4748336672782898\n",
      "Epoch 22219: train loss: 0.474833607673645\n",
      "Epoch 22220: train loss: 0.47483354806900024\n",
      "Epoch 22221: train loss: 0.47483348846435547\n",
      "Epoch 22222: train loss: 0.4748334586620331\n",
      "Epoch 22223: train loss: 0.4748333990573883\n",
      "Epoch 22224: train loss: 0.47483333945274353\n",
      "Epoch 22225: train loss: 0.47483327984809875\n",
      "Epoch 22226: train loss: 0.47483325004577637\n",
      "Epoch 22227: train loss: 0.4748331308364868\n",
      "Epoch 22228: train loss: 0.4748331308364868\n",
      "Epoch 22229: train loss: 0.47483304142951965\n",
      "Epoch 22230: train loss: 0.47483304142951965\n",
      "Epoch 22231: train loss: 0.4748329818248749\n",
      "Epoch 22232: train loss: 0.4748329222202301\n",
      "Epoch 22233: train loss: 0.47483283281326294\n",
      "Epoch 22234: train loss: 0.47483283281326294\n",
      "Epoch 22235: train loss: 0.4748327136039734\n",
      "Epoch 22236: train loss: 0.4748327136039734\n",
      "Epoch 22237: train loss: 0.4748326241970062\n",
      "Epoch 22238: train loss: 0.4748326241970062\n",
      "Epoch 22239: train loss: 0.47483256459236145\n",
      "Epoch 22240: train loss: 0.4748324453830719\n",
      "Epoch 22241: train loss: 0.4748324453830719\n",
      "Epoch 22242: train loss: 0.4748324155807495\n",
      "Epoch 22243: train loss: 0.47483229637145996\n",
      "Epoch 22244: train loss: 0.4748322367668152\n",
      "Epoch 22245: train loss: 0.4748322367668152\n",
      "Epoch 22246: train loss: 0.474832147359848\n",
      "Epoch 22247: train loss: 0.47483208775520325\n",
      "Epoch 22248: train loss: 0.47483202815055847\n",
      "Epoch 22249: train loss: 0.4748319983482361\n",
      "Epoch 22250: train loss: 0.4748319983482361\n",
      "Epoch 22251: train loss: 0.47483187913894653\n",
      "Epoch 22252: train loss: 0.47483181953430176\n",
      "Epoch 22253: train loss: 0.47483178973197937\n",
      "Epoch 22254: train loss: 0.4748317301273346\n",
      "Epoch 22255: train loss: 0.47483161091804504\n",
      "Epoch 22256: train loss: 0.47483158111572266\n",
      "Epoch 22257: train loss: 0.4748315215110779\n",
      "Epoch 22258: train loss: 0.4748314619064331\n",
      "Epoch 22259: train loss: 0.4748314619064331\n",
      "Epoch 22260: train loss: 0.47483137249946594\n",
      "Epoch 22261: train loss: 0.47483137249946594\n",
      "Epoch 22262: train loss: 0.47483131289482117\n",
      "Epoch 22263: train loss: 0.4748311936855316\n",
      "Epoch 22264: train loss: 0.4748311936855316\n",
      "Epoch 22265: train loss: 0.47483116388320923\n",
      "Epoch 22266: train loss: 0.47483110427856445\n",
      "Epoch 22267: train loss: 0.4748309850692749\n",
      "Epoch 22268: train loss: 0.4748309552669525\n",
      "Epoch 22269: train loss: 0.47483089566230774\n",
      "Epoch 22270: train loss: 0.47483083605766296\n",
      "Epoch 22271: train loss: 0.4748307764530182\n",
      "Epoch 22272: train loss: 0.474830687046051\n",
      "Epoch 22273: train loss: 0.474830687046051\n",
      "Epoch 22274: train loss: 0.474830687046051\n",
      "Epoch 22275: train loss: 0.4748305678367615\n",
      "Epoch 22276: train loss: 0.4748305082321167\n",
      "Epoch 22277: train loss: 0.4748304784297943\n",
      "Epoch 22278: train loss: 0.47483041882514954\n",
      "Epoch 22279: train loss: 0.47483035922050476\n",
      "Epoch 22280: train loss: 0.47483029961586\n",
      "Epoch 22281: train loss: 0.4748302698135376\n",
      "Epoch 22282: train loss: 0.4748302102088928\n",
      "Epoch 22283: train loss: 0.47483015060424805\n",
      "Epoch 22284: train loss: 0.47483015060424805\n",
      "Epoch 22285: train loss: 0.4748300015926361\n",
      "Epoch 22286: train loss: 0.4748300015926361\n",
      "Epoch 22287: train loss: 0.47482994198799133\n",
      "Epoch 22288: train loss: 0.47482988238334656\n",
      "Epoch 22289: train loss: 0.47482988238334656\n",
      "Epoch 22290: train loss: 0.4748297929763794\n",
      "Epoch 22291: train loss: 0.4748297333717346\n",
      "Epoch 22292: train loss: 0.47482967376708984\n",
      "Epoch 22293: train loss: 0.4748295843601227\n",
      "Epoch 22294: train loss: 0.4748295843601227\n",
      "Epoch 22295: train loss: 0.4748295247554779\n",
      "Epoch 22296: train loss: 0.47482946515083313\n",
      "Epoch 22297: train loss: 0.47482943534851074\n",
      "Epoch 22298: train loss: 0.4748293161392212\n",
      "Epoch 22299: train loss: 0.4748293161392212\n",
      "Epoch 22300: train loss: 0.4748292565345764\n",
      "Epoch 22301: train loss: 0.47482922673225403\n",
      "Epoch 22302: train loss: 0.47482916712760925\n",
      "Epoch 22303: train loss: 0.4748290479183197\n",
      "Epoch 22304: train loss: 0.4748290181159973\n",
      "Epoch 22305: train loss: 0.47482895851135254\n",
      "Epoch 22306: train loss: 0.47482895851135254\n",
      "Epoch 22307: train loss: 0.47482889890670776\n",
      "Epoch 22308: train loss: 0.4748287498950958\n",
      "Epoch 22309: train loss: 0.47482869029045105\n",
      "Epoch 22310: train loss: 0.47482869029045105\n",
      "Epoch 22311: train loss: 0.4748286306858063\n",
      "Epoch 22312: train loss: 0.4748286008834839\n",
      "Epoch 22313: train loss: 0.4748285412788391\n",
      "Epoch 22314: train loss: 0.47482848167419434\n",
      "Epoch 22315: train loss: 0.4748283922672272\n",
      "Epoch 22316: train loss: 0.4748283326625824\n",
      "Epoch 22317: train loss: 0.4748282730579376\n",
      "Epoch 22318: train loss: 0.47482821345329285\n",
      "Epoch 22319: train loss: 0.4748282730579376\n",
      "Epoch 22320: train loss: 0.4748281240463257\n",
      "Epoch 22321: train loss: 0.4748281240463257\n",
      "Epoch 22322: train loss: 0.4748280644416809\n",
      "Epoch 22323: train loss: 0.47482797503471375\n",
      "Epoch 22324: train loss: 0.47482797503471375\n",
      "Epoch 22325: train loss: 0.4748278558254242\n",
      "Epoch 22326: train loss: 0.4748278558254242\n",
      "Epoch 22327: train loss: 0.4748277962207794\n",
      "Epoch 22328: train loss: 0.47482770681381226\n",
      "Epoch 22329: train loss: 0.47482770681381226\n",
      "Epoch 22330: train loss: 0.4748276472091675\n",
      "Epoch 22331: train loss: 0.4748275578022003\n",
      "Epoch 22332: train loss: 0.47482749819755554\n",
      "Epoch 22333: train loss: 0.47482743859291077\n",
      "Epoch 22334: train loss: 0.474827378988266\n",
      "Epoch 22335: train loss: 0.474827378988266\n",
      "Epoch 22336: train loss: 0.47482728958129883\n",
      "Epoch 22337: train loss: 0.47482728958129883\n",
      "Epoch 22338: train loss: 0.4748271703720093\n",
      "Epoch 22339: train loss: 0.4748271405696869\n",
      "Epoch 22340: train loss: 0.4748270809650421\n",
      "Epoch 22341: train loss: 0.47482702136039734\n",
      "Epoch 22342: train loss: 0.47482696175575256\n",
      "Epoch 22343: train loss: 0.4748269319534302\n",
      "Epoch 22344: train loss: 0.4748268723487854\n",
      "Epoch 22345: train loss: 0.4748268127441406\n",
      "Epoch 22346: train loss: 0.47482675313949585\n",
      "Epoch 22347: train loss: 0.4748266935348511\n",
      "Epoch 22348: train loss: 0.4748266637325287\n",
      "Epoch 22349: train loss: 0.4748266041278839\n",
      "Epoch 22350: train loss: 0.47482654452323914\n",
      "Epoch 22351: train loss: 0.47482648491859436\n",
      "Epoch 22352: train loss: 0.4748263955116272\n",
      "Epoch 22353: train loss: 0.474826455116272\n",
      "Epoch 22354: train loss: 0.4748263359069824\n",
      "Epoch 22355: train loss: 0.4748263359069824\n",
      "Epoch 22356: train loss: 0.47482624650001526\n",
      "Epoch 22357: train loss: 0.4748261868953705\n",
      "Epoch 22358: train loss: 0.4748261272907257\n",
      "Epoch 22359: train loss: 0.47482606768608093\n",
      "Epoch 22360: train loss: 0.47482603788375854\n",
      "Epoch 22361: train loss: 0.47482597827911377\n",
      "Epoch 22362: train loss: 0.474825918674469\n",
      "Epoch 22363: train loss: 0.4748258590698242\n",
      "Epoch 22364: train loss: 0.47482576966285706\n",
      "Epoch 22365: train loss: 0.4748257100582123\n",
      "Epoch 22366: train loss: 0.4748256206512451\n",
      "Epoch 22367: train loss: 0.4748256206512451\n",
      "Epoch 22368: train loss: 0.4748256206512451\n",
      "Epoch 22369: train loss: 0.47482556104660034\n",
      "Epoch 22370: train loss: 0.47482550144195557\n",
      "Epoch 22371: train loss: 0.4748254120349884\n",
      "Epoch 22372: train loss: 0.4748254120349884\n",
      "Epoch 22373: train loss: 0.47482529282569885\n",
      "Epoch 22374: train loss: 0.4748252332210541\n",
      "Epoch 22375: train loss: 0.4748252034187317\n",
      "Epoch 22376: train loss: 0.4748251438140869\n",
      "Epoch 22377: train loss: 0.47482508420944214\n",
      "Epoch 22378: train loss: 0.47482508420944214\n",
      "Epoch 22379: train loss: 0.474824994802475\n",
      "Epoch 22380: train loss: 0.4748249351978302\n",
      "Epoch 22381: train loss: 0.4748248755931854\n",
      "Epoch 22382: train loss: 0.47482481598854065\n",
      "Epoch 22383: train loss: 0.47482478618621826\n",
      "Epoch 22384: train loss: 0.4748246669769287\n",
      "Epoch 22385: train loss: 0.4748246669769287\n",
      "Epoch 22386: train loss: 0.47482460737228394\n",
      "Epoch 22387: train loss: 0.47482457756996155\n",
      "Epoch 22388: train loss: 0.4748245179653168\n",
      "Epoch 22389: train loss: 0.474824458360672\n",
      "Epoch 22390: train loss: 0.4748243987560272\n",
      "Epoch 22391: train loss: 0.47482436895370483\n",
      "Epoch 22392: train loss: 0.4748242497444153\n",
      "Epoch 22393: train loss: 0.4748242497444153\n",
      "Epoch 22394: train loss: 0.4748241901397705\n",
      "Epoch 22395: train loss: 0.4748241603374481\n",
      "Epoch 22396: train loss: 0.4748241603374481\n",
      "Epoch 22397: train loss: 0.47482404112815857\n",
      "Epoch 22398: train loss: 0.4748239517211914\n",
      "Epoch 22399: train loss: 0.4748239517211914\n",
      "Epoch 22400: train loss: 0.47482389211654663\n",
      "Epoch 22401: train loss: 0.4748237729072571\n",
      "Epoch 22402: train loss: 0.4748237431049347\n",
      "Epoch 22403: train loss: 0.4748236835002899\n",
      "Epoch 22404: train loss: 0.47482362389564514\n",
      "Epoch 22405: train loss: 0.47482362389564514\n",
      "Epoch 22406: train loss: 0.47482356429100037\n",
      "Epoch 22407: train loss: 0.474823534488678\n",
      "Epoch 22408: train loss: 0.4748234152793884\n",
      "Epoch 22409: train loss: 0.47482335567474365\n",
      "Epoch 22410: train loss: 0.47482335567474365\n",
      "Epoch 22411: train loss: 0.47482332587242126\n",
      "Epoch 22412: train loss: 0.47482332587242126\n",
      "Epoch 22413: train loss: 0.47482314705848694\n",
      "Epoch 22414: train loss: 0.47482314705848694\n",
      "Epoch 22415: train loss: 0.47482311725616455\n",
      "Epoch 22416: train loss: 0.474822998046875\n",
      "Epoch 22417: train loss: 0.474822998046875\n",
      "Epoch 22418: train loss: 0.47482287883758545\n",
      "Epoch 22419: train loss: 0.47482284903526306\n",
      "Epoch 22420: train loss: 0.4748227894306183\n",
      "Epoch 22421: train loss: 0.47482267022132874\n",
      "Epoch 22422: train loss: 0.47482267022132874\n",
      "Epoch 22423: train loss: 0.47482264041900635\n",
      "Epoch 22424: train loss: 0.4748225808143616\n",
      "Epoch 22425: train loss: 0.4748225212097168\n",
      "Epoch 22426: train loss: 0.4748225212097168\n",
      "Epoch 22427: train loss: 0.47482243180274963\n",
      "Epoch 22428: train loss: 0.47482237219810486\n",
      "Epoch 22429: train loss: 0.4748223125934601\n",
      "Epoch 22430: train loss: 0.4748222529888153\n",
      "Epoch 22431: train loss: 0.47482216358184814\n",
      "Epoch 22432: train loss: 0.47482216358184814\n",
      "Epoch 22433: train loss: 0.4748220443725586\n",
      "Epoch 22434: train loss: 0.4748220443725586\n",
      "Epoch 22435: train loss: 0.4748220145702362\n",
      "Epoch 22436: train loss: 0.47482195496559143\n",
      "Epoch 22437: train loss: 0.4748218357563019\n",
      "Epoch 22438: train loss: 0.4748218357563019\n",
      "Epoch 22439: train loss: 0.4748218357563019\n",
      "Epoch 22440: train loss: 0.4748217463493347\n",
      "Epoch 22441: train loss: 0.47482162714004517\n",
      "Epoch 22442: train loss: 0.47482162714004517\n",
      "Epoch 22443: train loss: 0.474821537733078\n",
      "Epoch 22444: train loss: 0.4748214781284332\n",
      "Epoch 22445: train loss: 0.4748214781284332\n",
      "Epoch 22446: train loss: 0.47482141852378845\n",
      "Epoch 22447: train loss: 0.4748213291168213\n",
      "Epoch 22448: train loss: 0.4748213291168213\n",
      "Epoch 22449: train loss: 0.47482120990753174\n",
      "Epoch 22450: train loss: 0.47482120990753174\n",
      "Epoch 22451: train loss: 0.47482118010520935\n",
      "Epoch 22452: train loss: 0.4748210608959198\n",
      "Epoch 22453: train loss: 0.474821001291275\n",
      "Epoch 22454: train loss: 0.47482097148895264\n",
      "Epoch 22455: train loss: 0.47482091188430786\n",
      "Epoch 22456: train loss: 0.4748208522796631\n",
      "Epoch 22457: train loss: 0.4748207926750183\n",
      "Epoch 22458: train loss: 0.4748207926750183\n",
      "Epoch 22459: train loss: 0.4748207628726959\n",
      "Epoch 22460: train loss: 0.47482064366340637\n",
      "Epoch 22461: train loss: 0.4748205840587616\n",
      "Epoch 22462: train loss: 0.4748205542564392\n",
      "Epoch 22463: train loss: 0.47482043504714966\n",
      "Epoch 22464: train loss: 0.47482043504714966\n",
      "Epoch 22465: train loss: 0.4748203754425049\n",
      "Epoch 22466: train loss: 0.4748203754425049\n",
      "Epoch 22467: train loss: 0.4748202860355377\n",
      "Epoch 22468: train loss: 0.4748202860355377\n",
      "Epoch 22469: train loss: 0.4748201370239258\n",
      "Epoch 22470: train loss: 0.4748201370239258\n",
      "Epoch 22471: train loss: 0.474820077419281\n",
      "Epoch 22472: train loss: 0.47482001781463623\n",
      "Epoch 22473: train loss: 0.47481995820999146\n",
      "Epoch 22474: train loss: 0.47481992840766907\n",
      "Epoch 22475: train loss: 0.4748198688030243\n",
      "Epoch 22476: train loss: 0.4748198091983795\n",
      "Epoch 22477: train loss: 0.47481971979141235\n",
      "Epoch 22478: train loss: 0.47481971979141235\n",
      "Epoch 22479: train loss: 0.4748196601867676\n",
      "Epoch 22480: train loss: 0.4748196005821228\n",
      "Epoch 22481: train loss: 0.47481951117515564\n",
      "Epoch 22482: train loss: 0.47481951117515564\n",
      "Epoch 22483: train loss: 0.4748193919658661\n",
      "Epoch 22484: train loss: 0.4748193919658661\n",
      "Epoch 22485: train loss: 0.4748193323612213\n",
      "Epoch 22486: train loss: 0.4748193025588989\n",
      "Epoch 22487: train loss: 0.4748191833496094\n",
      "Epoch 22488: train loss: 0.4748191833496094\n",
      "Epoch 22489: train loss: 0.4748190641403198\n",
      "Epoch 22490: train loss: 0.4748190641403198\n",
      "Epoch 22491: train loss: 0.47481903433799744\n",
      "Epoch 22492: train loss: 0.4748189151287079\n",
      "Epoch 22493: train loss: 0.4748189151287079\n",
      "Epoch 22494: train loss: 0.4748188257217407\n",
      "Epoch 22495: train loss: 0.4748188257217407\n",
      "Epoch 22496: train loss: 0.47481870651245117\n",
      "Epoch 22497: train loss: 0.4748186469078064\n",
      "Epoch 22498: train loss: 0.4748186469078064\n",
      "Epoch 22499: train loss: 0.474818617105484\n",
      "Epoch 22500: train loss: 0.47481849789619446\n",
      "Epoch 22501: train loss: 0.4748184084892273\n",
      "Epoch 22502: train loss: 0.4748184084892273\n",
      "Epoch 22503: train loss: 0.47481828927993774\n",
      "Epoch 22504: train loss: 0.47481828927993774\n",
      "Epoch 22505: train loss: 0.47481822967529297\n",
      "Epoch 22506: train loss: 0.4748181998729706\n",
      "Epoch 22507: train loss: 0.4748181402683258\n",
      "Epoch 22508: train loss: 0.47481808066368103\n",
      "Epoch 22509: train loss: 0.47481802105903625\n",
      "Epoch 22510: train loss: 0.47481799125671387\n",
      "Epoch 22511: train loss: 0.4748179316520691\n",
      "Epoch 22512: train loss: 0.4748178720474243\n",
      "Epoch 22513: train loss: 0.47481781244277954\n",
      "Epoch 22514: train loss: 0.47481778264045715\n",
      "Epoch 22515: train loss: 0.4748177230358124\n",
      "Epoch 22516: train loss: 0.4748176634311676\n",
      "Epoch 22517: train loss: 0.4748176038265228\n",
      "Epoch 22518: train loss: 0.47481757402420044\n",
      "Epoch 22519: train loss: 0.4748174548149109\n",
      "Epoch 22520: train loss: 0.4748174548149109\n",
      "Epoch 22521: train loss: 0.4748173952102661\n",
      "Epoch 22522: train loss: 0.4748173654079437\n",
      "Epoch 22523: train loss: 0.47481730580329895\n",
      "Epoch 22524: train loss: 0.4748172461986542\n",
      "Epoch 22525: train loss: 0.4748171865940094\n",
      "Epoch 22526: train loss: 0.474817156791687\n",
      "Epoch 22527: train loss: 0.47481703758239746\n",
      "Epoch 22528: train loss: 0.4748169779777527\n",
      "Epoch 22529: train loss: 0.4748169779777527\n",
      "Epoch 22530: train loss: 0.4748169481754303\n",
      "Epoch 22531: train loss: 0.4748168885707855\n",
      "Epoch 22532: train loss: 0.47481676936149597\n",
      "Epoch 22533: train loss: 0.47481676936149597\n",
      "Epoch 22534: train loss: 0.4748167395591736\n",
      "Epoch 22535: train loss: 0.47481662034988403\n",
      "Epoch 22536: train loss: 0.47481656074523926\n",
      "Epoch 22537: train loss: 0.47481656074523926\n",
      "Epoch 22538: train loss: 0.4748164713382721\n",
      "Epoch 22539: train loss: 0.4748164117336273\n",
      "Epoch 22540: train loss: 0.47481635212898254\n",
      "Epoch 22541: train loss: 0.47481632232666016\n",
      "Epoch 22542: train loss: 0.4748162627220154\n",
      "Epoch 22543: train loss: 0.4748162031173706\n",
      "Epoch 22544: train loss: 0.4748162031173706\n",
      "Epoch 22545: train loss: 0.47481611371040344\n",
      "Epoch 22546: train loss: 0.47481611371040344\n",
      "Epoch 22547: train loss: 0.47481605410575867\n",
      "Epoch 22548: train loss: 0.4748159348964691\n",
      "Epoch 22549: train loss: 0.4748159348964691\n",
      "Epoch 22550: train loss: 0.47481584548950195\n",
      "Epoch 22551: train loss: 0.4748157858848572\n",
      "Epoch 22552: train loss: 0.4748157262802124\n",
      "Epoch 22553: train loss: 0.47481569647789\n",
      "Epoch 22554: train loss: 0.47481569647789\n",
      "Epoch 22555: train loss: 0.47481557726860046\n",
      "Epoch 22556: train loss: 0.4748155176639557\n",
      "Epoch 22557: train loss: 0.4748154282569885\n",
      "Epoch 22558: train loss: 0.4748154282569885\n",
      "Epoch 22559: train loss: 0.474815309047699\n",
      "Epoch 22560: train loss: 0.474815309047699\n",
      "Epoch 22561: train loss: 0.4748152494430542\n",
      "Epoch 22562: train loss: 0.4748152494430542\n",
      "Epoch 22563: train loss: 0.47481516003608704\n",
      "Epoch 22564: train loss: 0.47481510043144226\n",
      "Epoch 22565: train loss: 0.4748150408267975\n",
      "Epoch 22566: train loss: 0.4748150110244751\n",
      "Epoch 22567: train loss: 0.4748149514198303\n",
      "Epoch 22568: train loss: 0.47481483221054077\n",
      "Epoch 22569: train loss: 0.47481483221054077\n",
      "Epoch 22570: train loss: 0.4748148024082184\n",
      "Epoch 22571: train loss: 0.47481468319892883\n",
      "Epoch 22572: train loss: 0.47481468319892883\n",
      "Epoch 22573: train loss: 0.47481462359428406\n",
      "Epoch 22574: train loss: 0.47481459379196167\n",
      "Epoch 22575: train loss: 0.4748144745826721\n",
      "Epoch 22576: train loss: 0.4748144745826721\n",
      "Epoch 22577: train loss: 0.47481441497802734\n",
      "Epoch 22578: train loss: 0.4748143255710602\n",
      "Epoch 22579: train loss: 0.4748143255710602\n",
      "Epoch 22580: train loss: 0.47481420636177063\n",
      "Epoch 22581: train loss: 0.47481411695480347\n",
      "Epoch 22582: train loss: 0.47481411695480347\n",
      "Epoch 22583: train loss: 0.4748140573501587\n",
      "Epoch 22584: train loss: 0.4748140573501587\n",
      "Epoch 22585: train loss: 0.47481396794319153\n",
      "Epoch 22586: train loss: 0.47481396794319153\n",
      "Epoch 22587: train loss: 0.474813848733902\n",
      "Epoch 22588: train loss: 0.4748137891292572\n",
      "Epoch 22589: train loss: 0.4748137593269348\n",
      "Epoch 22590: train loss: 0.47481369972229004\n",
      "Epoch 22591: train loss: 0.47481369972229004\n",
      "Epoch 22592: train loss: 0.4748135805130005\n",
      "Epoch 22593: train loss: 0.4748135507106781\n",
      "Epoch 22594: train loss: 0.4748134911060333\n",
      "Epoch 22595: train loss: 0.4748134911060333\n",
      "Epoch 22596: train loss: 0.4748133718967438\n",
      "Epoch 22597: train loss: 0.4748133718967438\n",
      "Epoch 22598: train loss: 0.4748132824897766\n",
      "Epoch 22599: train loss: 0.4748132824897766\n",
      "Epoch 22600: train loss: 0.47481316328048706\n",
      "Epoch 22601: train loss: 0.4748131334781647\n",
      "Epoch 22602: train loss: 0.4748130738735199\n",
      "Epoch 22603: train loss: 0.4748130142688751\n",
      "Epoch 22604: train loss: 0.47481295466423035\n",
      "Epoch 22605: train loss: 0.47481292486190796\n",
      "Epoch 22606: train loss: 0.4748128652572632\n",
      "Epoch 22607: train loss: 0.4748128056526184\n",
      "Epoch 22608: train loss: 0.47481274604797363\n",
      "Epoch 22609: train loss: 0.47481271624565125\n",
      "Epoch 22610: train loss: 0.47481265664100647\n",
      "Epoch 22611: train loss: 0.4748125374317169\n",
      "Epoch 22612: train loss: 0.4748125374317169\n",
      "Epoch 22613: train loss: 0.47481250762939453\n",
      "Epoch 22614: train loss: 0.47481244802474976\n",
      "Epoch 22615: train loss: 0.474812388420105\n",
      "Epoch 22616: train loss: 0.474812388420105\n",
      "Epoch 22617: train loss: 0.47481223940849304\n",
      "Epoch 22618: train loss: 0.47481223940849304\n",
      "Epoch 22619: train loss: 0.4748121201992035\n",
      "Epoch 22620: train loss: 0.4748121201992035\n",
      "Epoch 22621: train loss: 0.4748120903968811\n",
      "Epoch 22622: train loss: 0.47481197118759155\n",
      "Epoch 22623: train loss: 0.47481197118759155\n",
      "Epoch 22624: train loss: 0.4748119115829468\n",
      "Epoch 22625: train loss: 0.4748118817806244\n",
      "Epoch 22626: train loss: 0.47481176257133484\n",
      "Epoch 22627: train loss: 0.47481176257133484\n",
      "Epoch 22628: train loss: 0.4748116731643677\n",
      "Epoch 22629: train loss: 0.4748116135597229\n",
      "Epoch 22630: train loss: 0.4748115539550781\n",
      "Epoch 22631: train loss: 0.47481149435043335\n",
      "Epoch 22632: train loss: 0.4748114347457886\n",
      "Epoch 22633: train loss: 0.4748114049434662\n",
      "Epoch 22634: train loss: 0.4748113453388214\n",
      "Epoch 22635: train loss: 0.47481128573417664\n",
      "Epoch 22636: train loss: 0.47481122612953186\n",
      "Epoch 22637: train loss: 0.4748111963272095\n",
      "Epoch 22638: train loss: 0.4748111367225647\n",
      "Epoch 22639: train loss: 0.4748110771179199\n",
      "Epoch 22640: train loss: 0.4748110771179199\n",
      "Epoch 22641: train loss: 0.47481098771095276\n",
      "Epoch 22642: train loss: 0.4748108685016632\n",
      "Epoch 22643: train loss: 0.4748108685016632\n",
      "Epoch 22644: train loss: 0.47481080889701843\n",
      "Epoch 22645: train loss: 0.47481077909469604\n",
      "Epoch 22646: train loss: 0.47481071949005127\n",
      "Epoch 22647: train loss: 0.4748106598854065\n",
      "Epoch 22648: train loss: 0.4748106002807617\n",
      "Epoch 22649: train loss: 0.47481057047843933\n",
      "Epoch 22650: train loss: 0.47481051087379456\n",
      "Epoch 22651: train loss: 0.4748104512691498\n",
      "Epoch 22652: train loss: 0.474810391664505\n",
      "Epoch 22653: train loss: 0.4748103618621826\n",
      "Epoch 22654: train loss: 0.47481024265289307\n",
      "Epoch 22655: train loss: 0.47481024265289307\n",
      "Epoch 22656: train loss: 0.4748101830482483\n",
      "Epoch 22657: train loss: 0.4748101532459259\n",
      "Epoch 22658: train loss: 0.47481009364128113\n",
      "Epoch 22659: train loss: 0.47481003403663635\n",
      "Epoch 22660: train loss: 0.4748099744319916\n",
      "Epoch 22661: train loss: 0.4748099446296692\n",
      "Epoch 22662: train loss: 0.4748099446296692\n",
      "Epoch 22663: train loss: 0.47480982542037964\n",
      "Epoch 22664: train loss: 0.47480976581573486\n",
      "Epoch 22665: train loss: 0.4748097360134125\n",
      "Epoch 22666: train loss: 0.4748096764087677\n",
      "Epoch 22667: train loss: 0.4748096168041229\n",
      "Epoch 22668: train loss: 0.47480952739715576\n",
      "Epoch 22669: train loss: 0.47480952739715576\n",
      "Epoch 22670: train loss: 0.4748094081878662\n",
      "Epoch 22671: train loss: 0.4748094081878662\n",
      "Epoch 22672: train loss: 0.47480931878089905\n",
      "Epoch 22673: train loss: 0.47480931878089905\n",
      "Epoch 22674: train loss: 0.4748091995716095\n",
      "Epoch 22675: train loss: 0.4748091399669647\n",
      "Epoch 22676: train loss: 0.47480911016464233\n",
      "Epoch 22677: train loss: 0.47480905055999756\n",
      "Epoch 22678: train loss: 0.4748089909553528\n",
      "Epoch 22679: train loss: 0.474808931350708\n",
      "Epoch 22680: train loss: 0.4748089015483856\n",
      "Epoch 22681: train loss: 0.47480878233909607\n",
      "Epoch 22682: train loss: 0.47480878233909607\n",
      "Epoch 22683: train loss: 0.47480878233909607\n",
      "Epoch 22684: train loss: 0.4748086929321289\n",
      "Epoch 22685: train loss: 0.47480863332748413\n",
      "Epoch 22686: train loss: 0.47480857372283936\n",
      "Epoch 22687: train loss: 0.4748085141181946\n",
      "Epoch 22688: train loss: 0.4748084843158722\n",
      "Epoch 22689: train loss: 0.4748084247112274\n",
      "Epoch 22690: train loss: 0.47480836510658264\n",
      "Epoch 22691: train loss: 0.47480830550193787\n",
      "Epoch 22692: train loss: 0.4748082756996155\n",
      "Epoch 22693: train loss: 0.4748081564903259\n",
      "Epoch 22694: train loss: 0.4748081564903259\n",
      "Epoch 22695: train loss: 0.47480809688568115\n",
      "Epoch 22696: train loss: 0.47480806708335876\n",
      "Epoch 22697: train loss: 0.474808007478714\n",
      "Epoch 22698: train loss: 0.4748079478740692\n",
      "Epoch 22699: train loss: 0.47480788826942444\n",
      "Epoch 22700: train loss: 0.47480785846710205\n",
      "Epoch 22701: train loss: 0.4748077392578125\n",
      "Epoch 22702: train loss: 0.4748077392578125\n",
      "Epoch 22703: train loss: 0.47480762004852295\n",
      "Epoch 22704: train loss: 0.47480759024620056\n",
      "Epoch 22705: train loss: 0.4748075306415558\n",
      "Epoch 22706: train loss: 0.4748075306415558\n",
      "Epoch 22707: train loss: 0.474807471036911\n",
      "Epoch 22708: train loss: 0.47480741143226624\n",
      "Epoch 22709: train loss: 0.4748073220252991\n",
      "Epoch 22710: train loss: 0.4748073220252991\n",
      "Epoch 22711: train loss: 0.4748072624206543\n",
      "Epoch 22712: train loss: 0.4748072028160095\n",
      "Epoch 22713: train loss: 0.47480711340904236\n",
      "Epoch 22714: train loss: 0.47480711340904236\n",
      "Epoch 22715: train loss: 0.4748069941997528\n",
      "Epoch 22716: train loss: 0.4748069941997528\n",
      "Epoch 22717: train loss: 0.4748069643974304\n",
      "Epoch 22718: train loss: 0.47480690479278564\n",
      "Epoch 22719: train loss: 0.47480684518814087\n",
      "Epoch 22720: train loss: 0.4748067855834961\n",
      "Epoch 22721: train loss: 0.47480669617652893\n",
      "Epoch 22722: train loss: 0.47480663657188416\n",
      "Epoch 22723: train loss: 0.4748065769672394\n",
      "Epoch 22724: train loss: 0.4748065769672394\n",
      "Epoch 22725: train loss: 0.47480642795562744\n",
      "Epoch 22726: train loss: 0.4748064875602722\n",
      "Epoch 22727: train loss: 0.47480636835098267\n",
      "Epoch 22728: train loss: 0.47480636835098267\n",
      "Epoch 22729: train loss: 0.4748063385486603\n",
      "Epoch 22730: train loss: 0.4748062789440155\n",
      "Epoch 22731: train loss: 0.47480615973472595\n",
      "Epoch 22732: train loss: 0.47480612993240356\n",
      "Epoch 22733: train loss: 0.474806010723114\n",
      "Epoch 22734: train loss: 0.474806010723114\n",
      "Epoch 22735: train loss: 0.47480595111846924\n",
      "Epoch 22736: train loss: 0.47480592131614685\n",
      "Epoch 22737: train loss: 0.4748058617115021\n",
      "Epoch 22738: train loss: 0.4748057425022125\n",
      "Epoch 22739: train loss: 0.4748057425022125\n",
      "Epoch 22740: train loss: 0.47480565309524536\n",
      "Epoch 22741: train loss: 0.47480571269989014\n",
      "Epoch 22742: train loss: 0.47480565309524536\n",
      "Epoch 22743: train loss: 0.4748055338859558\n",
      "Epoch 22744: train loss: 0.4748055338859558\n",
      "Epoch 22745: train loss: 0.47480544447898865\n",
      "Epoch 22746: train loss: 0.47480538487434387\n",
      "Epoch 22747: train loss: 0.4748053252696991\n",
      "Epoch 22748: train loss: 0.4748053252696991\n",
      "Epoch 22749: train loss: 0.47480523586273193\n",
      "Epoch 22750: train loss: 0.47480517625808716\n",
      "Epoch 22751: train loss: 0.47480508685112\n",
      "Epoch 22752: train loss: 0.47480508685112\n",
      "Epoch 22753: train loss: 0.47480496764183044\n",
      "Epoch 22754: train loss: 0.47480490803718567\n",
      "Epoch 22755: train loss: 0.47480490803718567\n",
      "Epoch 22756: train loss: 0.4748048186302185\n",
      "Epoch 22757: train loss: 0.47480475902557373\n",
      "Epoch 22758: train loss: 0.47480475902557373\n",
      "Epoch 22759: train loss: 0.47480466961860657\n",
      "Epoch 22760: train loss: 0.47480466961860657\n",
      "Epoch 22761: train loss: 0.4748046100139618\n",
      "Epoch 22762: train loss: 0.47480449080467224\n",
      "Epoch 22763: train loss: 0.47480446100234985\n",
      "Epoch 22764: train loss: 0.4748044013977051\n",
      "Epoch 22765: train loss: 0.4748043417930603\n",
      "Epoch 22766: train loss: 0.4748042821884155\n",
      "Epoch 22767: train loss: 0.47480425238609314\n",
      "Epoch 22768: train loss: 0.47480419278144836\n",
      "Epoch 22769: train loss: 0.4748041331768036\n",
      "Epoch 22770: train loss: 0.4748040735721588\n",
      "Epoch 22771: train loss: 0.4748040437698364\n",
      "Epoch 22772: train loss: 0.4748040437698364\n",
      "Epoch 22773: train loss: 0.47480398416519165\n",
      "Epoch 22774: train loss: 0.47480398416519165\n",
      "Epoch 22775: train loss: 0.4748038649559021\n",
      "Epoch 22776: train loss: 0.47480377554893494\n",
      "Epoch 22777: train loss: 0.47480371594429016\n",
      "Epoch 22778: train loss: 0.47480371594429016\n",
      "Epoch 22779: train loss: 0.4748036563396454\n",
      "Epoch 22780: train loss: 0.4748035669326782\n",
      "Epoch 22781: train loss: 0.4748035669326782\n",
      "Epoch 22782: train loss: 0.47480350732803345\n",
      "Epoch 22783: train loss: 0.47480344772338867\n",
      "Epoch 22784: train loss: 0.4748033583164215\n",
      "Epoch 22785: train loss: 0.47480329871177673\n",
      "Epoch 22786: train loss: 0.47480323910713196\n",
      "Epoch 22787: train loss: 0.4748031795024872\n",
      "Epoch 22788: train loss: 0.4748031497001648\n",
      "Epoch 22789: train loss: 0.4748031497001648\n",
      "Epoch 22790: train loss: 0.47480303049087524\n",
      "Epoch 22791: train loss: 0.47480303049087524\n",
      "Epoch 22792: train loss: 0.4748029410839081\n",
      "Epoch 22793: train loss: 0.4748028814792633\n",
      "Epoch 22794: train loss: 0.47480282187461853\n",
      "Epoch 22795: train loss: 0.47480276226997375\n",
      "Epoch 22796: train loss: 0.47480273246765137\n",
      "Epoch 22797: train loss: 0.4748026728630066\n",
      "Epoch 22798: train loss: 0.4748026132583618\n",
      "Epoch 22799: train loss: 0.4748026132583618\n",
      "Epoch 22800: train loss: 0.47480252385139465\n",
      "Epoch 22801: train loss: 0.4748024642467499\n",
      "Epoch 22802: train loss: 0.4748023450374603\n",
      "Epoch 22803: train loss: 0.4748024046421051\n",
      "Epoch 22804: train loss: 0.4748023450374603\n",
      "Epoch 22805: train loss: 0.47480225563049316\n",
      "Epoch 22806: train loss: 0.4748021960258484\n",
      "Epoch 22807: train loss: 0.4748021364212036\n",
      "Epoch 22808: train loss: 0.4748021066188812\n",
      "Epoch 22809: train loss: 0.47480204701423645\n",
      "Epoch 22810: train loss: 0.4748019874095917\n",
      "Epoch 22811: train loss: 0.4748019278049469\n",
      "Epoch 22812: train loss: 0.4748018980026245\n",
      "Epoch 22813: train loss: 0.47480177879333496\n",
      "Epoch 22814: train loss: 0.47480177879333496\n",
      "Epoch 22815: train loss: 0.4748016893863678\n",
      "Epoch 22816: train loss: 0.4748016893863678\n",
      "Epoch 22817: train loss: 0.474801629781723\n",
      "Epoch 22818: train loss: 0.47480157017707825\n",
      "Epoch 22819: train loss: 0.4748014807701111\n",
      "Epoch 22820: train loss: 0.4748014807701111\n",
      "Epoch 22821: train loss: 0.47480136156082153\n",
      "Epoch 22822: train loss: 0.47480136156082153\n",
      "Epoch 22823: train loss: 0.47480127215385437\n",
      "Epoch 22824: train loss: 0.47480127215385437\n",
      "Epoch 22825: train loss: 0.4748012125492096\n",
      "Epoch 22826: train loss: 0.47480109333992004\n",
      "Epoch 22827: train loss: 0.47480109333992004\n",
      "Epoch 22828: train loss: 0.47480106353759766\n",
      "Epoch 22829: train loss: 0.4748010039329529\n",
      "Epoch 22830: train loss: 0.47480088472366333\n",
      "Epoch 22831: train loss: 0.47480088472366333\n",
      "Epoch 22832: train loss: 0.47480085492134094\n",
      "Epoch 22833: train loss: 0.47480079531669617\n",
      "Epoch 22834: train loss: 0.4748007357120514\n",
      "Epoch 22835: train loss: 0.4748006761074066\n",
      "Epoch 22836: train loss: 0.47480064630508423\n",
      "Epoch 22837: train loss: 0.47480058670043945\n",
      "Epoch 22838: train loss: 0.4748004674911499\n",
      "Epoch 22839: train loss: 0.4748004674911499\n",
      "Epoch 22840: train loss: 0.4748004376888275\n",
      "Epoch 22841: train loss: 0.47480031847953796\n",
      "Epoch 22842: train loss: 0.4748002290725708\n",
      "Epoch 22843: train loss: 0.4748002290725708\n",
      "Epoch 22844: train loss: 0.474800169467926\n",
      "Epoch 22845: train loss: 0.47480010986328125\n",
      "Epoch 22846: train loss: 0.4748000502586365\n",
      "Epoch 22847: train loss: 0.4747999906539917\n",
      "Epoch 22848: train loss: 0.4747999608516693\n",
      "Epoch 22849: train loss: 0.47479990124702454\n",
      "Epoch 22850: train loss: 0.474799782037735\n",
      "Epoch 22851: train loss: 0.474799782037735\n",
      "Epoch 22852: train loss: 0.4747997522354126\n",
      "Epoch 22853: train loss: 0.4747996926307678\n",
      "Epoch 22854: train loss: 0.47479963302612305\n",
      "Epoch 22855: train loss: 0.47479957342147827\n",
      "Epoch 22856: train loss: 0.4747994840145111\n",
      "Epoch 22857: train loss: 0.4747994840145111\n",
      "Epoch 22858: train loss: 0.47479936480522156\n",
      "Epoch 22859: train loss: 0.47479936480522156\n",
      "Epoch 22860: train loss: 0.47479933500289917\n",
      "Epoch 22861: train loss: 0.4747992753982544\n",
      "Epoch 22862: train loss: 0.4747992157936096\n",
      "Epoch 22863: train loss: 0.47479912638664246\n",
      "Epoch 22864: train loss: 0.47479912638664246\n",
      "Epoch 22865: train loss: 0.47479912638664246\n",
      "Epoch 22866: train loss: 0.4747990071773529\n",
      "Epoch 22867: train loss: 0.47479894757270813\n",
      "Epoch 22868: train loss: 0.47479891777038574\n",
      "Epoch 22869: train loss: 0.47479885816574097\n",
      "Epoch 22870: train loss: 0.4747987389564514\n",
      "Epoch 22871: train loss: 0.4747987389564514\n",
      "Epoch 22872: train loss: 0.47479864954948425\n",
      "Epoch 22873: train loss: 0.47479864954948425\n",
      "Epoch 22874: train loss: 0.4747985899448395\n",
      "Epoch 22875: train loss: 0.4747985899448395\n",
      "Epoch 22876: train loss: 0.4747985005378723\n",
      "Epoch 22877: train loss: 0.47479838132858276\n",
      "Epoch 22878: train loss: 0.47479838132858276\n",
      "Epoch 22879: train loss: 0.474798321723938\n",
      "Epoch 22880: train loss: 0.4747982323169708\n",
      "Epoch 22881: train loss: 0.47479817271232605\n",
      "Epoch 22882: train loss: 0.47479817271232605\n",
      "Epoch 22883: train loss: 0.4747981131076813\n",
      "Epoch 22884: train loss: 0.4747980237007141\n",
      "Epoch 22885: train loss: 0.4747980237007141\n",
      "Epoch 22886: train loss: 0.47479790449142456\n",
      "Epoch 22887: train loss: 0.47479790449142456\n",
      "Epoch 22888: train loss: 0.4747978150844574\n",
      "Epoch 22889: train loss: 0.4747978150844574\n",
      "Epoch 22890: train loss: 0.47479769587516785\n",
      "Epoch 22891: train loss: 0.47479766607284546\n",
      "Epoch 22892: train loss: 0.47479766607284546\n",
      "Epoch 22893: train loss: 0.4747975468635559\n",
      "Epoch 22894: train loss: 0.47479748725891113\n",
      "Epoch 22895: train loss: 0.47479748725891113\n",
      "Epoch 22896: train loss: 0.47479745745658875\n",
      "Epoch 22897: train loss: 0.47479739785194397\n",
      "Epoch 22898: train loss: 0.4747972786426544\n",
      "Epoch 22899: train loss: 0.47479724884033203\n",
      "Epoch 22900: train loss: 0.47479724884033203\n",
      "Epoch 22901: train loss: 0.4747971296310425\n",
      "Epoch 22902: train loss: 0.4747970700263977\n",
      "Epoch 22903: train loss: 0.4747970402240753\n",
      "Epoch 22904: train loss: 0.47479698061943054\n",
      "Epoch 22905: train loss: 0.47479692101478577\n",
      "Epoch 22906: train loss: 0.474796861410141\n",
      "Epoch 22907: train loss: 0.4747968316078186\n",
      "Epoch 22908: train loss: 0.4747968316078186\n",
      "Epoch 22909: train loss: 0.47479677200317383\n",
      "Epoch 22910: train loss: 0.47479671239852905\n",
      "Epoch 22911: train loss: 0.4747966229915619\n",
      "Epoch 22912: train loss: 0.4747965633869171\n",
      "Epoch 22913: train loss: 0.4747965633869171\n",
      "Epoch 22914: train loss: 0.4747964143753052\n",
      "Epoch 22915: train loss: 0.47479644417762756\n",
      "Epoch 22916: train loss: 0.4747963547706604\n",
      "Epoch 22917: train loss: 0.47479623556137085\n",
      "Epoch 22918: train loss: 0.47479623556137085\n",
      "Epoch 22919: train loss: 0.4747961759567261\n",
      "Epoch 22920: train loss: 0.4747961461544037\n",
      "Epoch 22921: train loss: 0.4747960865497589\n",
      "Epoch 22922: train loss: 0.47479602694511414\n",
      "Epoch 22923: train loss: 0.47479596734046936\n",
      "Epoch 22924: train loss: 0.474795937538147\n",
      "Epoch 22925: train loss: 0.4747958779335022\n",
      "Epoch 22926: train loss: 0.47479575872421265\n",
      "Epoch 22927: train loss: 0.47479575872421265\n",
      "Epoch 22928: train loss: 0.47479572892189026\n",
      "Epoch 22929: train loss: 0.4747956693172455\n",
      "Epoch 22930: train loss: 0.4747956097126007\n",
      "Epoch 22931: train loss: 0.47479555010795593\n",
      "Epoch 22932: train loss: 0.47479546070098877\n",
      "Epoch 22933: train loss: 0.47479546070098877\n",
      "Epoch 22934: train loss: 0.474795401096344\n",
      "Epoch 22935: train loss: 0.474795401096344\n",
      "Epoch 22936: train loss: 0.47479531168937683\n",
      "Epoch 22937: train loss: 0.47479525208473206\n",
      "Epoch 22938: train loss: 0.4747951328754425\n",
      "Epoch 22939: train loss: 0.4747951030731201\n",
      "Epoch 22940: train loss: 0.4747951030731201\n",
      "Epoch 22941: train loss: 0.4747951030731201\n",
      "Epoch 22942: train loss: 0.47479498386383057\n",
      "Epoch 22943: train loss: 0.4747948944568634\n",
      "Epoch 22944: train loss: 0.4747948944568634\n",
      "Epoch 22945: train loss: 0.47479483485221863\n",
      "Epoch 22946: train loss: 0.47479477524757385\n",
      "Epoch 22947: train loss: 0.4747947156429291\n",
      "Epoch 22948: train loss: 0.4747946262359619\n",
      "Epoch 22949: train loss: 0.4747946262359619\n",
      "Epoch 22950: train loss: 0.47479456663131714\n",
      "Epoch 22951: train loss: 0.47479450702667236\n",
      "Epoch 22952: train loss: 0.47479447722435\n",
      "Epoch 22953: train loss: 0.4747943580150604\n",
      "Epoch 22954: train loss: 0.47479429841041565\n",
      "Epoch 22955: train loss: 0.47479426860809326\n",
      "Epoch 22956: train loss: 0.47479426860809326\n",
      "Epoch 22957: train loss: 0.4747942090034485\n",
      "Epoch 22958: train loss: 0.4747941493988037\n",
      "Epoch 22959: train loss: 0.47479405999183655\n",
      "Epoch 22960: train loss: 0.4747940003871918\n",
      "Epoch 22961: train loss: 0.4747940003871918\n",
      "Epoch 22962: train loss: 0.4747938811779022\n",
      "Epoch 22963: train loss: 0.4747938811779022\n",
      "Epoch 22964: train loss: 0.47479385137557983\n",
      "Epoch 22965: train loss: 0.4747937321662903\n",
      "Epoch 22966: train loss: 0.4747936427593231\n",
      "Epoch 22967: train loss: 0.4747936427593231\n",
      "Epoch 22968: train loss: 0.4747936427593231\n",
      "Epoch 22969: train loss: 0.47479358315467834\n",
      "Epoch 22970: train loss: 0.4747934639453888\n",
      "Epoch 22971: train loss: 0.4747934341430664\n",
      "Epoch 22972: train loss: 0.4747934341430664\n",
      "Epoch 22973: train loss: 0.47479331493377686\n",
      "Epoch 22974: train loss: 0.4747932553291321\n",
      "Epoch 22975: train loss: 0.4747932553291321\n",
      "Epoch 22976: train loss: 0.4747932255268097\n",
      "Epoch 22977: train loss: 0.4747931659221649\n",
      "Epoch 22978: train loss: 0.47479304671287537\n",
      "Epoch 22979: train loss: 0.474793016910553\n",
      "Epoch 22980: train loss: 0.474793016910553\n",
      "Epoch 22981: train loss: 0.4747928977012634\n",
      "Epoch 22982: train loss: 0.47479280829429626\n",
      "Epoch 22983: train loss: 0.47479283809661865\n",
      "Epoch 22984: train loss: 0.4747927486896515\n",
      "Epoch 22985: train loss: 0.4747926890850067\n",
      "Epoch 22986: train loss: 0.47479262948036194\n",
      "Epoch 22987: train loss: 0.47479259967803955\n",
      "Epoch 22988: train loss: 0.4747925400733948\n",
      "Epoch 22989: train loss: 0.47479248046875\n",
      "Epoch 22990: train loss: 0.4747924208641052\n",
      "Epoch 22991: train loss: 0.47479236125946045\n",
      "Epoch 22992: train loss: 0.47479236125946045\n",
      "Epoch 22993: train loss: 0.4747922718524933\n",
      "Epoch 22994: train loss: 0.4747922122478485\n",
      "Epoch 22995: train loss: 0.47479215264320374\n",
      "Epoch 22996: train loss: 0.47479215264320374\n",
      "Epoch 22997: train loss: 0.4747920632362366\n",
      "Epoch 22998: train loss: 0.4747920036315918\n",
      "Epoch 22999: train loss: 0.474791944026947\n",
      "Epoch 23000: train loss: 0.47479191422462463\n",
      "Epoch 23001: train loss: 0.47479185461997986\n",
      "Epoch 23002: train loss: 0.4747917950153351\n",
      "Epoch 23003: train loss: 0.4747917354106903\n",
      "Epoch 23004: train loss: 0.4747917056083679\n",
      "Epoch 23005: train loss: 0.47479164600372314\n",
      "Epoch 23006: train loss: 0.47479158639907837\n",
      "Epoch 23007: train loss: 0.4747915267944336\n",
      "Epoch 23008: train loss: 0.4747914969921112\n",
      "Epoch 23009: train loss: 0.4747914969921112\n",
      "Epoch 23010: train loss: 0.47479137778282166\n",
      "Epoch 23011: train loss: 0.4747913181781769\n",
      "Epoch 23012: train loss: 0.4747912883758545\n",
      "Epoch 23013: train loss: 0.4747912883758545\n",
      "Epoch 23014: train loss: 0.47479116916656494\n",
      "Epoch 23015: train loss: 0.47479110956192017\n",
      "Epoch 23016: train loss: 0.4747910797595978\n",
      "Epoch 23017: train loss: 0.474791020154953\n",
      "Epoch 23018: train loss: 0.4747909605503082\n",
      "Epoch 23019: train loss: 0.47479090094566345\n",
      "Epoch 23020: train loss: 0.4747908115386963\n",
      "Epoch 23021: train loss: 0.4747908115386963\n",
      "Epoch 23022: train loss: 0.4747907519340515\n",
      "Epoch 23023: train loss: 0.47479066252708435\n",
      "Epoch 23024: train loss: 0.4747906029224396\n",
      "Epoch 23025: train loss: 0.4747905433177948\n",
      "Epoch 23026: train loss: 0.4747905433177948\n",
      "Epoch 23027: train loss: 0.47479048371315\n",
      "Epoch 23028: train loss: 0.47479045391082764\n",
      "Epoch 23029: train loss: 0.47479045391082764\n",
      "Epoch 23030: train loss: 0.4747902750968933\n",
      "Epoch 23031: train loss: 0.4747902750968933\n",
      "Epoch 23032: train loss: 0.4747902452945709\n",
      "Epoch 23033: train loss: 0.47479018568992615\n",
      "Epoch 23034: train loss: 0.4747900664806366\n",
      "Epoch 23035: train loss: 0.4747900664806366\n",
      "Epoch 23036: train loss: 0.4747900664806366\n",
      "Epoch 23037: train loss: 0.47478997707366943\n",
      "Epoch 23038: train loss: 0.47478991746902466\n",
      "Epoch 23039: train loss: 0.4747898578643799\n",
      "Epoch 23040: train loss: 0.4747898578643799\n",
      "Epoch 23041: train loss: 0.4747897684574127\n",
      "Epoch 23042: train loss: 0.47478964924812317\n",
      "Epoch 23043: train loss: 0.4747896194458008\n",
      "Epoch 23044: train loss: 0.4747896194458008\n",
      "Epoch 23045: train loss: 0.474789559841156\n",
      "Epoch 23046: train loss: 0.47478950023651123\n",
      "Epoch 23047: train loss: 0.47478944063186646\n",
      "Epoch 23048: train loss: 0.47478941082954407\n",
      "Epoch 23049: train loss: 0.4747893512248993\n",
      "Epoch 23050: train loss: 0.47478923201560974\n",
      "Epoch 23051: train loss: 0.47478920221328735\n",
      "Epoch 23052: train loss: 0.47478920221328735\n",
      "Epoch 23053: train loss: 0.4747890830039978\n",
      "Epoch 23054: train loss: 0.474789023399353\n",
      "Epoch 23055: train loss: 0.474789023399353\n",
      "Epoch 23056: train loss: 0.47478899359703064\n",
      "Epoch 23057: train loss: 0.47478893399238586\n",
      "Epoch 23058: train loss: 0.4747888743877411\n",
      "Epoch 23059: train loss: 0.4747888147830963\n",
      "Epoch 23060: train loss: 0.47478872537612915\n",
      "Epoch 23061: train loss: 0.47478872537612915\n",
      "Epoch 23062: train loss: 0.4747886061668396\n",
      "Epoch 23063: train loss: 0.4747885465621948\n",
      "Epoch 23064: train loss: 0.47478851675987244\n",
      "Epoch 23065: train loss: 0.47478845715522766\n",
      "Epoch 23066: train loss: 0.4747883975505829\n",
      "Epoch 23067: train loss: 0.4747883975505829\n",
      "Epoch 23068: train loss: 0.4747883081436157\n",
      "Epoch 23069: train loss: 0.4747883081436157\n",
      "Epoch 23070: train loss: 0.47478824853897095\n",
      "Epoch 23071: train loss: 0.4747881293296814\n",
      "Epoch 23072: train loss: 0.4747881293296814\n",
      "Epoch 23073: train loss: 0.474788099527359\n",
      "Epoch 23074: train loss: 0.47478798031806946\n",
      "Epoch 23075: train loss: 0.4747879207134247\n",
      "Epoch 23076: train loss: 0.4747878909111023\n",
      "Epoch 23077: train loss: 0.47478777170181274\n",
      "Epoch 23078: train loss: 0.4747878909111023\n",
      "Epoch 23079: train loss: 0.47478777170181274\n",
      "Epoch 23080: train loss: 0.4747876822948456\n",
      "Epoch 23081: train loss: 0.4747876226902008\n",
      "Epoch 23082: train loss: 0.4747876226902008\n",
      "Epoch 23083: train loss: 0.47478750348091125\n",
      "Epoch 23084: train loss: 0.47478747367858887\n",
      "Epoch 23085: train loss: 0.4747873544692993\n",
      "Epoch 23086: train loss: 0.4747874140739441\n",
      "Epoch 23087: train loss: 0.4747873544692993\n",
      "Epoch 23088: train loss: 0.47478726506233215\n",
      "Epoch 23089: train loss: 0.4747872054576874\n",
      "Epoch 23090: train loss: 0.4747871458530426\n",
      "Epoch 23091: train loss: 0.4747871458530426\n",
      "Epoch 23092: train loss: 0.47478705644607544\n",
      "Epoch 23093: train loss: 0.47478699684143066\n",
      "Epoch 23094: train loss: 0.4747869372367859\n",
      "Epoch 23095: train loss: 0.4747868776321411\n",
      "Epoch 23096: train loss: 0.47478678822517395\n",
      "Epoch 23097: train loss: 0.4747868478298187\n",
      "Epoch 23098: train loss: 0.4747867286205292\n",
      "Epoch 23099: train loss: 0.4747866690158844\n",
      "Epoch 23100: train loss: 0.474786639213562\n",
      "Epoch 23101: train loss: 0.474786639213562\n",
      "Epoch 23102: train loss: 0.47478652000427246\n",
      "Epoch 23103: train loss: 0.47478652000427246\n",
      "Epoch 23104: train loss: 0.4747864305973053\n",
      "Epoch 23105: train loss: 0.4747864305973053\n",
      "Epoch 23106: train loss: 0.47478631138801575\n",
      "Epoch 23107: train loss: 0.47478631138801575\n",
      "Epoch 23108: train loss: 0.4747861623764038\n",
      "Epoch 23109: train loss: 0.4747861623764038\n",
      "Epoch 23110: train loss: 0.47478610277175903\n",
      "Epoch 23111: train loss: 0.47478610277175903\n",
      "Epoch 23112: train loss: 0.47478601336479187\n",
      "Epoch 23113: train loss: 0.4747858941555023\n",
      "Epoch 23114: train loss: 0.4747859537601471\n",
      "Epoch 23115: train loss: 0.47478580474853516\n",
      "Epoch 23116: train loss: 0.47478583455085754\n",
      "Epoch 23117: train loss: 0.4747857451438904\n",
      "Epoch 23118: train loss: 0.4747856855392456\n",
      "Epoch 23119: train loss: 0.47478562593460083\n",
      "Epoch 23120: train loss: 0.47478553652763367\n",
      "Epoch 23121: train loss: 0.47478553652763367\n",
      "Epoch 23122: train loss: 0.4747854769229889\n",
      "Epoch 23123: train loss: 0.4747854173183441\n",
      "Epoch 23124: train loss: 0.47478538751602173\n",
      "Epoch 23125: train loss: 0.4747852683067322\n",
      "Epoch 23126: train loss: 0.4747852683067322\n",
      "Epoch 23127: train loss: 0.4747852683067322\n",
      "Epoch 23128: train loss: 0.474785178899765\n",
      "Epoch 23129: train loss: 0.47478505969047546\n",
      "Epoch 23130: train loss: 0.47478505969047546\n",
      "Epoch 23131: train loss: 0.4747849702835083\n",
      "Epoch 23132: train loss: 0.4747849702835083\n",
      "Epoch 23133: train loss: 0.47478485107421875\n",
      "Epoch 23134: train loss: 0.474784791469574\n",
      "Epoch 23135: train loss: 0.474784791469574\n",
      "Epoch 23136: train loss: 0.4747847318649292\n",
      "Epoch 23137: train loss: 0.4747847020626068\n",
      "Epoch 23138: train loss: 0.47478458285331726\n",
      "Epoch 23139: train loss: 0.47478458285331726\n",
      "Epoch 23140: train loss: 0.4747844934463501\n",
      "Epoch 23141: train loss: 0.4747844934463501\n",
      "Epoch 23142: train loss: 0.4747844338417053\n",
      "Epoch 23143: train loss: 0.47478437423706055\n",
      "Epoch 23144: train loss: 0.47478431463241577\n",
      "Epoch 23145: train loss: 0.4747842252254486\n",
      "Epoch 23146: train loss: 0.4747842252254486\n",
      "Epoch 23147: train loss: 0.47478410601615906\n",
      "Epoch 23148: train loss: 0.47478410601615906\n",
      "Epoch 23149: train loss: 0.47478407621383667\n",
      "Epoch 23150: train loss: 0.4747840166091919\n",
      "Epoch 23151: train loss: 0.4747839570045471\n",
      "Epoch 23152: train loss: 0.47478386759757996\n",
      "Epoch 23153: train loss: 0.47478389739990234\n",
      "Epoch 23154: train loss: 0.4747837483882904\n",
      "Epoch 23155: train loss: 0.4747837483882904\n",
      "Epoch 23156: train loss: 0.47478365898132324\n",
      "Epoch 23157: train loss: 0.47478365898132324\n",
      "Epoch 23158: train loss: 0.47478359937667847\n",
      "Epoch 23159: train loss: 0.4747834801673889\n",
      "Epoch 23160: train loss: 0.47478345036506653\n",
      "Epoch 23161: train loss: 0.47478339076042175\n",
      "Epoch 23162: train loss: 0.474783331155777\n",
      "Epoch 23163: train loss: 0.4747832715511322\n",
      "Epoch 23164: train loss: 0.4747832715511322\n",
      "Epoch 23165: train loss: 0.47478318214416504\n",
      "Epoch 23166: train loss: 0.47478312253952026\n",
      "Epoch 23167: train loss: 0.4747830629348755\n",
      "Epoch 23168: train loss: 0.4747830331325531\n",
      "Epoch 23169: train loss: 0.4747829735279083\n",
      "Epoch 23170: train loss: 0.4747829735279083\n",
      "Epoch 23171: train loss: 0.4747828543186188\n",
      "Epoch 23172: train loss: 0.4747828543186188\n",
      "Epoch 23173: train loss: 0.4747827649116516\n",
      "Epoch 23174: train loss: 0.47478270530700684\n",
      "Epoch 23175: train loss: 0.47478270530700684\n",
      "Epoch 23176: train loss: 0.47478264570236206\n",
      "Epoch 23177: train loss: 0.4747825562953949\n",
      "Epoch 23178: train loss: 0.4747826159000397\n",
      "Epoch 23179: train loss: 0.4747824966907501\n",
      "Epoch 23180: train loss: 0.47478240728378296\n",
      "Epoch 23181: train loss: 0.47478240728378296\n",
      "Epoch 23182: train loss: 0.4747823476791382\n",
      "Epoch 23183: train loss: 0.47478222846984863\n",
      "Epoch 23184: train loss: 0.47478222846984863\n",
      "Epoch 23185: train loss: 0.47478219866752625\n",
      "Epoch 23186: train loss: 0.4747820794582367\n",
      "Epoch 23187: train loss: 0.4747820198535919\n",
      "Epoch 23188: train loss: 0.4747820198535919\n",
      "Epoch 23189: train loss: 0.47478193044662476\n",
      "Epoch 23190: train loss: 0.47478187084198\n",
      "Epoch 23191: train loss: 0.47478187084198\n",
      "Epoch 23192: train loss: 0.4747818112373352\n",
      "Epoch 23193: train loss: 0.47478172183036804\n",
      "Epoch 23194: train loss: 0.47478172183036804\n",
      "Epoch 23195: train loss: 0.4747816026210785\n",
      "Epoch 23196: train loss: 0.4747816026210785\n",
      "Epoch 23197: train loss: 0.47478151321411133\n",
      "Epoch 23198: train loss: 0.47478145360946655\n",
      "Epoch 23199: train loss: 0.47478145360946655\n",
      "Epoch 23200: train loss: 0.4747813642024994\n",
      "Epoch 23201: train loss: 0.4747813045978546\n",
      "Epoch 23202: train loss: 0.4747813045978546\n",
      "Epoch 23203: train loss: 0.47478118538856506\n",
      "Epoch 23204: train loss: 0.4747811555862427\n",
      "Epoch 23205: train loss: 0.4747810959815979\n",
      "Epoch 23206: train loss: 0.4747810363769531\n",
      "Epoch 23207: train loss: 0.47478097677230835\n",
      "Epoch 23208: train loss: 0.4747809171676636\n",
      "Epoch 23209: train loss: 0.4747809171676636\n",
      "Epoch 23210: train loss: 0.4747808277606964\n",
      "Epoch 23211: train loss: 0.47478076815605164\n",
      "Epoch 23212: train loss: 0.47478070855140686\n",
      "Epoch 23213: train loss: 0.4747806191444397\n",
      "Epoch 23214: train loss: 0.4747806191444397\n",
      "Epoch 23215: train loss: 0.4747805595397949\n",
      "Epoch 23216: train loss: 0.47478049993515015\n",
      "Epoch 23217: train loss: 0.47478047013282776\n",
      "Epoch 23218: train loss: 0.4747803509235382\n",
      "Epoch 23219: train loss: 0.4747803509235382\n",
      "Epoch 23220: train loss: 0.47478026151657104\n",
      "Epoch 23221: train loss: 0.47478026151657104\n",
      "Epoch 23222: train loss: 0.47478020191192627\n",
      "Epoch 23223: train loss: 0.4747801423072815\n",
      "Epoch 23224: train loss: 0.4747800827026367\n",
      "Epoch 23225: train loss: 0.4747800827026367\n",
      "Epoch 23226: train loss: 0.47477999329566956\n",
      "Epoch 23227: train loss: 0.4747799336910248\n",
      "Epoch 23228: train loss: 0.47477987408638\n",
      "Epoch 23229: train loss: 0.4747798442840576\n",
      "Epoch 23230: train loss: 0.47477978467941284\n",
      "Epoch 23231: train loss: 0.47477972507476807\n",
      "Epoch 23232: train loss: 0.4747796654701233\n",
      "Epoch 23233: train loss: 0.4747796356678009\n",
      "Epoch 23234: train loss: 0.47477957606315613\n",
      "Epoch 23235: train loss: 0.47477951645851135\n",
      "Epoch 23236: train loss: 0.47477951645851135\n",
      "Epoch 23237: train loss: 0.4747794270515442\n",
      "Epoch 23238: train loss: 0.4747793674468994\n",
      "Epoch 23239: train loss: 0.47477930784225464\n",
      "Epoch 23240: train loss: 0.47477924823760986\n",
      "Epoch 23241: train loss: 0.4747792184352875\n",
      "Epoch 23242: train loss: 0.4747790992259979\n",
      "Epoch 23243: train loss: 0.4747790992259979\n",
      "Epoch 23244: train loss: 0.47477903962135315\n",
      "Epoch 23245: train loss: 0.47477900981903076\n",
      "Epoch 23246: train loss: 0.474778950214386\n",
      "Epoch 23247: train loss: 0.4747788906097412\n",
      "Epoch 23248: train loss: 0.47477883100509644\n",
      "Epoch 23249: train loss: 0.47477880120277405\n",
      "Epoch 23250: train loss: 0.4747787415981293\n",
      "Epoch 23251: train loss: 0.4747786819934845\n",
      "Epoch 23252: train loss: 0.4747786223888397\n",
      "Epoch 23253: train loss: 0.47477859258651733\n",
      "Epoch 23254: train loss: 0.47477859258651733\n",
      "Epoch 23255: train loss: 0.4747784733772278\n",
      "Epoch 23256: train loss: 0.4747783839702606\n",
      "Epoch 23257: train loss: 0.47477832436561584\n",
      "Epoch 23258: train loss: 0.47477826476097107\n",
      "Epoch 23259: train loss: 0.4747782051563263\n",
      "Epoch 23260: train loss: 0.4747781753540039\n",
      "Epoch 23261: train loss: 0.4747781753540039\n",
      "Epoch 23262: train loss: 0.47477811574935913\n",
      "Epoch 23263: train loss: 0.47477805614471436\n",
      "Epoch 23264: train loss: 0.4747779965400696\n",
      "Epoch 23265: train loss: 0.4747779667377472\n",
      "Epoch 23266: train loss: 0.4747779071331024\n",
      "Epoch 23267: train loss: 0.47477778792381287\n",
      "Epoch 23268: train loss: 0.47477778792381287\n",
      "Epoch 23269: train loss: 0.4747777581214905\n",
      "Epoch 23270: train loss: 0.4747776389122009\n",
      "Epoch 23271: train loss: 0.4747776389122009\n",
      "Epoch 23272: train loss: 0.47477757930755615\n",
      "Epoch 23273: train loss: 0.47477754950523376\n",
      "Epoch 23274: train loss: 0.474777489900589\n",
      "Epoch 23275: train loss: 0.4747774302959442\n",
      "Epoch 23276: train loss: 0.47477734088897705\n",
      "Epoch 23277: train loss: 0.47477734088897705\n",
      "Epoch 23278: train loss: 0.4747772216796875\n",
      "Epoch 23279: train loss: 0.4747772216796875\n",
      "Epoch 23280: train loss: 0.47477710247039795\n",
      "Epoch 23281: train loss: 0.47477710247039795\n",
      "Epoch 23282: train loss: 0.47477707266807556\n",
      "Epoch 23283: train loss: 0.474776953458786\n",
      "Epoch 23284: train loss: 0.474776953458786\n",
      "Epoch 23285: train loss: 0.47477689385414124\n",
      "Epoch 23286: train loss: 0.47477686405181885\n",
      "Epoch 23287: train loss: 0.4747767448425293\n",
      "Epoch 23288: train loss: 0.4747767448425293\n",
      "Epoch 23289: train loss: 0.4747766852378845\n",
      "Epoch 23290: train loss: 0.47477659583091736\n",
      "Epoch 23291: train loss: 0.47477659583091736\n",
      "Epoch 23292: train loss: 0.4747765362262726\n",
      "Epoch 23293: train loss: 0.4747764766216278\n",
      "Epoch 23294: train loss: 0.4747764468193054\n",
      "Epoch 23295: train loss: 0.47477638721466064\n",
      "Epoch 23296: train loss: 0.4747762680053711\n",
      "Epoch 23297: train loss: 0.4747762680053711\n",
      "Epoch 23298: train loss: 0.4747762382030487\n",
      "Epoch 23299: train loss: 0.47477611899375916\n",
      "Epoch 23300: train loss: 0.47477611899375916\n",
      "Epoch 23301: train loss: 0.4747760593891144\n",
      "Epoch 23302: train loss: 0.474776029586792\n",
      "Epoch 23303: train loss: 0.4747759699821472\n",
      "Epoch 23304: train loss: 0.47477585077285767\n",
      "Epoch 23305: train loss: 0.47477585077285767\n",
      "Epoch 23306: train loss: 0.4747757613658905\n",
      "Epoch 23307: train loss: 0.4747757613658905\n",
      "Epoch 23308: train loss: 0.4747757017612457\n",
      "Epoch 23309: train loss: 0.4747757017612457\n",
      "Epoch 23310: train loss: 0.47477561235427856\n",
      "Epoch 23311: train loss: 0.474775493144989\n",
      "Epoch 23312: train loss: 0.474775493144989\n",
      "Epoch 23313: train loss: 0.47477543354034424\n",
      "Epoch 23314: train loss: 0.4747753441333771\n",
      "Epoch 23315: train loss: 0.4747752845287323\n",
      "Epoch 23316: train loss: 0.4747752845287323\n",
      "Epoch 23317: train loss: 0.4747752249240875\n",
      "Epoch 23318: train loss: 0.47477513551712036\n",
      "Epoch 23319: train loss: 0.4747750759124756\n",
      "Epoch 23320: train loss: 0.4747750163078308\n",
      "Epoch 23321: train loss: 0.4747750163078308\n",
      "Epoch 23322: train loss: 0.4747749865055084\n",
      "Epoch 23323: train loss: 0.47477486729621887\n",
      "Epoch 23324: train loss: 0.47477486729621887\n",
      "Epoch 23325: train loss: 0.4747748076915741\n",
      "Epoch 23326: train loss: 0.47477471828460693\n",
      "Epoch 23327: train loss: 0.47477465867996216\n",
      "Epoch 23328: train loss: 0.47477465867996216\n",
      "Epoch 23329: train loss: 0.47477465867996216\n",
      "Epoch 23330: train loss: 0.474774569272995\n",
      "Epoch 23331: train loss: 0.4747745096683502\n",
      "Epoch 23332: train loss: 0.47477439045906067\n",
      "Epoch 23333: train loss: 0.47477439045906067\n",
      "Epoch 23334: train loss: 0.4747743010520935\n",
      "Epoch 23335: train loss: 0.4747743010520935\n",
      "Epoch 23336: train loss: 0.47477418184280396\n",
      "Epoch 23337: train loss: 0.47477418184280396\n",
      "Epoch 23338: train loss: 0.4747740924358368\n",
      "Epoch 23339: train loss: 0.474774032831192\n",
      "Epoch 23340: train loss: 0.47477397322654724\n",
      "Epoch 23341: train loss: 0.47477394342422485\n",
      "Epoch 23342: train loss: 0.4747738838195801\n",
      "Epoch 23343: train loss: 0.4747738838195801\n",
      "Epoch 23344: train loss: 0.47477373480796814\n",
      "Epoch 23345: train loss: 0.4747737646102905\n",
      "Epoch 23346: train loss: 0.47477367520332336\n",
      "Epoch 23347: train loss: 0.47477367520332336\n",
      "Epoch 23348: train loss: 0.4747735559940338\n",
      "Epoch 23349: train loss: 0.4747735559940338\n",
      "Epoch 23350: train loss: 0.47477346658706665\n",
      "Epoch 23351: train loss: 0.4747734069824219\n",
      "Epoch 23352: train loss: 0.4747733473777771\n",
      "Epoch 23353: train loss: 0.4747732877731323\n",
      "Epoch 23354: train loss: 0.4747732877731323\n",
      "Epoch 23355: train loss: 0.47477325797080994\n",
      "Epoch 23356: train loss: 0.4747731387615204\n",
      "Epoch 23357: train loss: 0.4747730791568756\n",
      "Epoch 23358: train loss: 0.4747730493545532\n",
      "Epoch 23359: train loss: 0.4747730493545532\n",
      "Epoch 23360: train loss: 0.47477293014526367\n",
      "Epoch 23361: train loss: 0.4747728705406189\n",
      "Epoch 23362: train loss: 0.4747728705406189\n",
      "Epoch 23363: train loss: 0.4747728407382965\n",
      "Epoch 23364: train loss: 0.47477272152900696\n",
      "Epoch 23365: train loss: 0.4747726619243622\n",
      "Epoch 23366: train loss: 0.4747726619243622\n",
      "Epoch 23367: train loss: 0.474772572517395\n",
      "Epoch 23368: train loss: 0.47477251291275024\n",
      "Epoch 23369: train loss: 0.47477245330810547\n",
      "Epoch 23370: train loss: 0.4747724235057831\n",
      "Epoch 23371: train loss: 0.4747723639011383\n",
      "Epoch 23372: train loss: 0.47477230429649353\n",
      "Epoch 23373: train loss: 0.47477230429649353\n",
      "Epoch 23374: train loss: 0.47477221488952637\n",
      "Epoch 23375: train loss: 0.47477221488952637\n",
      "Epoch 23376: train loss: 0.4747721552848816\n",
      "Epoch 23377: train loss: 0.4747720956802368\n",
      "Epoch 23378: train loss: 0.47477200627326965\n",
      "Epoch 23379: train loss: 0.47477200627326965\n",
      "Epoch 23380: train loss: 0.4747718870639801\n",
      "Epoch 23381: train loss: 0.4747718870639801\n",
      "Epoch 23382: train loss: 0.4747718274593353\n",
      "Epoch 23383: train loss: 0.47477179765701294\n",
      "Epoch 23384: train loss: 0.4747716784477234\n",
      "Epoch 23385: train loss: 0.4747716188430786\n",
      "Epoch 23386: train loss: 0.4747715890407562\n",
      "Epoch 23387: train loss: 0.47477152943611145\n",
      "Epoch 23388: train loss: 0.47477152943611145\n",
      "Epoch 23389: train loss: 0.4747714102268219\n",
      "Epoch 23390: train loss: 0.4747713804244995\n",
      "Epoch 23391: train loss: 0.47477132081985474\n",
      "Epoch 23392: train loss: 0.47477126121520996\n",
      "Epoch 23393: train loss: 0.4747712016105652\n",
      "Epoch 23394: train loss: 0.4747711718082428\n",
      "Epoch 23395: train loss: 0.474771112203598\n",
      "Epoch 23396: train loss: 0.47477099299430847\n",
      "Epoch 23397: train loss: 0.47477099299430847\n",
      "Epoch 23398: train loss: 0.4747709035873413\n",
      "Epoch 23399: train loss: 0.4747709035873413\n",
      "Epoch 23400: train loss: 0.47477084398269653\n",
      "Epoch 23401: train loss: 0.47477078437805176\n",
      "Epoch 23402: train loss: 0.47477075457572937\n",
      "Epoch 23403: train loss: 0.4747706949710846\n",
      "Epoch 23404: train loss: 0.4747706949710846\n",
      "Epoch 23405: train loss: 0.47477054595947266\n",
      "Epoch 23406: train loss: 0.47477054595947266\n",
      "Epoch 23407: train loss: 0.4747704863548279\n",
      "Epoch 23408: train loss: 0.4747704267501831\n",
      "Epoch 23409: train loss: 0.4747704267501831\n",
      "Epoch 23410: train loss: 0.47477033734321594\n",
      "Epoch 23411: train loss: 0.47477027773857117\n",
      "Epoch 23412: train loss: 0.4747702181339264\n",
      "Epoch 23413: train loss: 0.4747701585292816\n",
      "Epoch 23414: train loss: 0.47477006912231445\n",
      "Epoch 23415: train loss: 0.47477006912231445\n",
      "Epoch 23416: train loss: 0.4747700095176697\n",
      "Epoch 23417: train loss: 0.4747699499130249\n",
      "Epoch 23418: train loss: 0.4747699201107025\n",
      "Epoch 23419: train loss: 0.47476986050605774\n",
      "Epoch 23420: train loss: 0.47476980090141296\n",
      "Epoch 23421: train loss: 0.4747697412967682\n",
      "Epoch 23422: train loss: 0.4747697114944458\n",
      "Epoch 23423: train loss: 0.474769651889801\n",
      "Epoch 23424: train loss: 0.47476959228515625\n",
      "Epoch 23425: train loss: 0.4747695326805115\n",
      "Epoch 23426: train loss: 0.4747694730758667\n",
      "Epoch 23427: train loss: 0.4747694432735443\n",
      "Epoch 23428: train loss: 0.47476938366889954\n",
      "Epoch 23429: train loss: 0.47476926445961\n",
      "Epoch 23430: train loss: 0.47476926445961\n",
      "Epoch 23431: train loss: 0.4747692346572876\n",
      "Epoch 23432: train loss: 0.4747691750526428\n",
      "Epoch 23433: train loss: 0.47476911544799805\n",
      "Epoch 23434: train loss: 0.47476905584335327\n",
      "Epoch 23435: train loss: 0.4747690260410309\n",
      "Epoch 23436: train loss: 0.4747689664363861\n",
      "Epoch 23437: train loss: 0.47476890683174133\n",
      "Epoch 23438: train loss: 0.47476884722709656\n",
      "Epoch 23439: train loss: 0.47476881742477417\n",
      "Epoch 23440: train loss: 0.4747687578201294\n",
      "Epoch 23441: train loss: 0.4747686982154846\n",
      "Epoch 23442: train loss: 0.47476863861083984\n",
      "Epoch 23443: train loss: 0.47476860880851746\n",
      "Epoch 23444: train loss: 0.47476860880851746\n",
      "Epoch 23445: train loss: 0.4747684895992279\n",
      "Epoch 23446: train loss: 0.47476842999458313\n",
      "Epoch 23447: train loss: 0.47476840019226074\n",
      "Epoch 23448: train loss: 0.47476834058761597\n",
      "Epoch 23449: train loss: 0.4747682809829712\n",
      "Epoch 23450: train loss: 0.4747682213783264\n",
      "Epoch 23451: train loss: 0.47476819157600403\n",
      "Epoch 23452: train loss: 0.47476813197135925\n",
      "Epoch 23453: train loss: 0.4747680723667145\n",
      "Epoch 23454: train loss: 0.4747680127620697\n",
      "Epoch 23455: train loss: 0.4747679829597473\n",
      "Epoch 23456: train loss: 0.47476792335510254\n",
      "Epoch 23457: train loss: 0.47476786375045776\n",
      "Epoch 23458: train loss: 0.474767804145813\n",
      "Epoch 23459: train loss: 0.4747677743434906\n",
      "Epoch 23460: train loss: 0.4747677147388458\n",
      "Epoch 23461: train loss: 0.4747675955295563\n",
      "Epoch 23462: train loss: 0.47476765513420105\n",
      "Epoch 23463: train loss: 0.4747675657272339\n",
      "Epoch 23464: train loss: 0.4747675061225891\n",
      "Epoch 23465: train loss: 0.47476744651794434\n",
      "Epoch 23466: train loss: 0.47476738691329956\n",
      "Epoch 23467: train loss: 0.4747673571109772\n",
      "Epoch 23468: train loss: 0.4747672975063324\n",
      "Epoch 23469: train loss: 0.4747672379016876\n",
      "Epoch 23470: train loss: 0.47476717829704285\n",
      "Epoch 23471: train loss: 0.47476714849472046\n",
      "Epoch 23472: train loss: 0.4747670292854309\n",
      "Epoch 23473: train loss: 0.4747670292854309\n",
      "Epoch 23474: train loss: 0.47476696968078613\n",
      "Epoch 23475: train loss: 0.47476696968078613\n",
      "Epoch 23476: train loss: 0.47476688027381897\n",
      "Epoch 23477: train loss: 0.47476688027381897\n",
      "Epoch 23478: train loss: 0.4747667610645294\n",
      "Epoch 23479: train loss: 0.47476673126220703\n",
      "Epoch 23480: train loss: 0.47476673126220703\n",
      "Epoch 23481: train loss: 0.47476667165756226\n",
      "Epoch 23482: train loss: 0.4747665524482727\n",
      "Epoch 23483: train loss: 0.4747665226459503\n",
      "Epoch 23484: train loss: 0.47476646304130554\n",
      "Epoch 23485: train loss: 0.47476640343666077\n",
      "Epoch 23486: train loss: 0.474766343832016\n",
      "Epoch 23487: train loss: 0.4747663140296936\n",
      "Epoch 23488: train loss: 0.47476625442504883\n",
      "Epoch 23489: train loss: 0.4747661352157593\n",
      "Epoch 23490: train loss: 0.47476619482040405\n",
      "Epoch 23491: train loss: 0.4747660458087921\n",
      "Epoch 23492: train loss: 0.4747660458087921\n",
      "Epoch 23493: train loss: 0.47476598620414734\n",
      "Epoch 23494: train loss: 0.47476592659950256\n",
      "Epoch 23495: train loss: 0.4747658371925354\n",
      "Epoch 23496: train loss: 0.4747658371925354\n",
      "Epoch 23497: train loss: 0.4747657775878906\n",
      "Epoch 23498: train loss: 0.4747657775878906\n",
      "Epoch 23499: train loss: 0.4747656583786011\n",
      "Epoch 23500: train loss: 0.4747656285762787\n",
      "Epoch 23501: train loss: 0.4747655689716339\n",
      "Epoch 23502: train loss: 0.47476550936698914\n",
      "Epoch 23503: train loss: 0.47476544976234436\n",
      "Epoch 23504: train loss: 0.474765419960022\n",
      "Epoch 23505: train loss: 0.4747653603553772\n",
      "Epoch 23506: train loss: 0.4747653007507324\n",
      "Epoch 23507: train loss: 0.47476521134376526\n",
      "Epoch 23508: train loss: 0.47476521134376526\n",
      "Epoch 23509: train loss: 0.4747651517391205\n",
      "Epoch 23510: train loss: 0.4747650921344757\n",
      "Epoch 23511: train loss: 0.47476503252983093\n",
      "Epoch 23512: train loss: 0.47476500272750854\n",
      "Epoch 23513: train loss: 0.474764883518219\n",
      "Epoch 23514: train loss: 0.474764883518219\n",
      "Epoch 23515: train loss: 0.47476479411125183\n",
      "Epoch 23516: train loss: 0.47476473450660706\n",
      "Epoch 23517: train loss: 0.47476473450660706\n",
      "Epoch 23518: train loss: 0.4747646152973175\n",
      "Epoch 23519: train loss: 0.4747646152973175\n",
      "Epoch 23520: train loss: 0.4747645854949951\n",
      "Epoch 23521: train loss: 0.47476446628570557\n",
      "Epoch 23522: train loss: 0.47476446628570557\n",
      "Epoch 23523: train loss: 0.4747644066810608\n",
      "Epoch 23524: train loss: 0.4747643768787384\n",
      "Epoch 23525: train loss: 0.47476431727409363\n",
      "Epoch 23526: train loss: 0.47476425766944885\n",
      "Epoch 23527: train loss: 0.4747641980648041\n",
      "Epoch 23528: train loss: 0.4747641682624817\n",
      "Epoch 23529: train loss: 0.47476404905319214\n",
      "Epoch 23530: train loss: 0.47476404905319214\n",
      "Epoch 23531: train loss: 0.47476404905319214\n",
      "Epoch 23532: train loss: 0.47476398944854736\n",
      "Epoch 23533: train loss: 0.4747639000415802\n",
      "Epoch 23534: train loss: 0.4747638404369354\n",
      "Epoch 23535: train loss: 0.47476378083229065\n",
      "Epoch 23536: train loss: 0.47476375102996826\n",
      "Epoch 23537: train loss: 0.4747636318206787\n",
      "Epoch 23538: train loss: 0.4747636318206787\n",
      "Epoch 23539: train loss: 0.47476357221603394\n",
      "Epoch 23540: train loss: 0.47476354241371155\n",
      "Epoch 23541: train loss: 0.4747634828090668\n",
      "Epoch 23542: train loss: 0.474763423204422\n",
      "Epoch 23543: train loss: 0.4747633635997772\n",
      "Epoch 23544: train loss: 0.47476333379745483\n",
      "Epoch 23545: train loss: 0.47476327419281006\n",
      "Epoch 23546: train loss: 0.4747632145881653\n",
      "Epoch 23547: train loss: 0.4747631251811981\n",
      "Epoch 23548: train loss: 0.4747631251811981\n",
      "Epoch 23549: train loss: 0.47476306557655334\n",
      "Epoch 23550: train loss: 0.47476300597190857\n",
      "Epoch 23551: train loss: 0.4747629463672638\n",
      "Epoch 23552: train loss: 0.4747629165649414\n",
      "Epoch 23553: train loss: 0.47476285696029663\n",
      "Epoch 23554: train loss: 0.4747627377510071\n",
      "Epoch 23555: train loss: 0.4747627079486847\n",
      "Epoch 23556: train loss: 0.4747627079486847\n",
      "Epoch 23557: train loss: 0.4747626483440399\n",
      "Epoch 23558: train loss: 0.47476258873939514\n",
      "Epoch 23559: train loss: 0.47476258873939514\n",
      "Epoch 23560: train loss: 0.474762499332428\n",
      "Epoch 23561: train loss: 0.4747624397277832\n",
      "Epoch 23562: train loss: 0.4747623801231384\n",
      "Epoch 23563: train loss: 0.4747623801231384\n",
      "Epoch 23564: train loss: 0.47476229071617126\n",
      "Epoch 23565: train loss: 0.4747621715068817\n",
      "Epoch 23566: train loss: 0.4747621715068817\n",
      "Epoch 23567: train loss: 0.47476208209991455\n",
      "Epoch 23568: train loss: 0.47476208209991455\n",
      "Epoch 23569: train loss: 0.47476208209991455\n",
      "Epoch 23570: train loss: 0.4747620224952698\n",
      "Epoch 23571: train loss: 0.4747619032859802\n",
      "Epoch 23572: train loss: 0.47476181387901306\n",
      "Epoch 23573: train loss: 0.4747617542743683\n",
      "Epoch 23574: train loss: 0.4747617542743683\n",
      "Epoch 23575: train loss: 0.47476163506507874\n",
      "Epoch 23576: train loss: 0.47476163506507874\n",
      "Epoch 23577: train loss: 0.47476160526275635\n",
      "Epoch 23578: train loss: 0.4747615456581116\n",
      "Epoch 23579: train loss: 0.4747614860534668\n",
      "Epoch 23580: train loss: 0.474761426448822\n",
      "Epoch 23581: train loss: 0.47476139664649963\n",
      "Epoch 23582: train loss: 0.47476133704185486\n",
      "Epoch 23583: train loss: 0.4747612774372101\n",
      "Epoch 23584: train loss: 0.4747612178325653\n",
      "Epoch 23585: train loss: 0.4747611880302429\n",
      "Epoch 23586: train loss: 0.47476112842559814\n",
      "Epoch 23587: train loss: 0.4747610092163086\n",
      "Epoch 23588: train loss: 0.4747610092163086\n",
      "Epoch 23589: train loss: 0.4747609794139862\n",
      "Epoch 23590: train loss: 0.47476091980934143\n",
      "Epoch 23591: train loss: 0.47476086020469666\n",
      "Epoch 23592: train loss: 0.4747608006000519\n",
      "Epoch 23593: train loss: 0.4747608006000519\n",
      "Epoch 23594: train loss: 0.4747607111930847\n",
      "Epoch 23595: train loss: 0.4747607111930847\n",
      "Epoch 23596: train loss: 0.47476059198379517\n",
      "Epoch 23597: train loss: 0.47476059198379517\n",
      "Epoch 23598: train loss: 0.4747605621814728\n",
      "Epoch 23599: train loss: 0.474760502576828\n",
      "Epoch 23600: train loss: 0.4747604429721832\n",
      "Epoch 23601: train loss: 0.47476038336753845\n",
      "Epoch 23602: train loss: 0.4747602939605713\n",
      "Epoch 23603: train loss: 0.4747602343559265\n",
      "Epoch 23604: train loss: 0.47476017475128174\n",
      "Epoch 23605: train loss: 0.4747600853443146\n",
      "Epoch 23606: train loss: 0.4747600853443146\n",
      "Epoch 23607: train loss: 0.4747600257396698\n",
      "Epoch 23608: train loss: 0.474759966135025\n",
      "Epoch 23609: train loss: 0.47475993633270264\n",
      "Epoch 23610: train loss: 0.47475987672805786\n",
      "Epoch 23611: train loss: 0.4747598171234131\n",
      "Epoch 23612: train loss: 0.4747597575187683\n",
      "Epoch 23613: train loss: 0.4747597277164459\n",
      "Epoch 23614: train loss: 0.47475966811180115\n",
      "Epoch 23615: train loss: 0.47475966811180115\n",
      "Epoch 23616: train loss: 0.47475960850715637\n",
      "Epoch 23617: train loss: 0.4747595191001892\n",
      "Epoch 23618: train loss: 0.47475939989089966\n",
      "Epoch 23619: train loss: 0.4747593402862549\n",
      "Epoch 23620: train loss: 0.4747593402862549\n",
      "Epoch 23621: train loss: 0.4747592508792877\n",
      "Epoch 23622: train loss: 0.4747592508792877\n",
      "Epoch 23623: train loss: 0.47475913166999817\n",
      "Epoch 23624: train loss: 0.4747591018676758\n",
      "Epoch 23625: train loss: 0.4747591018676758\n",
      "Epoch 23626: train loss: 0.47475898265838623\n",
      "Epoch 23627: train loss: 0.47475898265838623\n",
      "Epoch 23628: train loss: 0.47475892305374146\n",
      "Epoch 23629: train loss: 0.47475889325141907\n",
      "Epoch 23630: train loss: 0.4747588336467743\n",
      "Epoch 23631: train loss: 0.4747588336467743\n",
      "Epoch 23632: train loss: 0.47475871443748474\n",
      "Epoch 23633: train loss: 0.47475871443748474\n",
      "Epoch 23634: train loss: 0.4747586250305176\n",
      "Epoch 23635: train loss: 0.4747585654258728\n",
      "Epoch 23636: train loss: 0.474758505821228\n",
      "Epoch 23637: train loss: 0.47475847601890564\n",
      "Epoch 23638: train loss: 0.4747583568096161\n",
      "Epoch 23639: train loss: 0.4747582972049713\n",
      "Epoch 23640: train loss: 0.4747582972049713\n",
      "Epoch 23641: train loss: 0.4747582674026489\n",
      "Epoch 23642: train loss: 0.47475820779800415\n",
      "Epoch 23643: train loss: 0.4747581481933594\n",
      "Epoch 23644: train loss: 0.4747580885887146\n",
      "Epoch 23645: train loss: 0.4747580289840698\n",
      "Epoch 23646: train loss: 0.47475799918174744\n",
      "Epoch 23647: train loss: 0.4747578799724579\n",
      "Epoch 23648: train loss: 0.4747578799724579\n",
      "Epoch 23649: train loss: 0.4747578203678131\n",
      "Epoch 23650: train loss: 0.4747577905654907\n",
      "Epoch 23651: train loss: 0.47475767135620117\n",
      "Epoch 23652: train loss: 0.47475767135620117\n",
      "Epoch 23653: train loss: 0.4747576117515564\n",
      "Epoch 23654: train loss: 0.4747576117515564\n",
      "Epoch 23655: train loss: 0.47475752234458923\n",
      "Epoch 23656: train loss: 0.47475746273994446\n",
      "Epoch 23657: train loss: 0.4747574031352997\n",
      "Epoch 23658: train loss: 0.4747573733329773\n",
      "Epoch 23659: train loss: 0.4747573137283325\n",
      "Epoch 23660: train loss: 0.47475725412368774\n",
      "Epoch 23661: train loss: 0.47475719451904297\n",
      "Epoch 23662: train loss: 0.4747571647167206\n",
      "Epoch 23663: train loss: 0.4747571051120758\n",
      "Epoch 23664: train loss: 0.47475704550743103\n",
      "Epoch 23665: train loss: 0.47475695610046387\n",
      "Epoch 23666: train loss: 0.47475695610046387\n",
      "Epoch 23667: train loss: 0.4747568964958191\n",
      "Epoch 23668: train loss: 0.4747568368911743\n",
      "Epoch 23669: train loss: 0.47475677728652954\n",
      "Epoch 23670: train loss: 0.47475674748420715\n",
      "Epoch 23671: train loss: 0.47475674748420715\n",
      "Epoch 23672: train loss: 0.4747566282749176\n",
      "Epoch 23673: train loss: 0.4747565686702728\n",
      "Epoch 23674: train loss: 0.47475653886795044\n",
      "Epoch 23675: train loss: 0.47475647926330566\n",
      "Epoch 23676: train loss: 0.4747564196586609\n",
      "Epoch 23677: train loss: 0.4747563302516937\n",
      "Epoch 23678: train loss: 0.4747563302516937\n",
      "Epoch 23679: train loss: 0.47475627064704895\n",
      "Epoch 23680: train loss: 0.4747562110424042\n",
      "Epoch 23681: train loss: 0.4747561514377594\n",
      "Epoch 23682: train loss: 0.474756121635437\n",
      "Epoch 23683: train loss: 0.474756121635437\n",
      "Epoch 23684: train loss: 0.47475600242614746\n",
      "Epoch 23685: train loss: 0.47475600242614746\n",
      "Epoch 23686: train loss: 0.4747559130191803\n",
      "Epoch 23687: train loss: 0.4747558534145355\n",
      "Epoch 23688: train loss: 0.47475579380989075\n",
      "Epoch 23689: train loss: 0.47475573420524597\n",
      "Epoch 23690: train loss: 0.4747557044029236\n",
      "Epoch 23691: train loss: 0.4747556447982788\n",
      "Epoch 23692: train loss: 0.4747556447982788\n",
      "Epoch 23693: train loss: 0.47475558519363403\n",
      "Epoch 23694: train loss: 0.47475549578666687\n",
      "Epoch 23695: train loss: 0.4747554361820221\n",
      "Epoch 23696: train loss: 0.4747553765773773\n",
      "Epoch 23697: train loss: 0.47475531697273254\n",
      "Epoch 23698: train loss: 0.47475528717041016\n",
      "Epoch 23699: train loss: 0.47475528717041016\n",
      "Epoch 23700: train loss: 0.4747551679611206\n",
      "Epoch 23701: train loss: 0.47475510835647583\n",
      "Epoch 23702: train loss: 0.47475510835647583\n",
      "Epoch 23703: train loss: 0.47475501894950867\n",
      "Epoch 23704: train loss: 0.47475501894950867\n",
      "Epoch 23705: train loss: 0.4747548997402191\n",
      "Epoch 23706: train loss: 0.47475486993789673\n",
      "Epoch 23707: train loss: 0.47475481033325195\n",
      "Epoch 23708: train loss: 0.4747547507286072\n",
      "Epoch 23709: train loss: 0.4747547507286072\n",
      "Epoch 23710: train loss: 0.47475466132164\n",
      "Epoch 23711: train loss: 0.47475460171699524\n",
      "Epoch 23712: train loss: 0.47475460171699524\n",
      "Epoch 23713: train loss: 0.4747544825077057\n",
      "Epoch 23714: train loss: 0.4747544527053833\n",
      "Epoch 23715: train loss: 0.4747543931007385\n",
      "Epoch 23716: train loss: 0.47475433349609375\n",
      "Epoch 23717: train loss: 0.47475433349609375\n",
      "Epoch 23718: train loss: 0.4747542142868042\n",
      "Epoch 23719: train loss: 0.4747542142868042\n",
      "Epoch 23720: train loss: 0.47475412487983704\n",
      "Epoch 23721: train loss: 0.47475406527519226\n",
      "Epoch 23722: train loss: 0.4747540056705475\n",
      "Epoch 23723: train loss: 0.4747540056705475\n",
      "Epoch 23724: train loss: 0.4747539758682251\n",
      "Epoch 23725: train loss: 0.47475385665893555\n",
      "Epoch 23726: train loss: 0.47475379705429077\n",
      "Epoch 23727: train loss: 0.4747537672519684\n",
      "Epoch 23728: train loss: 0.4747537076473236\n",
      "Epoch 23729: train loss: 0.4747537076473236\n",
      "Epoch 23730: train loss: 0.47475358843803406\n",
      "Epoch 23731: train loss: 0.47475358843803406\n",
      "Epoch 23732: train loss: 0.4747534990310669\n",
      "Epoch 23733: train loss: 0.4747534394264221\n",
      "Epoch 23734: train loss: 0.47475337982177734\n",
      "Epoch 23735: train loss: 0.47475335001945496\n",
      "Epoch 23736: train loss: 0.4747532308101654\n",
      "Epoch 23737: train loss: 0.4747532308101654\n",
      "Epoch 23738: train loss: 0.4747532308101654\n",
      "Epoch 23739: train loss: 0.47475317120552063\n",
      "Epoch 23740: train loss: 0.47475314140319824\n",
      "Epoch 23741: train loss: 0.4747530221939087\n",
      "Epoch 23742: train loss: 0.4747529625892639\n",
      "Epoch 23743: train loss: 0.47475293278694153\n",
      "Epoch 23744: train loss: 0.47475287318229675\n",
      "Epoch 23745: train loss: 0.47475287318229675\n",
      "Epoch 23746: train loss: 0.474752813577652\n",
      "Epoch 23747: train loss: 0.4747527241706848\n",
      "Epoch 23748: train loss: 0.47475266456604004\n",
      "Epoch 23749: train loss: 0.47475260496139526\n",
      "Epoch 23750: train loss: 0.47475260496139526\n",
      "Epoch 23751: train loss: 0.4747525155544281\n",
      "Epoch 23752: train loss: 0.4747524559497833\n",
      "Epoch 23753: train loss: 0.47475239634513855\n",
      "Epoch 23754: train loss: 0.47475239634513855\n",
      "Epoch 23755: train loss: 0.4747523069381714\n",
      "Epoch 23756: train loss: 0.4747523069381714\n",
      "Epoch 23757: train loss: 0.4747522473335266\n",
      "Epoch 23758: train loss: 0.47475212812423706\n",
      "Epoch 23759: train loss: 0.47475212812423706\n",
      "Epoch 23760: train loss: 0.4747520387172699\n",
      "Epoch 23761: train loss: 0.4747520387172699\n",
      "Epoch 23762: train loss: 0.4747519791126251\n",
      "Epoch 23763: train loss: 0.47475191950798035\n",
      "Epoch 23764: train loss: 0.4747518301010132\n",
      "Epoch 23765: train loss: 0.4747517704963684\n",
      "Epoch 23766: train loss: 0.47475171089172363\n",
      "Epoch 23767: train loss: 0.47475168108940125\n",
      "Epoch 23768: train loss: 0.47475162148475647\n",
      "Epoch 23769: train loss: 0.47475162148475647\n",
      "Epoch 23770: train loss: 0.4747515022754669\n",
      "Epoch 23771: train loss: 0.47475147247314453\n",
      "Epoch 23772: train loss: 0.47475141286849976\n",
      "Epoch 23773: train loss: 0.47475141286849976\n",
      "Epoch 23774: train loss: 0.4747512936592102\n",
      "Epoch 23775: train loss: 0.4747512638568878\n",
      "Epoch 23776: train loss: 0.47475120425224304\n",
      "Epoch 23777: train loss: 0.47475120425224304\n",
      "Epoch 23778: train loss: 0.4747510850429535\n",
      "Epoch 23779: train loss: 0.4747510552406311\n",
      "Epoch 23780: train loss: 0.47475099563598633\n",
      "Epoch 23781: train loss: 0.47475099563598633\n",
      "Epoch 23782: train loss: 0.47475093603134155\n",
      "Epoch 23783: train loss: 0.4747508764266968\n",
      "Epoch 23784: train loss: 0.4747508466243744\n",
      "Epoch 23785: train loss: 0.47475072741508484\n",
      "Epoch 23786: train loss: 0.47475072741508484\n",
      "Epoch 23787: train loss: 0.47475066781044006\n",
      "Epoch 23788: train loss: 0.4747505784034729\n",
      "Epoch 23789: train loss: 0.4747505784034729\n",
      "Epoch 23790: train loss: 0.47475045919418335\n",
      "Epoch 23791: train loss: 0.4747503995895386\n",
      "Epoch 23792: train loss: 0.4747503697872162\n",
      "Epoch 23793: train loss: 0.4747503101825714\n",
      "Epoch 23794: train loss: 0.4747503101825714\n",
      "Epoch 23795: train loss: 0.47475025057792664\n",
      "Epoch 23796: train loss: 0.4747501611709595\n",
      "Epoch 23797: train loss: 0.4747501611709595\n",
      "Epoch 23798: train loss: 0.4747500419616699\n",
      "Epoch 23799: train loss: 0.47474998235702515\n",
      "Epoch 23800: train loss: 0.47474998235702515\n",
      "Epoch 23801: train loss: 0.47474995255470276\n",
      "Epoch 23802: train loss: 0.474749892950058\n",
      "Epoch 23803: train loss: 0.4747498333454132\n",
      "Epoch 23804: train loss: 0.47474977374076843\n",
      "Epoch 23805: train loss: 0.47474968433380127\n",
      "Epoch 23806: train loss: 0.4747496247291565\n",
      "Epoch 23807: train loss: 0.4747495651245117\n",
      "Epoch 23808: train loss: 0.4747495651245117\n",
      "Epoch 23809: train loss: 0.47474953532218933\n",
      "Epoch 23810: train loss: 0.47474947571754456\n",
      "Epoch 23811: train loss: 0.474749356508255\n",
      "Epoch 23812: train loss: 0.474749356508255\n",
      "Epoch 23813: train loss: 0.47474926710128784\n",
      "Epoch 23814: train loss: 0.47474920749664307\n",
      "Epoch 23815: train loss: 0.47474920749664307\n",
      "Epoch 23816: train loss: 0.4747491478919983\n",
      "Epoch 23817: train loss: 0.47474905848503113\n",
      "Epoch 23818: train loss: 0.47474905848503113\n",
      "Epoch 23819: train loss: 0.4747489392757416\n",
      "Epoch 23820: train loss: 0.4747489392757416\n",
      "Epoch 23821: train loss: 0.4747489094734192\n",
      "Epoch 23822: train loss: 0.4747488498687744\n",
      "Epoch 23823: train loss: 0.47474879026412964\n",
      "Epoch 23824: train loss: 0.4747487008571625\n",
      "Epoch 23825: train loss: 0.4747486412525177\n",
      "Epoch 23826: train loss: 0.4747486412525177\n",
      "Epoch 23827: train loss: 0.4747485816478729\n",
      "Epoch 23828: train loss: 0.47474849224090576\n",
      "Epoch 23829: train loss: 0.47474849224090576\n",
      "Epoch 23830: train loss: 0.474748432636261\n",
      "Epoch 23831: train loss: 0.4747483730316162\n",
      "Epoch 23832: train loss: 0.47474828362464905\n",
      "Epoch 23833: train loss: 0.47474828362464905\n",
      "Epoch 23834: train loss: 0.4747481644153595\n",
      "Epoch 23835: train loss: 0.4747481644153595\n",
      "Epoch 23836: train loss: 0.4747481048107147\n",
      "Epoch 23837: train loss: 0.47474801540374756\n",
      "Epoch 23838: train loss: 0.4747479557991028\n",
      "Epoch 23839: train loss: 0.474747896194458\n",
      "Epoch 23840: train loss: 0.474747896194458\n",
      "Epoch 23841: train loss: 0.4747478663921356\n",
      "Epoch 23842: train loss: 0.47474780678749084\n",
      "Epoch 23843: train loss: 0.47474774718284607\n",
      "Epoch 23844: train loss: 0.4747476875782013\n",
      "Epoch 23845: train loss: 0.4747476577758789\n",
      "Epoch 23846: train loss: 0.47474753856658936\n",
      "Epoch 23847: train loss: 0.4747474789619446\n",
      "Epoch 23848: train loss: 0.4747474789619446\n",
      "Epoch 23849: train loss: 0.4747474491596222\n",
      "Epoch 23850: train loss: 0.4747473895549774\n",
      "Epoch 23851: train loss: 0.47474732995033264\n",
      "Epoch 23852: train loss: 0.47474727034568787\n",
      "Epoch 23853: train loss: 0.4747472405433655\n",
      "Epoch 23854: train loss: 0.4747471213340759\n",
      "Epoch 23855: train loss: 0.4747471213340759\n",
      "Epoch 23856: train loss: 0.47474706172943115\n",
      "Epoch 23857: train loss: 0.47474703192710876\n",
      "Epoch 23858: train loss: 0.4747469127178192\n",
      "Epoch 23859: train loss: 0.4747469127178192\n",
      "Epoch 23860: train loss: 0.47474682331085205\n",
      "Epoch 23861: train loss: 0.47474682331085205\n",
      "Epoch 23862: train loss: 0.4747467041015625\n",
      "Epoch 23863: train loss: 0.4747466444969177\n",
      "Epoch 23864: train loss: 0.4747466444969177\n",
      "Epoch 23865: train loss: 0.47474658489227295\n",
      "Epoch 23866: train loss: 0.4747464954853058\n",
      "Epoch 23867: train loss: 0.4747464954853058\n",
      "Epoch 23868: train loss: 0.474746435880661\n",
      "Epoch 23869: train loss: 0.47474637627601624\n",
      "Epoch 23870: train loss: 0.47474634647369385\n",
      "Epoch 23871: train loss: 0.4747462272644043\n",
      "Epoch 23872: train loss: 0.4747462272644043\n",
      "Epoch 23873: train loss: 0.47474613785743713\n",
      "Epoch 23874: train loss: 0.47474613785743713\n",
      "Epoch 23875: train loss: 0.47474607825279236\n",
      "Epoch 23876: train loss: 0.4747459590435028\n",
      "Epoch 23877: train loss: 0.4747459590435028\n",
      "Epoch 23878: train loss: 0.4747459292411804\n",
      "Epoch 23879: train loss: 0.47474586963653564\n",
      "Epoch 23880: train loss: 0.47474581003189087\n",
      "Epoch 23881: train loss: 0.4747457206249237\n",
      "Epoch 23882: train loss: 0.4747457206249237\n",
      "Epoch 23883: train loss: 0.47474566102027893\n",
      "Epoch 23884: train loss: 0.47474566102027893\n",
      "Epoch 23885: train loss: 0.4747455418109894\n",
      "Epoch 23886: train loss: 0.474745512008667\n",
      "Epoch 23887: train loss: 0.4747454524040222\n",
      "Epoch 23888: train loss: 0.47474539279937744\n",
      "Epoch 23889: train loss: 0.47474533319473267\n",
      "Epoch 23890: train loss: 0.4747452437877655\n",
      "Epoch 23891: train loss: 0.4747452437877655\n",
      "Epoch 23892: train loss: 0.4747451841831207\n",
      "Epoch 23893: train loss: 0.47474512457847595\n",
      "Epoch 23894: train loss: 0.47474509477615356\n",
      "Epoch 23895: train loss: 0.4747450351715088\n",
      "Epoch 23896: train loss: 0.4747450351715088\n",
      "Epoch 23897: train loss: 0.47474491596221924\n",
      "Epoch 23898: train loss: 0.47474488615989685\n",
      "Epoch 23899: train loss: 0.4747448265552521\n",
      "Epoch 23900: train loss: 0.4747447669506073\n",
      "Epoch 23901: train loss: 0.4747447073459625\n",
      "Epoch 23902: train loss: 0.47474467754364014\n",
      "Epoch 23903: train loss: 0.47474461793899536\n",
      "Epoch 23904: train loss: 0.4747445583343506\n",
      "Epoch 23905: train loss: 0.4747444987297058\n",
      "Epoch 23906: train loss: 0.4747444689273834\n",
      "Epoch 23907: train loss: 0.4747444689273834\n",
      "Epoch 23908: train loss: 0.47474440932273865\n",
      "Epoch 23909: train loss: 0.4747442901134491\n",
      "Epoch 23910: train loss: 0.4747442603111267\n",
      "Epoch 23911: train loss: 0.4747442603111267\n",
      "Epoch 23912: train loss: 0.47474414110183716\n",
      "Epoch 23913: train loss: 0.4747440814971924\n",
      "Epoch 23914: train loss: 0.4747440814971924\n",
      "Epoch 23915: train loss: 0.4747439920902252\n",
      "Epoch 23916: train loss: 0.4747439920902252\n",
      "Epoch 23917: train loss: 0.47474387288093567\n",
      "Epoch 23918: train loss: 0.4747438430786133\n",
      "Epoch 23919: train loss: 0.4747437834739685\n",
      "Epoch 23920: train loss: 0.47474372386932373\n",
      "Epoch 23921: train loss: 0.47474366426467896\n",
      "Epoch 23922: train loss: 0.47474363446235657\n",
      "Epoch 23923: train loss: 0.47474363446235657\n",
      "Epoch 23924: train loss: 0.4747435748577118\n",
      "Epoch 23925: train loss: 0.474743515253067\n",
      "Epoch 23926: train loss: 0.47474342584609985\n",
      "Epoch 23927: train loss: 0.47474342584609985\n",
      "Epoch 23928: train loss: 0.4747433066368103\n",
      "Epoch 23929: train loss: 0.4747432470321655\n",
      "Epoch 23930: train loss: 0.4747432470321655\n",
      "Epoch 23931: train loss: 0.47474321722984314\n",
      "Epoch 23932: train loss: 0.47474315762519836\n",
      "Epoch 23933: train loss: 0.4747430384159088\n",
      "Epoch 23934: train loss: 0.4747430086135864\n",
      "Epoch 23935: train loss: 0.4747430086135864\n",
      "Epoch 23936: train loss: 0.4747428894042969\n",
      "Epoch 23937: train loss: 0.4747428297996521\n",
      "Epoch 23938: train loss: 0.4747427701950073\n",
      "Epoch 23939: train loss: 0.4747427701950073\n",
      "Epoch 23940: train loss: 0.47474274039268494\n",
      "Epoch 23941: train loss: 0.4747426211833954\n",
      "Epoch 23942: train loss: 0.4747426211833954\n",
      "Epoch 23943: train loss: 0.4747425317764282\n",
      "Epoch 23944: train loss: 0.4747425317764282\n",
      "Epoch 23945: train loss: 0.47474247217178345\n",
      "Epoch 23946: train loss: 0.4747423231601715\n",
      "Epoch 23947: train loss: 0.4747423529624939\n",
      "Epoch 23948: train loss: 0.4747423231601715\n",
      "Epoch 23949: train loss: 0.4747423231601715\n",
      "Epoch 23950: train loss: 0.4747421443462372\n",
      "Epoch 23951: train loss: 0.4747421443462372\n",
      "Epoch 23952: train loss: 0.4747421145439148\n",
      "Epoch 23953: train loss: 0.47474205493927\n",
      "Epoch 23954: train loss: 0.47474199533462524\n",
      "Epoch 23955: train loss: 0.4747419059276581\n",
      "Epoch 23956: train loss: 0.4747419059276581\n",
      "Epoch 23957: train loss: 0.47474178671836853\n",
      "Epoch 23958: train loss: 0.47474178671836853\n",
      "Epoch 23959: train loss: 0.47474172711372375\n",
      "Epoch 23960: train loss: 0.47474169731140137\n",
      "Epoch 23961: train loss: 0.4747416377067566\n",
      "Epoch 23962: train loss: 0.4747415781021118\n",
      "Epoch 23963: train loss: 0.47474151849746704\n",
      "Epoch 23964: train loss: 0.4747414290904999\n",
      "Epoch 23965: train loss: 0.4747414290904999\n",
      "Epoch 23966: train loss: 0.4747413694858551\n",
      "Epoch 23967: train loss: 0.4747413098812103\n",
      "Epoch 23968: train loss: 0.47474128007888794\n",
      "Epoch 23969: train loss: 0.4747411608695984\n",
      "Epoch 23970: train loss: 0.47474122047424316\n",
      "Epoch 23971: train loss: 0.4747411012649536\n",
      "Epoch 23972: train loss: 0.47474101185798645\n",
      "Epoch 23973: train loss: 0.47474101185798645\n",
      "Epoch 23974: train loss: 0.4747409522533417\n",
      "Epoch 23975: train loss: 0.4747408926486969\n",
      "Epoch 23976: train loss: 0.4747408628463745\n",
      "Epoch 23977: train loss: 0.47474080324172974\n",
      "Epoch 23978: train loss: 0.4747406840324402\n",
      "Epoch 23979: train loss: 0.4747406840324402\n",
      "Epoch 23980: train loss: 0.474740594625473\n",
      "Epoch 23981: train loss: 0.4747406542301178\n",
      "Epoch 23982: train loss: 0.47474053502082825\n",
      "Epoch 23983: train loss: 0.47474047541618347\n",
      "Epoch 23984: train loss: 0.4747404456138611\n",
      "Epoch 23985: train loss: 0.4747404456138611\n",
      "Epoch 23986: train loss: 0.47474032640457153\n",
      "Epoch 23987: train loss: 0.47474023699760437\n",
      "Epoch 23988: train loss: 0.47474023699760437\n",
      "Epoch 23989: train loss: 0.4747401773929596\n",
      "Epoch 23990: train loss: 0.4747401177883148\n",
      "Epoch 23991: train loss: 0.47474005818367004\n",
      "Epoch 23992: train loss: 0.47474002838134766\n",
      "Epoch 23993: train loss: 0.4747399687767029\n",
      "Epoch 23994: train loss: 0.4747399091720581\n",
      "Epoch 23995: train loss: 0.47473984956741333\n",
      "Epoch 23996: train loss: 0.47473981976509094\n",
      "Epoch 23997: train loss: 0.47473976016044617\n",
      "Epoch 23998: train loss: 0.4747397005558014\n",
      "Epoch 23999: train loss: 0.4747397005558014\n",
      "Epoch 24000: train loss: 0.47473961114883423\n",
      "Epoch 24001: train loss: 0.47473955154418945\n",
      "Epoch 24002: train loss: 0.4747394919395447\n",
      "Epoch 24003: train loss: 0.4747394919395447\n",
      "Epoch 24004: train loss: 0.4747394025325775\n",
      "Epoch 24005: train loss: 0.47473934292793274\n",
      "Epoch 24006: train loss: 0.47473928332328796\n",
      "Epoch 24007: train loss: 0.4747392237186432\n",
      "Epoch 24008: train loss: 0.4747391939163208\n",
      "Epoch 24009: train loss: 0.474739134311676\n",
      "Epoch 24010: train loss: 0.47473907470703125\n",
      "Epoch 24011: train loss: 0.4747389554977417\n",
      "Epoch 24012: train loss: 0.4747389554977417\n",
      "Epoch 24013: train loss: 0.4747389256954193\n",
      "Epoch 24014: train loss: 0.47473886609077454\n",
      "Epoch 24015: train loss: 0.47473886609077454\n",
      "Epoch 24016: train loss: 0.474738746881485\n",
      "Epoch 24017: train loss: 0.4747386574745178\n",
      "Epoch 24018: train loss: 0.4747386574745178\n",
      "Epoch 24019: train loss: 0.47473859786987305\n",
      "Epoch 24020: train loss: 0.47473859786987305\n",
      "Epoch 24021: train loss: 0.47473853826522827\n",
      "Epoch 24022: train loss: 0.4747384488582611\n",
      "Epoch 24023: train loss: 0.47473838925361633\n",
      "Epoch 24024: train loss: 0.47473832964897156\n",
      "Epoch 24025: train loss: 0.47473829984664917\n",
      "Epoch 24026: train loss: 0.4747382402420044\n",
      "Epoch 24027: train loss: 0.4747381806373596\n",
      "Epoch 24028: train loss: 0.47473812103271484\n",
      "Epoch 24029: train loss: 0.47473812103271484\n",
      "Epoch 24030: train loss: 0.47473809123039246\n",
      "Epoch 24031: train loss: 0.4747380316257477\n",
      "Epoch 24032: train loss: 0.47473791241645813\n",
      "Epoch 24033: train loss: 0.47473791241645813\n",
      "Epoch 24034: train loss: 0.47473788261413574\n",
      "Epoch 24035: train loss: 0.47473782300949097\n",
      "Epoch 24036: train loss: 0.4747377038002014\n",
      "Epoch 24037: train loss: 0.47473767399787903\n",
      "Epoch 24038: train loss: 0.47473761439323425\n",
      "Epoch 24039: train loss: 0.47473761439323425\n",
      "Epoch 24040: train loss: 0.4747374951839447\n",
      "Epoch 24041: train loss: 0.4747374653816223\n",
      "Epoch 24042: train loss: 0.47473740577697754\n",
      "Epoch 24043: train loss: 0.47473734617233276\n",
      "Epoch 24044: train loss: 0.474737286567688\n",
      "Epoch 24045: train loss: 0.474737286567688\n",
      "Epoch 24046: train loss: 0.4747372567653656\n",
      "Epoch 24047: train loss: 0.47473713755607605\n",
      "Epoch 24048: train loss: 0.4747370779514313\n",
      "Epoch 24049: train loss: 0.47473713755607605\n",
      "Epoch 24050: train loss: 0.4747370481491089\n",
      "Epoch 24051: train loss: 0.4747369885444641\n",
      "Epoch 24052: train loss: 0.47473686933517456\n",
      "Epoch 24053: train loss: 0.47473686933517456\n",
      "Epoch 24054: train loss: 0.4747368395328522\n",
      "Epoch 24055: train loss: 0.4747367799282074\n",
      "Epoch 24056: train loss: 0.4747367203235626\n",
      "Epoch 24057: train loss: 0.47473666071891785\n",
      "Epoch 24058: train loss: 0.4747365713119507\n",
      "Epoch 24059: train loss: 0.4747365713119507\n",
      "Epoch 24060: train loss: 0.47473645210266113\n",
      "Epoch 24061: train loss: 0.47473645210266113\n",
      "Epoch 24062: train loss: 0.47473642230033875\n",
      "Epoch 24063: train loss: 0.4747363030910492\n",
      "Epoch 24064: train loss: 0.47473636269569397\n",
      "Epoch 24065: train loss: 0.4747362434864044\n",
      "Epoch 24066: train loss: 0.47473621368408203\n",
      "Epoch 24067: train loss: 0.47473615407943726\n",
      "Epoch 24068: train loss: 0.4747360944747925\n",
      "Epoch 24069: train loss: 0.4747360348701477\n",
      "Epoch 24070: train loss: 0.4747360050678253\n",
      "Epoch 24071: train loss: 0.47473588585853577\n",
      "Epoch 24072: train loss: 0.474735826253891\n",
      "Epoch 24073: train loss: 0.474735826253891\n",
      "Epoch 24074: train loss: 0.4747357964515686\n",
      "Epoch 24075: train loss: 0.47473573684692383\n",
      "Epoch 24076: train loss: 0.47473567724227905\n",
      "Epoch 24077: train loss: 0.4747356176376343\n",
      "Epoch 24078: train loss: 0.4747355878353119\n",
      "Epoch 24079: train loss: 0.4747355282306671\n",
      "Epoch 24080: train loss: 0.47473546862602234\n",
      "Epoch 24081: train loss: 0.47473540902137756\n",
      "Epoch 24082: train loss: 0.4747353792190552\n",
      "Epoch 24083: train loss: 0.4747353792190552\n",
      "Epoch 24084: train loss: 0.4747352600097656\n",
      "Epoch 24085: train loss: 0.47473520040512085\n",
      "Epoch 24086: train loss: 0.4747351408004761\n",
      "Epoch 24087: train loss: 0.4747351109981537\n",
      "Epoch 24088: train loss: 0.4747351109981537\n",
      "Epoch 24089: train loss: 0.4747350513935089\n",
      "Epoch 24090: train loss: 0.47473493218421936\n",
      "Epoch 24091: train loss: 0.474734902381897\n",
      "Epoch 24092: train loss: 0.4747348427772522\n",
      "Epoch 24093: train loss: 0.4747347831726074\n",
      "Epoch 24094: train loss: 0.4747347831726074\n",
      "Epoch 24095: train loss: 0.47473469376564026\n",
      "Epoch 24096: train loss: 0.4747346341609955\n",
      "Epoch 24097: train loss: 0.4747345745563507\n",
      "Epoch 24098: train loss: 0.4747345745563507\n",
      "Epoch 24099: train loss: 0.47473451495170593\n",
      "Epoch 24100: train loss: 0.47473442554473877\n",
      "Epoch 24101: train loss: 0.474734365940094\n",
      "Epoch 24102: train loss: 0.47473442554473877\n",
      "Epoch 24103: train loss: 0.47473427653312683\n",
      "Epoch 24104: train loss: 0.47473421692848206\n",
      "Epoch 24105: train loss: 0.47473421692848206\n",
      "Epoch 24106: train loss: 0.4747340977191925\n",
      "Epoch 24107: train loss: 0.4747340679168701\n",
      "Epoch 24108: train loss: 0.4747340679168701\n",
      "Epoch 24109: train loss: 0.47473394870758057\n",
      "Epoch 24110: train loss: 0.47473394870758057\n",
      "Epoch 24111: train loss: 0.4747338593006134\n",
      "Epoch 24112: train loss: 0.47473379969596863\n",
      "Epoch 24113: train loss: 0.47473379969596863\n",
      "Epoch 24114: train loss: 0.47473374009132385\n",
      "Epoch 24115: train loss: 0.47473374009132385\n",
      "Epoch 24116: train loss: 0.4747336506843567\n",
      "Epoch 24117: train loss: 0.4747335910797119\n",
      "Epoch 24118: train loss: 0.47473353147506714\n",
      "Epoch 24119: train loss: 0.47473347187042236\n",
      "Epoch 24120: train loss: 0.4747334420681\n",
      "Epoch 24121: train loss: 0.4747333824634552\n",
      "Epoch 24122: train loss: 0.4747333228588104\n",
      "Epoch 24123: train loss: 0.47473326325416565\n",
      "Epoch 24124: train loss: 0.47473323345184326\n",
      "Epoch 24125: train loss: 0.4747331738471985\n",
      "Epoch 24126: train loss: 0.4747331142425537\n",
      "Epoch 24127: train loss: 0.47473305463790894\n",
      "Epoch 24128: train loss: 0.47473302483558655\n",
      "Epoch 24129: train loss: 0.4747329652309418\n",
      "Epoch 24130: train loss: 0.474732905626297\n",
      "Epoch 24131: train loss: 0.47473281621932983\n",
      "Epoch 24132: train loss: 0.47473281621932983\n",
      "Epoch 24133: train loss: 0.4747326970100403\n",
      "Epoch 24134: train loss: 0.4747326970100403\n",
      "Epoch 24135: train loss: 0.4747326374053955\n",
      "Epoch 24136: train loss: 0.47473254799842834\n",
      "Epoch 24137: train loss: 0.47473254799842834\n",
      "Epoch 24138: train loss: 0.47473248839378357\n",
      "Epoch 24139: train loss: 0.4747324287891388\n",
      "Epoch 24140: train loss: 0.4747324287891388\n",
      "Epoch 24141: train loss: 0.47473233938217163\n",
      "Epoch 24142: train loss: 0.47473227977752686\n",
      "Epoch 24143: train loss: 0.47473227977752686\n",
      "Epoch 24144: train loss: 0.4747321903705597\n",
      "Epoch 24145: train loss: 0.4747321307659149\n",
      "Epoch 24146: train loss: 0.47473207116127014\n",
      "Epoch 24147: train loss: 0.47473207116127014\n",
      "Epoch 24148: train loss: 0.474731981754303\n",
      "Epoch 24149: train loss: 0.4747319221496582\n",
      "Epoch 24150: train loss: 0.4747319221496582\n",
      "Epoch 24151: train loss: 0.47473180294036865\n",
      "Epoch 24152: train loss: 0.47473180294036865\n",
      "Epoch 24153: train loss: 0.4747317135334015\n",
      "Epoch 24154: train loss: 0.4747316539287567\n",
      "Epoch 24155: train loss: 0.4747316539287567\n",
      "Epoch 24156: train loss: 0.47473159432411194\n",
      "Epoch 24157: train loss: 0.4747315049171448\n",
      "Epoch 24158: train loss: 0.4747314453125\n",
      "Epoch 24159: train loss: 0.4747313857078552\n",
      "Epoch 24160: train loss: 0.4747313857078552\n",
      "Epoch 24161: train loss: 0.47473129630088806\n",
      "Epoch 24162: train loss: 0.4747312366962433\n",
      "Epoch 24163: train loss: 0.4747311770915985\n",
      "Epoch 24164: train loss: 0.4747312366962433\n",
      "Epoch 24165: train loss: 0.47473111748695374\n",
      "Epoch 24166: train loss: 0.47473108768463135\n",
      "Epoch 24167: train loss: 0.4747310280799866\n",
      "Epoch 24168: train loss: 0.4747309684753418\n",
      "Epoch 24169: train loss: 0.474730908870697\n",
      "Epoch 24170: train loss: 0.47473087906837463\n",
      "Epoch 24171: train loss: 0.47473081946372986\n",
      "Epoch 24172: train loss: 0.47473081946372986\n",
      "Epoch 24173: train loss: 0.4747307002544403\n",
      "Epoch 24174: train loss: 0.4747306704521179\n",
      "Epoch 24175: train loss: 0.47473061084747314\n",
      "Epoch 24176: train loss: 0.47473055124282837\n",
      "Epoch 24177: train loss: 0.4747304916381836\n",
      "Epoch 24178: train loss: 0.47473040223121643\n",
      "Epoch 24179: train loss: 0.47473040223121643\n",
      "Epoch 24180: train loss: 0.4747302830219269\n",
      "Epoch 24181: train loss: 0.4747302830219269\n",
      "Epoch 24182: train loss: 0.4747302532196045\n",
      "Epoch 24183: train loss: 0.4747301936149597\n",
      "Epoch 24184: train loss: 0.47473013401031494\n",
      "Epoch 24185: train loss: 0.47473007440567017\n",
      "Epoch 24186: train loss: 0.4747300446033478\n",
      "Epoch 24187: train loss: 0.4747299253940582\n",
      "Epoch 24188: train loss: 0.474729984998703\n",
      "Epoch 24189: train loss: 0.4747299253940582\n",
      "Epoch 24190: train loss: 0.47472983598709106\n",
      "Epoch 24191: train loss: 0.4747297763824463\n",
      "Epoch 24192: train loss: 0.4747297763824463\n",
      "Epoch 24193: train loss: 0.47472965717315674\n",
      "Epoch 24194: train loss: 0.47472962737083435\n",
      "Epoch 24195: train loss: 0.4747295677661896\n",
      "Epoch 24196: train loss: 0.4747294485569\n",
      "Epoch 24197: train loss: 0.4747294485569\n",
      "Epoch 24198: train loss: 0.4747294485569\n",
      "Epoch 24199: train loss: 0.47472935914993286\n",
      "Epoch 24200: train loss: 0.4747292995452881\n",
      "Epoch 24201: train loss: 0.4747292399406433\n",
      "Epoch 24202: train loss: 0.4747292101383209\n",
      "Epoch 24203: train loss: 0.47472915053367615\n",
      "Epoch 24204: train loss: 0.47472909092903137\n",
      "Epoch 24205: train loss: 0.4747290313243866\n",
      "Epoch 24206: train loss: 0.4747290015220642\n",
      "Epoch 24207: train loss: 0.47472894191741943\n",
      "Epoch 24208: train loss: 0.47472894191741943\n",
      "Epoch 24209: train loss: 0.47472888231277466\n",
      "Epoch 24210: train loss: 0.4747288227081299\n",
      "Epoch 24211: train loss: 0.4747287333011627\n",
      "Epoch 24212: train loss: 0.47472867369651794\n",
      "Epoch 24213: train loss: 0.4747287333011627\n",
      "Epoch 24214: train loss: 0.47472861409187317\n",
      "Epoch 24215: train loss: 0.474728524684906\n",
      "Epoch 24216: train loss: 0.47472846508026123\n",
      "Epoch 24217: train loss: 0.47472840547561646\n",
      "Epoch 24218: train loss: 0.47472840547561646\n",
      "Epoch 24219: train loss: 0.47472837567329407\n",
      "Epoch 24220: train loss: 0.4747283160686493\n",
      "Epoch 24221: train loss: 0.4747282564640045\n",
      "Epoch 24222: train loss: 0.47472819685935974\n",
      "Epoch 24223: train loss: 0.47472816705703735\n",
      "Epoch 24224: train loss: 0.4747281074523926\n",
      "Epoch 24225: train loss: 0.474727988243103\n",
      "Epoch 24226: train loss: 0.474727988243103\n",
      "Epoch 24227: train loss: 0.47472795844078064\n",
      "Epoch 24228: train loss: 0.47472789883613586\n",
      "Epoch 24229: train loss: 0.4747278392314911\n",
      "Epoch 24230: train loss: 0.4747277796268463\n",
      "Epoch 24231: train loss: 0.4747277498245239\n",
      "Epoch 24232: train loss: 0.47472769021987915\n",
      "Epoch 24233: train loss: 0.4747276306152344\n",
      "Epoch 24234: train loss: 0.4747276306152344\n",
      "Epoch 24235: train loss: 0.4747275114059448\n",
      "Epoch 24236: train loss: 0.47472748160362244\n",
      "Epoch 24237: train loss: 0.47472742199897766\n",
      "Epoch 24238: train loss: 0.47472742199897766\n",
      "Epoch 24239: train loss: 0.4747273027896881\n",
      "Epoch 24240: train loss: 0.4747273623943329\n",
      "Epoch 24241: train loss: 0.47472721338272095\n",
      "Epoch 24242: train loss: 0.47472715377807617\n",
      "Epoch 24243: train loss: 0.4747270941734314\n",
      "Epoch 24244: train loss: 0.474727064371109\n",
      "Epoch 24245: train loss: 0.47472700476646423\n",
      "Epoch 24246: train loss: 0.47472694516181946\n",
      "Epoch 24247: train loss: 0.4747268855571747\n",
      "Epoch 24248: train loss: 0.4747268855571747\n",
      "Epoch 24249: train loss: 0.4747267961502075\n",
      "Epoch 24250: train loss: 0.47472673654556274\n",
      "Epoch 24251: train loss: 0.47472673654556274\n",
      "Epoch 24252: train loss: 0.4747266471385956\n",
      "Epoch 24253: train loss: 0.4747265875339508\n",
      "Epoch 24254: train loss: 0.4747265875339508\n",
      "Epoch 24255: train loss: 0.47472652792930603\n",
      "Epoch 24256: train loss: 0.47472646832466125\n",
      "Epoch 24257: train loss: 0.4747263789176941\n",
      "Epoch 24258: train loss: 0.4747263789176941\n",
      "Epoch 24259: train loss: 0.4747263193130493\n",
      "Epoch 24260: train loss: 0.47472622990608215\n",
      "Epoch 24261: train loss: 0.47472622990608215\n",
      "Epoch 24262: train loss: 0.4747261106967926\n",
      "Epoch 24263: train loss: 0.4747261106967926\n",
      "Epoch 24264: train loss: 0.4747260510921478\n",
      "Epoch 24265: train loss: 0.47472602128982544\n",
      "Epoch 24266: train loss: 0.47472596168518066\n",
      "Epoch 24267: train loss: 0.4747259020805359\n",
      "Epoch 24268: train loss: 0.4747258424758911\n",
      "Epoch 24269: train loss: 0.4747258126735687\n",
      "Epoch 24270: train loss: 0.4747258126735687\n",
      "Epoch 24271: train loss: 0.4747256934642792\n",
      "Epoch 24272: train loss: 0.4747256338596344\n",
      "Epoch 24273: train loss: 0.474725604057312\n",
      "Epoch 24274: train loss: 0.47472548484802246\n",
      "Epoch 24275: train loss: 0.47472548484802246\n",
      "Epoch 24276: train loss: 0.47472548484802246\n",
      "Epoch 24277: train loss: 0.4747253954410553\n",
      "Epoch 24278: train loss: 0.4747253358364105\n",
      "Epoch 24279: train loss: 0.47472527623176575\n",
      "Epoch 24280: train loss: 0.47472527623176575\n",
      "Epoch 24281: train loss: 0.47472521662712097\n",
      "Epoch 24282: train loss: 0.4747251868247986\n",
      "Epoch 24283: train loss: 0.47472506761550903\n",
      "Epoch 24284: train loss: 0.47472500801086426\n",
      "Epoch 24285: train loss: 0.47472500801086426\n",
      "Epoch 24286: train loss: 0.47472497820854187\n",
      "Epoch 24287: train loss: 0.4747248589992523\n",
      "Epoch 24288: train loss: 0.4747248589992523\n",
      "Epoch 24289: train loss: 0.47472476959228516\n",
      "Epoch 24290: train loss: 0.47472476959228516\n",
      "Epoch 24291: train loss: 0.4747246503829956\n",
      "Epoch 24292: train loss: 0.47472459077835083\n",
      "Epoch 24293: train loss: 0.47472456097602844\n",
      "Epoch 24294: train loss: 0.47472456097602844\n",
      "Epoch 24295: train loss: 0.47472450137138367\n",
      "Epoch 24296: train loss: 0.4747244417667389\n",
      "Epoch 24297: train loss: 0.4747243821620941\n",
      "Epoch 24298: train loss: 0.47472435235977173\n",
      "Epoch 24299: train loss: 0.4747242331504822\n",
      "Epoch 24300: train loss: 0.4747242331504822\n",
      "Epoch 24301: train loss: 0.474724143743515\n",
      "Epoch 24302: train loss: 0.474724143743515\n",
      "Epoch 24303: train loss: 0.47472408413887024\n",
      "Epoch 24304: train loss: 0.47472402453422546\n",
      "Epoch 24305: train loss: 0.4747239649295807\n",
      "Epoch 24306: train loss: 0.4747239351272583\n",
      "Epoch 24307: train loss: 0.4747238755226135\n",
      "Epoch 24308: train loss: 0.47472381591796875\n",
      "Epoch 24309: train loss: 0.474723756313324\n",
      "Epoch 24310: train loss: 0.4747236967086792\n",
      "Epoch 24311: train loss: 0.4747236669063568\n",
      "Epoch 24312: train loss: 0.47472360730171204\n",
      "Epoch 24313: train loss: 0.47472354769706726\n",
      "Epoch 24314: train loss: 0.4747234880924225\n",
      "Epoch 24315: train loss: 0.4747234582901001\n",
      "Epoch 24316: train loss: 0.4747234582901001\n",
      "Epoch 24317: train loss: 0.4747233986854553\n",
      "Epoch 24318: train loss: 0.47472327947616577\n",
      "Epoch 24319: train loss: 0.4747231900691986\n",
      "Epoch 24320: train loss: 0.4747231900691986\n",
      "Epoch 24321: train loss: 0.47472307085990906\n",
      "Epoch 24322: train loss: 0.47472307085990906\n",
      "Epoch 24323: train loss: 0.47472304105758667\n",
      "Epoch 24324: train loss: 0.4747229814529419\n",
      "Epoch 24325: train loss: 0.47472286224365234\n",
      "Epoch 24326: train loss: 0.47472286224365234\n",
      "Epoch 24327: train loss: 0.47472286224365234\n",
      "Epoch 24328: train loss: 0.47472286224365234\n",
      "Epoch 24329: train loss: 0.4747227728366852\n",
      "Epoch 24330: train loss: 0.47472265362739563\n",
      "Epoch 24331: train loss: 0.47472265362739563\n",
      "Epoch 24332: train loss: 0.47472256422042847\n",
      "Epoch 24333: train loss: 0.47472256422042847\n",
      "Epoch 24334: train loss: 0.4747224450111389\n",
      "Epoch 24335: train loss: 0.4747224450111389\n",
      "Epoch 24336: train loss: 0.47472235560417175\n",
      "Epoch 24337: train loss: 0.47472235560417175\n",
      "Epoch 24338: train loss: 0.474722295999527\n",
      "Epoch 24339: train loss: 0.474722295999527\n",
      "Epoch 24340: train loss: 0.4747222065925598\n",
      "Epoch 24341: train loss: 0.47472214698791504\n",
      "Epoch 24342: train loss: 0.4747220277786255\n",
      "Epoch 24343: train loss: 0.4747220277786255\n",
      "Epoch 24344: train loss: 0.4747219383716583\n",
      "Epoch 24345: train loss: 0.47472187876701355\n",
      "Epoch 24346: train loss: 0.4747218191623688\n",
      "Epoch 24347: train loss: 0.4747218191623688\n",
      "Epoch 24348: train loss: 0.4747217893600464\n",
      "Epoch 24349: train loss: 0.4747217297554016\n",
      "Epoch 24350: train loss: 0.47472167015075684\n",
      "Epoch 24351: train loss: 0.47472161054611206\n",
      "Epoch 24352: train loss: 0.4747215807437897\n",
      "Epoch 24353: train loss: 0.4747215807437897\n",
      "Epoch 24354: train loss: 0.4747215211391449\n",
      "Epoch 24355: train loss: 0.47472140192985535\n",
      "Epoch 24356: train loss: 0.4747213125228882\n",
      "Epoch 24357: train loss: 0.4747213125228882\n",
      "Epoch 24358: train loss: 0.47472119331359863\n",
      "Epoch 24359: train loss: 0.4747212529182434\n",
      "Epoch 24360: train loss: 0.47472119331359863\n",
      "Epoch 24361: train loss: 0.47472110390663147\n",
      "Epoch 24362: train loss: 0.47472110390663147\n",
      "Epoch 24363: train loss: 0.4747209846973419\n",
      "Epoch 24364: train loss: 0.4747209846973419\n",
      "Epoch 24365: train loss: 0.47472089529037476\n",
      "Epoch 24366: train loss: 0.47472089529037476\n",
      "Epoch 24367: train loss: 0.4747207760810852\n",
      "Epoch 24368: train loss: 0.4747207462787628\n",
      "Epoch 24369: train loss: 0.4747207462787628\n",
      "Epoch 24370: train loss: 0.47472068667411804\n",
      "Epoch 24371: train loss: 0.4747205674648285\n",
      "Epoch 24372: train loss: 0.4747205376625061\n",
      "Epoch 24373: train loss: 0.47472047805786133\n",
      "Epoch 24374: train loss: 0.47472047805786133\n",
      "Epoch 24375: train loss: 0.4747203588485718\n",
      "Epoch 24376: train loss: 0.4747203588485718\n",
      "Epoch 24377: train loss: 0.4747202694416046\n",
      "Epoch 24378: train loss: 0.4747202694416046\n",
      "Epoch 24379: train loss: 0.47472020983695984\n",
      "Epoch 24380: train loss: 0.47472015023231506\n",
      "Epoch 24381: train loss: 0.4747201204299927\n",
      "Epoch 24382: train loss: 0.4747201204299927\n",
      "Epoch 24383: train loss: 0.47471994161605835\n",
      "Epoch 24384: train loss: 0.47471994161605835\n",
      "Epoch 24385: train loss: 0.47471994161605835\n",
      "Epoch 24386: train loss: 0.4747198820114136\n",
      "Epoch 24387: train loss: 0.4747197926044464\n",
      "Epoch 24388: train loss: 0.47471973299980164\n",
      "Epoch 24389: train loss: 0.47471967339515686\n",
      "Epoch 24390: train loss: 0.4747196435928345\n",
      "Epoch 24391: train loss: 0.4747195839881897\n",
      "Epoch 24392: train loss: 0.4747195243835449\n",
      "Epoch 24393: train loss: 0.47471946477890015\n",
      "Epoch 24394: train loss: 0.47471946477890015\n",
      "Epoch 24395: train loss: 0.474719375371933\n",
      "Epoch 24396: train loss: 0.4747193157672882\n",
      "Epoch 24397: train loss: 0.47471925616264343\n",
      "Epoch 24398: train loss: 0.4747193157672882\n",
      "Epoch 24399: train loss: 0.47471922636032104\n",
      "Epoch 24400: train loss: 0.4747191071510315\n",
      "Epoch 24401: train loss: 0.4747191071510315\n",
      "Epoch 24402: train loss: 0.4747190475463867\n",
      "Epoch 24403: train loss: 0.47471895813941956\n",
      "Epoch 24404: train loss: 0.47471895813941956\n",
      "Epoch 24405: train loss: 0.47471883893013\n",
      "Epoch 24406: train loss: 0.47471883893013\n",
      "Epoch 24407: train loss: 0.47471874952316284\n",
      "Epoch 24408: train loss: 0.47471868991851807\n",
      "Epoch 24409: train loss: 0.47471868991851807\n",
      "Epoch 24410: train loss: 0.4747186303138733\n",
      "Epoch 24411: train loss: 0.4747186005115509\n",
      "Epoch 24412: train loss: 0.47471854090690613\n",
      "Epoch 24413: train loss: 0.47471848130226135\n",
      "Epoch 24414: train loss: 0.4747184216976166\n",
      "Epoch 24415: train loss: 0.4747183918952942\n",
      "Epoch 24416: train loss: 0.4747183918952942\n",
      "Epoch 24417: train loss: 0.47471827268600464\n",
      "Epoch 24418: train loss: 0.47471821308135986\n",
      "Epoch 24419: train loss: 0.4747181832790375\n",
      "Epoch 24420: train loss: 0.4747181832790375\n",
      "Epoch 24421: train loss: 0.4747180640697479\n",
      "Epoch 24422: train loss: 0.47471797466278076\n",
      "Epoch 24423: train loss: 0.47471797466278076\n",
      "Epoch 24424: train loss: 0.47471797466278076\n",
      "Epoch 24425: train loss: 0.4747178554534912\n",
      "Epoch 24426: train loss: 0.47471779584884644\n",
      "Epoch 24427: train loss: 0.47471776604652405\n",
      "Epoch 24428: train loss: 0.47471776604652405\n",
      "Epoch 24429: train loss: 0.4747177064418793\n",
      "Epoch 24430: train loss: 0.4747175872325897\n",
      "Epoch 24431: train loss: 0.47471755743026733\n",
      "Epoch 24432: train loss: 0.47471749782562256\n",
      "Epoch 24433: train loss: 0.4747174382209778\n",
      "Epoch 24434: train loss: 0.4747174382209778\n",
      "Epoch 24435: train loss: 0.4747173488140106\n",
      "Epoch 24436: train loss: 0.47471728920936584\n",
      "Epoch 24437: train loss: 0.47471728920936584\n",
      "Epoch 24438: train loss: 0.47471722960472107\n",
      "Epoch 24439: train loss: 0.4747171700000763\n",
      "Epoch 24440: train loss: 0.4747171401977539\n",
      "Epoch 24441: train loss: 0.4747171401977539\n",
      "Epoch 24442: train loss: 0.47471702098846436\n",
      "Epoch 24443: train loss: 0.4747169315814972\n",
      "Epoch 24444: train loss: 0.4747168719768524\n",
      "Epoch 24445: train loss: 0.47471681237220764\n",
      "Epoch 24446: train loss: 0.47471681237220764\n",
      "Epoch 24447: train loss: 0.4747167229652405\n",
      "Epoch 24448: train loss: 0.4747167229652405\n",
      "Epoch 24449: train loss: 0.4747166633605957\n",
      "Epoch 24450: train loss: 0.4747166037559509\n",
      "Epoch 24451: train loss: 0.4747166037559509\n",
      "Epoch 24452: train loss: 0.47471651434898376\n",
      "Epoch 24453: train loss: 0.4747163951396942\n",
      "Epoch 24454: train loss: 0.4747163951396942\n",
      "Epoch 24455: train loss: 0.47471630573272705\n",
      "Epoch 24456: train loss: 0.47471633553504944\n",
      "Epoch 24457: train loss: 0.4747162461280823\n",
      "Epoch 24458: train loss: 0.4747161865234375\n",
      "Epoch 24459: train loss: 0.47471606731414795\n",
      "Epoch 24460: train loss: 0.47471606731414795\n",
      "Epoch 24461: train loss: 0.47471603751182556\n",
      "Epoch 24462: train loss: 0.4747159779071808\n",
      "Epoch 24463: train loss: 0.474715918302536\n",
      "Epoch 24464: train loss: 0.47471585869789124\n",
      "Epoch 24465: train loss: 0.47471585869789124\n",
      "Epoch 24466: train loss: 0.4747157692909241\n",
      "Epoch 24467: train loss: 0.4747157692909241\n",
      "Epoch 24468: train loss: 0.4747157096862793\n",
      "Epoch 24469: train loss: 0.47471562027931213\n",
      "Epoch 24470: train loss: 0.47471562027931213\n",
      "Epoch 24471: train loss: 0.4747155010700226\n",
      "Epoch 24472: train loss: 0.4747155010700226\n",
      "Epoch 24473: train loss: 0.4747155010700226\n",
      "Epoch 24474: train loss: 0.47471535205841064\n",
      "Epoch 24475: train loss: 0.47471529245376587\n",
      "Epoch 24476: train loss: 0.4747152328491211\n",
      "Epoch 24477: train loss: 0.4747152328491211\n",
      "Epoch 24478: train loss: 0.4747152030467987\n",
      "Epoch 24479: train loss: 0.47471514344215393\n",
      "Epoch 24480: train loss: 0.4747150242328644\n",
      "Epoch 24481: train loss: 0.4747150242328644\n",
      "Epoch 24482: train loss: 0.4747149348258972\n",
      "Epoch 24483: train loss: 0.4747149348258972\n",
      "Epoch 24484: train loss: 0.47471487522125244\n",
      "Epoch 24485: train loss: 0.47471481561660767\n",
      "Epoch 24486: train loss: 0.4747147262096405\n",
      "Epoch 24487: train loss: 0.4747147262096405\n",
      "Epoch 24488: train loss: 0.4747146666049957\n",
      "Epoch 24489: train loss: 0.47471460700035095\n",
      "Epoch 24490: train loss: 0.47471457719802856\n",
      "Epoch 24491: train loss: 0.4747145175933838\n",
      "Epoch 24492: train loss: 0.474714457988739\n",
      "Epoch 24493: train loss: 0.47471439838409424\n",
      "Epoch 24494: train loss: 0.47471436858177185\n",
      "Epoch 24495: train loss: 0.4747143089771271\n",
      "Epoch 24496: train loss: 0.4747143089771271\n",
      "Epoch 24497: train loss: 0.4747141897678375\n",
      "Epoch 24498: train loss: 0.47471415996551514\n",
      "Epoch 24499: train loss: 0.47471410036087036\n",
      "Epoch 24500: train loss: 0.47471410036087036\n",
      "Epoch 24501: train loss: 0.4747140407562256\n",
      "Epoch 24502: train loss: 0.4747139811515808\n",
      "Epoch 24503: train loss: 0.47471389174461365\n",
      "Epoch 24504: train loss: 0.47471389174461365\n",
      "Epoch 24505: train loss: 0.4747137725353241\n",
      "Epoch 24506: train loss: 0.4747137427330017\n",
      "Epoch 24507: train loss: 0.47471368312835693\n",
      "Epoch 24508: train loss: 0.47471368312835693\n",
      "Epoch 24509: train loss: 0.4747135639190674\n",
      "Epoch 24510: train loss: 0.4747135639190674\n",
      "Epoch 24511: train loss: 0.474713534116745\n",
      "Epoch 24512: train loss: 0.4747134745121002\n",
      "Epoch 24513: train loss: 0.47471341490745544\n",
      "Epoch 24514: train loss: 0.47471335530281067\n",
      "Epoch 24515: train loss: 0.4747132658958435\n",
      "Epoch 24516: train loss: 0.4747132658958435\n",
      "Epoch 24517: train loss: 0.47471320629119873\n",
      "Epoch 24518: train loss: 0.47471311688423157\n",
      "Epoch 24519: train loss: 0.47471311688423157\n",
      "Epoch 24520: train loss: 0.4747130572795868\n",
      "Epoch 24521: train loss: 0.4747130572795868\n",
      "Epoch 24522: train loss: 0.47471293807029724\n",
      "Epoch 24523: train loss: 0.47471290826797485\n",
      "Epoch 24524: train loss: 0.4747128486633301\n",
      "Epoch 24525: train loss: 0.4747128486633301\n",
      "Epoch 24526: train loss: 0.4747127294540405\n",
      "Epoch 24527: train loss: 0.4747127294540405\n",
      "Epoch 24528: train loss: 0.47471269965171814\n",
      "Epoch 24529: train loss: 0.4747125804424286\n",
      "Epoch 24530: train loss: 0.4747125208377838\n",
      "Epoch 24531: train loss: 0.4747125208377838\n",
      "Epoch 24532: train loss: 0.47471243143081665\n",
      "Epoch 24533: train loss: 0.4747123718261719\n",
      "Epoch 24534: train loss: 0.4747123718261719\n",
      "Epoch 24535: train loss: 0.4747122526168823\n",
      "Epoch 24536: train loss: 0.4747122526168823\n",
      "Epoch 24537: train loss: 0.47471222281455994\n",
      "Epoch 24538: train loss: 0.4747121036052704\n",
      "Epoch 24539: train loss: 0.4747120440006256\n",
      "Epoch 24540: train loss: 0.4747120141983032\n",
      "Epoch 24541: train loss: 0.4747120141983032\n",
      "Epoch 24542: train loss: 0.47471195459365845\n",
      "Epoch 24543: train loss: 0.47471195459365845\n",
      "Epoch 24544: train loss: 0.47471189498901367\n",
      "Epoch 24545: train loss: 0.4747118055820465\n",
      "Epoch 24546: train loss: 0.47471174597740173\n",
      "Epoch 24547: train loss: 0.47471168637275696\n",
      "Epoch 24548: train loss: 0.4747116267681122\n",
      "Epoch 24549: train loss: 0.4747116267681122\n",
      "Epoch 24550: train loss: 0.474711537361145\n",
      "Epoch 24551: train loss: 0.47471147775650024\n",
      "Epoch 24552: train loss: 0.47471141815185547\n",
      "Epoch 24553: train loss: 0.47471141815185547\n",
      "Epoch 24554: train loss: 0.4747113287448883\n",
      "Epoch 24555: train loss: 0.47471120953559875\n",
      "Epoch 24556: train loss: 0.47471126914024353\n",
      "Epoch 24557: train loss: 0.47471120953559875\n",
      "Epoch 24558: train loss: 0.47471117973327637\n",
      "Epoch 24559: train loss: 0.4747110605239868\n",
      "Epoch 24560: train loss: 0.47471100091934204\n",
      "Epoch 24561: train loss: 0.47471097111701965\n",
      "Epoch 24562: train loss: 0.47471097111701965\n",
      "Epoch 24563: train loss: 0.4747108519077301\n",
      "Epoch 24564: train loss: 0.4747108519077301\n",
      "Epoch 24565: train loss: 0.4747107923030853\n",
      "Epoch 24566: train loss: 0.47471070289611816\n",
      "Epoch 24567: train loss: 0.47471076250076294\n",
      "Epoch 24568: train loss: 0.4747106432914734\n",
      "Epoch 24569: train loss: 0.4747105538845062\n",
      "Epoch 24570: train loss: 0.4747105538845062\n",
      "Epoch 24571: train loss: 0.4747104346752167\n",
      "Epoch 24572: train loss: 0.4747104346752167\n",
      "Epoch 24573: train loss: 0.4747103750705719\n",
      "Epoch 24574: train loss: 0.4747103452682495\n",
      "Epoch 24575: train loss: 0.47471028566360474\n",
      "Epoch 24576: train loss: 0.47471022605895996\n",
      "Epoch 24577: train loss: 0.47471022605895996\n",
      "Epoch 24578: train loss: 0.4747101366519928\n",
      "Epoch 24579: train loss: 0.474710077047348\n",
      "Epoch 24580: train loss: 0.47471001744270325\n",
      "Epoch 24581: train loss: 0.47470995783805847\n",
      "Epoch 24582: train loss: 0.4747099280357361\n",
      "Epoch 24583: train loss: 0.47470980882644653\n",
      "Epoch 24584: train loss: 0.47470980882644653\n",
      "Epoch 24585: train loss: 0.47470980882644653\n",
      "Epoch 24586: train loss: 0.47470971941947937\n",
      "Epoch 24587: train loss: 0.4747096598148346\n",
      "Epoch 24588: train loss: 0.4747096002101898\n",
      "Epoch 24589: train loss: 0.4747096002101898\n",
      "Epoch 24590: train loss: 0.47470951080322266\n",
      "Epoch 24591: train loss: 0.47470951080322266\n",
      "Epoch 24592: train loss: 0.4747093915939331\n",
      "Epoch 24593: train loss: 0.47470933198928833\n",
      "Epoch 24594: train loss: 0.47470933198928833\n",
      "Epoch 24595: train loss: 0.4747091829776764\n",
      "Epoch 24596: train loss: 0.4747091829776764\n",
      "Epoch 24597: train loss: 0.4747091829776764\n",
      "Epoch 24598: train loss: 0.47470909357070923\n",
      "Epoch 24599: train loss: 0.47470903396606445\n",
      "Epoch 24600: train loss: 0.47470903396606445\n",
      "Epoch 24601: train loss: 0.4747089743614197\n",
      "Epoch 24602: train loss: 0.4747088849544525\n",
      "Epoch 24603: train loss: 0.4747088849544525\n",
      "Epoch 24604: train loss: 0.47470882534980774\n",
      "Epoch 24605: train loss: 0.47470876574516296\n",
      "Epoch 24606: train loss: 0.4747087061405182\n",
      "Epoch 24607: train loss: 0.474708616733551\n",
      "Epoch 24608: train loss: 0.474708616733551\n",
      "Epoch 24609: train loss: 0.47470855712890625\n",
      "Epoch 24610: train loss: 0.47470855712890625\n",
      "Epoch 24611: train loss: 0.4747084379196167\n",
      "Epoch 24612: train loss: 0.4747084081172943\n",
      "Epoch 24613: train loss: 0.47470834851264954\n",
      "Epoch 24614: train loss: 0.47470834851264954\n",
      "Epoch 24615: train loss: 0.47470822930336\n",
      "Epoch 24616: train loss: 0.47470822930336\n",
      "Epoch 24617: train loss: 0.4747081398963928\n",
      "Epoch 24618: train loss: 0.47470808029174805\n",
      "Epoch 24619: train loss: 0.47470802068710327\n",
      "Epoch 24620: train loss: 0.4747079908847809\n",
      "Epoch 24621: train loss: 0.4747079312801361\n",
      "Epoch 24622: train loss: 0.4747079312801361\n",
      "Epoch 24623: train loss: 0.47470781207084656\n",
      "Epoch 24624: train loss: 0.47470778226852417\n",
      "Epoch 24625: train loss: 0.47470778226852417\n",
      "Epoch 24626: train loss: 0.4747077226638794\n",
      "Epoch 24627: train loss: 0.47470760345458984\n",
      "Epoch 24628: train loss: 0.47470760345458984\n",
      "Epoch 24629: train loss: 0.47470757365226746\n",
      "Epoch 24630: train loss: 0.4747074544429779\n",
      "Epoch 24631: train loss: 0.4747074544429779\n",
      "Epoch 24632: train loss: 0.47470739483833313\n",
      "Epoch 24633: train loss: 0.47470739483833313\n",
      "Epoch 24634: train loss: 0.4747072458267212\n",
      "Epoch 24635: train loss: 0.4747072458267212\n",
      "Epoch 24636: train loss: 0.4747071862220764\n",
      "Epoch 24637: train loss: 0.47470715641975403\n",
      "Epoch 24638: train loss: 0.47470709681510925\n",
      "Epoch 24639: train loss: 0.4747070372104645\n",
      "Epoch 24640: train loss: 0.4747069776058197\n",
      "Epoch 24641: train loss: 0.4747069478034973\n",
      "Epoch 24642: train loss: 0.47470688819885254\n",
      "Epoch 24643: train loss: 0.47470682859420776\n",
      "Epoch 24644: train loss: 0.474706768989563\n",
      "Epoch 24645: train loss: 0.474706768989563\n",
      "Epoch 24646: train loss: 0.4747067391872406\n",
      "Epoch 24647: train loss: 0.4747066795825958\n",
      "Epoch 24648: train loss: 0.4747065603733063\n",
      "Epoch 24649: train loss: 0.4747065603733063\n",
      "Epoch 24650: train loss: 0.4747064709663391\n",
      "Epoch 24651: train loss: 0.47470641136169434\n",
      "Epoch 24652: train loss: 0.47470635175704956\n",
      "Epoch 24653: train loss: 0.47470635175704956\n",
      "Epoch 24654: train loss: 0.4747062623500824\n",
      "Epoch 24655: train loss: 0.4747062027454376\n",
      "Epoch 24656: train loss: 0.4747062027454376\n",
      "Epoch 24657: train loss: 0.47470614314079285\n",
      "Epoch 24658: train loss: 0.47470611333847046\n",
      "Epoch 24659: train loss: 0.4747059941291809\n",
      "Epoch 24660: train loss: 0.47470593452453613\n",
      "Epoch 24661: train loss: 0.47470593452453613\n",
      "Epoch 24662: train loss: 0.47470593452453613\n",
      "Epoch 24663: train loss: 0.47470584511756897\n",
      "Epoch 24664: train loss: 0.4747057855129242\n",
      "Epoch 24665: train loss: 0.47470569610595703\n",
      "Epoch 24666: train loss: 0.47470569610595703\n",
      "Epoch 24667: train loss: 0.47470563650131226\n",
      "Epoch 24668: train loss: 0.4747055768966675\n",
      "Epoch 24669: train loss: 0.4747055172920227\n",
      "Epoch 24670: train loss: 0.4747054874897003\n",
      "Epoch 24671: train loss: 0.47470542788505554\n",
      "Epoch 24672: train loss: 0.47470536828041077\n",
      "Epoch 24673: train loss: 0.474705308675766\n",
      "Epoch 24674: train loss: 0.47470521926879883\n",
      "Epoch 24675: train loss: 0.47470521926879883\n",
      "Epoch 24676: train loss: 0.47470521926879883\n",
      "Epoch 24677: train loss: 0.4747051000595093\n",
      "Epoch 24678: train loss: 0.4747050702571869\n",
      "Epoch 24679: train loss: 0.4747050106525421\n",
      "Epoch 24680: train loss: 0.47470495104789734\n",
      "Epoch 24681: train loss: 0.47470489144325256\n",
      "Epoch 24682: train loss: 0.4747048616409302\n",
      "Epoch 24683: train loss: 0.4747048020362854\n",
      "Epoch 24684: train loss: 0.4747047424316406\n",
      "Epoch 24685: train loss: 0.47470468282699585\n",
      "Epoch 24686: train loss: 0.47470468282699585\n",
      "Epoch 24687: train loss: 0.4747045934200287\n",
      "Epoch 24688: train loss: 0.4747045934200287\n",
      "Epoch 24689: train loss: 0.4747045338153839\n",
      "Epoch 24690: train loss: 0.47470447421073914\n",
      "Epoch 24691: train loss: 0.474704384803772\n",
      "Epoch 24692: train loss: 0.4747043251991272\n",
      "Epoch 24693: train loss: 0.474704384803772\n",
      "Epoch 24694: train loss: 0.4747042655944824\n",
      "Epoch 24695: train loss: 0.47470420598983765\n",
      "Epoch 24696: train loss: 0.4747041165828705\n",
      "Epoch 24697: train loss: 0.4747041165828705\n",
      "Epoch 24698: train loss: 0.4747040569782257\n",
      "Epoch 24699: train loss: 0.4747040569782257\n",
      "Epoch 24700: train loss: 0.47470396757125854\n",
      "Epoch 24701: train loss: 0.47470396757125854\n",
      "Epoch 24702: train loss: 0.47470390796661377\n",
      "Epoch 24703: train loss: 0.4747037887573242\n",
      "Epoch 24704: train loss: 0.47470375895500183\n",
      "Epoch 24705: train loss: 0.47470375895500183\n",
      "Epoch 24706: train loss: 0.4747036397457123\n",
      "Epoch 24707: train loss: 0.4747036397457123\n",
      "Epoch 24708: train loss: 0.4747035801410675\n",
      "Epoch 24709: train loss: 0.4747035503387451\n",
      "Epoch 24710: train loss: 0.47470349073410034\n",
      "Epoch 24711: train loss: 0.47470343112945557\n",
      "Epoch 24712: train loss: 0.4747033417224884\n",
      "Epoch 24713: train loss: 0.4747033417224884\n",
      "Epoch 24714: train loss: 0.47470328211784363\n",
      "Epoch 24715: train loss: 0.47470322251319885\n",
      "Epoch 24716: train loss: 0.4747031629085541\n",
      "Epoch 24717: train loss: 0.4747030735015869\n",
      "Epoch 24718: train loss: 0.4747030735015869\n",
      "Epoch 24719: train loss: 0.47470301389694214\n",
      "Epoch 24720: train loss: 0.47470295429229736\n",
      "Epoch 24721: train loss: 0.474702924489975\n",
      "Epoch 24722: train loss: 0.4747028052806854\n",
      "Epoch 24723: train loss: 0.4747028052806854\n",
      "Epoch 24724: train loss: 0.4747028052806854\n",
      "Epoch 24725: train loss: 0.47470271587371826\n",
      "Epoch 24726: train loss: 0.4747026562690735\n",
      "Epoch 24727: train loss: 0.4747025966644287\n",
      "Epoch 24728: train loss: 0.47470253705978394\n",
      "Epoch 24729: train loss: 0.47470250725746155\n",
      "Epoch 24730: train loss: 0.4747024476528168\n",
      "Epoch 24731: train loss: 0.474702388048172\n",
      "Epoch 24732: train loss: 0.4747023284435272\n",
      "Epoch 24733: train loss: 0.4747023284435272\n",
      "Epoch 24734: train loss: 0.47470229864120483\n",
      "Epoch 24735: train loss: 0.47470223903656006\n",
      "Epoch 24736: train loss: 0.4747021794319153\n",
      "Epoch 24737: train loss: 0.4747020900249481\n",
      "Epoch 24738: train loss: 0.4747020900249481\n",
      "Epoch 24739: train loss: 0.47470197081565857\n",
      "Epoch 24740: train loss: 0.4747019112110138\n",
      "Epoch 24741: train loss: 0.4747018814086914\n",
      "Epoch 24742: train loss: 0.47470182180404663\n",
      "Epoch 24743: train loss: 0.47470182180404663\n",
      "Epoch 24744: train loss: 0.47470176219940186\n",
      "Epoch 24745: train loss: 0.4747016727924347\n",
      "Epoch 24746: train loss: 0.4747016727924347\n",
      "Epoch 24747: train loss: 0.4747016131877899\n",
      "Epoch 24748: train loss: 0.4747016131877899\n",
      "Epoch 24749: train loss: 0.47470149397850037\n",
      "Epoch 24750: train loss: 0.474701464176178\n",
      "Epoch 24751: train loss: 0.4747014045715332\n",
      "Epoch 24752: train loss: 0.4747014045715332\n",
      "Epoch 24753: train loss: 0.47470128536224365\n",
      "Epoch 24754: train loss: 0.47470125555992126\n",
      "Epoch 24755: train loss: 0.47470125555992126\n",
      "Epoch 24756: train loss: 0.4747011363506317\n",
      "Epoch 24757: train loss: 0.47470104694366455\n",
      "Epoch 24758: train loss: 0.47470104694366455\n",
      "Epoch 24759: train loss: 0.4747009873390198\n",
      "Epoch 24760: train loss: 0.474700927734375\n",
      "Epoch 24761: train loss: 0.4747008681297302\n",
      "Epoch 24762: train loss: 0.47470080852508545\n",
      "Epoch 24763: train loss: 0.47470077872276306\n",
      "Epoch 24764: train loss: 0.4747007191181183\n",
      "Epoch 24765: train loss: 0.4747006595134735\n",
      "Epoch 24766: train loss: 0.47470059990882874\n",
      "Epoch 24767: train loss: 0.47470059990882874\n",
      "Epoch 24768: train loss: 0.4747005105018616\n",
      "Epoch 24769: train loss: 0.4747004508972168\n",
      "Epoch 24770: train loss: 0.474700391292572\n",
      "Epoch 24771: train loss: 0.474700391292572\n",
      "Epoch 24772: train loss: 0.47470030188560486\n",
      "Epoch 24773: train loss: 0.47470030188560486\n",
      "Epoch 24774: train loss: 0.4747001826763153\n",
      "Epoch 24775: train loss: 0.4747001826763153\n",
      "Epoch 24776: train loss: 0.4747001528739929\n",
      "Epoch 24777: train loss: 0.47470003366470337\n",
      "Epoch 24778: train loss: 0.47470003366470337\n",
      "Epoch 24779: train loss: 0.47470003366470337\n",
      "Epoch 24780: train loss: 0.4746999442577362\n",
      "Epoch 24781: train loss: 0.47469982504844666\n",
      "Epoch 24782: train loss: 0.4746997654438019\n",
      "Epoch 24783: train loss: 0.4746997654438019\n",
      "Epoch 24784: train loss: 0.4746997356414795\n",
      "Epoch 24785: train loss: 0.4746996760368347\n",
      "Epoch 24786: train loss: 0.47469961643218994\n",
      "Epoch 24787: train loss: 0.47469955682754517\n",
      "Epoch 24788: train loss: 0.4746995270252228\n",
      "Epoch 24789: train loss: 0.474699467420578\n",
      "Epoch 24790: train loss: 0.474699467420578\n",
      "Epoch 24791: train loss: 0.47469934821128845\n",
      "Epoch 24792: train loss: 0.47469931840896606\n",
      "Epoch 24793: train loss: 0.4746992588043213\n",
      "Epoch 24794: train loss: 0.4746992588043213\n",
      "Epoch 24795: train loss: 0.47469913959503174\n",
      "Epoch 24796: train loss: 0.47469913959503174\n",
      "Epoch 24797: train loss: 0.4746990501880646\n",
      "Epoch 24798: train loss: 0.4746989905834198\n",
      "Epoch 24799: train loss: 0.4746989905834198\n",
      "Epoch 24800: train loss: 0.47469890117645264\n",
      "Epoch 24801: train loss: 0.47469884157180786\n",
      "Epoch 24802: train loss: 0.47469884157180786\n",
      "Epoch 24803: train loss: 0.4746987819671631\n",
      "Epoch 24804: train loss: 0.4746987223625183\n",
      "Epoch 24805: train loss: 0.4746986925601959\n",
      "Epoch 24806: train loss: 0.47469863295555115\n",
      "Epoch 24807: train loss: 0.47469863295555115\n",
      "Epoch 24808: train loss: 0.4746985137462616\n",
      "Epoch 24809: train loss: 0.47469842433929443\n",
      "Epoch 24810: train loss: 0.4746984839439392\n",
      "Epoch 24811: train loss: 0.47469836473464966\n",
      "Epoch 24812: train loss: 0.4746983051300049\n",
      "Epoch 24813: train loss: 0.4746983051300049\n",
      "Epoch 24814: train loss: 0.4746982157230377\n",
      "Epoch 24815: train loss: 0.47469815611839294\n",
      "Epoch 24816: train loss: 0.47469809651374817\n",
      "Epoch 24817: train loss: 0.47469809651374817\n",
      "Epoch 24818: train loss: 0.474698007106781\n",
      "Epoch 24819: train loss: 0.474698007106781\n",
      "Epoch 24820: train loss: 0.47469788789749146\n",
      "Epoch 24821: train loss: 0.47469788789749146\n",
      "Epoch 24822: train loss: 0.47469785809516907\n",
      "Epoch 24823: train loss: 0.4746977388858795\n",
      "Epoch 24824: train loss: 0.4746977388858795\n",
      "Epoch 24825: train loss: 0.47469767928123474\n",
      "Epoch 24826: train loss: 0.47469764947891235\n",
      "Epoch 24827: train loss: 0.4746975898742676\n",
      "Epoch 24828: train loss: 0.474697470664978\n",
      "Epoch 24829: train loss: 0.474697470664978\n",
      "Epoch 24830: train loss: 0.47469744086265564\n",
      "Epoch 24831: train loss: 0.47469738125801086\n",
      "Epoch 24832: train loss: 0.47469738125801086\n",
      "Epoch 24833: train loss: 0.4746972620487213\n",
      "Epoch 24834: train loss: 0.4746972620487213\n",
      "Epoch 24835: train loss: 0.47469717264175415\n",
      "Epoch 24836: train loss: 0.47469717264175415\n",
      "Epoch 24837: train loss: 0.4746971130371094\n",
      "Epoch 24838: train loss: 0.4746969938278198\n",
      "Epoch 24839: train loss: 0.47469690442085266\n",
      "Epoch 24840: train loss: 0.47469696402549744\n",
      "Epoch 24841: train loss: 0.4746968448162079\n",
      "Epoch 24842: train loss: 0.4746967852115631\n",
      "Epoch 24843: train loss: 0.4746967554092407\n",
      "Epoch 24844: train loss: 0.4746967554092407\n",
      "Epoch 24845: train loss: 0.47469663619995117\n",
      "Epoch 24846: train loss: 0.47469663619995117\n",
      "Epoch 24847: train loss: 0.4746965765953064\n",
      "Epoch 24848: train loss: 0.47469648718833923\n",
      "Epoch 24849: train loss: 0.47469642758369446\n",
      "Epoch 24850: train loss: 0.47469642758369446\n",
      "Epoch 24851: train loss: 0.4746963679790497\n",
      "Epoch 24852: train loss: 0.4746963381767273\n",
      "Epoch 24853: train loss: 0.47469621896743774\n",
      "Epoch 24854: train loss: 0.47469621896743774\n",
      "Epoch 24855: train loss: 0.47469615936279297\n",
      "Epoch 24856: train loss: 0.4746961295604706\n",
      "Epoch 24857: train loss: 0.4746960699558258\n",
      "Epoch 24858: train loss: 0.47469601035118103\n",
      "Epoch 24859: train loss: 0.47469592094421387\n",
      "Epoch 24860: train loss: 0.47469592094421387\n",
      "Epoch 24861: train loss: 0.47469592094421387\n",
      "Epoch 24862: train loss: 0.4746958017349243\n",
      "Epoch 24863: train loss: 0.47469574213027954\n",
      "Epoch 24864: train loss: 0.47469571232795715\n",
      "Epoch 24865: train loss: 0.47469571232795715\n",
      "Epoch 24866: train loss: 0.4746955931186676\n",
      "Epoch 24867: train loss: 0.4746955335140228\n",
      "Epoch 24868: train loss: 0.47469550371170044\n",
      "Epoch 24869: train loss: 0.47469544410705566\n",
      "Epoch 24870: train loss: 0.4746953845024109\n",
      "Epoch 24871: train loss: 0.4746953845024109\n",
      "Epoch 24872: train loss: 0.4746953248977661\n",
      "Epoch 24873: train loss: 0.4746952950954437\n",
      "Epoch 24874: train loss: 0.47469523549079895\n",
      "Epoch 24875: train loss: 0.4746951758861542\n",
      "Epoch 24876: train loss: 0.4746951758861542\n",
      "Epoch 24877: train loss: 0.47469502687454224\n",
      "Epoch 24878: train loss: 0.47469502687454224\n",
      "Epoch 24879: train loss: 0.47469496726989746\n",
      "Epoch 24880: train loss: 0.4746949076652527\n",
      "Epoch 24881: train loss: 0.4746948778629303\n",
      "Epoch 24882: train loss: 0.4746948182582855\n",
      "Epoch 24883: train loss: 0.47469475865364075\n",
      "Epoch 24884: train loss: 0.47469469904899597\n",
      "Epoch 24885: train loss: 0.4746946692466736\n",
      "Epoch 24886: train loss: 0.4746946096420288\n",
      "Epoch 24887: train loss: 0.47469449043273926\n",
      "Epoch 24888: train loss: 0.47469455003738403\n",
      "Epoch 24889: train loss: 0.47469446063041687\n",
      "Epoch 24890: train loss: 0.4746944010257721\n",
      "Epoch 24891: train loss: 0.4746943414211273\n",
      "Epoch 24892: train loss: 0.47469428181648254\n",
      "Epoch 24893: train loss: 0.47469428181648254\n",
      "Epoch 24894: train loss: 0.47469425201416016\n",
      "Epoch 24895: train loss: 0.4746941328048706\n",
      "Epoch 24896: train loss: 0.4746941328048706\n",
      "Epoch 24897: train loss: 0.4746941328048706\n",
      "Epoch 24898: train loss: 0.47469404339790344\n",
      "Epoch 24899: train loss: 0.4746939241886139\n",
      "Epoch 24900: train loss: 0.4746939241886139\n",
      "Epoch 24901: train loss: 0.4746938645839691\n",
      "Epoch 24902: train loss: 0.47469377517700195\n",
      "Epoch 24903: train loss: 0.4746937155723572\n",
      "Epoch 24904: train loss: 0.4746937155723572\n",
      "Epoch 24905: train loss: 0.4746936559677124\n",
      "Epoch 24906: train loss: 0.47469362616539\n",
      "Epoch 24907: train loss: 0.47469356656074524\n",
      "Epoch 24908: train loss: 0.47469350695610046\n",
      "Epoch 24909: train loss: 0.4746934175491333\n",
      "Epoch 24910: train loss: 0.4746934175491333\n",
      "Epoch 24911: train loss: 0.4746933579444885\n",
      "Epoch 24912: train loss: 0.47469329833984375\n",
      "Epoch 24913: train loss: 0.47469329833984375\n",
      "Epoch 24914: train loss: 0.4746931791305542\n",
      "Epoch 24915: train loss: 0.47469308972358704\n",
      "Epoch 24916: train loss: 0.47469308972358704\n",
      "Epoch 24917: train loss: 0.47469303011894226\n",
      "Epoch 24918: train loss: 0.4746929705142975\n",
      "Epoch 24919: train loss: 0.4746929705142975\n",
      "Epoch 24920: train loss: 0.4746928811073303\n",
      "Epoch 24921: train loss: 0.47469282150268555\n",
      "Epoch 24922: train loss: 0.47469282150268555\n",
      "Epoch 24923: train loss: 0.47469276189804077\n",
      "Epoch 24924: train loss: 0.4746927320957184\n",
      "Epoch 24925: train loss: 0.4746926724910736\n",
      "Epoch 24926: train loss: 0.47469261288642883\n",
      "Epoch 24927: train loss: 0.47469255328178406\n",
      "Epoch 24928: train loss: 0.47469252347946167\n",
      "Epoch 24929: train loss: 0.4746924042701721\n",
      "Epoch 24930: train loss: 0.4746924042701721\n",
      "Epoch 24931: train loss: 0.47469234466552734\n",
      "Epoch 24932: train loss: 0.47469231486320496\n",
      "Epoch 24933: train loss: 0.4746922552585602\n",
      "Epoch 24934: train loss: 0.47469213604927063\n",
      "Epoch 24935: train loss: 0.47469213604927063\n",
      "Epoch 24936: train loss: 0.47469210624694824\n",
      "Epoch 24937: train loss: 0.47469204664230347\n",
      "Epoch 24938: train loss: 0.4746919870376587\n",
      "Epoch 24939: train loss: 0.4746919274330139\n",
      "Epoch 24940: train loss: 0.4746919274330139\n",
      "Epoch 24941: train loss: 0.47469189763069153\n",
      "Epoch 24942: train loss: 0.474691778421402\n",
      "Epoch 24943: train loss: 0.4746917188167572\n",
      "Epoch 24944: train loss: 0.4746916890144348\n",
      "Epoch 24945: train loss: 0.4746916890144348\n",
      "Epoch 24946: train loss: 0.47469156980514526\n",
      "Epoch 24947: train loss: 0.4746915102005005\n",
      "Epoch 24948: train loss: 0.4746915102005005\n",
      "Epoch 24949: train loss: 0.4746914803981781\n",
      "Epoch 24950: train loss: 0.4746914207935333\n",
      "Epoch 24951: train loss: 0.4746914207935333\n",
      "Epoch 24952: train loss: 0.4746913015842438\n",
      "Epoch 24953: train loss: 0.4746912121772766\n",
      "Epoch 24954: train loss: 0.47469115257263184\n",
      "Epoch 24955: train loss: 0.47469115257263184\n",
      "Epoch 24956: train loss: 0.47469109296798706\n",
      "Epoch 24957: train loss: 0.4746910035610199\n",
      "Epoch 24958: train loss: 0.4746910631656647\n",
      "Epoch 24959: train loss: 0.4746910035610199\n",
      "Epoch 24960: train loss: 0.47469088435173035\n",
      "Epoch 24961: train loss: 0.47469088435173035\n",
      "Epoch 24962: train loss: 0.4746907949447632\n",
      "Epoch 24963: train loss: 0.4746907353401184\n",
      "Epoch 24964: train loss: 0.47469067573547363\n",
      "Epoch 24965: train loss: 0.47469067573547363\n",
      "Epoch 24966: train loss: 0.47469064593315125\n",
      "Epoch 24967: train loss: 0.47469058632850647\n",
      "Epoch 24968: train loss: 0.4746905267238617\n",
      "Epoch 24969: train loss: 0.47469043731689453\n",
      "Epoch 24970: train loss: 0.47469043731689453\n",
      "Epoch 24971: train loss: 0.474690318107605\n",
      "Epoch 24972: train loss: 0.474690318107605\n",
      "Epoch 24973: train loss: 0.4746902585029602\n",
      "Epoch 24974: train loss: 0.47469016909599304\n",
      "Epoch 24975: train loss: 0.47469016909599304\n",
      "Epoch 24976: train loss: 0.47469010949134827\n",
      "Epoch 24977: train loss: 0.4746900498867035\n",
      "Epoch 24978: train loss: 0.47468996047973633\n",
      "Epoch 24979: train loss: 0.47468996047973633\n",
      "Epoch 24980: train loss: 0.47468990087509155\n",
      "Epoch 24981: train loss: 0.4746898412704468\n",
      "Epoch 24982: train loss: 0.4746898114681244\n",
      "Epoch 24983: train loss: 0.4746897518634796\n",
      "Epoch 24984: train loss: 0.47468969225883484\n",
      "Epoch 24985: train loss: 0.4746896028518677\n",
      "Epoch 24986: train loss: 0.4746896028518677\n",
      "Epoch 24987: train loss: 0.4746896028518677\n",
      "Epoch 24988: train loss: 0.4746894836425781\n",
      "Epoch 24989: train loss: 0.47468942403793335\n",
      "Epoch 24990: train loss: 0.4746893644332886\n",
      "Epoch 24991: train loss: 0.4746893346309662\n",
      "Epoch 24992: train loss: 0.4746893346309662\n",
      "Epoch 24993: train loss: 0.47468921542167664\n",
      "Epoch 24994: train loss: 0.47468915581703186\n",
      "Epoch 24995: train loss: 0.4746891260147095\n",
      "Epoch 24996: train loss: 0.4746891260147095\n",
      "Epoch 24997: train loss: 0.4746890068054199\n",
      "Epoch 24998: train loss: 0.4746890068054199\n",
      "Epoch 24999: train loss: 0.47468894720077515\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 25000\n",
    "errors = []\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), y_train)\n",
    "    errors.append(loss.item())    \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after Training 0.4653743505477905\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(X_val)\n",
    "after_train = criterion(y_pred.squeeze(), y_val)\n",
    "print('Test loss after Training' , after_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plotcharts(errors):\n",
    "    errors = np.array(errors)    \n",
    "    plt.figure(figsize=(12, 5))   \n",
    "    graf02 = plt.subplot(1, 2, 1) # nrows, ncols, index\n",
    "    graf02.set_title('Errors')\n",
    "    plt.plot(errors, '-')\n",
    "    plt.xlabel('Epochs')    \n",
    "    graf03 = plt.subplot(1, 2, 2)\n",
    "    graf03.set_title('Tests')\n",
    "    a = plt.plot(X_val.numpy(), 'yo', label='Real')\n",
    "    plt.setp(a, markersize=10)\n",
    "    a = plt.plot(y_pred.detach().numpy(), 'b+', label='Predicted')\n",
    "    plt.setp(a, markersize=10)\n",
    "    plt.legend(loc=7)\n",
    "    plt.show()\n",
    "plotcharts(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_edited = torch.FloatTensor(np.array(test_edited))\n",
    "y_pred = model(test_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1176],\n",
      "        [0.1748],\n",
      "        [0.2107],\n",
      "        ...,\n",
      "        [0.1620],\n",
      "        [0.1043],\n",
      "        [0.2833]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c120166613a30323498f3baa97198a2d3e3dd7e83ea28a4ee6b037cfde0c0779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
